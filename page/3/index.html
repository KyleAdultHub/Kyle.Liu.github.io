<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="刘小恺" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="刘小恺(Kyle) 的个人博客">
<meta property="og:url" content="http://blog.kyleliu.cn/page/3/index.html">
<meta property="og:site_name" content="刘小恺(Kyle) 的个人博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="刘小恺(Kyle) 的个人博客">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="刘小恺(Kyle) 的个人博客" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>刘小恺(Kyle) 的个人博客</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">刘小恺</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/octave/">octave</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/博客/">博客</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/梯度下降/">梯度下降</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">刘小恺</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">刘小恺</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-4.python爬虫/爬虫基础/Request模块使用" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/Request模块使用/" class="article-date">
      <time datetime="2018-10-18T12:22:23.198Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Request模块的介绍<br>request 库的特点<br>requests的底层实现就是urllib<br>requests在python2 和python3中通用，方法完全一样<br>requests简单易用<br>requests能够自动帮助我们解压(gzip压缩的等)网页内容<br>requests的作用<br>发送网络请求，返回响应数据<br>中文文档 API：<a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html" target="_blank" rel="noopener">http://docs.python-requests.org/zh_CN/latest/user/quickstart.html</a><br>request/response的常用方法和属性<br>requests.get()    get请求<br>使用方法：<br>response = requests.get(url, headers=headers， params=params)    返回响应对象<br>get方法的参数<br>url：为请求的url地址<br>headers：字典形式的命名参数，传递请求头。模拟浏览器获取想获取的数据，防止反扒检测；<br>示例：headers = {“User-Agent”: “Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36”}<br>params：字典形式命名参数，传递请求参数，获取想要获取的数据<br>示例：params = {‘wd’:’长城’}<br>request.post()     post请求<br>使用方法：<br>response = requests.post(url, headers=headers, data= data)<br>post方法的参数<br>url：为请求的url地址（不包含锚点）<br>headers：字典形式的命名参数，传递请求头。模拟浏览器获取想获取的数据，防止反扒检测；<br>示例：headers = {“User-Agent”: “Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36”}<br>data：<br>当时普通的post请求的时候，传递的参数是字典形式命名参数<br>Formdata参数示例：data = {‘wd’:’长城’}<br>当时ajax异步post请求的时候，传递的的字符串形式的参数<br>Payload参数示例：data = json.dumps({‘wd’: ‘长城’})<br>还要加上请求头 ： ‘Content-Type’: ‘application/json’<br>requests.utils   工具<br>requests.utils.qoute(url)     将url编码<br>requests.utils.unqoute(url)     将url进行解码<br>parsed = requests.utils.urlparse(url)     将url进行  协议、ip+端口、文件路径、参数等的拆分<br>parsed.scheme       获取协议名<br>parsed.netloc    获取ip+端口（或者是域名+端口）<br>parsed.path    获取路径<br>parsed.params     获取问号后面的参数<br>response 响应的常用属性和方法<br>response.text               获取响应内容html字符串<br>获取html字符串，结果是<code>str</code>类型<br>其编码方式，是requests根据响应头做出的有根据的推测，尝试使用这个编码方式来解码<br>response.encoding               规定解码的格式<br>可以在使用text属性获取响应字符串之前先规定编码格式，按照设定的编码格式进行解码<br>respones.content         获取响应内容的二进制(bytes)数据<br>通常我们在获取到内容的时候尝试对二进制数据进行解码，下面三种方法能够解决后续我们100%的编解码的需求<br>response.content.deocde()           获取二进制响应内容并解码成为utf-8编码格式的字符串<br>response.content.deocde(“gbk”)         获取二进制响应内容并解码成为gbk编码格式的字符串<br>response.text      在使用content解码失败的时候，可以尝试让request去根据推测进行解码<br>response.status_code       查看响应状态码<br>response.request.headers     查看请求头<br>response.headers      查看响应头<br>response.request.url     查看响应url<br>response.url       查看请求url<br>响应的url可能与请求url不同，比如304重定向，响应的url为重定向的url</p>
<p><strong><em>找到想要请求url和请求参数的方法</em></strong><br>寻找想要的url<br>第一种情况：表单提交请求，并且表单有action属性<br>通过form表单的action属性/a标签的href属性来找到对应的请求url<br>form表单提交时获取表单中所有的name、value键值对<br>构造成字典，通过requests函数的data参数进行请求<br>第二种情况：通过ajax异步发出的请求（非action表单提交请求）<br>查看抓包结果，存在我们想要最多的数据会是我们想要的响应，其url为主要的请求url<br>当有一些数据不存在该主要的请求中，那说明有js进行了其他异步请求获取了数据<br>我们可以先找到其他请求的响应，找到其url地址，其url地址一般有两种构建途径<br>1.一般的情况下和url相关的信息会存在主请求的响应中，我们可以通过在主响应中获取其他请求的请求url关键字，进行url的构建<br>2.有的时候url地址或参数是通过js动态生成的，这时候，我们需要去寻找对应的js文件来观察js是怎样生成的动态url和参数等（参考下面）<br>寻找想要的js文件（当不是通过form表单的action/a标签href属性提交请求的时候）<br>找到进行url请求的js文件<br>第一种方法<br>使用开发者选择工具，点击会触发请求js的元素<br>点击右边的event Listener，会获取到对应的点击事件<br>点击事件所在的js文件<br>第二种方法<br>先了解js文件中会出现的关键字，一般可以查找url中动态参数名称；<br>通过点击右上角的菜单search all file<br>查找关键字，找到后点击进入对应的js文件<br>会跳转到source界面，点击{}展开代码，显示文件全部的代码<br>找到想要找的事件函数，并在想要查看实现方法的函数前边加上断点<br>通过点击右边的调试工具，可以查看函数的具体执行步骤和实现方法<br>了解了实现的方法后，可以在爬虫请求的时候，实现同样的方法，然后生成参数和url地址，进行请求<br>使用代理获取响应<br>什么叫代理<br>在对网站爬取的时候，通过访问代理服务器，让代理服务器帮我们对目标服务器进行请求，然后通过代理服务器将响应返回给我们；<br>代理的作用<br>在需要大量的进行数据的爬取的时候<br>防止在同一时间，使用同一个ip地址对同一个服务器进行大量的访问，被反扒出来<br>在爬取网站的时候，通过代理服务器可以隐藏我们的真实ip地址，隐藏我们的身份，但是有的代理服务器不能隐匿我们的mac地址，如果想要隐匿我们的mac地址需要使用高匿代理服务器<br>代理服务器的原理</p>
<p>正向代理/反向代理<br>正向代理<br>在我们请求代理服务器的时候，我们知道我们的最终目标ip地址，比如我们使用代理服务器爬取百度服务器的内容<br>反向代理<br>我们在请求服务器的时候，不知道最终目标ip地址，比如我们在访问nginx反向代理的服务器的时候，我们只是在访问nginx服务器，我们并不知道nginx去访问哪一个ip地址<br>requests使用代理<br>使用方法：<br>requests.get(url, proxies = proxies)<br>需要参数<br>proxies：字典类型命名参数，指定代理服务器支持的协议，和代理服务器的请求地址<br>示例：proxies = {“协议”:”协议://ip:port”}，传递代理服务器支持协议类型（http/https），和代理服务器的ip和端口<br>备注：proxies字典参数最多只能接受两个键值对，一个键是http，一个键是https，定义在进行http/https请求的时候分别使用的代理服务器<br>代理服务器注意点：<br>http的url地址要使用http的代理，https的要使用https的代理<br>透明度低的代理能够被对方服务器找到我们的真实的ip，可能会导致代理的效果不明显<br>cookie与session的请求<br>cookie和session的区别<br>cookie存在浏览器本地，session在服务端<br>cookie不安全，session不会将数据暴露在客户端，比较安全<br>session占用性能，会加长请求的时间<br>cookie存储是有上限的，session没有<br>请求带上cookies的好处<br>能够请求登陆后的页面<br>带上cookie反反扒，用登录成功的cookie来进行伪造<br>伪造请求带上cookie的不好的地方<br>使用同一个cookie，不间断的访问同一个服务器的时候，可以被对方识别为爬虫<br>解决方法：使用多个用户名密码，多账号，随机选用账号进行服务器的访问，模拟多人访问<br>模拟登录cookie/session请求的方法<br>第一种<br>session请求登录接口<br>实例化一个session对象<br>session = requests.session()<br>使用session请求登录接口，session对象会将响应中的cookie进行保存<br>session.post(url, data=data, headers=headers)<br>也可以使用get请求<br>再使用session对象请求其他需要登录的url地址，会自动带上cookie进行访问<br>response =session.get(url, params=params, headers=headers)<br>session对象的作用<br>对响应的cookie进行保存，后面的访问会自动携带cookie<br>第二种<br>要获取了登录后的cookie请求字符串<br>headers中放cookie请求头字符串<br>第三种<br>把cookie的每一个name和value组成一个字典<br>将字典传给requests请求中的cookies参数接收<br>获取response中的cookie的方法<br>获取response中的cookie对象<br>response.cookies      获取respone的cookie对象<br>response.cookies只能获取服务器主动设置的cookie，不能获取我们手动创建的cookie<br>返回的数据类型是列表嵌套字典，每一个字典包含一个cookie的所有信息<br>将cookie对象转化为字典类型的方法<br>requests.utils.dict_from_cookiejar(response.cookies)<br>将python字典类型转化为cookie对象类型<br>requests.utils.cookiejar_from_dict( {‘key’: value } )<br>requests请求常见问题<br> SSl证书验证问题<br>问题产生原因<br>请求协议为https的网站需要向机构申请证书，这样用户才能直接通过https访问，但是有的网站（比如12306）的整数是自己研发的，这样浏览器不会在进行请求的时候，会产生一个证书异常，我们需要点击浏览器上的继续访问，来请求服务器，当我们使用爬虫来进行访问的时候，会直接报出异常<br>解决办法：<br>response = requests.get(“<a href="https://www.12306.cn/mormhweb/" target="_blank" rel="noopener">https://www.12306.cn/mormhweb/</a> “, verify=False)<br>在请求非机构证书的网站的时候，我们需要在请求方法中加上verify=False 的参数，就不会报异常<br>请求超时问题<br>问题产生原因<br>当我们进行请求的时候，我们可能因为在请求摸一个url的时候产生一些异常，导致长时间没有请求成功，由于请求一直在进行，导致后面的程序无法正常执行，验证影响程序的效率或者导致程序终止<br>解决办法<br>应用到的方法<br>from retrying import retry<br>@retry(stop_max_attempt_number=n)      装饰函数，表示函数如果报错将会再次执行，直到第n次如果依然报错，那将抛出异常<br> response = requests.get(url,timeout=10)     设置请求的超时时间，如果限定时间内没有请求成功，将会抛出异常<br>assert response.status_code == m      assert为断言关键字，如果后面的条件表达式为false将会排出异常<br>示例：</p>
<p>数据处理的技巧<br>字符串的格式化<br>“abc{}abc”.format()<br>不同于%号的格式化，{ }方式的格式化，可以接收任意类型的格式化<br>format  接收的参数与格式化{ }的数量相等<br>在通过字符串格式化构建url进行requests请求时候，尽量使用{ }来格式化，因为在浏览器会将url格式化，%有时候会按照特殊字符处理；<br> json数据与字符串之间的转化<br>import json<br>json.dumps( dict)    把python类型字典数据转化为json字符串<br>json.loads(‘json’ )    把json数据转化为python的字典类型<br>扁平化赋值表达式<br>name = “a” if lang==b else “c”<br>if后面的条件如果成立，那么就把if前的值赋给str<br>否则if否面的条件不成立，就把else后的值赋给str<br>name = a and ‘b’ or ‘c’<br>如果a为true，name等于b<br>如果a为false，name等于c<br>列表推导式和字典推导式<br>列表推倒式<br>[i for i in range(10) if i%2==0]<br>字典推导式<br>{i+1:i for i in range(10) if i%3==0}<br>注意：<br>在列表推导式中可以使用if判断，但是不能够使用else<br>tips<br>linux命令重命名<br>作用<br>可以将一段的linux命令，重名为其他比较短而且容易记忆的命令，方便我们的调用<br>设置方法<br>修改家目录下的.bashrc文件<br>添加  alias  新的命令=’原命令 -选项/参数’<br>保存退出   source .bashrc<br>已经可以使用新的命令了<br>将数据写入文件<br>文件存储内容的方法<br>with open(‘文件路径’, encoding=’utf-8’) as f:<br>f.write(content)<br>encoding 如果文件不存在，相当于我们创建一个文件进行写入，我们可以通过encoding来指定写入内容的编码格式，如果不指定，可能会报错</p>
<p>常见的反扒思路<br>尽量减少请求的次数<br>能抓列表页不抓详细页<br>保存html页面，有利于重复使用<br>多分析一个网站的不同类型页面<br>手机极速版页面<br>wap手机版页面<br>web网页<br>app抓包软件<br>进行请求伪装<br>多带一些请求头，有的时候请求头带的不同，服务器返回的结果不同<br>代理ip，设置代理ip池，定期更新代理ip池<br>在浏览器会根据cookie内容判断爬虫的时候，携带cookie（但是注意不要一直携带同一个cookie）<br>需要获取登录之后的数据的时候，要进行模拟登录，获取cookie后，在进行数据的获取<br>利用多线程/分布式（尽可能的快速抓取）<br>在可能的情况下尽可能的使用多线程和分布式爬虫</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/Puppeteer动态爬取" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/Puppeteer动态爬取/" class="article-date">
      <time datetime="2018-10-18T12:22:23.178Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Puppeteer简介<br>Puppeteer介绍<br>Puppeteer 是一个node库，他提供了一组用来操纵Chrome的API, 通俗来说就是一个 headless chrome浏览器 (当然你也可以配置成有UI的，默认是没有的)。既然是浏览器，那么我们手工可以在浏览器上做的事情 Puppeteer 都能胜任, 另外，Puppeteer 翻译成中文是”木偶”意思，所以听名字就知道，操纵起来很方便，你可以很方便的操纵她去实现：<br>运行环境</p>
<ol>
<li>Nodejs 的版本不能低于 v7.6.0, 需要支持 async, await.</li>
<li><p>需要最新的 chrome driver, 这个你在通过 npm 安装 Puppeteer 的时候系统会自动下载的<br>npm install puppeteer<br>3.安装pyppeteer<br>pip install pyppeteer<br>Puppeteer的官方文档<br><a href="https://github.com/miyakogi/pyppeteer" target="_blank" rel="noopener">https://github.com/miyakogi/pyppeteer</a><br>知识前提<br>Puppeteer 几乎所有的操作都是 异步的, 为了使用大量的 then 使得代码的可读性降低，本文所有 demo 代码都是用 async, await 方式实现<br>由于文档上大量使用async和await关键字,  需要提前了解Async/Await 异步编程<br>Puppeteer的基本用法<br>python示例</p>
</li>
<li><p>先通过launch() 创建一个浏览器实例 Browser 对象</p>
</li>
<li>然后通过 Browser 对象创建页面 Page 对象</li>
<li>然后 page.goto() 跳转到指定的页面</li>
<li>调用 page.screenshot() 对页面进行截图</li>
<li>关闭浏览器<br>Puppeteer的常用属性/方法/对象<br>launch(options)方法<br>介绍<br>使用launch 方法会返回一个browser对象, 创建browser对象的时候可以选择性的配置如下选项<br>常用参数</li>
</ol>
<p>Browser对象<br>介绍<br>Browser对象相当于selenium的driver对象, 常用的有如下方法<br>当返回值是promise的时候, 在python中返回的是异步函数, 需要这样的函数需要使用await关键字调用<br>常用方法</p>
<p>browser.userAgent()    设置browser的请求头<br>Page对象<br>介绍<br>Page对象相当于selenium的每一个标签页<br>我们对页面的元素进行操作就是操作page对象<br>常用方法<br>常用方法<br>page.content()     获取页面的源码<br>page.cookies(…urls)      获取当前的cookies<br>page.goto()     请求某个页面<br>page.click(selector[, options])     点击元素<br>page.focus(selector)    聚焦某元素<br>page.close(options)     关闭当前标签页<br>page.url()   当前页面的url<br>page.reload(options)     重新加载页面<br>page.type(selector, text[, options])    向聚焦的元素框中输入内容<br>获取元素<br>Page.quirySelector()      选择一个元素<br>Page.querySelectorAll()     选择一组元素<br>Page.xpath()     通过xpath选择元素<br>evaluate()方法(运行js脚本/获取元素的属性)<br>evaluate接收字符串为参数， 字符串的内容可以使javascript的原生函数， 或者是表达式， 当字符串的内容是表达式的时候我们通常可以用来获取元素的属性， 当字符串的内容是js函数的时候通常可以用来在当前页面执行js代码<br>当字符串的内容是表达式的时候可以用force_expr属性来强调, 不然可能会识别失败报错<br>示例：</p>
<p>修改driver的配置</p>
<ol>
<li>Page.setViewport() 修改浏览器视窗大小</li>
<li>Page.setUserAgent() 设置浏览器的 UserAgent 信息</li>
<li>Page.emulateMedia() 更改页面的CSS媒体类型，用于进行模拟媒体仿真。 可选值为 “screen”, “print”, “null”, 如果设置为 null 则表示禁用媒体仿真。<br>键盘keyboard<br>keyboard.down(key[, options]) 触发 keydown 事件<br>keyboard.press(key[, options]) 按下某个键，key 表示键的名称，比如 ‘ArrowLeft’ 向左键，详细的键名映射请戳这里<br>keyboard.sendCharacter(char) 输入一个字符<br>keyboard.type(text, options) 输入一个字符串<br>keyboard.up(key) 触发 keyup 事件<br>鼠标mouse<br>mouse.click(x, y, [options]) 移动鼠标指针到指定的位置，然后按下鼠标，这个其实 mouse.move 和 mouse.down 或 mouse.up 的快捷操作<br>mouse.down([options]) 触发 mousedown 事件，options 可配置:<br>options.button 按下了哪个键，可选值为[left, right, middle], 默认是 left, 表示鼠标左键<br>options.clickCount 按下的次数，单击，双击或者其他次数<br>delay 按键延时时间<br>mouse.move(x, y, [options]) 移动鼠标到指定位置， options.steps 表示移动的步长<br>mouse.up([options]) 触发 mouseup 事件<br>waitFor()等待方法<br>page.waitFor(selectorOrFunctionOrTimeout[, options[, …args]]) 下面三个的综合 API<br>page.waitForFunction(pageFunction[, options[, …args]]) 等待 pageFunction 执行完成之后<br>page.waitForNavigation(options) 等待页面基本元素加载完之后，比如同步的 HTML, CSS, JS 等代码<br>page.waitForSelector(selector[, options]) 等待某个选择器的元素加载之后，这个元素可以是异步加载的，这个 API 非常有用，你懂的。<br>事件的监听<br>使用方法<br>Page.on(“event”, callback)<br>event: 事件名称<br>callback: 监听到事件后执行的回调函数<br>所有可以监听的事件</li>
</ol>
<p>官方文档介绍的Python使用Puppeteer的区别<br>Keyword arguments for options<br>Puppeteer uses object (dictionary in python) for passing options to functions/methods. Pyppeteer accepts both dictionary and keyword arguments for options.<br>Dictionary style option (similar to puppeteer):<br>browser = await launch({‘headless’: True})<br>Keyword argument style option (more pythonic, isn’t it?):<br>browser = await launch(headless=True)<br>Element selector method name ($ -&gt; querySelector)<br>In python, $ is not usable for method name. So pyppeteer usesPage.querySelector()/Page.querySelectorAll()/Page.xpath() instead of Page.$()/Page.$$()/Page.$x(). Pyppeteer also has shorthands for these methods, Page.J(), Page.JJ(), and Page.Jx().<br>Arguments of Page.evaluate() and Page.querySelectorEval()<br>Puppeteer’s version of evaluate() takes JavaScript raw function or string of JavaScript expression, but pyppeteer takes string of JavaScript. JavaScript strings can be function or expression. Pyppeteer tries to automatically detect the string is function or expression, but sometimes it fails. If expression string is treated as function and error is raised, add force_expr=True option, which force pyppeteer to treat the string as expression.<br>Example to get page content:<br>content = await page.evaluate(‘document.body.textContent’, force_expr=True)<br>Example to get element’s inner text:<br>element = await page.querySelector(‘h1’)<br>title = await page.evaluate(‘(element) =&gt; element.textContent’, element)<br>python实现puppeteer同步使用库<br>github  ：  <a href="https://github.com/issacLiuUp/puppeteer" target="_blank" rel="noopener">https://github.com/issacLiuUp/puppeteer</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/数据快速写入Csv文件" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/数据快速写入Csv文件/" class="article-date">
      <time datetime="2018-10-18T12:22:23.144Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>csv文件的写入<br>以列表的方式写入<br>import  csv      导入csv模块<br>with open(‘data.csv’, ‘w’, encoding=’utf-8’) as csvfile:<br>writer = csv.writer(csvfile, delimiter=’,’)    创建阅读器<br>writer.writerow([‘id’, ‘name’, ‘age’])    向csv文件中写入一行数据<br>writer.writerows([ [‘1’, ‘2’, ‘3’], [‘2’, ‘3’, ‘4’] ])     向csv文件中写入多行数据<br>以字典的方式写入<br>import  csv      导入csv模块<br>with open(‘data.csv’, ‘w’, encoding=’utf-8’) as csvfile:<br>fieldnames = [‘id’, ‘name’, ‘age’]<br>writer = csv.DictWriter(csvfile, fieldnames=fieldnames)    创建阅读器， 并指定列索引<br>writer.writeheader()      向文件中写入列的索引<br>writer.writerow({‘id’: 1, ‘name’: 2, ‘age’: 3})    向csv文件中写入一行数据<br>csv文件的读取<br>csv文件按行读取<br>import csv<br>with open(‘data.csv’, ‘r’, encoding=’utf-8’) as csvfile:<br>reader = csv.reader(csvfile)      创建阅读器<br>for row in reader:<br>print(row)</p>
<blockquote>
<blockquote>
<p>[‘id’, ‘name’, ‘age’]<br>使用pandas进行csv文件的写入和读取<br>读取csv文件数据的方法<br>df_obj = pd.read_csv(‘filename’, [usecols=[‘col1’, ‘col2’], skiprows=n1, skipfooter=n2, index_col=n3, engine=’python’])     返回的是DataFrame对象<br>filename：表示读取的文件名<br>usecols：表示读取文件的哪些列<br>skiprows:  表示跳过文件的前几行<br>skipfooter： 表示跳过文件的后几行<br>index_col：表示使用那一列作为索引列<br>engine： 使用的解释器引擎， 当读取中文文件的时候需要指定<br>写入csv数据的方法<br>df_obj.to_csv(“filename”)<br>filename：表示读取的文件名,要加上.csv后缀；</p>
</blockquote>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/数据去重方法-HashBloom" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/数据去重方法-HashBloom/" class="article-date">
      <time datetime="2018-10-18T12:22:23.127Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>hash序列化过滤<br>使用hash哈希方法去重的场景<br>当数据量不大的时候，并且数据所占内存不多的时候<br>当只有几万条url去重的时候，可以直接使用hash+redis的set类型进行数据的去重<br>使用sha1-redis去重的实例<br>​        import hashlib<br>​        import redis<br>​        from redis import *</p>
<pre><code>class Filter_hash(object):
    # 创建redis客户端对象
    sr = StrictRedis(host=&apos;localhost&apos;, port=6379, db=0)
    # 定义存储hash后数据的key_name     
    key = &apos;hash_list&apos;        

    def add_hash_num(self, key_name，url):
            # 创建一个哈希对象
            fp = hashlib.sha1()
            # 对url进行哈希序列化
            fp.update(url)
            fp_num = fp.hexdiget()
            # 将十六进制后的序列化的hash数值进行存储
            added = self.server.sadd(self.key, fp_num)
            return   added   # 如果插入成功， 返回1，表示数据不重复插入成功，否则返回0

            filter_cur = Filter_hash()

if __name__ == &apos;__main__&apos;:
      filter_cur.add_hash_num(&apos;url&apos;， &apos;https://www.baidu.com&apos;)
</code></pre><p>bloom-布隆过滤<br>什么是布隆过滤器<br>是一种space efficient的概率模型数据结构，用于判断一个元素是否在集合中。<br>一个空的布隆过滤器是一个m bit的bitmap，每一位都初始化为0。布隆过滤器定义有k个hash函数，对输入的数据生成k个hash值，定义一个map函数将k个hash值映射到bitmap的k个位。<br>bitmap数据类型<br>bitmap介绍<br>Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32长度的位数组。<br>位操作方法可以被分为两组：<br>一、对单一位的操作，比如设置某一位为1或0，或者得到这一位的值；<br>二、对一组位的操作，比方说计算一定范围内的1的个数（比如计数）<br>bitmap的应用场景<br>bitmap一个最大的优势是它通常能在存储信息的时候节省大量空间。比方说一个用增量ID来辨别用户的系统，可以用仅仅512MB的空间来标识40亿个用户是否想要接受通知。<br>使用SETBIT和GETBIT命令来对位进行置数和检索（redis中实现的bitmap类型数据的操作）</p>
<blockquote>
<p>setbit key 10 1<br>(integer) 1<br>getbit key 10<br>(integer) 1<br>getbit key 11<br>(integer) 0<br>返回的是该位上之前的数值<br>SETBIT 如上所示，意思是将第10位置位为1，第二个参数可为0或1。如果设置的位超出了当前String的长度，那么会自动增长。（最长2^32，下同）<br>GETBIT 如上所示，返回第10位和第11位的数据，分别是1和0。如果查找的位超出了当前String的长度，那么会返回0。<br>接下来是三个对一组位进行操作的命令:<br>BITOP 执行不同字符串之间的逐位操作。所提供的操作有AND，OR，XOR和NOT。BITCOUNT<br>BITCOUNT 计数,返回bitmap里值为1的位的个数.<br>BITPOS 返回第一个0或1的位置<br>BITPOS和BITCOUNT不仅可以作用于整个bitmap，还可以作用于一定的范围,下面是一个BITCOUNT的例子<br>布隆过滤的原理<br>布隆过滤器需要的是一个位数组(和位图类似)和K个映射函数(和Hash表类似)，在初始状态时，对于长度为m的位数组array，它的所有位被置0　</p>
</blockquote>
<p>对于有n个元素的集合S={S1,S2…Sn},通过k个映射函数{f1,f2,……fk}，将集合S中的每个元素Sj(1&lt;=j&lt;=n)映射为K个值{g1,g2…gk}，然后再将位数组array中相对应的array[g1],array[g2]……array[gk]置为1：　</p>
<p>如果要查找某个元素item是否在S中，则通过映射函数{f1,f2,…fk}得到k个值{g1,g2…gk}，然后再判断array[g1],array[g2]…array[gk]是否都为1，若全为1，则item在S中，否则item不在S中。这个就是布隆过滤器的实现原理。<br>布隆过滤优点<br>相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数。<br>布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。<br>布隆过滤器可以表示全集，其它任何数据结构都不能；<br>k 和 m 相同，使用同一组 Hash 函数的两个布隆过滤器的交并差运算可以使用位操作进行。<br>缺点<br>误算率（False Positive），随着存入的元素数量增加，错判“在集合内”的概率就越大了，误算率随之增加。<br>一般情况下不能从布隆过滤器中删除元素. 我们很容易想到把位列阵变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全的删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。<br>布隆过滤器的应用场景<br>在对大量的数据进行去重的时候， 可以使用布隆过滤器判断元素是否已经在集合中，通过判断的结果，来对数据进行相应的操作<br>布隆过滤实现（包在下方）<br>构造HashMap类<br>这里新建了一个HashMap类。构造函数传入两个值，一个是m位数组的位数，另一个是种子值seed。不同的散列函数需要有不同的seed，这样可以保证不同的散列函数的结果不会碰撞。<br>在hash()方法的实现中，value是要被处理的内容。这里遍历了value的每一位，并利用ord()方法取到每一位的ASCII码值，然后混淆seed进行迭代求和运算，最终得到一个数值。这个数值的结果就由value和seed唯一确定。<br>我们再将这个数值和m进行按位与运算，即可获取到m位数组的映射结果，这样就实现了一个由字符串和seed来确定的散列函数。<br>当m固定时，只要seed值相同，散列函数就是相同的，相同的value必然会映射到相同的位置。<br>所以如果想要构造几个不同的散列函数，只需要改变其seed就好了。以上内容便是一个简易的散列函数的实现。</p>
<p>构造BloomFilter<br>Bloom Filter里面需要用到k个散列函数，这里要对这几个散列函数指定相同的m值和不同的seed值<br>由于我们需要亿级别的数据的去重，即前文介绍的算法中的n为1亿以上，散列函数的个数k大约取10左右的量级</p>
<p>实现判断元素是否重复和添加元素到集合的方法<br>insert方法<br>Bloom Filter算法会逐个调用散列函数对放入集合中的元素进行运算，得到在m位位数组中的映射位置，然后将位数组对应的位置置1。<br>这里代码中我们遍历了初始化好的散列函数，然后调用其hash()方法算出映射位置offset，再利用Redis的setbit()方法将该位置1。<br>exit方法<br>我们要实现判定是否重复的逻辑，方法参数value为待判断的元素。我们首先定义一个变量exist，遍历所有散列函数对value进行散列运算，得到映射位置，用getbit()方法取得该映射位置的结果，循环进行与运算。<br>这样只有每次getbit()得到的结果都为1时，最后的exist才为True，即代表value属于这个集合。如果其中只要有一次getbit()得到的结果为0，即m位数组中有对应的0位，那么最终的结果exist就为False，即代表value不属于这个集合。</p>
<p>布隆过滤源码包<br>bloomfilter.py<br>​            </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/mitmproxy&amp;mitmdump手机爬虫" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/mitmproxy&mitmdump手机爬虫/" class="article-date">
      <time datetime="2018-10-18T12:22:23.111Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>mitmproxy的介绍和安装配置<br>mitmproxy的作用<br>mitmproxy是一个支持HTTP和HTTPS的抓包程序，类似Fiddler、Charles的功能，只不过它通过控制台的形式操作。<br>mitmproxy还有两个关联组件，一个是mitmdump，它是mitmproxy的命令行接口，利用它可以对接Python脚本，实现监听后的处理；另一个是mitmweb，它是一个Web程序，通过它以清楚地观察到mitmproxy捕获的请求。<br>可以保存Http会话并进行分析<br>模拟客户端发起请求，模拟服务器端返回的响应<br>利用反向代理将流量发给指定的服务器<br>支持Mac和Linux上的透明代理<br>利用Python对Http请求和响应进行实时的处理<br>mitmproxy和mitmdump的区别<br>mitmproxy起到了代理服务的功能，手机和PC在同一个局域网内，可以将mitmproxy设置为手机的代理，这样数据都是通过mitmproxy妆发出去的，起到了中间人的作用<br>mtmdump可以实现将抓取到的请求和响应直接交给某个Python程序进行处理，比如提取和入库操作<br>mitmproxy相关链接<br>GitHub：<a href="https://github.com/mitmproxy/mitmproxy" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy</a><br>官方网站：<a href="https://mitmproxy.org" target="_blank" rel="noopener">https://mitmproxy.org</a><br>PyPI：<a href="https://pypi.python.org/pypi/mitmproxy" target="_blank" rel="noopener">https://pypi.python.org/pypi/mitmproxy</a><br>官方文档：<a href="http://docs.mitmproxy.org" target="_blank" rel="noopener">http://docs.mitmproxy.org</a><br>mitmdump脚本：<a href="http://docs.mitmproxy.org/en/stable/scripting/overview.html" target="_blank" rel="noopener">http://docs.mitmproxy.org/en/stable/scripting/overview.html</a><br>下载地址：<a href="https://github.com/mitmproxy/mitmproxy/releases" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy/releases</a><br>DockerHub：<a href="https://hub.docker.com/r/mitmproxy/mitmproxy" target="_blank" rel="noopener">https://hub.docker.com/r/mitmproxy/mitmproxy</a><br>mitmproxy的安装<br>linux下使用pip安装<br>pip3 install mitmproxy<br>这是最简单和通用的安装方式，执行完毕之后即可完成mitmproxy的安装，另外还附带安装了mitmdump和mitmweb这两个组件。<br>windows下安装<br>获取安装包，下载地址: <a href="https://github.com/mitmproxy/mitmproxy/releases/" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy/releases/</a><br>在Windows上不支持mitmproxy的控制台接口，但是可以使用mitmdump和mitmweb。<br>Linux下源码安装<br>下载源码包，下载地址: <a href="https://github.com/mitmproxy/mitmproxy/releases/" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy/releases/</a><br>它包含了最新版本的mitmproxy和内置的Python 3环境，以及最新的OpenSSL环境<br>tar -zxvf mitmproxy-2.0.2-linux.tar.gz<br>sudo mv mitmproxy mitmdump mitmweb /usr/bin<br>证书配置<br>证书配置的说明<br>对于mitmproxy来说，如果想要截获HTTPS请求，就需要设置证书。<br>mitmproxy在安装后会提供一套CA证书，只要客户端信任了mitmproxy提供的证书，就可以通过mitmproxy获取HTTPS请求的具体内容，否则mitmproxy是无法解析HTTPS请求的。<br>证书配置步骤<br>启动mitmdump<br>mitmdump<br>找到家目录.mitmproxy目录里面的CA证书<br>mitmproxy-ca.pem        PEM格式的证书私钥<br>mitmproxy-ca-cert.pem       PEM格式证书，适用于大多数非Windows平台<br>mitmproxy-ca-cert.p12        PKCS12格式的证书，适用于Windows平台<br>mitmproxy-ca-cert.cer           与mitmproxy-ca-cert.pem相同，只是改变了后缀，适用于部分Android平台<br>mitmproxy-dhparam.pem     PEM格式的秘钥文件，用于增强SSL安全性<br>在各平台上配置证书的过程<br>Windows平台<br>1.双击mitmproxy-ca.p12，会出现导入证书的页面向导<br>2.直接点击下一步，会出现密码设置的提示，这里不需要设置密码，直接点击下一步按钮即可<br>3.接下来选择证书的存储区域。这里点击第二选项，将所有的证书都放入下列存储，然后点击浏览按钮，选择证书的存储位置为收信人的根证书颁发急购，接着点击确定按钮，点击下一步<br>4.过程中出现安全警告直接点击是<br>Android<br>1.在Android手机上，需要将证书mitmproxy-ca-cert.pem文件发送到手机上，例如直接复制文件。<br>2.接下来，点击证书，便会出现一个提示窗口，这时输入证书的名称，然后点击“确定”按钮即可完成安装。<br>iOS<br>1.将mitmproxy-ca-cert.pem文件发送到iPhone上，推荐使用邮件方式发送，然后在iPhone上可以直接点击附件并识别安装。<br>点击“安装”按钮之后，会跳到安装描述文件的页面，点击“安装”按钮，此时会有警告提示，继续点击右上角的“安装”按钮，安装成功之后会有已安装的提示。<br>如果你的iOS版本是10.3及以上版本，还需要在“设置”→“通用”→“关于本机”→“证书信任设置”将mitmproxy的完全信任开关打开。<br>mitmproxy的使用<br>启动mitmproxy服务<br>mitmproxy    这样就会在8080端口上运行一个代理服务<br>将mitmproxy设置为手机端的代理<br>将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings<br>发送请求<br>在手机端进行网络请求，便可以在mitmproxy的界面上看到对应的请求<br>查看/处理请求和响应<br>查看请求的详情<br>光标移动到对应的请求位置，点击ENTER<br>在请求中查看Request/Response/Detail<br>将光标移动到对应的分栏上，点击Tab<br>重新编辑请求<br>1.在请求中，点击e键进行编辑<br>2.按照高亮的部分，选择想要编辑的内容(比如: q：修改请求方参数，m：修改请求方式)<br>进入修改页面后,可以直接对内容进行修改<br>点击a可以增加一行参数<br>修改完成后点击Esc退出修改，q返回上一级页面<br>3.敲击a进行修改的保存<br>4.对修改后的请求重新发起请求<br>mitdump的使用<br>编写请求和响应的处理脚本<br>日志输出<br>介绍<br>ctx模块提供了不同等级的log将会打印不一样的颜色<br>使用示例<br>from mitmproxy import ctx<br>def request(flow):<br>flow.request.headers[‘User-Agent’] = ‘MitmProxy’<br>ctx.log.info(“info”)<br>ctx.log.warn(“warn”)<br>ctx.log.error(“error”)<br>Request对象处理<br>介绍<br>我们可以通过request() 方法实现对请求进行修改<br>request对象包含的属性: url、headers、cookies、host、method、scheme、port<br>使用示例<br>def request(flow):<br>request = flow.request<br>print(request.url)<br>Response对象处理<br>介绍<br>可以通过response() 方法实现对响应的操作，比如入库等操作<br>response对象包含的属性: status_code、deaders、cookies、text<br>使用示例<br>def response(flow):<br>response = flow.response<br>print(response.text)<br>启动mitmproxy服务<br>mitmdump  [OPTIONS]<br>-w: 可以指定将接货的数据都保存到此文件中<br>-s: 可以指定scripts.py 脚本文件，用来处理请求和响应，它需要放在当前命令的执行目录下<br>将mitmdump设置为手机端的代理<br>将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings<br>发送请求<br>在手机端进行网络请求，便可以在mitmdump的日志中便可以看见对应的请求</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/Fiddler抓包工具安装、配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/Fiddler抓包工具安装、配置/" class="article-date">
      <time datetime="2018-10-18T12:22:23.095Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>使用前配置<br>安装证书(配置HTTPS网站的抓取证书)<br>点击工具栏tools<br>点击Telerlk FiddlerOptions<br>点击HTTPS选项<br>点击右上角的Actions–&gt;Trust Root Certificate<br>软件界面</p>
<p>手机app抓包方法<br>将Fiddler设置为代理<br>点击工具栏tools—-&gt;Connections<br>勾选 Allow remote computers to connect<br>端口号设置为8888（可以设置其他）<br>配置手机端的证书<br>配置方法见其他两篇介绍<br>配置移动端的官方文档：<a href="http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureForiOS" target="_blank" rel="noopener">http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureForiOS</a><br>使用手机访问网页</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/Fiddler在ios上配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/Fiddler在ios上配置/" class="article-date">
      <time datetime="2018-10-18T12:22:23.075Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Capture Traffic from iOS Device<br>Configure Fiddler<br>Click Tools &gt; Fiddler Options &gt; Connections.</p>
<p>Click the checkbox by Allow remote computers to connect.</p>
<p>Allow remote computers to connect</p>
<p>Restart Fiddler.</p>
<p>Ensure your firewall allows incoming connections to the Fiddler process.</p>
<p>Hover over the Online indicator at the far right of the Fiddler toolbar to display the IP addresses assigned to Fiddler’s machine.</p>
<p>Online Tooltip</p>
<p>Verify client iOS device can reach Fiddler by navigating in the browser to <a href="http://FiddlerMachineIP:8888" target="_blank" rel="noopener">http://FiddlerMachineIP:8888</a>. This address should return the Fiddler Echo Service page.</p>
<p>For iPhone: Disable the 3g/4g connection.</p>
<p>Set the iOS Device Proxy<br>Tap Settings &gt; General &gt; Network &gt; Wi-Fi.</p>
<p>Tap the settings for the Wi-Fi network.</p>
<p>Tap the Manual option in the HTTP Proxy section.</p>
<p>In the Server box, type the IP address or hostname of your Fiddler instance.</p>
<p>In the Port box, type the port Fiddler is listening on (usually 8888).</p>
<p>Ensure the Authentication slider is set to Off.</p>
<p>iOS Proxy Settings</p>
<p>Decrypt HTTPS Traffic from iOS Devices<br>Download the Certificate Maker plugin for Fiddler.</p>
<p>Install the Certificate Maker plugin.</p>
<p>Restart Fiddler.</p>
<p>Configure the device where Fiddler is installed to trust Fiddler root certificate.</p>
<p>On the iOS device, go to <a href="http://ipv4.fiddler:8888/" target="_blank" rel="noopener">http://ipv4.fiddler:8888/</a> in a browser.</p>
<p>From the bottom of the Fiddler Echo Service webpage, download the FiddlerRoot certificate.</p>
<p>Download FiddlerRoot Certificate</p>
<p>Open the FiddlerRoot.cer file.</p>
<p>Tap the Install button.</p>
<p>Install Profile</p>
<p>Tap the Install button again.</p>
<p>Warning</p>
<p>Uninstall FiddlerRoot Certificate<br>If you decide to uninstall the root certificate:</p>
<p>Tap the Settings app.</p>
<p>Tap General.</p>
<p>Scroll to Profiles.</p>
<p>Tap the DO_NOT_TRUST_FiddlerRoot* profile.</p>
<p>Tap Remove.</p>
<p>是否将当前网页翻译成中文 网页翻译 中英对照 </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/Fiddler在Android上配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/Fiddler在Android上配置/" class="article-date">
      <time datetime="2018-10-18T12:22:23.057Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Fiddler for Android / Google Nexus 7</p>
<p>Configure Fiddler<br>​    1.<br>Click Tools &gt; Fiddler Options &gt; Connections.<br>​    2.<br>Ensure that the checkbox by Allow remote computers to connect is checked.<br>​    3.<br>If you check the box, restart Fiddler.<br>​    4.<br>Hover over the Online indicator at the far right of the Fiddler toolbar to display the IP address of the Fiddler server.</p>
<p>Configure Nexus Device<br>​    1.<br>Swipe down from the top of the screen and tap the Settings icon.<br>​    2.<br>Tap Wi-Fi.<br>​    3.<br>Tap and hold your current Wi-Fi network. Select Modify Network.</p>
<pre><code>4. 
</code></pre><p>Tap the Show advanced options box.</p>
<pre><code>5. 
</code></pre><p>Tap the Proxy settings dropdown and select Manual.</p>
<pre><code>6. 
</code></pre><p>Type the IP address and port (usually 8888) of the Fiddler server.</p>
<pre><code>7. 
</code></pre><p>Tap Save.</p>
<p>To verify this configuration, go to <a href="http://ipv4.fiddler:8888/" target="_blank" rel="noopener">http://ipv4.fiddler:8888/</a>. Chrome should display the Fiddler Echo Service webpage, and the traffic should appear in Fiddler.Disable the proxy<br>After using Fiddler, return to the Proxy Settings screen above and remove the proxy.Decrypt HTTPS<br>​    1.<br>On the Fiddler Echo Service Webpage, click the FiddlerRoot Certificate link.</p>
<pre><code>2. 
</code></pre><p>If the download doesn’t open automatically, swipe down from the top and tap the Settings icon.<br>​    3.<br>Tap Personal &gt; Security.<br>​    4.<br>Under Credential Storage, tap Install from storage.</p>
<pre><code>5. 
</code></pre><p>Tap the FiddlerRoot.cer file.<br>​    6.<br>(Optional) Type a name for the certificate.</p>
<p>To verify this configuration, tap Trusted credentials &gt; User. This should display the Fiddler certificate.Disable HTTPS Decryption<br>To delete the FiddlerRoot certificate, tap Trusted credentials &gt; User and delete the certificate.</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/Crontab定时执行爬虫" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/Crontab定时执行爬虫/" class="article-date">
      <time datetime="2018-10-18T12:22:23.044Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Crontab的使用方法<br>安装cron软件<br>apt-get  install  cron<br>编辑crontab定时执行命令<br>进入crontab编辑界面<br>crontab  -e    进入编辑界面<br>crontab  -l     查看当前的定时任务<br>crontab -r    删除任务<br>编辑需要被定时执行的命令<br>编辑的格式<br>分(0-59)   小时(0-23)  日(1-31)   月(1-12)  星期(0-6)  命令(command)<br>示例<br>30  7  8  <em>  </em>  ls    指定每月的8日的7:30执行ls命令<br><em>/15  </em>  <em>  </em>  <em>  ls      每15分钟执行一次ls命令<br>0  </em>/2  <em>  </em>  *  ls     每隔两个小时执行一次ls<br>注意点</p>
<ul>
<li>/num 代表每隔多长时间的意思<br>当一个位置使用每隔符号的时候，其前边的时间位置，不能为<em><br>星期中0表示周日<br>使用Crontab定时爬虫<br>编辑python_spider命令<br>先把python的执行命令写入.sh脚本<br>#！/bin/sh     将脚本定义在可执行脚本目录中<br>cd <code>dirname</code> $0 || exit 1    cd到.sh文件所在目录(项目目录)，失败则退出，dirname两边的不是引号<br>所以我们要将.sh文件定义在项目目录中，执行的时候，会自动的执行cd命令<br>topython3    切换到python3的环境<br>scrapy crawl spider_name &gt;&gt; run.log 2&gt;&amp;1<br>将终端显示的内容重定向到log日志中，不会在终端显示错误、异常（2&gt;&amp;1，表示会将错误和异常也保存到日志中  ）<br>给.sh文件添加可执行权限<br>chmod  +  run.sh<br>在crontab中编辑脚本文件执行时间（注意些绝对路径）<br>0  6   </em>  <em>  </em>  /home/ubuntu/<strong><em>/myspider.sh &gt;&gt; /home/ubuntu/</em></strong> run_crontab.log 2&gt;&amp;1<br>将终端显示的内容重定向到log日志中，不会保存错误、异常（2&gt;&amp;1，表示会将错误和异常也保存到日志中  ）<br>动态查看log日志<br>tail  -f  log_name.log</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫扩展功能/Charles抓包工具安装、配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/Charles抓包工具安装、配置/" class="article-date">
      <time datetime="2018-10-18T12:22:23.027Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Charles介绍<br>Charles的作用<br>Charles是一个网络抓包工具，相比Fiddler，其功能更为强大，而且跨平台支持得更好，<br>可以选用它来作为主要的移动端抓包工具。<br>相关链接<br>官方网站：<a href="https://www.charlesproxy.com" target="_blank" rel="noopener">https://www.charlesproxy.com</a><br>下载链接：<a href="https://www.charlesproxy.com/download" target="_blank" rel="noopener">https://www.charlesproxy.com/download</a><br>Charles的配置<br>HTTPS证书的配置<br>Windows系统的证书配置<br>1.首先打开Charles，点击Help→SSL Proxying→Install Charles Root Certificate，即可进入证书的安装页面。<br>2.接下来，会弹出一个安装证书的页面，点击“安装证书”按钮，就会打开证书导入向导。<br>3.直接点击“下一步”按钮，此时需要选择证书的存储区域，点击第二个选项“将所有的证书放入下列存储”，然后点击“浏览”按钮，从中选择证书存储位置为“受信任的根证书颁发机构”，再点击“确定”按钮，然后点击“下一步”按钮。<br>4.再继续点击“下一步”按钮完成导入。<br>Mac系统的证书配置<br>1.如果你的PC是Mac系统，可以按照下面的操作进行证书配置。<br>2.同样是点击Help→SSL Proxying→Install Charles Root Certificate，即可进入证书的安装页面。<br>3.接下来，找到Charles的证书并双击，将“信任”设置为“始终信任”即可，如图1-48所示。</p>
<p>代理配置<br>具体操作是点击Proxy→Proxy Settings，打开代理设置页面，确保当前的HTTP代理是开启的，如图1-49所示。这里的代理端口为8888，也可以自行修改。</p>
<p>将手机连接到Charles并安装证书<br>ios系统<br>1.将手机和电脑连在同一个局域网下，可以将PC设置为热点，手机连接其热点<br>2.将PC的ip设置为手机的代理ip，Settings &gt; General &gt; Network &gt; Wi-Fi.</p>
<p>3.设置完毕后，电脑上会出现一个提示窗口，询问是否信任此设备，此时点击Allow按钮即可。这样手机就和PC连在同一个局域网内了，而且设置了Charles的代理，即Charles可以抓取到流经App的数据包了。<br>4.安装Charles的HTTPS证书，在电脑上打开Help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser</p>
<p>5.在手机浏览器中打开chls.pro/ssl下载证书，点击“设置”→“通用”→“关于本机”→“证书信任设置”中将证书的完全信任开关打开。<br> Android系统<br>1.将手机和电脑连在同一个局域网下，可以将PC设置为热点，手机连接其热点<br>2.将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings<br>3.设置完毕后，电脑上就会出现一个提示窗口，询问是否信任此设备, 此时直接点击Allow按钮即可。<br>4.安装Charles的HTTPS证书，在电脑上打开Help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser</p>
<ol start="5">
<li>在手机浏览器上打开chls.pro/ssl，这时会出现一个提示框，为证书添加一个名称，然后点击“确定”按钮即可完成证书的安装。</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2018 刘小恺
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>