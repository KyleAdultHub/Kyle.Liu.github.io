<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="刘小恺" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="刘小恺(Kyle) 的个人博客">
<meta property="og:url" content="http://blog.kyleliu.cn/page/4/index.html">
<meta property="og:site_name" content="刘小恺(Kyle) 的个人博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="刘小恺(Kyle) 的个人博客">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="刘小恺(Kyle) 的个人博客" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>刘小恺(Kyle) 的个人博客</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">刘小恺</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/octave/">octave</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/博客/">博客</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/梯度下降/">梯度下降</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">刘小恺</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">刘小恺</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-4.python爬虫/爬虫扩展功能/Appium手机爬虫" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫扩展功能/Appium手机爬虫/" class="article-date">
      <time datetime="2018-10-18T12:22:23.006Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Appium的介绍和安装<br>Appium的简介<br>Appium是移动端的自动化测试工具，类似于前面所说的Selenium，利用它可以驱动Android、iOS等设备完成自动化测试，比如模拟点击、滑动、输入等操作<br>Appium负责驱动移动端来完成一系列操作，对于iOS设备来说，它使用苹果的UIAutomation来实现驱动；对于Android来说，它使用UIAutomator和Selendroid来实现驱动。<br>同时Appium也相当于一个服务器，我们可以向它发送一些操作指令，它会根据不同的指令对移动设备进行驱动，以完成不同的动作。<br>Appium相关链接<br>GitHub：<a href="https://github.com/appium/appium" target="_blank" rel="noopener">https://github.com/appium/appium</a><br>官方网站：<a href="http://appium.io" target="_blank" rel="noopener">http://appium.io</a><br>官方文档：<a href="http://appium.io/introduction.html" target="_blank" rel="noopener">http://appium.io/introduction.html</a><br>下载链接：<a href="https://github.com/appium/appium-desktop/releases" target="_blank" rel="noopener">https://github.com/appium/appium-desktop/releases</a><br>Python Client：<a href="https://github.com/appium/python-client" target="_blank" rel="noopener">https://github.com/appium/python-client</a><br>Appium Api：<a href="https://testerhome.comm/topics/3711" target="_blank" rel="noopener">https://testerhome.comm/topics/3711</a><br>Appium的安装<br>通过Appium Desktop安装<br>Appium Desktop介绍<br>Appium Desktop支持全平台的安装，我们直接从GitHub的Releases里面安装即可<br>链接为<a href="https://github.com/appium/appium-desktop/releases。目前的最新版本" target="_blank" rel="noopener">https://github.com/appium/appium-desktop/releases。目前的最新版本</a><br>各平台的安装方法<br>下载exe安装包appium-desktop-Setup-1.1.0.exe<br>Mac平台可以下载dmg安装包如appium-desktop-1.1.0.dmg<br>Linux平台可以选择下载源码<br>通过Node.js安装<br>1.首先安装node.js<br>2.npm install -g appium<br>python-api安装<br>pip install Appium-Python-Client<br>开发环境的配置<br>Android开发环境配置<br>下载和配置Android Studio，下载地址为<a href="https://developer.android.com/studio/index.html?hl=zh-cn。下载后直接安装即可。" target="_blank" rel="noopener">https://developer.android.com/studio/index.html?hl=zh-cn。下载后直接安装即可。</a><br>打开Android Studio，直接打开首选项里面的Android SDK设置页面，勾选要安装的SDK版本，点击OK按钮即可下载和安装勾选的SDK版本。<br>添加环境变量，添加ANDROID_HOME为Android SDK所在路径，然后再添加SDK文件夹下的tools和platform-tools文件夹到PATH中。<br>更详细的配置可以参考Android Studio的官方文档：<a href="https://developer.android.com/studio/intro/index.html。" target="_blank" rel="noopener">https://developer.android.com/studio/intro/index.html。</a><br>Appium的使用(有界面版示例)<br>启动Appium服务<br>点击Appium<br>输入主机和端口号，点击Start Server按钮即可启动Appium服务<br>将手机连接到PC端<br>将手机通过数据线和PC相连<br>打开USB调试功能，确保PC可以连接到手机<br>可以在PC端测试是否连接成功<br>adb  devices  -l   查看连接的设备列表<br>如果找不到adb命令，请检查Android开发环境和环境变量是否配置成功<br>如果adb命令不显示设备信息，检查手机和PC的连接情况<br>启动App并操作<br>通过Appium内置驱动来操作<br>打开app<br>点击Appium中的Start New Session<br>配置启动App时的参数<br>platformName: 它是平台名称，需要区分Android或iOs，此处填Android<br>deviceName: 它是设备名称， 此处是手机的具体类型<br>appPackage： 它是App程序的包名<br>AppActivity: 它是入口Activity名，这里通常需要.开头<br>点击保存按钮，可以将此配置进行保存<br>点击Start Session，便会启动Android手机上的App，同时PC上会弹出一个调试窗口，里面包含了页面源码<br>操作应用App<br>选择元素<br>和浏览器的调试一样，只要点击作业页面中的元素，对应的元素就会高亮，中间栏会显示对应的源码<br>右侧是和元素包含的属性和事件等信息<br>操作元素<br>点击右侧元素的属性，便可以实现对元素的操作<br>操作的录制<br>点击中间栏的眼睛按钮，Appium会开始录制操作的动作，这时我们可以在窗口中操作APp的行为都会记录下来<br>Recorder处可以自动生成对应的语言代码。</p>
<p>通过Python代码来操作<br>打开app(初始化应用)<br>打开方法<br>用字典来配置Desired Capabilities 参数，新建一个Session<br>示例代码<br>server = “<a href="http://localhost:4723/wd/hub&quot;" target="_blank" rel="noopener">http://localhost:4723/wd/hub&quot;</a><br>desired_caps = {<br>“platformName”: “Android”,<br>“deviceName”: “MI_NOTE_Pro”,<br>“appPackage”: “com.tencent.mm”,<br>“appActivity”: “.ui.LauncherUI”<br>}<br>from appium import Webdriver<br>from selenium.webdriver.support.ui import WebDriverWait<br>driver = wevdriver.Remote(server, desired_caps)    # 启动app<br>查找元素<br>element = driver.find_element_by_android_uiautomator(‘new UiSelector().description(“Animation”)’)<br>elements = driver.find_elements_by_android_uiautomator(‘new UiSelector().description(“Animation”)’)<br> 点击操作<br>多点点击方法<br>driver.tap(self, positions, duration=None)<br>positions: 它是点击的位置组成的列表<br>duration: 它是点击持续的时间<br>通过tap方法实现触摸点击<br>该方法可以模拟手指点击，最多五个手指，也可以设置触摸的时间长短<br>单点点击方法<br>element.click()    对元素进行点击<br>示例代码<br>driver.tap([(100, 20), (100, 60), (100, 100)], 500)<br>屏幕拖动<br>元素间拖动方法<br>driver.scroll(self, origin_el, destination_el)<br>original_el: 拖动的起始元素<br>destination_el:拖动的终止元素<br>两点间的拖动方法<br>driver.swipe(self, start_x, start_y, end_x, end_y, duration=None)<br>start_x: 它是开始位置的横坐标<br>start_y: 它是开始位置的纵坐标<br>end_x: 它是终止位置的横坐标<br>end_y: 它是终止位置的纵坐标<br>duration: 它是持续时间，单位是毫秒<br>两点间的快速的滑动<br>driver.swipe()<br>start_x: 它是开始位置的横坐标<br>start_y: 它是开始位置的纵坐标<br>end_x: 它是终止位置的横坐标<br>end_y: 它是终止位置的纵坐标<br>示例代码<br>driver.scroll(el1, el2)<br>driver.swipe(100, 100, 100, 400, 5000)<br>driver.flick(100, 100, 100, 400)<br>元素的拖拽<br>拖拽方法<br>driver.drag_and_drop(self, origin_el, destination_el)<br>origin_el: 被拖拽的元素<br>destination_el：拖拽的目标元素<br>示例代码<br>driver.drag_and_drop(el1, el2)<br>文本的输入<br>输入的方法<br>ele.set_text(‘Hello’)<br>示例代码<br>element = driver.find_element_by_id(‘name’)<br>element.set_text(“Hello”)<br>动作链<br>使用动作链的方法<br>from appium import TouchAction()<br>action = TouchAction()<br>action.tap(element).perfoem()<br>常用的动作链还有: tap、press、long_press、release、move_to、wait、cancel<br>示例代码<br>from appium import TouchAction()<br>action = TouchAction()<br>a_ele = self.driver.find_element_by_class_name(“listView”)<br>action.press(a_ele).move_to(x=10, y=0), move_to(x=10, y=-600).release()<br>action.perform<br>示例代码<br>Appium爬取微信<br>MomentsAppium.zip<br>Appium+ mitmdump 爬取京东<br>MitmAppiumJD.zip</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫特殊反反爬/破解诡异字体" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫特殊反反爬/破解诡异字体/" class="article-date">
      <time datetime="2018-10-18T12:22:22.971Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>破解诡异字体</p>
<p>爬虫与诡异的字体-反爬与反反爬的奇技淫巧<br>从今天这篇文章开始，主要给大家过一些小众的反爬策略以及其中的利弊，所以一般来说都会有一个特定的示例，比如今天的反爬策略就来自反爬界大神之一的去哪儿（当然也是爬虫界的）手机版。<br>我们先来用chrome的手机模式开一下去哪儿的机票:</p>
<p>看着叫一个干净整洁，完全没毛病。但是！当我们掀开被子看源码的时候 震惊了：</p>
<p>不知道大家能不能看清图，我码字注解一下，显示的价格是560，而源码中的价格则是540！<br>我们这篇文章就来看看去哪儿手机版是怎么做到的（当然偷偷剧透下，下篇文章咱们主角还是去哪儿）<br>其实同学们只要智商在线，看到标题就知道，这是用字体实现，我们看右侧的css面板也可以看到，这个dom元素的字体是单独设置的。也就是去哪儿通过修改字体，让4显示成了6。<br>1.为什么字体可以反爬<br>感觉从这篇文章开始这个套路快进行不下去了。为什么字体可以反爬？ 首先这个迷惑性太大，很多马虎的工程师以为可以了，直接就爬下来，当然就是被老板骂的体无完肤了，对反反爬工程师的精神可以造成1万点伤害。同时呢，去哪儿这个字体是动态生成的字体，也就是说你也不知道下一次刷出来的4到底是几显示出来的。这个道行就比较高了。<br>2.怎么做好字体反爬？<br>这种反爬策略用在数字上确实天衣无缝，非常优雅。唯一值得提的就是一点，如何动态生成字体和页面来做好对应关系。其实这是一个工程性的问题，大部分编程语言都含有生成字体的库，如果对网站整体性能比较自信的话，完全可以每次请求都动态生成，当然这样确实会比较慢，比较推荐是通过定时任务，去更新一个字体池。每次有请求过来，从字体池中随机拿一个字体，换一个随机的名字（可以通过url rewrite来实现），并和现在的数字做一次映射，调整页面显示后整体输出，就可以在尽量不影响性能的情况下搞死反反爬。<br>当然使用字体也是有局限的，其中最大的问题莫过于@font-face的兼容性问题，所以去哪儿只在移动端采用这个反爬策略，可能也有兼容性的考虑吧，相比之下，去哪儿的pc端的反爬写法则丑陋很多。<br>3.遇到字体反爬怎么办？<br>好了，万一我就是要爬去哪儿机票了怎么办呢？其实字体反爬处理也并不复杂，就像前面说到的，大部分语言都有字体处理类库，而这种情况大概率来讲只有10个数字的字体，我们将字体解析后只要能找到对应关系，就简单了。这里如果是Java工程师的话，推荐用Apache.Fontbox，贴一段解析的代码，注意这段代码不保证现在可用，仅做参考：<br>int[] digits = null;<br>BASE64Decoder decoder = new BASE64Decoder();<br>TTFParser parser = new TTFParser();<br>try {<br>byte[] bytes = decoder.decodeBuffer(fontBase64);<br>InputStream inputStream = new ByteArrayInputStream(bytes);<br>TrueTypeFont ttf = parser.parse(inputStream);<br>if(ttf != null &amp;&amp; ttf.getCmap() != null &amp;&amp;<br>ttf.getCmap().getCmaps() != null &amp;&amp;<br>ttf.getCmap().getCmaps().length &gt; 0){<br>digits = new int[10];<br>CmapSubtable[] tables = ttf.getCmap().getCmaps();<br>CmapSubtable table = tables[0];// No matter what<br>for(int i =0;i&lt;10;i++){<br>digits[i] = table.getGlyphId(i+48)-1;<br>}<br>}<br>} catch (IOException e) {<br>digits = null;<br>}<br>return digits;</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫特殊反反爬/参数伪造" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫特殊反反爬/参数伪造/" class="article-date">
      <time datetime="2018-10-18T12:22:22.955Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>参数伪造<br>携程反爬中的Eleven参数-反爬与反反爬的奇技淫巧<br>今天我们要聊点什么呢，之前说要聊去哪儿的，不过暂且咱们再放一放，先聊一聊去哪儿的干爹携程吧，上次我记得看了携程工程师霸气回应说懂爬虫的来去哪儿，懂反爬的来携程。我觉得特别棒，这种开放的心态和自信，正是一个开放的互联网环境所需要的。<br>所以今天这节课虽然咱们以携程为例，但是我们还是以学习的目的为主，因此我不会把完整的代码放出来，大家掌握思路，拿到渔网比直接copy代码有用的多。<br>上篇文章用邮箱加密给大家演示了爬虫中简单的JS对抗，今天这节课咱们就用携程的Eleven参数来演示下复杂的JS对抗。<br>对了，这个题图，主要是因为携程给他们这个反爬的JS起了一个名字叫oceanball-海洋球，不明觉厉啊。<br>好了，言归正传。做过携程酒店爬虫的朋友，估计都研究过这个eleven参数，这个参数到底是哪里的呢，我们先看下页面请求：</p>
<p>就是这样一个页面，打开一个酒店页面会发现实际的酒店房型列表是一个ajax请求，如下：<br><a href="http://hotels.ctrip.com/Domestic/tool/AjaxHote1RoomListForDetai1.aspx?psid=&amp;MasterHotelID=441351&amp;hotel=441351&amp;EDM=F&amp;roomId=&amp;IncludeRoom=&amp;city=2&amp;showspothotel=T&amp;supplier=&amp;IsDecoupleSpotHotelAndGroup=F&amp;contrast=0&amp;brand=0&amp;startDate=2017-08-28&amp;depDate=2017-08-29&amp;IsFlash=F&amp;RequestTravelMoney=F&amp;hsids=&amp;IsJustConfirm=&amp;contyped=0&amp;priceInfo=-1&amp;equip=&amp;filter=&amp;productcode=&amp;couponList=&amp;abForHuaZhu=&amp;defaultLoad=T&amp;TmFromList=F&amp;eleven=c4350e460862b69d9d76724e1325a0a54ef23c2e0648636c855a329418018a85&amp;callback=CASuBCgrghIfIUqemNE&amp;_=1503884369495" target="_blank" rel="noopener">http://hotels.ctrip.com/Domestic/tool/AjaxHote1RoomListForDetai1.aspx?psid=&amp;MasterHotelID=441351&amp;hotel=441351&amp;EDM=F&amp;roomId=&amp;IncludeRoom=&amp;city=2&amp;showspothotel=T&amp;supplier=&amp;IsDecoupleSpotHotelAndGroup=F&amp;contrast=0&amp;brand=0&amp;startDate=2017-08-28&amp;depDate=2017-08-29&amp;IsFlash=F&amp;RequestTravelMoney=F&amp;hsids=&amp;IsJustConfirm=&amp;contyped=0&amp;priceInfo=-1&amp;equip=&amp;filter=&amp;productcode=&amp;couponList=&amp;abForHuaZhu=&amp;defaultLoad=T&amp;TmFromList=F&amp;eleven=c4350e460862b69d9d76724e1325a0a54ef23c2e0648636c855a329418018a85&amp;callback=CASuBCgrghIfIUqemNE&amp;_=1503884369495</a><br>前面咱说过，出于对反爬工程师工作的尊重，我们今天的文章不去完整介绍整个携程爬虫的做法，其实除了这eleven参数，携程还是在代码了下了不少毒的。<br>一般来说，一个ajax请求，如果没有做cookie限制的话，最难的问题就是能把所有的参数拼接完整，这个请求中最难的就是这个eleven参数的获取，那咱们遇到这种问题，应该具体如何处理，同时对于反爬工作来说，又有什么可以借鉴的地方呢。<br>一、反反爬中遇到复杂JS请求的处理流程<br>凡是题目总有一个解题思路，反反爬也不例外，解题思路很重要。不然就像没头苍蝇一样到处乱撞，今天这篇文章最重要的就是说说这个解题思路。<br>1.查看发起这个请求的JS来源<br>首先，Ajax请求大多都是由JS发起的（今天我们不讨论flash或者其他情况，不常见），我们使用Chrome工具，将鼠标移动到这个请求的Initiator这一栏上，就可以看到完整的调用栈，非常清晰。</p>
<p>2.确认核心JS文件<br>可以看到调用栈中有非常多的JS，那具体我们要分析哪个呢？一般来说我们可以首先排除掉VM开头的和常见库如jQuery这类，当然也不绝对，有些JS也会把自己注册到VM中去，这个另说，携程这里的情况挺明显，一眼就看到了那个不寻常的oceanball，看着就不是一般人，就他了。<br>3.分析JS文件<br>点击这个oceanball咱们就可以看到完整了源码了，下面也是最难的一步，分析JS文件。上一篇文章已经简单说了一些，首先，可读性非常重要，咱们复制完整的JS，贴进Snippet中，咱们大概看下，文件太大，没办法贴到文章中来：<br>一看就是在代码里面下毒，携程反爬工程师不简单啊。</p>
<p>不过呢，咱们看到这种很多数字的其实不要害怕，特别是在看到eval函数和String.fromCharCode，因为这就是把代码加密了一下，这种加密就跟没加密一样，咱们把eval函数直接换成console.log函数，运行一下这个Snippet。在控制台就可以看到原始代码。解密后如下：</p>
<p>看来这个是毒中毒啊，解密后的代码虽然比刚刚的可读性大为增加，但是依然可读性不强，不过根据经验，这样的文件已经没有一个特别简单的方案可以一举解密，只能逐行分析加部分替换了。当然直接Debug也是一个最省力的方案。</p>
<p>通过Debug调试，我们可以看到很多变量的中间值，这中间最重要的莫过于这最后一行，我们可以看到Eleven参数这个时候已经算出来了，而向CASttwHNetheyMWqSJ这个函数中传入一串实现，而这个实现正式返回eleven的值（这段代码是要把人绕死的节奏吗），所以我们只需要构造一个假的CASttwHNetheyMWqSJ来接收这段实现即可，而CASttwHNetheyMWqSJ这个函数名也是通过url传入的，好了，基本上大体的分析搞定了，下面就是构造这个代码的运行环境了。<br>4.构造运行环境<br>重要的话要重复很多遍：写爬虫最好的语言就是JS，因为JS对抗是爬虫中最难的部分，而一个合适的JS环境则可以事半功倍。全球最好的JS爬虫框架-在线网络爬虫/大数据分析/机器学习开发平台-神箭手云。<br>有了JS运行环境，我们其实只需要构造一个函数接受Eleven参数，请求oceanball，并直接运行他就可以了。我们看下我们构造的CASttwHNetheyMWqSJ函数：<br>var callbackFunc = randomCallback(15); var getEleven; eval(“var “+callbackFunc+” = function(a){getEleven = a}”);<br>这里对于初学者可能还是有点绕，简单说明下，首先我们随机了一个函数名，然后我们定义了一个空函数来接受oceanball中的返回eleven参数的函数实现，然后我把这个函数名定义为接受new Function的实现，这样我们后面就可以用getEleven来直接获取eleven参数了。<br>这里还得说下普通JS环境的缺陷，由于在普通JS环境中（非浏览器中）缺少一些重要的内置变量，如window，document等等，导致很多JS是运行不了的，这里我们补上这些变量：<br>var Image = function(){}; var window = {}; window.document = {}; var document = window.document; window.navigator = {“appCodeName”:”Mozilla”, “appName”:”Netscape”, “language”:”zh-CN”, “platform”:”Win”}; window.navigator.userAgent = site.getUserAgent(); var navigator = window.navigator; window.location = {}; window.location.href = “<a href="http://hotels.ctrip.com/hotel/&quot;+hotel_id+&quot;.html&quot;" target="_blank" rel="noopener">http://hotels.ctrip.com/hotel/&quot;+hotel_id+&quot;.html&quot;</a>; var location = window.location;<br>JS如果比较熟悉的话，应该能看出来这里有一个变量不常规，就是第一行的Image变量。这又是携程下的毒啊，我们回到刚刚oceanball的代码看下：</p>
<p>代码中有一句尝试new一个Image对象，如果失败了，则把某一个参数+1，而这个参数正是eleven参数中的某一位，也就是说eleven参数中有一位记录了JS运行环境是否支持new Image，我们这种伪造的浏览器环境当然不支持，所以我们得补上。<br>5.大功告成<br>好了，解了这么多毒，咱们终于可以获取到了eleven参数，当然这只是漫漫长征第一步，后面请求到的结果中，还有更深更辣的毒等着大家：</p>
<p>二、反爬中可以借鉴的地方<br>首先非常感谢携程反爬工程师给了一个教科书般的JS反爬案例，从这个例子我们也可以看出反爬中使用JS代码加密混淆的威力，基本上如果不是JS熟手，或者不熟悉解题思路的话，就是束手无策。JS混淆的方案有很多，大家可以到网上搜一搜，这里还是很推荐携程这种ajax配合回调+js二次混淆的方案，可以说极大提升了反反爬难度的同时也做到了基本不影响性能。但是这个方案依然对于直接渲染JS页面效果一般，还是建议配合之前文章提到的css:content的方案，这样处理之后那对于反反爬工程师的酸爽绝对够得上100桶统一老坛。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫特殊反反爬/CssContent字体破解" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫特殊反反爬/CssContent字体破解/" class="article-date">
      <time datetime="2018-10-18T12:22:22.938Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Css:Content字体破解</p>
<p>爬虫与汽车之家的Css:Content-反爬与反反爬的奇技淫巧<br>发布于</p>
<p>话说感觉这个系列的我起名字越来越不走心了。越写越像哈利波特的起名套路了-爬虫与混血王子。（嗯，一点都不违和）我这个爱扯犊子的性格真是很难收敛啊。<br>话说Css自从越来越强大之后，被很多反爬工程师看上了。上篇文章介绍的字体就需要用到Css中的font-face和font-family。这节我们通过汽车之家来看看Css在反爬中另外一个妙用-content。<br>本来网页上显示的字无非两种，一种就是文本，一种就是显示在图片上的。所以之前有一种常规的反爬，就是把字当做图片替换来显示，目前百度还经常喜欢这么处理，比如百度知道，百度指数（参考天坑），当然不少的电话号码和邮箱这类重要信息，依然也再沿用这个方案。<br>然后随着css的content兼容性越来越好之后，就又有了一个性能更好的反爬，就是用css的content来代替原来文本中的文字，辅以合适的随机生成的方法，确实也不错。算是一个性能和反爬折中的方案吧，比如我们今天要提到的 汽车之家论坛 就是采用这样一个反爬方案。<br>一.为什么Css:Content能反爬。<br>事实上任何能让原来html中文本隐藏混淆加密又不影响正常用户显示的方式都可以来做反爬，比如我们上一篇文章提到的字体方式混淆，比如我们后面要提到的JS加密。而这篇文章主要是采用Css的content属性来隐藏。<br>我们简单的看下content属性使用，大家可以尝试复制这段写入一个网页的Css中<br>div:before {<br>content: “神箭手”;<br>}<br>我们会发现每一个div的开头都加入的神箭手的文字，而且这些文字还是无法复制的。再加上并不像生成图片那样会严重拖慢服务器性能，所以算是一个挺折中的方案。<br>二.如何好好利用Css:Content反爬<br>我们一起来看看汽车之家的反爬学习一下：<br>我们打开一个汽车之家论坛网页，然后直接Ctrl+A全选，就可以很明显的看到用Css的Content插入进的字符，我们掀开被子仔细研究下：</p>
<p>可以看到，汽车之家是将一些常用的字，包括&lt;了&gt;&lt;，&gt;&lt;的&gt;&lt;九&gt;等等变换成Css，这样做优点是可以提前写好css，缺点是由于是固定的库，很容易先把映射的关系解析好，然后直接全文替换。因此比较推荐的，当然还是每次的动态生成，不过这依然产生一个工程性的优化问题，这里还是比较推荐用池的方案，或者用客户的IP做一个hash映射也不错。<br>不过汽车之家做的比较好的在于很好的隐藏了这段固定映射的Css，同时Css中的class也每次会替换一些名称部分，同时又不是每次都把所有的映射库输出出来，而是节选文章中有的文字，因此既缩小了输出大小，又隐藏了整体库，看得出工程师也是打得一手好牌啊。<br>三.如何应对Css:Content这种类型的反爬<br>我们就以汽车之家为例吧：<br>1.获取映射的Css<br>其实汽车之家这个例子来说，主要的问题并不是如何解决这种反爬，因为这种模式的反爬如果知道映射，搞起来太简单，直接正则替换就行了。这里最难的是怎么找到这个映射，汽车之家可以说隐藏的是教科书级的完美，解析方案我就不展开讲了，因为这个是Javascript对抗里的内容（后面我可能会拿出几篇文章单独讲JS对抗），重点就是解析跟在正文后面的这一段JS。</p>
<p>好的工具等于成功的一半，想要很好的解析JS，一定要有一个好的JS解释引擎。这样可以省掉大量的破解的工作而直接运行别人的JS，在代码中整合脚本引擎相当麻烦，最完美的莫过于找一个好的JS的爬虫框架。<br>具体的Javascript对抗我在后面再讲，这里简单说下，直接运行这段代码，我们可以看到在Nl_（可变的）这个变量中存储了完整的Css映射，因此我们想办法在运行中保存该变量，带入下一步即可。</p>
<p>2.根据映射进行内容替换<br>我们既然拥有了Css映射，那还有啥难的，直接读取CssRules，循环用正则替换把<br><span class="hs_kw4_mainYM"></span><br>这类标签直接替换成content中的文字：<br>九<br>好了，大功告成。<br>贴一下部分神箭手中运行的解析代码：<br>var code = extract(page.raw, “//div[@class=’conttxt’]/div/script”); if(code){ eval(code); var cssMapping = {}; if(rules.length &gt; 0){ for(var ri in rules){ var rule = rules[ri]; var matches1 = /content:\s*”([^”]+)”/.exec(rule) var matches2 = /.([^:]+)::before/.exec(rule) if(matches1 &amp;&amp; matches2){ cssMapping[matches2[1]] = matches1[1]; } } page.raw = page.raw.replace(/&lt;span\s+class=’(hs_kw\d+_[^’]+)’&gt;&lt;\/span&gt;/g, function(match,p1,p2){ return cssMapping[p1]; }); } } page.raw = exclude(page.raw, “//div[@class=’conttxt’]/div/script”);<br>————————————————最后说两句————————————————————<br>大家会发现，很多非常奇特的反爬措施，大多依赖Javascript加密的功力。当然有些Javascript加密可以直接通过JS渲染，如[Email Protection]，但是也有很多单纯通过渲染JS无法解决，如今天这篇文章中提到，因此也让Css的这种反爬措施有别于普通的JS加密形式，给反反爬工程师设置了更高的障碍，是我个人比较推荐的一种反爬手段。反过来对于反反爬来说：在有了好的工具前提下，熟练掌握JS则是一个必备内功之一。<br>文章导航</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫特殊反反爬/CloudFlare邮箱加密(cfemail)" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫特殊反反爬/CloudFlare邮箱加密(cfemail)/" class="article-date">
      <time datetime="2018-10-18T12:22:22.918Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>CloudFlare邮箱加密(cfemail)</p>
<p>爬虫与CloudFlare邮箱加密(cfemail)-反爬与反反爬的奇技淫巧<br>发布于</p>
<p>由于Javascript这块的内容很多，难度也不一样。所以这一篇文章希望给大家讲一些入门级别的。本来找来找去感觉都是高段位玩家，然而今天在做邮箱地址爬虫的过程中一段代码突然跳到了我的面前-邮箱地址加密JS。恰好难度适中，就今天拿出来跟大家一起聊一聊：</p>
<p>下面我们就进入正题，先来看看这个文章标题上的内容跟我们这篇文章到底有啥关系：<br>CloudFlare是一家美国的跨国科技企业，总部位于旧金山，在英国伦敦亦设有办事处。CloudFlare以向客户提供网站安全管理、性能优化及相关的技术支持为主要业务。通过基于反向代理的内容传递网络(ContentDeliveryNetwork,CDN)及分布式域名解析服务(DistributedDomainNameServer)，CloudFlare可以帮助受保护站点抵御包括拒绝服务攻击(DenialofService)在内的大多数网络攻击，确保该网站长期在线，同时提升网站的性能、访问速度以改善访客体验。<br>看完了好像还是没关系，别急，我们今天要讲的是CloudFlare中的一个功能，叫做Email Obfuscation，也就是邮箱混淆。<br>当我们使用了 CloudFlare 的服务，如果开启了 Email Obfuscation ，页面里真正的 Email 地址会被隐藏，具体隐藏的代码如下：<br><a class="__cf_email__" href="/cdn-cgi/l/email-protection" data-cfemail="a89b9b9c919d9b909a989ae8d9d986cbc7c5">[email&#160;protected]</a><script data-cfhash="f9e31" type="text/javascript">/<em> &lt;![CDATA[ </em>/!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName(‘script’),e=t.length;e–;)if(t[e].getAttribute(‘data-cfhash’))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute(‘data-cfemail’)){for(e=’’,r=’0x’+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+=’%’+(‘0’+(‘0x’+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/<em> ]]&gt; </em>/</script><br>做过爬虫的朋友应该都对这段代码不陌生，非常常见的一个邮箱混淆服务。<br>一.为什么要使用邮箱混淆服务。<br>事实上很多站长可能并不知道自己使用了这样的服务，因为CloudFlare主要还是以CDN为主，这个服务是附加的，而且CloudFlare也做的很贴心，几乎不用什么设置，就可以自动混淆。（实际上是识别了输出的页面里是否有邮箱，有邮箱就替换成[email protected]并在下面添加一段JS代码）。<br>无论邮箱和电话号码，即使主动留在互联网上，也不希望被人批量获取，所以邮箱混淆一直是件很重要的事情。包括用at代替@，用#代替@，还有生成图片的。而CloudFlare提供了一种完全不需要修改代码的方案，确实给了大家很大的方便。<br>二.CloudFlare邮箱混淆服务的优劣<br>那么我们使用这种方案有什么优点和缺点呢？<br>我们先说说优点：首先不用改代码，很方便；其次全局替换，不会有遗漏；最后JS混淆，比at和#更加彻底也对显示影响最小，毕竟at和#用的人多了，就和@一样了，有时候还会让真正想联系的客户摸不着头脑。<br>我们再说说缺点：我在验证码那篇文章中就提到过的成本回报比概念，当很多人用一个方案的时候，由于回报无限增大导致无论这个方案有多么好，他被破解的概率都会大大增加。当然CloudFlare混淆远远不止这个问题这么简单。他的这个混淆有时候恰恰使得这个页面中的邮箱标志更加明显，有点类似本来你把金子放在地上，很危险。现在把金子埋在地下，然后为了自己能找到，又画了一个此处有金子的标记。事实上在真正识别邮箱的爬虫中，CloudFlare反而降低获取Email的难度。所以先提前说反爬的结论，严重不推荐大家使用这个方案！<br>三.写爬虫时遇到CloudFlare邮箱混淆，如何解密？<br>说完了反爬，再说反反爬。这个我就不得不提工具的重要性了。有时候你遇到一个好工具，那真的一身轻松。这里需要再次强调，写爬虫最好的语言是JS，因为JS对抗是爬虫中最难的部分，框架本身就是JS的环境将使得事半功倍：<br>世界上最好的JS爬虫开发框架- 在线网络爬虫/大数据分析/机器学习开发平台-神箭手云。<br>好了，今天的盒饭有着落了。当然我们这篇文章还是得给爬虫工程师来点干货的。<br>我们今天就通过这个简单的例子来看看写爬虫时遇到复杂的JS到底怎么分析。<br>1.格式化代码<br>JS分析最重要的是格式，因为大部分JS代码都是混淆且压缩的。基本是不具备任何可读性，先格式化成可读的形式最重要。格式化的工具很多，这里我最推荐Chrome浏览其中的Snippet。因为格式化完了之后还可以调试，简直是神器。<br>我们把前面例子中HTML代码部分删除掉，留下JS部分，贴进Snippet，点击左下角的{}按钮格式化：</p>
<p>2.分析代码<br>是不是看着舒服太多了，已经到了人眼能看的级别了。我们贴出来看看：<br>!function(t, e, r, n, c, a, p) { try { t = document.currentScript || function() { for (t = document.getElementsByTagName(‘script’), e = t.length; e–; ) if (t[e].getAttribute(‘data-cfhash’)) return t[e] }(); if (t &amp;&amp; (c = t.previousSibling)) { p = t.parentNode; if (a = c.getAttribute(‘data-cfemail’)) { for (e = ‘’, r = ‘0x’ + a.substr(0, 2) | 0, n = 2; a.length - n; n += 2) e += ‘%’ + (‘0’ + (‘0x’ + a.substr(n, 2) ^ r).toString(16)).slice(-2); p.replaceChild(document.createTextNode(decodeURIComponent(e)), c) } p.removeChild(t) } } catch (u) {} }()<br>我们先大概看下整个代码，首先外层就是一个函数的定义及直接调用。大部分的JS库都会采用这种方法，既可以保证代码模块之间变量不相互污染，又可以通过传入参数实现内外部变量的传输。<br>再读代码第一段：获取变量t，这个过程我们结合前面的HTML代码可以看出，这个是在获取Email被加密后的Dom元素，为了后面获取data-cfemail做准备。<br>最后看第二段：显然就是从Dom元素中获取data-cfemail并解密出真实的Email并替换到页面显示中去。<br>3.整合进爬虫<br>一般来说，对于复杂的JS，我们还会有断点调试和其他分析的过程，这里的JS很简单，所以咱直接开始写代码。我们怎么在像神箭手这样的JS爬虫框架中处理呢，同样也非常简单：<br>var cfemails = extractList(content, “//*[@data-cfemail]/@data-cfemail”); for(var c in cfemails){ var a = cfemails[c]; for (e = ‘’, r = ‘0x’ + a.substr(0, 2) | 0, n = 2; a.length - n; n += 2) e += ‘%’ + (‘0’ + (‘0x’ + a.substr(n, 2) ^ r).toString(16)).slice(-2); var emailDecoded = decodeURIComponent(e); console.log(emailDecoded); }<br>可以看到，我们先通过xpath直接获取所有的data-cfemail的值，然后直接把CloudFlare这段解密JS复制过来就行了。运行后就可以直接获取该页面所有被CloudFlare混淆过的邮箱，简直比直接用正则提取邮箱还要简单还要准确。<br>————————————————最后再说两句——————————————————–<br>通过这篇文章中这个非常常见却又相对入门的例子，我们一起看了下Javascript在反爬与反反爬中扮演的角色，同时也看到了一个JS爬虫框架的重要性，如果是其他爬虫框架，要么需要自己整合JS引擎，要么就得读懂整段解密代码再翻译成那种语言，这个难度在我们后面会提到的一些JS加密中，将不敢想象。同时我们也可以看到类似CloudFlare这类通用邮箱混淆的脆弱性。<br>文章导航</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/celery分布式爬虫/分布式任务队列Celery" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/celery分布式爬虫/分布式任务队列Celery/" class="article-date">
      <time datetime="2018-10-18T12:22:22.884Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>分布式任务队列Celery</p>
<p>分布式任务队列Celery<br>Celery （芹菜）是基于Python开发的分布式任务队列。它支持使用任务队列的方式在分布的机器／进程／线程上执行任务调度。<br>结构<br>核心部件</p>
<p>broker<br>消息队列，由第三方消息中间件完成<br>常见有RabbitMQ, Redis, MongoDB等</p>
<p>worker<br>任务执行器<br>可以有多个worker进程<br>worker又可以起多个queue来并行消费消息</p>
<p>backend<br>后端存储，用于持久化任务执行结果<br>功能部件</p>
<p>beat<br>定时器，用于周期性调起任务</p>
<p>flower<br>web管理界面<br>任务<br>基本用法是在程序里引用celery，并将函数方法绑定到task</p>
<p>from celery import Celery</p>
<p>app = Celery(‘tasks’, backend=’amqp’, broker=’amqp://guest@localhost//‘)<br>app.conf.CELERY_RESULT_BACKEND = ‘db+sqlite:///results.sqlite’</p>
<p>@app.task<br>def add(x, y):<br>return x + y<br>然后调用相应方法即可(delay与apply_async都是异步调用)<br>from tasks import add<br>import time<br>result = add.delay(4,4)<br>while not result.ready():<br>print “not ready yet”<br>time.sleep(5)</p>
<p>print result.get()<br>由于是采用消息队列，因此任务提交之后，程序立刻返回一个任务ID。<br>之后可以通过该ID查询该任务的执行状态和结果。<br>关联任务<br>执行1个任务，完成后再执行第2个，第一个任务的结果做第二个任务的入参<br>add.apply_async((2, 2), link=add.s(16))<br>结果：2+2+16=20<br>还可以做错误处理<br>@app.task(bind=True)<br>def error_handler(self, uuid):<br>result = self.app.AsyncResult(uuid)<br>print(‘Task {0} raised exception: {1!r}\n{2!r}’.format(<br>uuid, result.result, result.traceback))</p>
<p>add.apply_async((2, 2), link_error=error_handler.s())<br>定时任务<br>让任务在指定的时间执行，与下文叙述的周期性任务是不同的。<br>ETA, 指定任务执行时间,注意时区<br>countdown, 倒计时,单位秒<br>from datetime import datetime, timedelta<br>tomorrow = datetime.utcnow() + timedelta(seconds=3)<br>add.apply_async((2, 2), eta=tomorrow)<br>result = add.apply_async((2, 2), countdown=3)<br>tip<br>任务的信息是保存在broker中的，因此关闭worker并不会丢失任务信息<br>回收任务(revoke)并非是将队列中的任务删除，而是在worker的内存中保存回收的任务task-id，不同worker之间会自动同步上述revoked task-id。<br>由于信息是保存在内存当中的，因此如果将所有worker都关闭了，revoked task-id信息就丢失了，回收过的任务就又可以执行了。要防治这点，需要在启动worker时指定一个文件用于保存信息<br>celery -A app.celery worker –loglevel=info &amp;&gt; celery_worker.log –statedb=/var/tmp/celery_worker.state<br>过期时间<br>expires单位秒，超过过期时间还未开始执行的任务会被回收<br>add.apply_async((10, 10), expires=60)<br>重试<br>max_retries:最大重试次数<br>interval_start:重试等待时间<br>interval_step:每次重试叠加时长，假设第一重试等待1s，第二次等待1＋n秒</p>
<p>interval_max:最大等待时间</p>
<p>add.apply_async((2, 2), retry=True, retry_policy={<br>‘max_retries’: 3,<br>‘interval_start’: 0,<br>‘interval_step’: 0.2,<br>‘interval_max’: 0.2,<br>})</p>
<p>序列化<br>将任务结果按照一定格式序列化处理，支持pickle, JSON, YAML and msgpack<br>add.apply_async((10, 10), serializer=’json’)<br>压缩<br>将任务结果压缩<br>add.apply_async((2, 2), compression=’zlib’)<br>任务路由<br>使用-Q参数为队列(queue)命名，然后调用任务时可以指定相应队列<br>$ celery -A proj worker -l info -Q celery,priority.high</p>
<p>add.apply_async(queue=’priority.high’)<br>工作流<br>按照一定关系一次调用多个任务<br>group: 并行调度<br>chain: 串行调度<br>chord: 类似group，但分header和body2个部分，header可以是一个group任务，执行完成后调用body的任务<br>map: 映射调度，通过输入多个入参来多次调度同一个任务<br>starmap: 类似map，入参类似＊args<br>chunks:将任务按照一定数量进行分组<br>group<br>from celery import group</p>
<blockquote>
<blockquote>
<blockquote>
<p>res = group(add.s(i, i) for i in xrange(10))()<br>res.get(timeout=1)<br>[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]<br>chain<br>from celery import chain</p>
</blockquote>
</blockquote>
</blockquote>
<h1 id="2-2-4-8"><a href="#2-2-4-8" class="headerlink" title="2 + 2 + 4 + 8"></a>2 + 2 + 4 + 8</h1><blockquote>
<blockquote>
<blockquote>
<p>res = chain(add.s(2, 2), add.s(4), add.s(8))()<br>res.get()<br>16</p>
</blockquote>
</blockquote>
</blockquote>
<p>可以用｜来表示chain</p>
<h1 id="4-16-2-4-8"><a href="#4-16-2-4-8" class="headerlink" title="((4 + 16)  2 + 4)  8"></a>((4 + 16) <em> 2 + 4) </em> 8</h1><blockquote>
<blockquote>
<blockquote>
<p>c2 = (add.s(4, 16) | mul.s(2) | (add.s(4) | mul.s(8)))</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>res = c2()<br>res.get()<br>chord<br>from celery import chord</p>
<p>#1<em>2+2</em>2+…9*2<br>res = chord((add.s(i, i) for i in xrange(10)), xsum.s())()<br>res.get()<br>90<br>map<br>from proj.tasks import add</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>~xsum.map([range(10), range(100)])<br>[45, 4950]<br>starmap<br>~add.starmap(zip(range(10), range(10)))<br>[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]<br>chunks<br>from proj.tasks import add</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>res = add.chunks(zip(range(100), range(100)), 10)()<br>res.get()<br>[[0, 2, 4, 6, 8, 10, 12, 14, 16, 18],<br>[20, 22, 24, 26, 28, 30, 32, 34, 36, 38],<br>[40, 42, 44, 46, 48, 50, 52, 54, 56, 58],<br>[60, 62, 64, 66, 68, 70, 72, 74, 76, 78],<br>[80, 82, 84, 86, 88, 90, 92, 94, 96, 98],<br>[100, 102, 104, 106, 108, 110, 112, 114, 116, 118],<br>[120, 122, 124, 126, 128, 130, 132, 134, 136, 138],<br>[140, 142, 144, 146, 148, 150, 152, 154, 156, 158],<br>[160, 162, 164, 166, 168, 170, 172, 174, 176, 178],<br>[180, 182, 184, 186, 188, 190, 192, 194, 196, 198]]<br>周期性任务<br>周期性任务就是按照一定的时间检查反复执行的任务。前面描述的定时任务值的是一次性的任务。<br>程序中引入并配置好周期性任务后，beat进程就会定期调起相关任务<br>beat进程是需要单独启动的<br>$ celery -A proj beat<br>或者在worker启动时一起拉起<br>$ celery -A proj worker -B<br>注意一套celery只能启一个beat进程<br>时区配置<br>由于python中时间默认是utc时间，因此最简便的方法是celery也用utc时区</p>
</blockquote>
</blockquote>
</blockquote>
<p>CELERY_TIMEZONE = ‘UTC’<br>这么配置可以保证任务调度的时间是准确的，但由于服务器一般都配置时区，因此flower、以及日志中的时间可能会有偏差<br>另外一种方法，就是配置正确的时区<br>CELERY_TIMEZONE = ‘Asia/Shanghai’<br>然后任务调起时，将时间带入时区配置<br>local_tz = pytz.timezone(app.config[‘CELERY_TIMEZONE’])<br>format_eta = local_tz.localize(datetime.strptime(eta.strip(), ‘%Y/%m/%d %H:%M:%S’))<br>add.apply_async((2, 2),eta=format_eta)<br>周期性任务配置<br>from datetime import timedelta</p>
<p>CELERYBEAT_SCHEDULE = {<br>‘add-every-30-seconds’: {<br>‘task’: ‘tasks.add’,<br>‘schedule’: timedelta(seconds=30),<br>‘args’: (16, 16)<br>},<br>}<br>周期性任务配置crontab<br>from celery.schedules import crontab</p>
<p>CELERYBEAT_SCHEDULE = {</p>
<h1 id="Executes-every-Monday-morning-at-7-30-A-M"><a href="#Executes-every-Monday-morning-at-7-30-A-M" class="headerlink" title="Executes every Monday morning at 7:30 A.M"></a>Executes every Monday morning at 7:30 A.M</h1><p>‘add-every-monday-morning’: {<br>‘task’: ‘tasks.add’,<br>‘schedule’: crontab(hour=7, minute=30, day_of_week=1),<br>‘args’: (16, 16),<br>},<br>}<br>本文参考官方文档</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/celery分布式爬虫/Celery构建一个分布式爬虫：理论篇" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/celery分布式爬虫/Celery构建一个分布式爬虫：理论篇/" class="article-date">
      <time datetime="2018-10-18T12:22:22.864Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Celery构建一个分布式爬虫：理论篇</p>
<p>如何构建一个分布式爬虫：理论篇<br>前言<br>本系列文章计划分三个章节进行讲述，分别是理论篇、基础篇和实战篇。理论篇主要为构建分布式爬虫而储备的理论知识，基础篇会基于理论篇的知识写一个简易的分布式爬虫，实战篇则会以微博为例，教大家做一个比较完整且足够健壮的分布式微博爬虫。通过这三篇文章，希望大家能掌握如何构建一个分布式爬虫的方法；能举一反三，将celery用于除爬虫外的其它场景。目前基本上的博客都是教大家使用scrapyd或者scrapy-redis构建分布式爬虫，本系列文章会从另外一个角度讲述如何用requests+celery构建一个健壮的、可伸缩并且可扩展的分布式爬虫。<br>本系列文章属于爬虫进阶文章，期望受众是具有一定Python基础知识和编程能力、有爬虫经验并且希望提升自己的同学。小白要是感兴趣，也可以看看，看不懂的话，可以等有了一定基础和经验后回过头来再看。<br>另外一点说明，本系列文章不是旨在构建一个分布式爬虫框架或者分布式任务调度框架，而是利用现有的分布式任务调度工具来实现分布式爬虫，所以请轻喷。<br>分布式爬虫概览<br>何谓分布式爬虫？<br>通俗的讲，分布式爬虫就是多台机器多个 spider 对多个 url 的同时处理问题，分布式的方式可以极大提高程序的抓取效率。<br>构建分布式爬虫通畅需要考虑的问题<br>（1）如何能保证多台机器同时抓取同一个URL？<br>（2）如果某个节点挂掉，会不会影响其它节点，任务如何继续？<br>（3）既然是分布式，如何保证架构的可伸缩性和可扩展性？不同优先级的抓取任务如何进行资源分配和调度？<br>基于上述问题，我选择使用celery作为分布式任务调度工具，是分布式爬虫中任务和资源调度的核心模块。它会把所有任务都通过消息队列发送给各个分布式节点进行执行，所以可以很好的保证url不会被重复抓取；它在检测到worker挂掉的情况下，会尝试向其他的worker重新发送这个任务信息，这样第二个问题也可以得到解决；celery自带任务路由，我们可以根据实际情况在不同的节点上运行不同的抓取任务（在实战篇我会讲到）。本文主要就是带大家了解一下celery的方方面面(有celery相关经验的同学和大牛可以直接跳过了)<br>Celery知识储备<br>celery基础讲解<br>按celery官网的介绍来说<br>Celery 是一个简单、灵活且可靠的，处理大量消息的分布式系统，并且提供维护这样一个系统的必需工具。它是一个专注于实时处理的任务队列，同时也支持任务调度。<br>下面几个关于celery的核心知识点<br>broker：翻译过来叫做中间人。它是一个消息传输的中间件，可以理解为一个邮箱。每当应用程序调用celery的异步任务的时候，会向broker传递消息，而后celery的worker将会取到消息，执行相应程序。这其实就是消费者和生产者之间的桥梁。<br>backend: 通常程序发送的消息，发完就完了，可能都不知道对方时候接受了。为此，celery实现了一个backend，用于存储这些消息以及celery执行的一些消息和结果。<br>worker: Celery类的实例，作用就是执行各种任务。注意在celery3.1.25后windows是不支持celery worker的！<br>producer: 发送任务，将其传递给broker<br>beat: celery实现的定时任务。可以将其理解为一个producer，因为它也是通过网络调用定时将任务发送给worker执行。注意在windows上celery是不支持定时任务的！<br>下面是关于celery的架构示意图，结合上面文字的话应该会更好理解</p>
<p>由于celery只是任务队列，而不是真正意义上的消息队列，它自身不具有存储数据的功能，所以broker和backend需要通过第三方工具来存储信息，celery官方推荐的是 RabbitMQ和Redis，另外mongodb等也可以作为broker或者backend，可能不会很稳定，我们这里选择Redis作为broker兼backend。<br>关于redis的安装和配置可以查看这里<br>实际例子<br>先安装celery<br>pip install celery<br>我们以官网给出的例子来做说明，并对其进行扩展。首先在项目根目录下，这里我新建一个项目叫做celerystudy，然后切换到该项目目录下，新建文件tasks.py，然后在其中输入下面代码<br>from celery import Celery</p>
<p>app = Celery(‘tasks’, broker=’redis://:<a href="mailto:&#39;&#39;@223.129.0.190" target="_blank" rel="noopener">&#39;&#39;@223.129.0.190</a>:6379/2’, backend=’redis://:<a href="mailto:&#39;&#39;@223.129.0.190" target="_blank" rel="noopener">&#39;&#39;@223.129.0.190</a>:6379/3’)</p>
<p>@app.task<br>def add(x, y):<br>return x + y<br>这里我详细讲一下代码：我们先通过app=Celery()来实例化一个celery对象，在这个过程中，我们指定了它的broker，是redis的db 2,也指定了它的backend,是redis的db3, broker和backend的连接形式大概是这样<br>redis://:password@hostname:port/db_number<br>然后定义了一个add函数，重点是@app.task，它的作用在我看来就是<strong>将add()<br>注册为一个类似服务的东西，本来只能通过本地调用的函数被它装饰后，就可以通过网络来调用。这个tasks.py中的app就是一个worker。它可以有很多任务，比如这里的任务函数add。我们再通过在命令行切换到项目根目录</strong>，执行<br>celery -A tasks worker -l info<br>启动成功后就是下图所示的样子<br>celery的worker启动成功<br>这里我说一下各个参数的意思，-A指定的是app(即Celery实例)所在的文件模块，我们的app是放在tasks.py中，所以这里是 tasks；worker表示当前以worker的方式运行，难道还有别的方式？对的，比如运行定时任务就不用指定worker这个关键字; -l info表示该worker节点的日志等级是info，更多关于启动worker的参数(比如-c、-Q等常用的)请使用<br>celery worker –help<br>进行查看<br>将worker启动起来后，我们就可以通过网络来调用add函数了。我们在后面的分布式爬虫构建中也是采用这种方式分发和消费url的。在命令行先切换到项目根目录，然后打开python交互端<br>from tasks import add<br>rs = add.delay(2, 2) # 这里的add.delay就是通过网络调用将任务发送给add所在的worker执行<br>这个时候我们可以在worker的界面看到接收的任务和计算的结果。<br>[2017-05-19 14:22:43,038: INFO/MainProcess] Received task: tasks.add[c0dfcd0b-d05f-4285-b944-0a8aba3e7e61] # worker接收的任务<br>[2017-05-19 14:22:43,065: INFO/MainProcess] Task tasks.add[c0dfcd0b-d05f-4285-b944-0a8aba3e7e61] succeeded in 0.025274309000451467s: 4 # 执行结果<br>这里是异步调用，如果我们需要返回的结果，那么要等rs的ready状态true才行。这里add看不出效果，不过试想一下，如果我们是调用的比较占时间的io任务，那么异步任务就比较有价值了<br>rs #<br>rs.ready() # true 表示已经返回结果了<br>rs.status # ‘SUCCESS’ 任务执行状态，失败还是成功<br>rs.successful() # True 表示执行成功<br>rs.result # 4 返回的结果<br>rs.get() # 4 返回的结果<br>from tasks import add</p>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>add.delay(5, 10)<br>这时候可以在celery的worker界面看到执行的结果<br>[2017-05-19 14:25:48,039: INFO/MainProcess] Received task: tasks.add[f5ed0d5e-a337-45a2-a6b3-38a58efd9760]<br>[2017-05-19 14:25:48,074: INFO/MainProcess] Task tasks.add[f5ed0d5e-a337-45a2-a6b3-38a58efd9760] succeeded in 0.03369094600020617s: 15<br>此外，我们还可以通过send_task()来调用，将excute_tasks.py改成这样<br>from tasks import app<br>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>app.send_task(‘tasks.add’, args=(10, 15),)<br>这种方式也是可以的。send_task()还可能接收到为注册（即通过@app.task装饰）的任务，这个时候worker会忽略这个消息<br>[2017-05-19 14:34:15,352: ERROR/MainProcess] Received unregistered task of type ‘tasks.adds’.<br>The message has been ignored and discarded.<br>定时任务<br>上面部分讲了怎么启动worker和调用worker的相关函数，这里再讲一下celery的定时任务。<br>爬虫由于其特殊性，可能需要定时做增量抓取，也可能需要定时做模拟登陆，以防止cookie过期，而celery恰恰就实现了定时任务的功能。在上述基础上，我们将tasks.py文件改成如下内容<br>from celery import Celery<br>app = Celery(‘add_tasks’, broker=’redis:’’//223.129.0.190:6379/2’, backend=’redis:’’//223.129.0.190:6379/3’)<br>app.conf.update(</p>
<h1 id="配置所在时区"><a href="#配置所在时区" class="headerlink" title="配置所在时区"></a>配置所在时区</h1><p>CELERY_TIMEZONE=’Asia/Shanghai’,<br>CELERY_ENABLE_UTC=True,</p>
<h1 id="官网推荐消息序列化方式为json"><a href="#官网推荐消息序列化方式为json" class="headerlink" title="官网推荐消息序列化方式为json"></a>官网推荐消息序列化方式为json</h1><p>CELERY_ACCEPT_CONTENT=[‘json’],<br>CELERY_TASK_SERIALIZER=’json’,<br>CELERY_RESULT_SERIALIZER=’json’,</p>
<h1 id="配置定时任务"><a href="#配置定时任务" class="headerlink" title="配置定时任务"></a>配置定时任务</h1><p>CELERYBEAT_SCHEDULE={<br>‘my_task’: {<br>‘task’: ‘tasks.add’, # tasks.py模块下的add方法<br>‘schedule’: 60, # 每隔60运行一次<br>‘args’: (23, 12),<br>}<br>}<br>)<br>@app.task<br>def add(x, y):<br>return x + y<br>然后先通过ctrl+c停掉前一个worker，因为我们代码改了，需要重启worker才会生效。我们再次以celery -A tasks worker -l info这个命令开启worker。<br>这个时候我们只是开启了worker，如果要让worker执行任务，那么还需要通过beat给它定时发送，我们再开一个命令行，切换到项目根目录，通过<br>celery beat -A tasks -l info<br>celery beat v3.1.25 (Cipater) is starting.<br><strong> - … </strong> - _<br>Configuration -&gt;<br>. broker -&gt; redis://223.129.0.190:6379/2<br>. loader -&gt; celery.loaders.app.AppLoader<br>. scheduler -&gt; celery.beat.PersistentScheduler<br>. db -&gt; celerybeat-schedule<br>. logfile -&gt; [stderr]@%INFO<br>. maxinterval -&gt; now (0s)<br>[2017-05-19 15:56:57,125: INFO/MainProcess] beat: Starting…<br>这样就表示定时任务已经开始运行了。<br>眼尖的同学可能看到我这里celery的版本是3.1.25，这是因为celery支持的windows最高版本是3.1.25。由于我的分布式微博爬虫的worker也同时部署在了windows上，所以我选择了使用 3.1.25。如果全是linux系统，建议使用celery4。<br>此外，还有一点需要注意，在celery4后，定时任务（通过schedule调度的会这样，通过crontab调度的会马上执行）会在当前时间再过定时间隔执行第一次任务，比如我这里设置的是60秒的间隔，那么第一次执行add会在我们通过celery beat -A tasks -l info启动定时任务后60秒才执行；celery3.1.25则会马上执行该任务。<br>关于定时任务更详细的请看官方文档celery定时任务</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/celery分布式爬虫/Celery构建一个分布式爬虫：基础篇" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/celery分布式爬虫/Celery构建一个分布式爬虫：基础篇/" class="article-date">
      <time datetime="2018-10-18T12:22:22.846Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Celery构建一个分布式爬虫：基础篇</p>
<p>如何构建一个分布式爬虫：基础篇<br>首先，我们新建目录distributedspider，然后再在其中新建文件workers.py,里面内容如下<br>from celery import Celery<br>app = Celery(‘crawl_task’, include=[‘tasks’], broker=’redis://223.129.0.190:6379/1’, backend=’redis://223.129.0.190:6379/2’)</p>
<h1 id="官方推荐使用json作为消息序列化方式"><a href="#官方推荐使用json作为消息序列化方式" class="headerlink" title="官方推荐使用json作为消息序列化方式"></a>官方推荐使用json作为消息序列化方式</h1><p>app.conf.update(<br>CELERY_TIMEZONE=’Asia/Shanghai’,<br>CELERY_ENABLE_UTC=True,<br>CELERY_ACCEPT_CONTENT=[‘json’],<br>CELERY_TASK_SERIALIZER=’json’,<br>CELERY_RESULT_SERIALIZER=’json’,<br>)<br>上述代码主要是做Celery实例的初始化工作，include是在初始化celery app的时候需要引入的内容，主要就是注册为网络调用的函数所在的文件。然后我们再编写任务函数，新建文件tasks.py,内容如下<br>import requests<br>from bs4 import BeautifulSoup<br>from workers import app<br>@app.task<br>def crawl(url):<br>print(‘正在抓取链接{}’.format(url))<br>resp_text = requests.get(url).text<br>soup = BeautifulSoup(resp_text, ‘html.parser’)<br>return soup.find(‘h1’).text<br>它的作用很简单，就是抓取指定的url，并且把标签为h1的元素提取出来<br>最后，我们新建文件task_dispatcher.py，内容如下<br>from workers import app<br>url_list = [<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/introduction.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/introduction.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/next-steps.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/next-steps.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/resources.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/resources.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/application.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/application.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/tasks.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/tasks.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/canvas.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/canvas.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/workers.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/workers.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/daemonizing.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/daemonizing.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html&#39;</a><br>]<br>def manage_crawl_task(urls):<br>for url in urls:<br>app.send_task(‘tasks.crawl’, args=(url,))<br>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>manage_crawl_task(url_list)<br>这段代码的作用主要就是给worker发送任务，任务是tasks.crawl，参数是url(元祖的形式)<br>现在，让我们在节点A(hostname为resolvewang的主机)上启动worker<br>celery -A workers worker -c 2 -l info<br>这里 -c指定了线程数为2， -l表示日志等级是info。我们把代码拷贝到节点B(节点名为wpm的主机)，同样以相同命令启动worker，便可以看到以下输出<br>两个节点<br>可以看到左边节点(A)先是all alone，表示只有一个节点；后来再节点B启动后，它便和B同步了<br>sync with celery@wpm<br>这个时候，我们运行给这两个worker节点发送抓取任务<br>python task_dispatcher.py<br>可以看到如下输出<br>分布式抓取示意图<br>可以看到两个节点都在执行抓取任务，并且它们的任务不会重复。我们再在redis里看看结果<br>backend示意图<br>可以看到一共有11条结果，说明 tasks.crawl中返回的数据都在db2(backend)中了，并且以json的形式存储了起来，除了返回的结果，还有执行是否成功等信息。<br>到此，我们就实现了一个很基础的分布式网络爬虫，但是它还不具有很好的扩展性，而且貌似太简单了…下一篇我将以微博数据采集为例来演示如何构建一个稳健的分布式网络爬虫。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/celery分布式爬虫/celery分布式：监控界面" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/celery分布式爬虫/celery分布式：监控界面/" class="article-date">
      <time datetime="2018-10-18T12:22:22.826Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>celery分布式：监控界面</p>
<p>flower监控服务器<br>flower安装与运行的方法</p>
<p>通过浏览器打开地址</p>
<p>可能遇到的错误<br>pymongo.errors.NotMasterError: not master<br>关闭mongodb的数据集<br>其他<br>当服务器数量较多的时候，管理起来会很不方便，可以使用python的supervisor来管理后台进程，遗憾的是它并不支持python3，不过也可以装在python2的环境<br>虽然用了supervisor可以很方便的管理python程序，但是还是得一个个登陆不同的服务器的去管理，咋办捏？<br>我在github上找到一个工具supervisor-easy，可以批量管理supervisor，如图:</p>
<p>地址：<a href="https://github.com/trytofix/supervisor-easy" target="_blank" rel="noopener">https://github.com/trytofix/supervisor-easy</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/celery分布式爬虫/Celery&amp;flower 后台运行部署" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/celery分布式爬虫/Celery&flower 后台运行部署/" class="article-date">
      <time datetime="2018-10-18T12:22:22.815Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Celery&amp;flower 后台运行部署<br>celery在后台运行<br>在生产环境中，你可能希望在后台运行worker.下面的文档中有详细的介绍：daemonization tutorial.<br>下面的脚本使用celery multi命令在后台启动一个或多个worker.<br>$ celery multi start w1 -A proj -l info<br>celery multi v4.0.0 (0today8)</p>
<blockquote>
<p>Starting nodes…<br>w1.halcyon.local: OK<br>你也可以重新启动:<br>$ celery multi restart w1 -A proj -l info<br>celery multi v4.0.0 (0today8)<br>Stopping nodes…<br>w1.halcyon.local: TERM -&gt; 64024<br>Waiting for 1 node…..<br>w1.halcyon.local: OK<br>Restarting node w1.halcyon.local: OK<br>celery multi v4.0.0 (0today8)<br>Stopping nodes…<br>w1.halcyon.local: TERM -&gt; 64052<br>或者停止它:<br>$ celery multi stop w1 -A proj -l info<br>stop命令是异步的，它不会等待所有的worker真正关闭。你可能需要使用stopwait命令，这个命令会保证当前所有的任务都执行完毕。<br>$ celery multi stopwait w1 -A proj -l info<br>注:celery multi命令不会保存workers的信息，所以当重新启动时你需要使用相同的命令行参数。当停止时，只有相同的pidfile和logfile参数是必须的。</p>
</blockquote>
<p>默认情况下，celery会在当前目录下创建pidfile和logfile.为了防止多个worker在启动时相互影响，你可以指定一个特定的目录。<br>$ mkdir -p /var/run/celery<br>$ mkdir -p /var/log/celery<br>$ celery multi start w1 -A proj -l info –pidfile=/var/run/celery/%n.pid \<br>–logfile=/var/log/celery/%n%I.log<br>通过multi命令你可以启动多个workers,另外这个命令还支持强大的命令行语法来为不同的workers指定不同的参数。<br>$ celery multi start 10 -A proj -l info -Q:1-3 images,video -Q:4,5 data \<br>-Q default -L:4,5 debug<br>更多关闭multi的举例请参考multi模块的API手册。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2018 刘小恺
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>