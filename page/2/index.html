<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="刘小恺" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="刘小恺(Kyle) 的个人博客">
<meta property="og:url" content="http://blog.kyleliu.cn/page/2/index.html">
<meta property="og:site_name" content="刘小恺(Kyle) 的个人博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="刘小恺(Kyle) 的个人博客">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="刘小恺(Kyle) 的个人博客" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>刘小恺(Kyle) 的个人博客</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">刘小恺</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/octave/">octave</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/博客/">博客</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/梯度下降/">梯度下降</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">刘小恺</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">刘小恺</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-4.python爬虫/scrapy框架/Scrapy-middleware" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/Scrapy-middleware/" class="article-date">
      <time datetime="2018-10-18T12:22:23.383Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Scrapy-middleware<br>中间件使用方法<br>使用方法<br>在middlewares文件中编写一个Downloader Middlewares的类，定义中间件<br>在setting中开启Downloader Middlewares，数值越小权限越大<br>request会先经过权限大的中间件，response会先经过权限小的中间件<br>​                SPIDER_MIDDLEWARES = {<br>​                    # 开启middleware，写清楚middleware所在的路径<br>​                    ‘login.middlewares.LoginSpiderMiddleware’: 543,<br>​                     }<br>常用自定义中间件<br>Downloader Middlewares（下载中间件）的方法<br>process_request(self, request, spider)<br>作用<br>当每个请求通过下载中间件时，该方法被调用,必须返回如下值的其中一个<br>返回内容<br>返回None<br>Scrapy将继续处理该请求，执行其他的中间件的相应方法，直到合适的下载器处理函数（下载处理程序）被调用，该请求被执行（其响应被下载）<br>返回Request对象<br>如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。<br>返回Response对象<br>Scrapy将不会调用任何其他的 process_request或 process_exception方法，或相应地下载函数； 将返回该response,已安装的中间件的 process_response() 方法则会在每个response返回时被调用。<br>抛出异常（包括抛出一个IgnoreRequest异常）<br>停止调用 process_request，下载中间件的 process_exception方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。<br>process_response（self, request, response, spider）<br>作用<br>当响应通过每个下载中间件的时候，调用方法，必须返回如下之一<br>返回内容<br>返回Response对象（可以与传入的响应相同，也可以是全新的对象）<br>该响应将被在链中的其他中间件的process_response方法处理。<br>返回Request对象<br>中间件链停止，返回的请求将被重新调度下载。处理类似于process_request()返回请求所做的那样。<br>产生异常（包括抛出一个IgnoreRequest异常）<br>停止调用 process_response，下载中间件链的 process_exception方法会被调用，如果没有任何一个方法处理该异常，则调用请求的errback（Request.errback）。如果没有代码处理抛出的异常，则该异常被忽略且不记录（不同于其他异常那样）。<br>process_exception(self, request，exception, spider)<br>作用：<br>当下载处理器（下载处理程序）、下载中间件的方法抛出异常（包括IgnoreRequest异常）时，Scrapy调用process_exception,返回如下几个参数之一<br>返回内容：<br>返回None：Scrapy将会继续处理该异常，调用接着已安装的其他中间件的process_exception方法，直到所有中间件都被调用完毕，则调用默认的异常处理。<br>返回Request对象，则返回的请求将被重新调用下载。这将停止中间件的process_exception方法执行，就如返回一个响应的那样。<br>常设置的中间件<br>给请求添加请求头的内容<br>​      from .settings import USER_AGENTS<br>​       class RandomUserAgent(object):    # 配置下载中间件<br>​           def process_request(self, request, spider):<br>​                ug_list = USER_AGENTS<br>​                user_agent = random.choice(ug_list)<br>​                # 在requests请求前添加自定义的USER_AGENT<br>​                request.headers[‘User-Agent’] = user_agent<br>给请求添加代理ip<br>​        class RandomProxy(object):<br>​               def process_request(self, request, spider):<br>​                    proxy = random.choice(PROXIES)</p>
<pre><code>if proxy[&apos;user_passwd&apos;] is None:
        # 没有代理账户验证的代理使用方式
        request.meta[&apos;proxy&apos;] = &quot;http://&quot; + proxy[&apos;ip_port&apos;]
else:
        # 对账户密码进行base64编码转换
        base64_userpasswd = base64.b64encode(proxy[&apos;user_passwd&apos;])
       # 对应到代理服务器的信令格式里
       request.headers[&apos;Proxy-Authorization&apos;] = &apos;Basic &apos; + base64_userpasswd
request.meta[&apos;proxy&apos;] = &quot;https://&quot; + proxy[&apos;ip_port&apos;]
</code></pre><p>将跳转（302）等状态码不是200的响应，重新发起请求（只适用于scrapy，不适用与scrapy-redis）<br>​    class Forbidden302Middleware(object):<br>​        def process_response(self, request, response, spider):<br>​            if response.status != 200:<br>​                print(‘捕获到一个相应状态码不是200的请求:’, request.url, ‘: ‘, response.status_code)<br>​                return request<br>​            return response<br>给请求添加cookie的中间件<br>​    import random<br>​    class CookiesMiddleware(object):<br>​        def process_request(self,request,spider):<br>​            cookie = random.choice(cookie_pool)  # cookie_poll是通过其他模块定期爬取获得的COOKIE集合<br>​            request.cookies = cooki<br>在下载中间件中通过selenium实现模拟登录</p>
<p>所有内置下载中间件<br>关闭内置中间件的方法<br>示例：’scrapy.downloadermiddlewares.retry.RetryMiddleware’: None<br>​    DOWNLOADER_MIDDLEWARES = {<br>​        # Engine side<br>​        ‘scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware’: 100,<br>​        ‘scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware’: 300,<br>​        ‘scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware’: 350,<br>​        ‘scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware’: 400,<br>​        ‘scrapy.downloadermiddlewares.useragent.UserAgentMiddleware’: 500,<br>​        ‘scrapy.downloadermiddlewares.retry.RetryMiddleware’: 550,<br>​        ‘scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware’: 560,<br>​        ‘scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware’: 580,<br>​        # 对压缩(gzip, deflate)数据的支持<br>​        ‘scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware’: 590,<br>​        ‘scrapy.downloadermiddlewares.redirect.RedirectMiddleware’: 600,<br>​        ‘scrapy.downloadermiddlewares.cookies.CookiesMiddleware’: 700,<br>​        ‘scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware’: 750,<br>​        ‘scrapy.downloadermiddlewares.stats.DownloaderStats’: 850,<br>​        ‘scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware’: 900,<br>​<br>常用的伪造User-Agent<br>user_agent_list = [<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1”,<br>​    “Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6”,<br>​    “Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1”,<br>​    “Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5”,<br>​    “Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3”,<br>​    “Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24”,<br>​    “Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/531.21.8 (KHTML, like Gecko) Version/4.0.4 Safari/531.21.10”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/533.17.8 (KHTML, like Gecko) Version/5.0.1 Safari/533.17.8”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/533.19.4 (KHTML, like Gecko) Version/5.0.2 Safari/533.18.5”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 6.1; en-GB; rv:1.9.1.17) Gecko/20110123 (like Firefox/3.x) SeaMonkey/2.0.12”,<br>​    “Mozilla/5.0 (Windows NT 5.2; rv:10.0.1) Gecko/20100101 Firefox/10.0.1 SeaMonkey/2.7.1”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_5_8; en-US) AppleWebKit/532.8 (KHTML, like Gecko) Chrome/4.0.302.2 Safari/532.8”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US) AppleWebKit/534.3 (KHTML, like Gecko) Chrome/6.0.464.0 Safari/534.3”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_5; en-US) AppleWebKit/534.13 (KHTML, like Gecko) Chrome/9.0.597.15 Safari/534.13”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.186 Safari/535.1”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.54 Safari/535.2”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7”,<br>​    “Mozilla/5.0 (Macintosh; U; Mac OS X Mach-O; en-US; rv:2.0a) Gecko/20040614 Firefox/3.0.0 “,<br>​    “Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10.5; en-US; rv:1.9.0.3) Gecko/2008092414 Firefox/3.0.3”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.14) Gecko/20110218 AlexaToolbar/alxf-2.0 Firefox/3.6.14”,<br>​    “Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10.5; en-US; rv:1.9.2.15) Gecko/20110303 Firefox/3.6.15”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1”<br>​    ]<br>代理ip代码示例<br>proxy_ip.pyrandomproxymiddleware.py<br>获取cookie_pool(cookie_list)的模块<br>get_cookie_list.py</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/Scrapy-items&amp;spider" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/Scrapy-items&spider/" class="article-date">
      <time datetime="2018-10-18T12:22:23.365Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Scrapy-items/spider</p>
<p>Items的配置<br>定义Item的作用<br>提前定义需要获取的数据，可以很清楚想要爬取的信息<br>没有定义的item属性不能被spider使用，键命名错误会报错，可以防止使用错误的键来存储数据<br>不同的爬虫使用不同的Item来存放不同的数据，在把数据交给pipeline的时候，可以通过isinstance(item, MyspiderItem) 来判断数据是属于哪个item，进行不同的数据(item)处理方式<br>item实例对象可以通过和字典相同的数据处理方式，来处理item对象的属性，所以我们可以直接将Myspider理解为一个字典<br>定义Item的方法（在item文件中）<br>​    class MyspiderItem(scrapy.Item):  # 创建一个item类<br>​        name = scrapy.Field()    # scrapy.Field( )只是占了一个位，并没有值<br>​                title = scrapy.Field( )<br>在Spider中使用item的方法（在spider文件中）<br>​    from Python import MyspiderItem<br>​    item = MyspiderItem()    # 创建一个自定义的item实例，item的操作和字典是一样的<br>​    item[‘title’] = ###     # 向item对象中插入数据<br>实现item对象和字典类型数据的转化<br>dict(item)      将item对象转化为dict类型数据<br>在将数据进行写入文件中的时候，一定要将其转化为json字符串，所以要先转化为字典<br>spider爬虫文件Spider类<br>scrapy_spider类的作用<br>传递给引擎request对象，或者需要处理的数据，接收引擎传递过来的response对象<br>配置spider类及类属性（在spider文件中）<br>​    class PythonSpider(scrapy.Spider)     # 自定义spider类，继承自scrapy.Spider类<br>​        name = ‘python’       # 创建爬虫时候的名字，不可以修改，和爬虫文件名称一致<br>​        # 创建爬虫时允许爬虫爬取范围，防止爬虫爬取到其他的网站，可以在一个列表中放置多个域名<br>​               #  从start_url后面爬取的url地址，必须要属于allow_domin域名下的地址<br>​        allowed_domain = [‘python.cn’]<br>​         # 定义开始爬取的网址，默认是可以被反复请求的<br>​        start_urls =  ‘<a href="http://www.itcast.cn/channel/teacher.shtml&#39;" target="_blank" rel="noopener">http://www.itcast.cn/channel/teacher.shtml&#39;</a>;<br>配置<strong>init</strong>方法（使用Redis_spider框架爬虫类继承crawlredisSpider的时候要动态获取allow_domin）<br>​        # 增加<strong>init</strong>()方法，根据从redis中获取的start_url动态设置allow_domin的方法<br>​    def <strong>init</strong>(self, <em>args, **kwargs):<br>​        domain = kwargs.pop(‘domain’, ‘’)<br>​        self.allowed_domains = filter(None, domain.split(‘,’))<br>​            # 要继承上方定义的爬虫类的原始的<strong>init</strong>方法<br>​        super(youyuanSpider, self).<strong>init</strong>(</em>args, **kwargs)<br>配置spider类的实例方法实现请求下一页<br>​    def parse(self,  response):  # 定义数据提取的方法，接收中间件传递过来的response对象（方法名不能改）<br>​       tr_list = response.xpath(“//table[@class=’tablelist’]/tr”)[1:-1]          # xpath分组提取<br>​              for tr in tr_list:<br>​                      item = {}<br>​                      item[“title”] = tr.xpath(“./td[1]/a/text()”).extract_first()<br>​                      item[“position_categary”] = tr.xpath(“./td[2]/text()”).extract_first()<br>​                      item[“url”] = response.url<br>​                      yield item        # 将item通过引擎传递给pipeline进行处理</p>
<pre><code>   next_url_temp = response.xpath(&quot;//a[@id=&apos;next&apos;]/@href&quot;).extract_first()
   if next_url_temp is not None and next_url_temp != &quot;javascript:;&quot;:
           next_url = &quot;http://hr.tencent.com/&quot;; + next_url_temp

# scrapy.Request构造requests对象给引擎，callback表示response交给哪个函数处理
yield scrapy.Request(next_page_url, callback=self.parse)  
</code></pre><p>多个解析函数中如何传递参数示例<br>​    def parse(self, response):<br>​        tr_list = response.xpath(‘//div[@class=”greyframe”]/table//table/tr’)   # xpath分组提取<br>​        for i in tr_list:<br>​            item = MyspiderItem（）  # 实例化item对象<br>​            ……..<br>​            …….<br>​                        #   将item参数封装成request对象通过引擎传递给调度器，引擎传递回来的Response对象给get_content函数<br>​            yield scrapy.Request(item[“href”],  callback=self.get_content,  meta{“item”:item})   </p>
<pre><code>def get_content(self, response):
    item = response.meta[&apos;item&apos;]    # 获取response.meta属性的item对象
    item[&apos;text&apos;] = response.xpath(&quot;//div[@class=&apos;content_text14_2&apos;]//text( )&quot;).extract( )
    yield  item    # 统一将数据传递给pipeline
</code></pre><p>start_requests方法<br>作用：<br>我们定义的start_urls都是默认交给start_requests处理的，所以如果我们想要在处理在请求之前处理request对象，那么我们可以重写start_requests方法，实现指定其响应的解析函数等功能<br>示例：<br>​         import scrapy<br>​            def start_requests(self):<br>​               for url in self.start_urls:<br>​                    yield scrapy.Request(<br>​                        url,<br>​                        callback = self.parse<br>​                )<br>​            def parse(self, response)<br>​                self.settings.get(‘KEY’, ‘’)<br>​                pass<br>spider中用到的方法/属性：<br>response对象的属性<br>response.url：当前响应的url地址<br>response.request.url：当前响应对应的请求的url地址<br>response.status: 响应的状态码<br>response.headers：获取响应头<br>response.request.headers：当前响应的请求头<br>response.body：响应体，也就是html代码，默认是bytes类型<br>response.meta: 解析函数之间传递的数据，字典类型<br>response.request.headers.getlist(‘Cookie’)    请求的cookie<br>response.headers.getlist(‘Set-Cookie’)   响应的cookie<br>response.xpath( )   方法<br>xpath( )  response.xpath( )  返回的是一个含有多个匹配结果的selector对象的列表，其具有如下方法<br>extract( )返回一个包含字符串数据的列表，将列表中所有select对象中的data属性的值提取出来<br>如果xpath（）提取的数据不存在，返回一个空列表<br>extract_first  返回列表中的第一个字符串，将列表第一个对象元素的data属性值提取出来<br>如果xpath（）提取的数据不存在，返回一个None类型数据<br>scrapy.Request(url, callback, method=’GET’, headers, body, cookies, meta, dont_filter=False )   get请求方法<br>url必须传递，为请求的url，将请求的结果作为响应传递给callback解析函数<br>callback: 指定传入的参数交给那个解析函数去处理<br>meta: 在不同的解析函数中传递数据， meta默认还会携带部分信息， 比如下载延迟，请求深度等<br>cookiejar ： 给对应的request一个cookiejar表示，任意的数字，在之后的request也携带该标示，那么将会自动把request对象携带对应的request的获取到的所有cookie内容<br>dont_filter: 默认url会经过allow_domain过滤，如果dont_filter设置为True，则已经爬过的地址不会被过滤<br>scrapy默认有url去重的功能,该功能对需要重复请求的url有重要用途<br>如果请求的页面是实时在变的，在有需要要抓实时的页面内容的时候，需要设置为true<br>start_urls的请求，dont_filter参数默认为true<br>method： 请求的方法，当使用POST的时候要传递body参数，在scrapy中body为字符串的形式<br>cookies： 传递的cookie，注意，在scrapy中cookie不能放在header中进行传递<br>headers： 使用自定义的header，优先级高于在setting中设置的默认headers<br>body：请求体，当使用post请求，发送数据的时候使用，并且当发送payload类型参数的时候，一定要使用<br>并且还要加上请求头 ： ‘Content-Type’: ‘application/json’<br>注意：<br>当请求被重定向后，redict中间件会自动进行重定向请求，并返回重定向后的响应<br>yield<br>将数据返回给引擎，并将方法挂起；<br>原理上，让这个函数成为一个生成器，每次遍历的时候会将每个结果读取到内存中，不会导致没存的占用量瞬间变高<br>yield只能返回[BaseItem， dict， None， Request ]类型的数据；<br>yield 的不是Request对象，表示将变量的数据传递给pipeline，接scrapy.Request方法表示将request传递给调度器，进行请求返回给callback函数response<br>scrapy.FormRequest（）      post表单提交<br>formdata：携带的post请求的参数，字典类型，只能发送formdata类型的参数<br>其他参数和scrapy.Request（）方法的参数使用基本一致<br>scrapy.FormRequest.from_resopnse（）    对响应中的表单自动进行表单提交<br>response：   解析函数接收到的响应<br>formdata：  需要填写的表单内容，，只能发送formdata类型的参数<br>其他参数和scrapy.Request（）方法的参数使用基本一致<br>Scrapy中的CrawlSpider类<br>CrawlSpider的作用<br>是scrapy的一个子类，应用CrawlSpider，我们可以从start_url返回的response中自动提取url地址<br>scrapy会自动的构造requests请求，发送给引擎，进行请求<br>还可以指定response是否传递给解析函数，或者是否需要继续提取响应中的url进行请求<br>使用CrawlSpider的方法<br>CrawlSpider和普通爬虫的区别<br>CrawlSpider爬虫类继承自CrawlSpide<br>在爬虫中定义一个rules对象，scrapy会自动根据rules定义的规则，过滤出符合规则的url，自动进行请求，将响应传递给解析函数，或者继续经过rules过滤符合规律的url进行请求获取响应<br>不需要自定义parse方法，因为scrapy自动的使用parse方法去请求rules过滤出来的url地址<br>生成crawlspider的命令<br>scrapy genspider -t crawl &lt;爬虫名字&gt;  &lt;允许爬取的域名&gt;<br>配置CrawlSpider爬虫示例<br>​         form scrapy.linkextractors import LinkExtractor<br>​         from scrapy.spiders import CrawSpider, Rule</p>
<pre><code>class CsdnspiderSpider(CrawlSpider):      # CrawlSpider要继承CrawlSpider
               name = &apos;csdnspider&apos;
               allowed_domains = [&apos;suning.com&apos;]
               start_urls = [&apos;http://snbook.suning.com/web/trd-fl/100301/46.htm&apos;]

               # 提取url，自动构造请求，把请求交给引擎获取响应
               rules = (
                     Rule(linkExtractor(allow=r&apos;/web/trd-fl/\d+.htm$&apos;), callback=&apos;parse_next_url&apos; follow=True),
                     Rule(linkExtractor(allow=r&apos;/web/prd/\d+.htm$&apos;), callback=&apos;parse_item&apos; follow=True),
</code></pre><p>spiders.Rule常见参数<br>linkextractor：是 一个Link Extractor对象，用于定义需要提取的链接，交给parse方法；<br>linkextractor可以接收正则、xpath、css等匹配连接方式，可以查看linkextractor的源码来查看其可以接收的匹配方式<br>callback：从link_extractor中每获取到连接的时候，参数所指定的值作为回调函数<br>follow：是一个布尔值，指定了根据改规则从response提取的链接是否需要linkextractor继续跟进（如果callback为None，follow默认设置为True，否则默认为False）<br>process_links：指定该spider中哪个函数将会被调用，从link_extractor中获取到连接列表时将会调用该函数，该方法主要用来过滤url<br>process_request：指定该Spider中哪个函数将会被调用，当构造完request列表时都会调用该函数，用来过滤request，必须返回request\None<br>LinkExtracort常见的参数<br>allow：满足括号中正则匹配的url会被提取，如果为空，则全部匹配(也可以是xpath匹配)<br>可以使用其他参数，可以是xpath、css等匹配方法<br>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接，及xpath满足范围内的url地址会被提取<br>deny：满足括号中的正则表达式的url一定不提取（优先级高于allow）<br>allow_domains：会被提取的连接的domain<br>deny_domains: 一定不会被提取连接的domains<br>使用ScrawlSpider注意点<br>当LinkExtractor提取到一个满足匹配规则的URL地址，后面的规则讲不会匹配该url地址，所以，注意不要将前面url地址匹配规则写的太详细，会被前面的的规则提取完不会经过后面的规则提取；<br>scrapy会自动的将LinkExtractor提取到的所有url地址给补全协议和域名、端口等，进行请求，不需要我们手动补全；<br>scrapy模拟登录<br>scrapy模拟登录有两种方法<br>直接携带cookie<br>找到发送post请求的url地址，带上信息，发送请求<br>使用scrapy框架自动登录<br>使用scrapy模拟登录注意点<br>当需要获取登录后才能获取到数据的时候<br>一个cookie一般对一个网站访问的时候，服务器可能设置阈值，这时候我们要限制访问的延时<br>注意有时候服务器会对我们的ip和cookie对应结果做判断，所以我们最好不要频繁的更换ip和cookie的搭配关系<br>如果想要持续的状态保持可以在setting中设置COOKIES_ENABLE = True<br>如果想要使用多cookie，多ip对数据进行爬取，可以使用分布式，一个ip搭配一个cookie对数据进行请<br>scrapy携带cookie模拟登录<br>应用场景<br>1、cookie过期时间很长，常见于一些不规范的网站<br>2、能在cookie过期之前把搜有的数据拿到<br>3、配合其他程序使用，比如其使用selenium把登陆之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie<br>模拟登录的示例（使用start_request来使请求携带cookie）<br>​        import scrapy<br>​                import re<br>​                class RenrenSpider(scrapy.Spider):<br>​                        name = ‘renren’<br>​                        allowed_domains = [‘renren.com’]<br>​                        start_urls = [‘<a href="http://www.renren.com/327550029/profile&#39;;]" target="_blank" rel="noopener">http://www.renren.com/327550029/profile&#39;;]</a></p>
<pre><code>def start_requests(self):                # 0.在start_process中对start_urls进行请求
        cookies_str = &quot;*******&quot;         # 1.获取到cookie,一般情况下我们也可以使用selenium进行获取
        cookies = {i.split(&quot;=&quot;)[0]:i.split(&quot;=&quot;)[-1] for i in cookies_str.split(&quot;; &quot;)}
        yield scrapy.Request(
                self.start_urls[0],
                callback=self.parse,
                cookies=cookies            # 2.直接带上cookie进行页面的请求
                )
def parse(self, response):          # 接下来的请求scrapy会直接将其带上cookie
    ret = re.findall(r&quot;你瞅啥&quot;,response.body.decode())
    print(ret)
    yield scrapy.Request(
            &quot;http://www.renren.com/327550029/profile?v=info_timeline&quot;;,
            callback=self.parse1
</code></pre><p>scrapy发送post请求模拟登录<br>使用场景<br>模拟的POST请求可以获取到请求参数和实际请求的URL地址（可以在之前请求的响应找到，或者可以通过解析js等方式获取到post请求的所需参数）<br>发送post请求获取cookie的示例<br>​        import scrapy<br>​        import re<br>​        class GithubSpider(scrapy.Spider):<br>​            name = ‘github’<br>​            allowed_domains = [‘github.com’]<br>​            start_urls = [‘<a href="https://github.com/login&#39;;;]" target="_blank" rel="noopener">https://github.com/login&#39;;;]</a><br>​            def parse(self, response):<br>​                form_data = {}<br>​                # 1.获取并构建post请求需要的参数<br>​                form_data[“authenticity_token”] =      # 可以在之前的响应中获取，或js解析<br>​                   response.xpath(“//input[@name=’authenticity_token’]/@value”).extract_first()<br>​                form_data[“commit”] = “Sign in”<br>​                form_data[“utf8”] = “✓”<br>​                form_data[“login”] = “noobpythoner”<br>​                form_data[“password”] = “zhoudawei123”<br>​            </p>
<pre><code>    # 2.使用scrapy.FormRequest携带参数对登录的url地址进行请求
    yield scrapy.FormRequest(
       &quot;https://github.com/session&quot;,      #post请求的url地址
        formdata = form_data,
        callback = self.after_login
    )
def after_login(self,response):
    ret = re.findall(r&quot;noobpythoner&quot;,response.body.decode())
    print(ret)
</code></pre><p>scrapy自动模拟登录<br>使用场景<br>在进行提交post请求的表单中，的form表单具有action属性<br>可以使用scrapy.FormRequest.from_resopnse方法，会自动帮我们获取form表单的提交地址<br>我们提供表单中需要填写的内容，可表单提交请求的响应交给哪个函数处理<br>scrapy自动模拟登录示例<br>​            import scrapy<br>​            import re<br>​            class RenrenSpider(scrapy.Spider):<br>​                 name = ‘renren’<br>​                 allowed_domains = [‘renren.com’]<br>​                 start_urls = [‘<a href="http://www.renren.com/327550029/profile&#39;]" target="_blank" rel="noopener">http://www.renren.com/327550029/profile&#39;]</a></p>
<pre><code>def parse(self, response):
                # 使用FormRequest.from_resopnse对表单地址进行请求
yield scrapy.FormRequest.from_resopnse(
     # 传入包含post请求表单的响应，表单内容，回调函数
     response,
     formdata={&apos;email&apos;: &apos;user_name&apos;, &apos;password&apos;: &apos;password&apos;},
     callback=self.parse_page
     )
def parse_page(self, response):
    print(response.url, &apos;*&apos;, * 100, response.starus)
    print(&apos;*&apos; * 100)
    print(re.findall(r&apos;user_name&apos;, response.body.decode()))                    
</code></pre><p>设置cookie允许在函数中传递<br>在setting中配置cookie的传递<br>​            COOKIES_ENABLE = True       # 设置允许cookie在不同的解析函数中传递，默认是允许的<br>​            COOKIES_DEBUG = True        # 可以看带cookie在函数中传递的过程</p>
<p>终端效果如下</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/Gerapy的介绍" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/Gerapy的介绍/" class="article-date">
      <time datetime="2018-10-18T12:22:23.343Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Gerapy的介绍<br>Gerapy的介绍<br>分布式的爬虫部署和管理工具，基于scrapy、scrapyd、scrapyd-API、Django、Vue.js<br>Gerapy的使用<br>Gerapy的安装<br>pip3 install gerapy<br>Gerapy应用步骤<br>1.初始化Gerapy<br>gerapy init<br>执行完毕之后，本地便会生成一个名字为 gerapy 的文件夹，接着进入该文件夹，可以看到有一个 projects 文件夹，我们后面会用到。<br>2.初始化数据库<br>cd gerapy<br>gerapy migrate<br>会在 gerapy 目录下生成一个 SQLite 数据库，同时建立数据库表。<br>3.启动Gerapy<br>gerapy runserver<br>这样我们就可以看到 Gerapy 已经在 8000 端口上运行了。接下来我们在浏览器中打开 <a href="http://localhost:8000/，就可以看到" target="_blank" rel="noopener">http://localhost:8000/，就可以看到</a> Gerapy 的主界面了<br>使用Gerapy管理项目<br>主机管理<br>我们可以点击左侧 Clients 选项卡，即主机管理页面，添加我们的 Scrapyd 远程服务，点击右上角的创建按钮即可添加我们需要管理的 Scrapyd 服务。<br>需要添加 IP、端口，以及名称，点击创建即可完成添加，点击返回即可看到当前添加的 Scrapyd 服务列表<br>添加主机前后<br>这样我们可以在状态一栏看到各个 Scrapyd 服务是否可用，同时可以一目了然当前所有 Scrapyd 服务列表，另外我们还可以自由地进行编辑和删除。</p>
<p>项目管理<br>Gerapy 的核心功能当然是项目管理，在这里我们可以自由地配置、编辑、部署我们的 Scrapy 项目，点击左侧的 Projects ，即项目管理选项，我们可以看到如下空白的页面：</p>
<p>将项目拖动到刚才 gerapy 运行目录的 projects 文件夹下，例如我这里写好了一个 Scrapy 项目，这时刷新页面，我们便可以看到 Gerapy 检测到了这个项目，同时它是不可配置、没有打包的</p>
<p>这时我们可以点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于 Git 的 commit 信息，然后点击打包按钮，即可发现 Gerapy 会提示打包成功，同时在左侧显示打包的结果和打包名称<br>打包成功之后，我们便可以进行部署了，我们可以选择需要部署的主机，点击后方的部署按钮进行部署，同时也可以批量选择主机进行部署，示例如下</p>
<p>监控任务<br>部署完毕之后就可以回到主机管理页面进行任务调度了，任选一台主机，点击调度按钮即可进入任务管理页面，此页面可以查看当前 Scrapyd 服务的所有项目、所有爬虫及运行状态<br>我们可以通过点击新任务、停止等按钮来实现任务的启动和停止等操作，同时也可以通过展开任务条目查看日志详情</p>
<p>编辑项目<br>同时 Gerapy 还支持项目编辑功能，有了它我们不再需要 IDE 即可完成项目的编写，我们点击项目的编辑按钮即可进入到编辑页面，如图所示</p>
<p>CrawlSpider代码自动生成<br>在 Scrapy 中，其实提供了一个可配置化的爬虫 CrawlSpider，它可以利用一些规则来完成爬取规则和解析规则的配置，这样可配置化程度就非常高，这样我们只需要维护爬取规则、提取逻辑就可以了。如果要新增一个爬虫，我们只需要写好对应的规则即可，这类爬虫就叫做可配置化爬虫。<br>Gerapy 可以做到：我们写好爬虫规则，它帮我们自动生成 Scrapy 项目代码。<br>我们可以点击项目页面的右上角的创建按钮，增加一个可配置化爬虫，接着我们便可以在此处添加提取实体、爬取规则、抽取规则了，例如这里的解析器，我们可以配置解析成为哪个实体，每个字段使用怎样的解析方式，如 XPath 或 CSS 解析器、直接获取属性、直接添加值等多重方式，另外还可以指定处理器进行数据清洗，或直接指定正则表达式进行解析等等，通过这些流程我们可以做到任何字段的解析。</p>
<p>再比如爬取规则，我们可以指定从哪个链接开始爬取，允许爬取的域名是什么，该链接提取哪些跟进的链接，用什么解析方法来处理等等配置。通过这些配置，我们可以完成爬取规则的设置。</p>
<p>最后点击生成按钮即可完成代码的生成。</p>
<p>生成的代码示例结果如图所示，可见其结构和 Scrapy 代码是完全一致的。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/网络爬虫介绍" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/网络爬虫介绍/" class="article-date">
      <time datetime="2018-10-18T12:22:23.307Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>爬虫的介绍<br>爬虫的定义<br>  模拟浏览器其发送请求，获取到和浏览器一模一样的数据<br>  浏览器能够看到的，我们爬虫才能够获取到，否则是没有办法获取到的，所以，只要浏览器能做的事情，原则上，爬虫都能做<br>爬虫获取的数据的用途<br>呈现数据，呈现在app或者在网站上<br>伪造网站请求，进行自动的访问网站<br>进行数据分析，获得结论<br>爬虫的分类<br>通用爬虫：<br>搜索引擎的爬虫，通常指搜索引擎的爬虫，通用网络爬虫利用种子url从互联网中搜集网页，采集信息，这些网页信息用于为搜索引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。<br>聚焦爬虫：<br>针对特定网站的爬虫，指定url进行爬取，有明确的爬取目标<br>通用爬虫工作原理<br>第一步：数据抓取<br>首先选取一部分的种子URL，将这些URL放入待抓取URL队列；<br>取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。<br>分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环….<br>第二步：数据存储<br>搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。<br>搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。<br>第三步：预处理<br>搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。<br>提取文字<br>中文分词<br>消除噪音（比如版权声明文字、导航条、广告等……）<br>索引处理<br>链接关系计算<br>特殊文件处理<br>第四步：提供检索服务，网站排名<br>搜索引擎在对信息进行组织和处理后，为用户提供关键字检索服务，将用户检索相关的信息展示给用户。<br>同时会根据页面的PageRank值（链接的访问量排名）来进行网站排名，这样Rank值高的网站在搜索结果中会排名较前，当然也可以直接使用 Money 购买搜索引擎网站排名，简单粗暴。<br>通用爬虫的局限性<br>通用搜索引擎所返回的网页里90%的内容无用。<br>图片、音频、视频多媒体的内容通用搜索引擎无能为力<br>不同用户搜索的目的不全相同，但是返回内容相同<br>聚焦爬虫的工作流程</p>
<p>爬虫爬取哪些数据<br>教育机构：其他教育机构的开班，招生，就业，口碑<br>资讯公司：特定领域的新闻数据的爬虫<br>金融公司：关于各个公司的动态的信息，<br>酒店/旅游：携程，去哪儿的酒店价格信息/机票，景点价格，其他旅游公司价格信息<br>房地产、高铁：10大房地产楼盘门户网站，政府动态等<br>强生保健医药：医疗数据，价格，目前的市场的行情<br>Robots协议：<br>网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。<br>这个协议之是道德层面的协议，这个协议并不能从技术上阻止去对其网站进行爬取。<br>例如：<a href="https://www.taobao.com/robots.txt" target="_blank" rel="noopener">https://www.taobao.com/robots.txt</a><br>HTTP/HTTPS协议<br>url的形式<br>url表现形式：scheme://host[:port#]/path/…/[?query-string][#anchor]<br>scheme：协议(例如：http, https, ftp)<br>host：服务器的IP地址或者域名<br>port：服务器的端口（如果是走协议默认端口，80  or    443）<br>path：访问资源的路径<br>query-string：参数，发送给http服务器的数据<br>anchor：锚（跳转到网页的指定锚点位置）<br>示例：<a href="http://localhost:4000/file/part01/1.2.htmlhttp://item.jd.com/11936238.html#product-detail" target="_blank" rel="noopener">http://localhost:4000/file/part01/1.2.htmlhttp://item.jd.com/11936238.html#product-detail</a><br>HTTP/HTTPS的区别<br>HTTP<br>超文本传输协议<br>默认端口号:80<br>HTTPS<br>HTTP + SSL(安全套接字层)<br>默认端口号：443<br>HTTP和HTTPS区别<br>浏览器默认请求服务器是以HTTP协议进行请求，如果服务器支持HTTPS协议，会返回给浏览器端一个协议相关的响应，浏览器会重新发起HTTPS协议的请求；</p>
<p>HTTP请求报文的形式</p>
<p>http的请求过程<br>域名—&gt;dns（拿ip）—&gt;浏览器请求ip—&gt;服务器—&gt;返回资源<br>HTTP常见的请求头</p>
<ol>
<li>Host (主机和端口号)<br>对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的Host部分</li>
<li>Connection (链接类型)<br>keep-alive  客户端携带表示支持长连接<br>keep-alive  服务器端如果回复keep-alive， 代表允许双方建立长连接<br>close： 服务器端回复close，代表不允许建立长连接，浏览器接收到响应后会主动断开连接</li>
<li>Upgrade-Insecure-Requests (浏览器支持升级为HTTPS请求)<br>升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。</li>
<li>User-Agent (浏览器名称)<br>对方的服务器通过User-Agent判断出来我们是一个手机版的浏览器还是电脑版的，同时能够判断出来浏览器的平台，型号，版本，内核版本</li>
<li>Accept (传输文件类型)<br>Accept: <em>/</em>：表示什么都可以接收。<br>Accept：image/gif：表明客户端希望接受GIF图像格式的资源；<br>Accept：text/html：表明客户端希望接受html文本。<br>Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。<br>q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。<br>text：用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；<br>application用于传输应用程序数据或者二进制数据。</li>
<li>Referer (页面跳转处)<br>表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。<br>防盗链：有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载。</li>
<li>Accept-Encoding（文件编解码格式）<br>指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。<br>举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0</li>
<li>Cookie （Cookie）<br>cookies 能够记录用户信息，会在请求的时候一起传递给对方的服务器，对方的服务器能够根据cookie判断出来是否登陆过（用户的状态）</li>
<li>x-requested-with :XMLHttpRequest  (是Ajax 异步请求)<br>10.Accept-Language   (接受语言)<br>指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。<br>11.Accept-Charset（字符编码）<br>指出浏览器可以接受的字符编码。<br>举例：Accept-Charset:iso-8859-1,gb2312,utf-8<br>12.Content-Type (POST数据类型)<br>Content-Type：POST请求里用来表示的内容类型。<br>举例：Content-Type = Text/XML; charset=gb2312：<br>0.请求体<br>常见的请求方法<br>GET<br>特点：请求所带参数包含在url中，显示在地址栏，请求数据可以被缓存，请求参数有长度限制，请求速度快<br>POST<br>特点：请求所带参数在请求体中，请求数据不可以被缓存，请求数据没有长度限制，请求速度慢<br>常用的响应报头</li>
<li>Cache-Control：must-revalidate, no-cache, private。<br>告诉客户端，对资源的缓存建议；</li>
<li>Connection：keep-alive<br>作为回应客户端的Connection：keep-alive，告诉客户端是否同意建立长连接；</li>
<li>Content-Encoding:gzip<br>告诉客户端，服务端发送的资源是采用gzip编码的；</li>
<li>Content-Type：text/html;charset=UTF-8<br>告诉客户端，资源文件的类型，还有字符编码；</li>
<li>Date：Sun, 21 Sep 2016 06:18:21 GMT<br>这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。</li>
<li>Expires:Sun, 1 Jan 2000 01:00:00 GMT<br>告诉客户端在这个时间前，缓存的到期时间</li>
<li>Pragma:no-cache<br>这个含义与Cache-Control等同。<br>8.Server：Tengine/1.4.6<br>这个是服务器和相对应的版本，只是告诉客户端服务器的信息。</li>
<li>Transfer-Encoding：chunked<br>这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。<br>HTTP常见响应状态码<br>100~199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。<br>200：表示服务器成功接收请求并已完成整个处理过程<br>200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。<br>302：临时转移至新的url<br>300~399表示请求转<br>307：临时转移至新的url<br>404：not found<br>400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。<br>500：服务器内部错误<br>为什么浏览器渲染出来的页面和爬虫请求的页面不一样<br>爬虫请求结果<br>爬虫只会请求当前url地址，不会主动请求js，所以往往当前URL对应的响应和element的内容不一样<br>浏览器<br>浏览器请求服务器渲染出来的界面，以及最终的在终端element显示的内容，和爬虫爬取到的不一样，因为浏览器会对页面中的静态文件的url进行请求，将请求结果渲染到浏览器中，导致最终的网页代码和显示的结果，已经是被js文件进行修改过；<br>在哪里查看当前url地址对应的响应（不包括对静态文件请求的响应）：<br>抓包（network），network下的第一个url地址，当前url地址的response<br>右键显示网页源码</li>
</ol>
<p>字符串和字节（str/ bytes）<br>python3 字符串应用的字符集<br>str ：unicode的呈现形式（python3应用的unicode的子集utf-8的形式对字符串进行呈现）<br>bytes：二进制互联网上数据的都是以二进制的方式传输的，bytes是二进制格式的字符串<br>对服务器的请求，要将请求数据先转化为bytes格式；<br>获取到服务器的响应要将获取到的bytes类型的数据，进行字符串的转化<br>Unicode/UTF8/ASCII字符集的介绍<br>字符(Character)<br>各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等<br>字符集(Character set)<br>多个字符的集合字符集包括：<br>ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集<br>ASCII编码是1个字节，而Unicode编码通常是2个字节，UTF-8是Unicode的实现方式之一，它是一种变长的编码方式，可以是1，2，3个字节（一句字符的内容而定）<br>str到bytes之间的转化（str、bytes）<br>python3中字节和字符之间转化的方法<br>从str—&gt; bytes：str使用encode方法转化为 bytes<br>从bytes—&gt; str：bytes通过decode转化为str<br>注意：<br>编码方式解码方式必须一样，否则就会出现乱码</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/爬虫相关工具列表" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/爬虫相关工具列表/" class="article-date">
      <time datetime="2018-10-18T12:22:23.290Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Python 爬虫的工具列表<br>爬虫网络库<br>通用<br>urllib -网络库(stdlib)。<br>requests -网络库。<br>grab – 网络库(基于pycurl)。<br>pycurl – 网络库(绑定libcurl)。<br>urllib3 – Python HTTP库，安全连接池、支持文件post、可用性高。<br>httplib2 – 网络库。<br>RoboBrowser – 一个简单的、极具Python风格的Python库，无需独立的浏览器即可浏览网页。<br>MechanicalSoup – 一个与网站自动交互Python库。<br>mechanize -有状态、可编程的Web浏览库。<br>socket – 底层网络接口(stdlib)。<br>Unirest for Python – Unirest是一套可用于多种语言的轻量级的HTTP库。<br>hyper – Python的HTTP/2客户端。<br>PySocks – SocksiPy更新并积极维护的版本，包括错误修复和一些其他的特征。作为socket模块的直接替换。<br>异步<br>treq – 类似于requests的API(基于twisted)。<br>aiohttp – asyncio的HTTP客户端/服务器(PEP-3156)。<br>网络爬虫框架<br>功能齐全的爬虫<br>grab – 网络爬虫框架(基于pycurl/multicur)。<br>scrapy – 网络爬虫框架(基于twisted)。<br>pyspider – 一个强大的爬虫系统。<br>cola – 一个分布式爬虫框架。<br>其他<br>portia – 基于Scrapy的可视化爬虫。<br>restkit – Python的HTTP资源工具包。它可以让你轻松地访问HTTP资源，并围绕它建立的对象。<br>demiurge – 基于PyQuery的爬虫微框架。<br>HTML/XML解析器<br>通用<br>lxml – C语言编写高效HTML/ XML处理库。支持XPath。<br>cssselect – 解析DOM树和CSS选择器。<br>pyquery – 解析DOM树和jQuery选择器。<br>BeautifulSoup – 低效HTML/ XML处理库，纯Python实现。<br>html5lib – 根据WHATWG规范生成HTML/ XML文档的DOM。该规范被用在现在所有的浏览器上。<br>feedparser – 解析RSS/ATOM feeds。<br>MarkupSafe – 为XML/HTML/XHTML提供了安全转义的字符串。<br>xmltodict – 一个可以让你在处理XML时感觉像在处理JSON一样的Python模块。<br>xhtml2pdf – 将HTML/CSS转换为PDF。<br>untangle – 轻松实现将XML文件转换为Python对象。<br>清理<br>Bleach – 清理HTML(需要html5lib)。<br>sanitize – 为混乱的数据世界带来清明。<br>文本处理<br>用于解析和操作简单文本的库。<br>通用<br>difflib – (Python标准库)帮助进行差异化比较。<br>Levenshtein – 快速计算Levenshtein距离和字符串相似度。<br>fuzzywuzzy – 模糊字符串匹配。<br>esmre – 正则表达式加速器。<br>ftfy – 自动整理Unicode文本，减少碎片化。<br>转换<br>unidecode – 将Unicode文本转为ASCII。<br>字符编码<br>uniout – 打印可读字符，而不是被转义的字符串。<br>chardet – 兼容 Python的2/3的字符编码器。<br>xpinyin – 一个将中国汉字转为拼音的库。<br>pangu.py – 格式化文本中CJK和字母数字的间距。<br>Slug化<br>awesome-slugify – 一个可以保留unicode的Python slugify库。<br>python-slugify – 一个可以将Unicode转为ASCII的Python slugify库。<br>unicode-slugify – 一个可以将生成Unicode slugs的工具。<br>pytils – 处理俄语字符串的简单工具(包括pytils.translit.slugify)。<br>通用解析器<br>PLY – lex和yacc解析工具的Python实现。<br>pyparsing – 一个通用框架的生成语法分析器。<br>人的名字<br>python-nameparser -解析人的名字的组件。<br>电话号码<br>phonenumbers -解析，格式化，存储和验证国际电话号码。<br>用户代理字符串<br>python-user-agents – 浏览器用户代理的解析器。<br>HTTP Agent Parser – Python的HTTP代理分析器。<br>特定格式文件处理<br>解析和处理特定文本格式的库。<br>通用<br>tablib – 一个把数据导出为XLS、CSV、JSON、YAML等格式的模块。<br>textract – 从各种文件中提取文本，比如 Word、PowerPoint、PDF等。<br>messytables – 解析混乱的表格数据的工具。<br>rows – 一个常用数据接口，支持的格式很多(目前支持CSV，HTML，XLS，TXT – 将来还会提供更多!)。<br>Office<br>python-docx – 读取，查询和修改的Microsoft Word2007/2008的docx文件。<br>xlwt / xlrd – 从Excel文件读取写入数据和格式信息。<br>XlsxWriter – 一个创建Excel.xlsx文件的Python模块。<br>xlwings – 一个BSD许可的库，可以很容易地在Excel中调用Python，反之亦然。<br>openpyxl – 一个用于读取和写入的Excel2010 XLSX/ XLSM/ xltx/ XLTM文件的库。<br>Marmir – 提取Python数据结构并将其转换为电子表格。<br>PDF<br>PDFMiner – 一个从PDF文档中提取信息的工具。<br>PyPDF2 – 一个能够分割、合并和转换PDF页面的库。<br>ReportLab – 允许快速创建丰富的PDF文档。<br>pdftables – 直接从PDF文件中提取表格。<br>Markdown<br>Python-Markdown – 一个用Python实现的John Gruber的Markdown。<br>Mistune – 速度最快，功能全面的Markdown纯Python解析器。<br>markdown2 – 一个完全用Python实现的快速的Markdown。<br>YAML<br>PyYAML – 一个Python的YAML解析器。<br>CSS<br>cssutils – 一个Python的CSS库。<br>ATOM/RSS<br>feedparser – 通用的feed解析器。<br>SQL<br>sqlparse – 一个非验证的SQL语句分析器。<br>HTTP<br>http-parser – C语言实现的HTTP请求/响应消息解析器。<br>微格式<br>opengraph – 一个用来解析Open Graph协议标签的Python模块。<br>可移植的执行体<br>pefile – 一个多平台的用于解析和处理可移植执行体(即PE)文件的模块。<br>PSD<br>psd-tools – 将Adobe Photoshop PSD(即PE)文件读取到Python数据结构。<br>自然语言处理<br>处理人类语言问题的库。<br>NLTK -编写Python程序来处理人类语言数据的最好平台。<br>Pattern – Python的网络挖掘模块。他有自然语言处理工具，机器学习以及其它。<br>TextBlob – 为深入自然语言处理任务提供了一致的API。是基于NLTK以及Pattern的巨人之肩上发展的。<br>jieba – 中文分词工具。<br>SnowNLP – 中文文本处理库。<br>loso – 另一个中文分词库。<br>genius – 基于条件随机域的中文分词。<br>langid.py – 独立的语言识别系统。<br>Korean – 一个韩文形态库。<br>pymorphy2 – 俄语形态分析器(词性标注+词形变化引擎)。<br>PyPLN – 用Python编写的分布式自然语言处理通道。这个项目的目标是创建一种简单的方法使用NLTK通过网络接口处理大语言库。<br>浏览器自动化与仿真<br>selenium – 自动化真正的浏览器(Chrome浏览器，火狐浏览器，Opera浏览器，IE浏览器)。<br>Ghost.py – 对PyQt的webkit的封装(需要PyQT)。<br>Spynner – 对PyQt的webkit的封装(需要PyQT)。<br>Splinter – 通用API浏览器模拟器(selenium web驱动，Django客户端，Zope)。<br>多重处理<br>threading – Python标准库的线程运行。对于I/O密集型任务很有效。对于CPU绑定的任务没用，因为python GIL。<br>multiprocessing – 标准的Python库运行多进程。<br>celery – 基于分布式消息传递的异步任务队列/作业队列。<br>concurrent-futures – concurrent-futures 模块为调用异步执行提供了一个高层次的接口。<br>异步<br>异步网络编程库<br>asyncio – (在Python 3.4 +版本以上的 Python标准库)异步I/O，时间循环，协同程序和任务。<br>Twisted – 基于事件驱动的网络引擎框架。<br>Tornado – 一个网络框架和异步网络库。<br>pulsar – Python事件驱动的并发框架。<br>diesel – Python的基于绿色事件的I/O框架。<br>gevent – 一个使用greenlet 的基于协程的Python网络库。<br>eventlet – 有WSGI支持的异步框架。<br>Tomorrow – 异步代码的奇妙的修饰语法。<br>队列<br>celery – 基于分布式消息传递的异步任务队列/作业队列。<br>huey – 小型多线程任务队列。<br>mrq – Mr. Queue – 使用redis &amp; Gevent 的Python分布式工作任务队列。<br>RQ – 基于Redis的轻量级任务队列管理器。<br>simpleq – 一个简单的，可无限扩展，基于Amazon SQS的队列。<br>python-gearman – Gearman的Python API。<br>云计算<br>picloud – 云端执行Python代码。<br>dominoup.com – 云端执行R，Python和matlab代码。<br>电子邮件<br>电子邮件解析库<br>flanker – 电子邮件地址和Mime解析库。<br>Talon – Mailgun库用于提取消息的报价和签名。<br>网址和网络地址操作<br>解析/修改网址和网络地址库。<br>URL<br>furl – 一个小的Python库，使得操纵URL简单化。<br>purl – 一个简单的不可改变的URL以及一个干净的用于调试和操作的API。<br>urllib.parse – 用于打破统一资源定位器(URL)的字符串在组件(寻址方案，网络位置，路径等)之间的隔断，为了结合组件到一个URL字符串，并将“相对URL”转化为一个绝对URL，称之为“基本URL”。<br>tldextract – 从URL的注册域和子域中准确分离TLD，使用公共后缀列表。<br>网络地址<br>netaddr – 用于显示和操纵网络地址的Python库。<br>网页内容提取<br>提取网页内容的库。<br>HTML页面的文本和元数据<br>newspaper – 用Python进行新闻提取、文章提取和内容策展。<br>html2text – 将HTML转为Markdown格式文本。<br>python-goose – HTML内容/文章提取器。<br>lassie – 人性化的网页内容检索工具<br>micawber – 一个从网址中提取丰富内容的小库。<br>sumy -一个自动汇总文本文件和HTML网页的模块<br>Haul – 一个可扩展的图像爬虫。<br>python-readability – arc90 readability工具的快速Python接口。<br>scrapely – 从HTML网页中提取结构化数据的库。给出了一些Web页面和数据提取的示例，scrapely为所有类似的网页构建一个分析器。<br>视频<br>youtube-dl – 一个从YouTube下载视频的小命令行程序。<br>you-get – Python3的YouTube、优酷/ Niconico视频下载器。<br>维基<br>WikiTeam – 下载和保存wikis的工具。<br>WebSocket<br>用于WebSocket的库。<br>Crossbar – 开源的应用消息传递路由器(Python实现的用于Autobahn的WebSocket和WAMP)。<br>AutobahnPython – 提供了WebSocket协议和WAMP协议的Python实现并且开源。<br>WebSocket-for-Python – Python 2和3以及PyPy的WebSocket客户端和服务器库。<br>DNS解析<br>dnsyo – 在全球超过1500个的DNS服务器上检查你的DNS。<br>pycares – c-ares的接口。c-ares是进行DNS请求和异步名称决议的C语言库。<br>计算机视觉<br>OpenCV – 开源计算机视觉库。<br>SimpleCV – 用于照相机、图像处理、特征提取、格式转换的简介，可读性强的接口(基于OpenCV)。<br>mahotas – 快速计算机图像处理算法(完全使用 C++ 实现)，完全基于 numpy 的数组作为它的数据类型。<br>代理服务器<br>shadowsocks – 一个快速隧道代理，可帮你穿透防火墙(支持TCP和UDP，TFO，多用户和平滑重启，目的IP黑名单)。<br>tproxy – tproxy是一个简单的TCP路由代理(第7层)，基于Gevent，用Python进行配置。<br>其他Python工具列表<br>awesome-python<br>pycrumbs<br>python-github-projects<br>python_reference<br>pythonidae</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/正则表达式" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/正则表达式/" class="article-date">
      <time datetime="2018-10-18T12:22:23.281Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>正则表达式介绍</p>
<pre><code>* 什么是正则表达式

    * Regular Expression, 又称规则表达式。
    * 正则表达式就是用事先定义好的一些特定字符（组合），组成一个“规则字符串”，这个“规则字符串”用来描述一种字符串的匹配模式（pattern）；
* 正则的作用

    * 可以用来检查一个字串是否包含某种子串、将匹配的子串替换或者取出
* 正则的特点

    * 灵活性、逻辑性和功能性非常强大
</code></pre><p>正则表达式的使用方法<br>​    * re模块常用函数和方法</p>
<pre><code>    * import  re
    * result_obj = re.search(正则表达式， 数据，flag=0）     ----查找数据中第一个符合匹配规则的字符串

        * search()函数从数据中只能查找到第一个符合正则数据放到result_obj中，  如果没有匹配到想要匹配的结果会返回None
    * result_obj.group()      ----查看正则匹配的结果内容

        * result_obj.group(1， 2/ 组名) 返回需要组的匹配的结果，返回一个包含多个组匹配结果的元组；    result_obj.group() == result_obj.group(0) == 正个正则表达式所有匹配的字符
* re模块高级函数

    * 
    * 补充：

        * re.compile( )       编译

            * 作用

                * 对正则表达式匹配规则进行预编译，在大量使用到正则的时候，可以提高匹配的速度
            * 使用方法

                * p = re.compile(&apos;匹配规则&apos;,  re.DATALL)
                * p.search(&apos;字符串&apos;)       按照编译的规则对字符串进行匹配
</code></pre><p>正则表达式中的特殊字符<br>​    * 匹配单个字符</p>
<pre><code>    * 
    * 空白字符\s == [ \f\n\r\t\v]   非空白字符 \S == [^\f\t\v\n\r]   
    * 在正则表达式中若只是想要匹配一个像特殊字符的普通字符需要在特殊字符前面加转义字符“\” 例如“\.”
    * 特殊字符在[ ]中例如：[.    |   * + ？等 ]没有特殊功能只代表普通字符
    * 在[ ]中若是想使用“-”普通字符要加上转义字符\
* 匹配多个字符

    * 
* 常用定位符

    * 
* 正则表达式的分组

    * 
    * 在使用（|）的时候尽量使特殊的或者通用的变量放在前边
    * 在引用分组的时候注意：\num表示八进制数num所表示的普通ASCII码字符，所以在引用的时候会默认表示ascii码字符，所以要注意转义或者使用原生字符串
</code></pre><p>正则表达式的贪婪与懒惰<br>​    * 概念：</p>
<pre><code>    * 贪婪-尽可能多的匹配
    * 懒惰-尽可能少的匹配
* 默认为贪婪模式的匹配模式

    * 在python中 +/*/{m,n}默认情况下总是贪婪的
* 如何让贪婪模式变为懒惰模式

    * 在量词后加上一个?
    * 例子：

        * 
</code></pre><p>原生字符串的应用<br>​    * 特殊字符的转义</p>
<pre><code>    * 在表达式中如果包含“\”表示转义\后面的字符为八进制数字代表的ascii对应的特殊字符，在python中会对ascii包含的数字或者字符进行转义，这种情况会导致会将匹配规则的字符进行转义，结果不能匹配到想要匹配的字符串内容
* 解决办法：

    * 取消转义

        * 在每一个&apos;\&apos;字符前加上&apos;\&apos;，对&quot;\&apos;进行转义，这样会取消\的转义功能，将\\只代表一个\字符，不会对后边的字符进行转义

            * ascii不包括的字符，如果前边有转义字符\，不需要加以转义，python会自动转义
    * 原生字符串

        * 如果在表达式或字符串前边加上r“”对字符串中的\字符自动转义  

            * 在使用的时候，匹配规则可以和想要匹配的内容写法相同，r会自动帮我们转义；
        * 示例： re.search(r&apos;abc\nabc&apos;, &apos;abc\nabc&apos;)
</code></pre><p>正则表达式的常见问题<br>​    * 如何让 . 特殊符号可以匹配所有内容（包括\n）</p>
<pre><code>* 解决办法：

    * 使用re.DOTALL 参数
* 示例：

    * re.findall(r&apos;abc.&apos;,  &apos;abc\n\nsfgs&apos;, re.DOTALL)

        * 也可以使用re.S 代替re.DOTALL 效果上是一样的
</code></pre><p>匹配所有的汉字的方法<br>​    * re.compole(r’[\u4e00-\u9fa5]’)</p>
<pre><code>* 匹配所有unicode编码的中文
</code></pre><p>Ascii码对应关系</p>
<pre><code>* 在字符串，或者正则表达式中，\n\t等控制字符 或者 \数字（表示八进制的num所表示的普通ascii码）等显示字符，在应用的时候会默认为在调用ascii码的控制字符或者是显示字符，所以如果只是想表达单纯的字符 需要用\\n或者r&quot;&quot;这种形式进行转意，可以使用chr（八进制数）来查询对应的ascii字符
* ascii码表
</code></pre><p><img src="../../../img/1539862996513.png" alt="1539862996513"></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/数据提取 json  Xpath" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/数据提取 json  Xpath/" class="article-date">
      <time datetime="2018-10-18T12:22:23.266Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/多线程爬虫" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/多线程爬虫/" class="article-date">
      <time datetime="2018-10-18T12:22:23.249Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>多线程爬虫所需应用<br>Queue（对列对象）<br>介绍Queue<br>Queue是python中的标准库，可以直接import Queue引用;队列是线程间最常用的交换数据的形式。<br>Queue的作用<br>python下多线程对于资源的处理，加锁是个重要的环节，因为python原生的list,dict等，都是not thread safe的，所以使用Queue，可以不用手动添加资源锁就能解决资源竞争的问题。<br>在使用Queue进行参数传递的时候，只有Queue列队中有数据，线程就可以获得并执行任务，不需要等待其他线程对数据的处理并传递，更方便数据的传递和处理；<br>使用方法<br>创建队列对象<br>from queue import Queue<br>queue = Queue(maxsize = n)        创建一个最大容量为n的queue列队<br>将一个数值放在队列中<br>queue.put( )      向列队中添加任务，列队满后会阻塞等待<br>myqueue.put_nowait( )    想列队中添加任务，列队满后会报错<br>将一个数值从队列中取出<br>queue.get( )     从列队中取出任务<br>queue.get([block[, timeout]])        从列队中取出任务，超时后会报错<br>备注：从列队取出任务后，列队计数并不会减一，需要标记任务task_done<br>任务完成，列队技术减一<br>queue.task_done( )      列队计数减一<br>列队任务计数不为0时，阻塞线程<br>Queue.join( )                 列队如果不为空，阻塞主线程<br>其他查询列队状态的方法<br>Queue.qsize()              返回队列当前计数大小<br>Queue.empty()           如果队列为空，返回True,反之False<br>Queue.full()                  如果队列满了，返回True,反之False<br>Queue.full                    与 maxsize 大小对应<br>线程的使用（复习）<br>创建子线程<br>import threading<br>t1 = threading.Thread(target=function, args=(, ))<br>将子线程设置为守护进程<br>t1.setDeamon(True)<br>启动子线程<br>t1.start( )<br>线程的使用策略<br>使用策略<br>创建不同功能任务需求的列队<br>在处理时间较长的功能函数处，使用比较多的线程<br>在处理时间比较快的函数处，使用比较少的线程<br>少创建一些线程，线程之间会进行资源的竞争，导致请求失败率增加<br>多设置一些最大请求次数，增加请求成功的几率<br>将失败的请求，任务，防止到一个列表/对列中，单独再对失败的进程进行统一执行<br>断点续爬的思想<br>场景<br>在我们爬取数据的时候，想要先停止爬取，将爬取的状态和数据保存，并想要下次爬取的时候再接着当前的状态继续爬取<br>实现方法<br>可以保存当前的所有的queue列队，在程序结束之前将queue对象存储到redis中<br>当下次启动程序的时候，从redis数据库中读取上次停止之前的queue对象<br>程序判断queue对象的状态，是否全部为空，如果不为空，那么便不需要再重新获取参数想queue列队中添加，直接接着之前的queue的装填进行爬取<br>多线程爬虫代码实例</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/Splash动态爬取" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/Splash动态爬取/" class="article-date">
      <time datetime="2018-10-18T12:22:23.232Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Splash的功能介绍和运行<br>Splash的简介<br>Splash是一个JavaScript渲染服务，是一个带有HTTP API的轻量级浏览器，同时它对接了Python中的Twisted和QT库。利用它，我们同样可以实现动态渲染页面的抓取。<br>Splash文档<br>Splash官方文档<br><a href="https://splash.readthedocs.io/en/stable/scripting-ref.html" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/scripting-ref.html</a><br>Splash-api官方文档<br><a href="https://splash.readthedocs.io/en/stable/api.html" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/api.html</a><br>Splash的功能<br>异步方式处理多个网页渲染过程；<br>获取渲染后的页面的源代码或截图；<br>通过关闭图片渲染或者使用Adblock规则来加快页面渲染速度；<br>可执行特定的JavaScript脚本；<br>可通过Lua脚本来控制页面渲染过程；<br>获取渲染的详细过程并通过HAR（HTTP Archive）格式呈现。<br>Splash服务的运行<br>docker run -d -p 8050:8050 scrapinghub/splash<br>打开<a href="http://localhost:8050/即可看到其Web页面" target="_blank" rel="noopener">http://localhost:8050/即可看到其Web页面</a><br>Splash-Lua脚本介绍<br>Splash的运行机制<br>Splash加载和操作渲染过程，可以理解是由一段Lua代码来操控<br>Splash留出来了一个main() 函数的接口，我们可以通过编写该段函数，操作其传入的splash对象，Splash会调用其指令并执行对应的操作<br>可以在浏览器界面编写并点击render me来启动，也可以通过调用splash-api的方式来调用<br>Splash Lua脚本示例<br>function main(splash, args)<br>assert(splash:go(args.url))         请求url<br>assert(splash:wait(0.5))       等待0.5s<br>return {       返回结果<br>html = splash:html(),            获取页面源码<br>png = splash:png(),        获取页面截图<br>har = splash:har(),        获取请求的详细har信息<br>}<br>end<br>Splash异步处理<br>Splash异步的介绍<br>在脚本内调用的wait()方法类似于Python中的sleep()，其参数为等待的秒数。当Splash执行到此方法时，它会转而去处理其他任务，然后在指定的时间过后再回来继续处理。<br>另外，这里做了加载时的异常检测。go()方法会返回加载页面的结果状态，如果页面出现4xx或5xx状态码，ok变量就为空<br>Splash异步脚本示例<br>function main(splash, args)<br>local example_urls = {“<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">www.baidu.com&quot;</a>, “<a href="http://www.taobao.com&quot;" target="_blank" rel="noopener">www.taobao.com&quot;</a>, “<a href="http://www.zhihu.com&quot;}" target="_blank" rel="noopener">www.zhihu.com&quot;}</a><br>local urls = args.urls or example_urls<br>local results = {}<br>for index, url in ipairs(urls) do<br>local ok, reason = splash:go(“http://“ .. url)<br>if ok then<br>splash:wait(2)<br>results[url] = splash:png()<br>end<br>end<br>return results<br>end<br>Splash负载均衡的配置<br>搭建多个splash服务节点<br>docker run -d -p 8050:8050 scrapinghub/splash<br>配置负载均衡服务(使用nginx)<br>nginx配置节点<br>​            http {<br>​                upstream splash {<br>​                    least_conn;<br>​                    server 41.159.27.223:8050 weight=4;<br>​                    server 41.159.27.221:8050 weight=2;<br>​                    server 41.159.27.9:8050 weight=2;<br>​                    server 41.159.117.119:8050 weight=1;<br>​                }<br>​                server {<br>​                    listen 8050;<br>​                    location / {<br>​                        proxy_pass <a href="http://splash" target="_blank" rel="noopener">http://splash</a>;<br>​                    }<br>​                }<br>​            }<br>nginx配置认证<br>现在Splash是可以公开访问的，如果不想让其公开访问，还可以配置认证，这仍然借助于Nginx。可以在server的location字段中添加auth_basic和auth_basic_user_file字段，具体配置如下：<br>这里使用的用户名和密码配置放置在/etc/nginx/conf.d目录下，我们需要使用htpasswd命令创建。例如，创建一个用户名为admin的文件，相关命令如下<br>htpasswd -c .htpasswd admin<br>接下来就会提示我们输入密码，输入两次之后，就会生成密码文件，其内容如下：<br>配置完成后，重启一下Nginx服务： sudo nginx -s reload<br>​                server {<br>​                    listen 8050;<br>​                    location / {<br>​                        proxy_pass <a href="http://splash" target="_blank" rel="noopener">http://splash</a>;<br>​                        auth_basic “Restricted”;<br>​                        auth_basic_user_file /etc/nginx/conf.d/.htpasswd;<br>​                    }<br>​                }<br>Splash对象的常用属性<br>加载参数<br>获取方法一<br>​            function main(splash, args)<br>​                local url = args.url<br>​            end<br>获取方法二<br>​            function main(splash)<br>​                local url = splash.args.url<br>​            end<br>开启关闭javascript<br>splash.js_enabled = false/true<br>设置加载的超时时间<br>splash.resource_timeout = 2<br>单位是秒。如果设置为0或nil（类似Python中的None），代表不检测超时。<br>禁止加载图片<br>splash.images_enabled = true/false<br>禁止开启插件(flash等)<br>splash.plugins_enabled = true/false<br>滚动条<br>splash.scroll_position = {x=100, y=400}<br>滚动条向右滚动100像素，向下滚动400像素<br>Splash对象常用方法<br>请求某链接<br>使用方法<br>splash:go{url, baseurl=nil, headers=nil, http_method=”GET”, body=nil, formdata=nil}<br>url：请求的URL。<br>baseurl：可选参数，默认为空，表示资源加载相对路径。<br>headers：可选参数，默认为空，表示请求头。<br>http_method：可选参数，默认为GET，同时支持POST。<br>body：可选参数，默认为空，发POST请求时的表单数据，使用的Content-type为application/json。<br>formdata：可选参数，默认为空，POST的时候的表单数据，使用的Content-type为application/x-www-form-urlencoded。<br>return: 结果ok和原因reason的组合，如果ok为空，代表网页加载出现了错误，此时reason变量中包含了错误的原因<br>使用示例<br>​            function main(splash, args)<br>​                local ok, reason = splash:go{“<a href="http://httpbin.org/post&quot;" target="_blank" rel="noopener">http://httpbin.org/post&quot;</a>, http_method=”POST”, body=”name=Germey”}<br>​                if ok then<br>​                    return splash:html()<br>​                end<br>​            end<br>设置定时任务<br>使用示例(0.2s后获取网页截图)<br>​                    function main(splash, args)<br>​                             local snapshots = {}<br>​                             local timer = splash:call_later(function()<br>​                                 snapshots[“a”] = splash:png()<br>​                                 splash:wait(1.0)<br>​                                 snapshots[“b”] = splash:png()<br>​                                 end, 0.2)<br>​                             splash:go(“<a href="https://www.taobao.com&quot;" target="_blank" rel="noopener">https://www.taobao.com&quot;</a>)<br>​                             splash:wait(3.0)<br>​                             return snapshots<br>​                    end<br>模拟发送get请求<br>使用方法<br>response = splash:http_get{url, headers=nil, follow_redirects=true}<br>url：请求URL。<br>headers：可选参数，默认为空，请求头。<br>follow_redirects：可选参数，表示是否启动自动重定向，默认为true<br>使用示例<br>​            function main(splash, args)<br>​                local treat = require(“treat”)<br>​                local response = splash:http_get(“<a href="http://httpbin.org/get&quot;" target="_blank" rel="noopener">http://httpbin.org/get&quot;</a>)<br>​                    return {<br>​                        html=treat.as_string(response.body),<br>​                        url=response.url,<br>​                        status=response.status<br>​                    }<br>​            end<br>模拟发送post请求<br>使用方法<br>response = splash:http_post{url, headers=nil, follow_redirects=true, body=nil}<br>url：请求URL。<br>headers：可选参数，默认为空，请求头。<br>follow_redirects：可选参数，表示是否启动自动重定向，默认为true。<br>body：可选参数，即表单数据，默认为空。<br>使用示例<br>​            function main(splash, args)<br>​                local treat = require(“treat”)<br>​                local json = require(“json”)<br>​                local response = splash:http_post{“<a href="http://httpbin.org/post&quot;" target="_blank" rel="noopener">http://httpbin.org/post&quot;</a>,<br>​                    body=json.encode({name=”Germey”}),<br>​                    headers={[“content-type”]=”application/json”}<br>​                }<br>​                return {<br>​                    html=treat.as_string(response.body),<br>​                    url=response.url,<br>​                    status=response.status<br>​                }<br>​            end<br>获取页面加载过程描述等信息<br>使用方法<br>splash:har()        获取加载过程详细信息<br>splash:url()        获取正在访问的url<br>页面cookie相关操作<br>使用方法<br>splash:get_cookies()       获取当前页面的cookie<br>cookies = splash:add_cookie{name, value, path=nil, domain=nil, expires=nil, httpOnly=nil, secure=nil}    给页面添加cookie<br>splash:clear_cookies()    清空当前页面所有cookie<br>设置页面大小<br>使用方法<br>splash:get_viewport_size()    获取页面宽高<br>splash:set_viewport_size(width, height)    设置页面宽高<br>splash:set_viewport_full()    设置页面铺满<br>设置User-Agent<br>使用示例<br>​                    function main(splash)<br>​                            splash:set_user_agent(‘Splash’)<br>​                            splash:go(“<a href="http://httpbin.org/get&quot;" target="_blank" rel="noopener">http://httpbin.org/get&quot;</a>)<br>​                            return splash:html()<br>​                    end<br>设置请求头<br>使用示例<br>​                    function main(splash)<br>​                        splash:set_custom_headers({<br>​                                [“User-Agent”] = “Splash”,<br>​                                [“Site”] = “Splash”,<br>​                        })<br>​                        splash:go(“<a href="http://httpbin.org/get&quot;" target="_blank" rel="noopener">http://httpbin.org/get&quot;</a>)<br>​                        return splash:html()<br>​                    end<br>节点操作<br>使用方法<br>element = splash:select(“#kw”)    通过css选择器选择元素, 返回选中的第一个元素<br>element = splash:select_all(“#kw”)    通过css选择器选择元素, 返回选中所有元素<br>element.send_text(‘test’)     向节点输入文本<br>element.mouse_click()    点击节点<br>等待wait<br>使用方法<br>ok, reason = splash:wait{time, cancel_on_redirect=false, cancel_on_error=true}<br>time：等待的秒数。<br>cancel_on_redirect：可选参数，默认为false，表示如果发生了重定向就停止等待，并返回重定向结果。<br>cancel_on_error：可选参数，默认为false，表示如果发生了加载错误，就停止等待。<br>return: 结果ok和原因reason的组合。<br>使用示例<br>​            function main(splash)<br>​                splash:go(“<a href="https://www.taobao.com&quot;" target="_blank" rel="noopener">https://www.taobao.com&quot;</a>)<br>​                splash:wait(2)<br>​                return {html=splash:html()}<br>​            end<br>调用JavaScript定义的方法<br>使用示例<br>​                    function main(splash, args)<br>​                            local get_div_count = splash:jsfunc([[<br>​                                    function () {<br>​                                    var body = document.body;<br>​                                    var divs = body.getElementsByTagName(‘div’);<br>​                                    return divs.length;<br>​                                    }<br>​                                    ]])<br>​                            splash:go(“<a href="https://www.baidu.com&quot;" target="_blank" rel="noopener">https://www.baidu.com&quot;</a>)<br>​                            return (“There are %s DIVs”):format(get_div_count())<br>​                    end<br>执行JavaScript代码(表达式类)<br>使用示例<br>local title = splash:evaljs(“document.title”)  执行JavaScript代码并返回最后一条JavaScript语句的返回结果<br>执行JavaScript代码(声明类)<br>使用示例<br>​                    function main(splash, args)<br>​                         splash:go(“<a href="https://www.baidu.com&quot;" target="_blank" rel="noopener">https://www.baidu.com&quot;</a>)<br>​                          splash:runjs(“foo = function() { return ‘bar’ }”)<br>​                          local result = splash:evaljs(“foo()”)<br>​                          return result<br>​                    end<br>加载js代码<br>使用方法<br>此方法只负责加载JavaScript代码或库，不执行任何操作。如果要执行操作，可以调用evaljs()或runjs()<br>ok, reason = splash:autoload{source_or_url, source=nil, url=nil}<br>source_or_url：JavaScript代码或者JavaScript库链接。<br>source：JavaScript代码。<br>url：JavaScript库链接<br>使用示例<br>​                    function main(splash, args)<br>​                          splash:autoload([[<br>​                                  function get_document_title(){<br>​                                         return document.title;<br>​                            }<br>​                          ]])<br>​                          splash:go(“<a href="https://www.baidu.com&quot;" target="_blank" rel="noopener">https://www.baidu.com&quot;</a>)<br>​                          return splash:evaljs(“get_document_title()”)<br>​                    end<br>页面内容的操作方法<br>使用方法<br>splash:set_content(“<html><body><h1>hello</h1></body></html>“)     添加页面内容<br>splash:html()    获取当前页面源码<br>获取png/jpeg格式的页面截图<br>使用方法<br>splash:png()<br>splash:jpeg()<br>设置代理<br>使用示例<br>​                    function main(splash, args)<br>​                           request = splash:on_request(function(request)<br>​                                    request:set_proxy{host, port, username=nil, password=nil, type=’HTTP/SOCKS5’)<br>​                                    end)<br>​                    end<br>Splash-API的使用<br>Splash API的介绍<br>Splash Lua脚本的用法，但这些脚本是在Splash页面中测试运行的<br>Splash API就是我们可以通过python程序使用Splash服务的方法<br>Splash给我们提供了一些HTTP API接口，我们只需要请求这些接口并传递相应的参数即可<br>常用Splash API<br>render.png<br>接口介绍<br>通过width和height来控制宽高，它返回的是PNG格式的图片二进制数据。示例如下<br>常用参数<br>width、height、render_all<br>使用示例<br>​                        import requests<br>​                        url = ‘<a href="http://localhost:8050/render.png?url=https://www.jd.com&amp;wait=5&amp;width=1000&amp;height=700&#39;" target="_blank" rel="noopener">http://localhost:8050/render.png?url=https://www.jd.com&amp;wait=5&amp;width=1000&amp;height=700&#39;</a><br>​                        response = requests.get(url)<br>​                        with open(‘taobao.png’, ‘wb’) as f:<br>​                               f.write(response.content)<br>render.html<br>接口介绍<br>此接口用于获取JavaScript渲染的页面的HTML代码，接口地址就是Splash的运行地址加此接口名称<br>常用参数<br>url 、baseurl、timeout、resource_timeout、wait、proxy、js、js_source、filter、allowed_domin<br>allowed_content_types、viewport、images、headers、body、heep_method、save_args、load_args<br>使用示例<br>​            import requests<br>​            url = ‘<a href="http://localhost:8050/render.html?url=https://www.baidu.com&#39;" target="_blank" rel="noopener">http://localhost:8050/render.html?url=https://www.baidu.com&#39;</a><br>​            response = requests.get(url)<br>​            print(response.text)<br>render.har<br>接口介绍<br>此接口用于获取页面加载的HAR数据<br>使用示例<br>curl <a href="http://localhost:8050/render.har?url=https://www.jd.com&amp;wait=5" target="_blank" rel="noopener">http://localhost:8050/render.har?url=https://www.jd.com&amp;wait=5</a><br>render.json<br>接口介绍<br>此接口包含了前面接口的所有功能，返回结果是JSON格式<br>以JSON形式返回了相应的请求数据<br>可以通过传入不同参数控制其返回结果。比如，传入html=1，返回结果即会增加源代码数据；传入png=1，返回结果即会增加页面PNG截图数据；传入har=1，则会获得页面HAR数据<br>使用示例<br>curl <a href="http://localhost:8050/render.json?url=https://httpbin.org" target="_blank" rel="noopener">http://localhost:8050/render.json?url=https://httpbin.org</a><br>execute<br>接口介绍<br>用此接口便可实现与Lua脚本的对接<br>可以用该接口，通过原生的Lua脚本来调用splash<br>可以通过lua_source参数传递Lua脚本给splash服务器进行请求和渲染服务<br>常用参数<br>timeout、allowed_domains、proxy、filters、save_args、load_args<br>使用示例<br>​                        import requests<br>​                        from urllib.parse import quote<br>​                        lua = ‘’’<br>​                            function main(splash)<br>​                                return ‘hello’<br>​                            end<br>​                         ‘’’<br>​                        url = ‘<a href="http://localhost:8050/execute?lua_source=&#39;" target="_blank" rel="noopener">http://localhost:8050/execute?lua_source=&#39;</a> + quote(lua)<br>​                        response = requests.get(url)<br>​                        print(response.text)</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/爬虫基础/Selenium动态爬取" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/爬虫基础/Selenium动态爬取/" class="article-date">
      <time datetime="2018-10-18T12:22:23.215Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>动态HTML相关技术<br> JavaScript<br>JavaScript 是网络上最常用也是支持者最多的客户端脚本语言。它可以收集 用户的跟踪数据,不需要重载页面直接提交表单，在页面嵌入多媒体文件，甚至运行网页游戏。<br>jQuery<br>jQuery 是一个十分常见的库,70% 最流行的网站(约 200 万)和约 30% 的其他网站(约 2 亿)都在使用。一个网站使用 jQuery 的特征,就是源代码里包含了 jQuery 入口。如果你在一个网站上看到了 jQuery，那么采集这个网站数据的时候要格外小心。jQuery 可 以动态地创建 HTML 内容,只有在 JavaScript 代码执行之后才会显示。如果你用传统的方 法采集页面内容,就只能获得 JavaScript 代码执行之前页面上的内容。<br>Ajax  / DHTML<br>我们与网站服务器通信的唯一方式，就是发出 HTTP 请求获取新页面。如果提交表单之后，或从服务器获取信息之后，网站的页面不需要重新刷新，那么你访问的网站就在用Ajax 技术。<br>Ajax 其实并不是一门语言,而是用来完成网络任务(可以认为 它与网络数据采集差不多)的一系列技术。Ajax 全称是 Asynchronous JavaScript and XML(异步 JavaScript 和 XML)，网站不需要使用单独的页面请求就可以和网络服务器进行交互 (收发信息)。<br>动态HTML技术的影响<br>动态技术的效果对爬虫的影响（如下情况我们最好使用selenium来解决）<br>动态生成HTML内容，在获取内容的时候，只能获取到js处理之前的内容（可能是js再次发起异步请求，或者是对内容传输过程加密，js解密显示）<br>动态的生成url地址，在请求的时候，不知道其js的处理逻辑，不能解析js处理后的url地址<br>动态的生成请求参数，在请求的时候 ，不知道其js的处理逻辑，不知道js处理后的请求参数<br>服务器端动态生成的验证码（请求同一验证码url地址，返回图片不同），这样无法通过请求url下载图片来获取当前请求的验证码<br>怎么解决<br>直接从 JavaScript 代码里采集内容（费时费力）<br>分析js代码的功能逻辑<br>用python来重新实现其功能，完成参数或者内容的解析<br>用 Python 的 第三方库运行 JavaScript，直接模拟浏览器操作，并获取element中的js渲染后的代码内容。<br>通过使用selenium库来实现直接在浏览器上运行的效果，可以获取浏览器中elements中的源码（js解析后的内容）<br>通过模拟浏览器进行请求，不需要我们构建请求的url地址和请求参数等内容<br>获取模拟登录后的cookie等，保持登录状态<br>Selenium的使用<br>所需工具介绍<br>selenium的介绍<br>Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏<br>使用selenium的场景<br>在模拟登录的时候，可以模拟获取到动态的url、请求参数、等进行登录，还可以获取到cookie<br>获取动态HTML页面内容，即请求页面中的内容是经过js修改过动态计算生成的内容<br>各种driver的下载<br>chromedriver官网网站<br><a href="https://sites.google.com/a/chromium.org/chromedriver" target="_blank" rel="noopener">https://sites.google.com/a/chromium.org/chromedriver</a><br>chromedriver下载地址<br><a href="https://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">https://chromedriver.storage.googleapis.com/index.html</a><br>geckodriver官方网址<br><a href="https://github.com/mozilla/geckodriver" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver</a><br>geckodriver下载地址<br><a href="https://github.com/mozilla/geckodriver/releases" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver/releases</a><br>selenium官方文档<br><a href="http://selenium-python.readthedocs.io/api.html" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html</a><br>创建driver对象、并设置窗口大小<br>from selenium import webdriver<br>from selenium.webdriver import ActionChains<br>from selenium.webdriver.common.by import By<br>from selenium.webdriver.support.ui import WevDriverWait<br>from selenium.webdriver.support import export expected_conditions as EC<br>from selenium.webdriver.remote.command import Command<br>driver = webdriver.PhantomJS( )<br>driver.set_window_size(width, height)      设置浏览器的窗口大小，截图可以根据窗口大小来截取<br>driver.max_windiw( )       让窗口显示最大<br>加载网页<br>driver = webdriver.PhantomJS( )      加载浏览器driver<br>driver.get(“<a href="http://www.baidu.com/&quot;" target="_blank" rel="noopener">http://www.baidu.com/&quot;</a>)    请求url地址<br>寻找元素<br>driver.find_element_by_id()                                  通过id找到元素<br>driver.find_element_by_name()                           通过name属性寻找元素<br>driver.find_element_by_xpath()                            通过xpath来寻找元素<br>driver.find_element_by_link_text()                       通过文本内容寻找a标签<br>driver.find_element_by_partial_link_text()            通过文本内容包含内容寻找a标签<br>driver.find_element_by_tag_name()                     通过标签名获取元素<br>driver.find_element_by_class_name()                  通过类型获取元素<br>driver.find_element_by_css_selector()                  通过css选择器选择元素<br>备注：<br>find_element 和find_elements的区别：find_elements用来寻找多个元素, 返回一个和返回一个列表<br>by_css_selector的用法示例： #food span.dairy.aged<br>元素的交互<br>element.send_keys(“长城”)            在input元素value中输入内容<br>element.clear()                                  清空input标签中输入的内容<br>element.click( )                                  触发元素的事件<br>动作链的使用<br>source_ele = driver.find_element_by_css_selector(‘#draggable’)<br>target_ele = driver.find_element_by_css_selector(‘#droppable’)<br>actions = ActionChains(driver)         创建动作链<br>actions.drag_and_drop(source_ele, target_ele)         调用动作链的方法<br>actions.persorm()     执行动作链<br>指定原生Js代码<br>js_code = “window.scrollTo(0, document.body.scrollHeight)”    定义js代码<br>driver.execute_script(js_code)          指定js代码<br>获取节点元素的属性和文本值等<br>element.get_attribute(‘属性名’)     可以获取元素的属性<br>element.text                                    可以获取文本内容<br>element.id                                        获取元素的id属性<br>element.size                                     获取元素的长宽<br>element.size[‘width’]                    获取元素的宽<br>element.size[‘height’]                  获取元素的高<br>element.tag_name                       获取元素的标签名称<br>element.location                           获取元素的坐标<br>切换Frame<br>driver.switch_to.frame(‘frame_id’ )            切换到iframe标签页面中<br>driver.switch_to.parent_frame()                      切换到父frame中<br>driver.switch_to.default_content()             切换到默认的frame中<br>等待<br>隐式等待<br>等待规则<br>如果selenium没有在DOM中找到节点元素，将等待一段时间<br>时间到达设定的时候后, 再次查找元素，若找不到元素则抛出找不到节点的异常，默认时间是0<br>等待设置方法<br>driver = webdriver.Chrome()<br>driver.implicity_wait(10)     给driver设置了10秒的隐式等待时间<br>driver.get(‘<a href="https://www.baidu.com&#39;" target="_blank" rel="noopener">https://www.baidu.com&#39;</a>)<br>ele = driver.find_element_by_class_name(‘kw’)      寻找元素, 找不到元素的话会等待10秒钟, 若还找不到元素则抛出异常<br>显示等待<br>等待规则<br>规定了一个最长的等待时间，如果在规定的时间内加载出来了这个节点, 就返回找到节点<br>如果没有找点节点则抛出超时异常<br>等待设置方法<br>driver = webdriver.Chrome()<br>driver.get(‘<a href="https://www.baidu.com&#39;" target="_blank" rel="noopener">https://www.baidu.com&#39;</a>)<br>wait = WebDriverWait(driver, 10)        设置了10秒的最大显示等待时间<br>ele = wait.until(EC.presence_of_element_located((By.ID, ‘kw’)))        设置等待条件，并等待寻找到节点<br>ele_1 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ‘#bt’)))    设置等待条件，并等待寻找到节点<br>常用的等待条件<br>title_is             标题是某内容<br>title_contains             标题包含某内容<br>presence_of_element_located               节点加载出来<br>visibility_of_element_located                 节点课件, 传入定位元组<br>visibility_of                    节点可见，传入节点对象<br>presence_of_all_elements_located          所有节点加载出来<br>text_to_be_present_in_element             某个节点中包含某文字<br>text_to_be_present_in_element_value          某个节点值包含某文字<br>frame_to_be_available_and_switch_to_it      加载并切换<br>invisibility_of_element_located             节点不可见<br>element_to_be_clickable     节点可点击<br>element_to_be_seleted    节点可选择，传入节点对象<br>element_located_to_be_selected     节点可选择，传入定位元组<br>element_selection_state_to_be          传入节点对象以及状态，相等返回True, 否则返回false<br>element_located_selection_state_to_be    传入定位元组以及状态，相等返回True，否则返回False<br>alert_is_present    是否出现警告<br>其他的等待<br>driver.set_page_load_timeout(10)         设置页面加载的超时时间<br>driver.set_script_timeout(10)          设置执行script脚本的超时时间<br>鼠标的移动和点击<br>鼠标的移动<br>actions = ActionChains(driver)<br>actions.move_to_element(element).move_by_offset( lx , ly )<br>actions.perform()<br>element   移动到哪个元素里面<br>lx  相对元素左边界的距离<br>ly 相对元素上边界的距离<br>鼠标的点击<br>from selenium.webdriver.remote.command import Command<br>driver.execute(Command.MOUSE_DOWN, pamas)<br>第一个参数为时间类型<br>第二个参数为命令的附加命令，为字典类型<br>操作driver<br>页面请求参数相关<br>driver.page_source                获取浏览器中element中的内容（即页面的源码）<br>driver.current_url()                获取当前页面url地址<br>driver.back()                          退回上一个页面<br>driver.forward()                     返回到前面一个页面<br>driver.flush()                          刷新当前页面<br>driver.get_cookies()               获取所有的cookie，返回的是列表嵌套字典（存储的每个cookie信息）<br>driver.delete_cookie(“key”)    删除一条cookie<br>driver.delete_all_cookies()      删除所有的cookie内容<br>{cookie[‘name’]: cookie[‘value’] for cookie in driver.get_cookies( )}   将cookie构造请requests请求参数<br>driver.execute_script(‘window.open()’)     开启一个浏览器的选项卡<br>driver.switch_to_window(driver.window_handles[1])         切换到第二个选项卡窗口<br>窗口截图相关<br>from PIL import Image<br>driver.save_screenshot(“长城.png”)     截图浏览器，没有图形界面可以通过截图来观察界面<br>img = Image.open(io.BytesIO(driver.get_screenshot_as_png()))            获取截图并直接读取到内存<br>img_small = img.crop((x0, y0, x1, y1)).convert(‘L’)     对图片进行截图，四个参数代表(左、上、右、下)坐标，并将彩图转化为灰度图<br>img_small.save(‘–path–’)    将图片进行保存<br>width  =  img.size[0]    获取图片的宽<br>height  =  img.size[1]    获取图片的高<br>img1.load( )[ i, j ]       获取图片该像素点的rgb值<br>浏览器开关相关<br>driver.close()    关闭当前页面，当页面全部都关闭后，退出浏览器driver<br>driver.quit()      退出浏览器driver<br>selenium 的driver配置<br>代理的设置<br>phantomjs设置代理ip<br>browser=webdriver.PhantomJS(PATH_PHANTOMJS)<br>proxy=webdriver.Proxy()<br>proxy.proxy_type=ProxyType.MANUAL<br>proxy.http_proxy=’1.9.171.51:800’<br>proxy.add_to_capabilities(webdriver.DesiredCapabilities.PHANTOMJS)<br>browser.start_session(webdriver.DesiredCapabilities.PHANTOMJS)<br>browser.get(‘<a href="http://1212.ip138.com/ic.asp&#39;;" target="_blank" rel="noopener">http://1212.ip138.com/ic.asp&#39;;</a>)<br>Chrome设置代理ip</p>
<p>firefox设置代理ip<br>方法一</p>
<p>方法二</p>
<p>禁用插件和无头浏览器的设置<br>chrome配置方法<br>设置无头浏览器</p>
<p>设置禁用图片和javascript</p>
<p>firefox配置方法<br>firefox_options = webdriver.FirefoxOptions()<br>firefox_options.set_preference(u”permissions.default.stylesheet”, 2)   # 禁用css<br>firefox_options.set_preference(u”permissions.default.image”, 2)       # 禁用图片<br>firefox_options.set_preference(u”dom.ipc.plugins.enabled.libflashplayer.so”, “false”)       # 禁用flash插件<br>firefox_options.set_headless()     # 设置为无头浏览器<br>driver = webdriver.Firefox(firefox_options=firefox_options)<br>设置请求头的方法</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2018 刘小恺
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>