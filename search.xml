<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[以太坊技术栈]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F4.%E4%BB%A5%E5%A4%AA%E5%9D%8A%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[以太坊的介绍以太坊和比特币的区别以太坊的目的是创建一种去中心化应用的协议，提供一套对大量去中心化应用程序非常有用的新方案，特别强调快速开发，对小的和少数人使用的应用也非常安全（小而使用人少的应用容易被51%攻破），以及不同应用程序之间能够有效的互动。以太坊用过建立在本质上是抽象的基础层来完成这一工作；一个区块链其内置了图灵完备的编程语言，允许任何人编写智能合约和去中心化的应用程序，在这些应用程序中，他们可以创建任意的属于他们自己的规则、交易格式和状态转换函数。名字币的一个简单版本在以太坊可以用两行代码来编写完成，而其他协议如货币和信用系统则可以用不到 20 行的代码来构建。智能合约-包含价值而且只有满足某些条件才能打开的加密箱子-也能在我们的平台上构建，并且因为图灵完备性、价值知晓（value-awareness）、区块链知晓（blockchain-awareness）和多状态所增加的力量，远比特币脚本所能提供的功能强大得多； 以太坊虚拟机以太坊是一个可编程的区块链，不仅仅是给用户一些预定义操作（如比特币只交易），以太坊允许用户创建属于他们自己的复杂的操作。以太坊作为一个平台为不同的区块链应用提供服务。 狭义来说，以太坊是一系列协议，其核心就是一个以太坊虚拟机，能执行遵守协议的任何复杂的代码。以太坊虚拟机是图灵完备的，开发者可以在虚拟机上使用像 javascript，python 这样的友好的编程语言来创建应用。 和任何的区块链一样，以太坊包含了一个点对点的网络协议。这以太坊区块链是被链接着这个网络的各个节点维护和更新的。网络中的各个节点的虚拟机都执行相同的指令来共同维护区块数据库，因为这个原因，以太坊有时候被人称为“世界计算机”。 以太坊全网的大规模并行计算不是只为了提计算效率，而是为了保证全网的数据一致性。实际上，这使得在以太网上的 运算要比传统的电脑慢的多，成本也昂贵得多。全网中的每一台虚拟机的运行都是为确保全网数据库的一致性。去中心化的一致性给全网极端的容错能力;抗审查能力和永不宕机能力等。 以太坊账户以太坊的基本单元是账号。每一个账户都有一个 20 个字节长度的地址 。以太坊区块链跟踪每一个账号的状态，区块链上所有状态的转移都是账户之间的令牌（令牌即以太币）和信息的转移。以太坊有 2 种账户类型： 外部账号，简称 EOA,是由私钥来控制的。 有一个以太币的余额 可以发送交易(以太币转账或者激活合约代码) 通过私钥控制 没有相关联的代码 合约帐户,由合约代码来控制,且只能由一个 EOA 账号来操作 有一个以太币余额 相关联的代码 代码执行是通过交易后者其他的合约发送的call来激活 当被执行时 – 运行在随机复杂度(图灵完备性) – 只能操作其拥有的特定存储， 例如可以拥有其永久的state – 可以call其他合约 对于大多数用户来说，最基本的区别在于，用户掌握着 EOA 账号，因为用户掌握着控制 EOA 账号的私钥。而合约账号由内部程序代码来控制的，当然掌控私钥的 EOA 账户可以通过编写特定的程序代码来掌控合约账户。流行的术语“智能合约”就是合约账号中的代码，当一个交易被发送到该账户时，合约中的代码就会被执行。用户可以通过把代码部署到区块链中来创建一个新合约，也即创建了一个新的合约账户。 合约账户只有在 EOA 账户发出一个指令的时候才会去执行一个操作。所以一个合约账户是不可能自己去执行一个操作的，如生产一个随机数或执行一个 API 调用等，它只有在 EOA 账户作出确认的情况下才会去做这些事情。这是因为以太坊要求节点能够对计算的结果无论对错都达成一致，这就对操作有了一个必定会执行的要求。在以太坊中，全网的状态是由被“账户”的对象组成的，账户之间可以直接的进行价值和信息的转移，一个以太坊的账户包含下面 4 个字段: 随机数, 一个计数器，用以确保每个交易都只会被处理一次 账户当前的以太币额度 账户的合约代码, 如果有的话 这个账户的 存储 (默认空) “以太币” 是以太坊主要的内部加密燃料，并且被用来支付交易的费用。一般情况下，有 2 种类型的账户: 外部拥有的账户,被私钥控制的，和合约账户, 被合约代码控制的。外部拥有的账户没有代码，用户可以通过一个外部账户来创建和签名一个交易来送一个消息；合约账户中，每次当这个合约收到一个消息的时候，它的代码就会被激活，允许它读取这个消息，并且写入到内部存储中，然后按照一定顺序发送其他的消息或创建合约等。 应用以太坊框架本身并没有什么特别的功能。就好像程序语言一样，它做什么，都是由企业或开发者自己来决定的。如，复杂的金融合约的自动化。比特币可以让用户不通过第三方机构，如银行，政府等就可以直接兑换货币。但是以太坊的介入可能会产生更加深远的影响，因为任何复杂的金融操作都是可以自动被执行的，并且可以写成代码在以太坊上运行。当然除了金融外，任何情况下，只要对信用、安全、和持久有极高的要求，比如资产注册登记，投票，管理和物联网等都有可能受到以太坊平台的影响。 一般来说，在以太坊上有三种类型的应用。第一种是金融应用，这包括子货币，金融衍生品，套期保值合约，和一些雇佣合同等。第二类是半金融应用，这里有钱的存在但也有很重的非金钱的方面；最后，还有在线投票和去中心化治理这样的完全的非金融应用。 消息和交易交易名词“交易”在以太坊中是指签名的数据包，这个数据包中存储了从外部账户发送的消息，交易包含以下内容: 消息的接受者 一个可以识别发送者的签名 发送方给接收方的以太币数量 一个可选的数据字段 一个 STARTGAS 值, 表示执行这个交易允许消耗的最大计算步骤 一个 GASPRICE 值, 表示发送方的每个计算步骤的费用 前面三个是每一个加密货币都有的标准字段。默认情况下第四个数据字段没有任何功能，但是合约可以访问这里的数据；举个例子，如果一个合约是在一个区块链上提供域名注册服务的，那么它就会想把这数据字段中的数据解析成 2 个字段，第一个字段是域名，第二个字段是域名对应的 IP 地址。这个合约会从数据字段中读取这些值，然后适当把它们保存下来。 这个 STARTGAS 和 GASPRICE 字段 是以太坊的预防拒绝式攻击用的，非常重要。为了防止在代码中出现意外或敌对的无限循环或其他计算浪费，每个交易都需要设置一个限制，以限制它的计算总步骤是一个明确的值。这计算的基本单位是“汽油（gas）”； 通常，一个计算成本是一个 1 滴汽油，但是一些操作需要消耗更多的汽油，因为它们的计算成本更高。在交易数据中每一个字节需要消耗 5 滴汽油。这样做的目的是为了让攻击者为他们所消耗的每一种资源，包括计算，带宽和存储支付费用；所以消耗网络资源越多，则交易成本就越大。 消息合约具有发送”消息”到其他合约的能力。消息是一个永不串行且只在以太坊执行环境中存在的虚拟对象。他们可以被理解为函数调用（function calls）。 一个消息包括： 明确的消息发送者 消息的接收者 一个可选的数据域，这是合约实际上的输入数据 一个GASLIMIT值，用来限制这个消息出发的代码执行可用的最大gas数量 总的来说，一个消息就像是一个交易，除了它不是由外部账户生成，而是合约账户生成。当合约正在执行的代码中运行了CALL 或者DELEGATECALL这两个命令时，就会生成一个消息。消息有的时候也被称为”内部交易”。与一个交易类似，一个消息会引导接收的账户运行它的代码。因此，合约账户可以与其他合约账户发生关系，这点和外部账户一样。有许多人会误用交易这个词指代消息，所以可能消息这个词已经由于社区的共识而慢慢退出大家的视野，不再被使用。 以太坊状态转移函数 以太坊的状态转移函数 APPLY(S,TX) -&gt; S’ 可以被定义成下面的: 检查这个交易是不是合法的 ,签名是不是合法的, 这随机数是不是匹配这个发送者的账户，如果答案是否定的，那返回错误。 用 STARTGAS * GASPRICE 计算交易的费用，并且从签名中确定这个发送者的地址。 从发送者的余额中减去费用，并且增加发送者的随机值。如果余额不够，则返回错误。 初始化 GAS = STARTGAS, 并根据这交易中的字节数拿走一定量的汽油。 把交易的值从发送的账户转移到接收者的账户。如果接收者的账户还不存在，就创建一个。如果这个接收者的账户是一个合约，那么就运行合约的代码直到完成，或者报汽油消耗光的异常。 如果值转移失败了，因为发送者没有足够多的余额，或代码执行消耗光了汽油，恢复除了支付的费用外的所有的状态，并且把这个费用添加到矿工的账户上。 另外,把所有剩下的汽油退还给发送者，然后把用于支付费用的汽油发送给矿工。举例，假设合约的代码是这样的:if !self.storage[calldataload(0)]: self.storage[calldataload(0)] = calldataload(32)注意，真实的合约代码是用底层的 EVM 代码编写的；这个列子是用一个叫 Serpent 的高级语言写的。假设这个合约的存储开始是空的，并且发送了一个交易，其中包含 10 个以太币，2000 个汽油，汽油价格是 0.001 比特币，和 64 字节的数据，其中 0-31 字节代表数字 2,32-63 字节代表字符串 CHARLIE。在这个案例中，这状态转移函数的处理如下： 检查者交易是否有效并且格式完好。 检查者交易的发送者是否至少有 2000 * 0.001 = 2 以太币。如果有，则从发送者的账户中减去 2 以太币。 初始化 汽油（gas）= 2000;假设这个交易是 170 个字节长度并且每个字节的费用是 5，那么减去 850，汽油还剩 1150。 从发送者的账户减去 10 个以太币，并且添加到合约的账户中。 运行合约的代码. 在这里例子中:检查合约的存储的第 2 个索引是否已经被使用，注意到它没 有，然后就把这数据存储的第二个索引的值设置为 CHARLIE. 假设这个操作消耗了 187 个汽 油，那么剩下的汽油总量是 1150 – 187 = 963 把 963 * 0.001 = 0.963 以太币加到发送者的账户，然后反正结果状态。 如果交易的接收端没有合约，那么这总的交易费用就简单的等于汽油的价格乘以这个交易的字节长 度，与交易一起发送的数据字段的数据将无关重要。 注意，在恢复这个方面，消息和交易的处理方式是相同的： 如果一个消息执行消耗光了汽油，那么 这消息的执行和其他被触发的执行都会被恢复，但是父类的执行不会恢复。 Account 和 UTXO模型对比在当前区块链世界中，主要有两种记录保存方式，UTXO 模式（Unspent Transaction Output) 和 Account 模式。Bitcoin 采用的是 UTXO 模型，Ethereum 采用的 Account 模型，同样 CITA 也采用了 Account 模型。 Bitcoin 的设计初衷是点对点的电子现金系统，在比特币中，每个交易消耗之前交易生成的 UTXO 然后生成新的 UTXO，账户的余额即所有属于该地址的未花费 UTXO 集合，Bitcoin 的全局状态即当前所有未花费的 UTXO 集合。Ethereum 意图创建一个更为通用的协议，该协议支持图灵完备的编程语言，在此协议上用户可以编写智能合约，创建各种去中心化的应用。 由于 UTXO 模型在状态保存以及可编程性方面的缺陷，Ethereum 引入了 Account 模型。在乙太坊网络中，账户的状态信息是全局的，这些状态会被一种特殊的数据结果(MPT：默克尔前缀树)保存到每一个区块中，比方说账户A的地址，余额，交易的次数等等，比方说合约账户的地址，余额，合约代码等等。下面我们对两种模型的优缺点做进一步展开。 UTXO模型UTXO 模型中，交易只是代表了 UTXO 集合的变更。而账户和余额的概念是在 UTXO 集合上更高的抽象，账号和余额的概念只存在于钱包中。 优点： 计算是在链外的，交易本身既是结果也是证明。节点只做验证即可，不需要对交易进行额外的计算，也没有额外的状态存储。交易本身的输出 UTXO 的计算是在钱包完成的，这样交易的计算负担完全由钱包来承担，一定程度上减少了链的负担。 除 Coinbase 交易外，交易的 Input 始终是链接在某个 UTXO 后面。交易无法被重放，并且交易的先后顺序和依赖关系容易被验证，交易是否被消费也容易被举证。 UTXO 模型是无状态的，更容易并发处理。 对于 P2SH 类型的交易，具有更好的隐私性。交易中的 Input 是互不相关联的，可以使用 CoinJoin 这样的技术，来增加一定的隐私性。 缺点： 无法实现一些比较复杂的逻辑，可编程性差。对于复杂逻辑，或者需要状态保存的合约，实现难度大，且状态空间利用率比较低。 当 Input 较多时，见证脚本也会增多。而签名本身是比较消耗 CPU 和存储空间的。 ACCOUNT模型对于 Account 模型，Account 模型保存了世界状态，链的状态一般在区块中以 StateRoot 和 ReceiptRoot 等形式进行共识。交易只是事件本身，不包含结果，交易的共识和状态的共识本质上可以隔离的。 优点： 合约以代码形式保存在 Account 中，并且 Account 拥有自身状态。这种模型具有更好的可编程性，容易开发人员理解，场景更广泛。 批量交易的成本较低。设想矿池向矿工支付手续费，UTXO 中因为每个 Input 和 Out 都需要单独 Witness script 或者 Locking script，交易本身会非常大，签名验证和交易存储都需要消耗链上宝贵的资源。而 Account 模型可以通过合约的方式极大的降低成本。 缺点： Account 模型交易之间没有依赖性，需要解决重放问题。 对于实现闪电网络/雷电网络，Plasma 等，用户举证需要更复杂的 Proof 证明机制，子链向主链进行状态迁移需要更复杂的协议。 UTXO和ACCOUNT区别 计算问题 UTXO 交易本身对于区块链并没有复杂的计算，这样简单的讲其实并不完全准确，原因分有两个，一是 Bitcoin 本身的交易多为 P2SH，且 Witness script 是非图灵完备的，不存在循环语句。而对于 Account 模型，例如 Ethereum，由于计算多在链上，且为图灵完备，一般计算较为复杂，同时合约安全性就容易成为一个比较大的问题。当然是否图灵完备对于是否是账户模型并没有直接关联。但是账户模型引入之后，合约可以作为一个不受任何人控制的独立实体存在，这一点意义重大。 UTXO更易并发 在 UTXO 模型中，世界状态即为 UTXO 的集合，节点为了更快的验证交易，需要在内存中存储所有的 UTXO 的索引，因此 UTXO 是非常昂贵的。对于长期不消费的 UTXO，会一直占用节点的内存。所以对于此种模型，理论上应该鼓励用户减少生产 UTXO，多消耗 UTXO。但是如果要使用 UTXO 进行并行交易则需要更多的 UTXO 作为输入，同时要产生更多的 UTXO 来保证并发性，这本质上是对网络进行了粉尘攻击。并且由于交易是在钱包内构造，所以需要钱包更复杂的设计。反观 Account 模型，每个账户可以看成是单独的互不影响的状态机，账户之间通过消息进行通信。所以理论上用户发起多笔交易时，当这些交易之间不会互相调用同一 Account 时，交易是完全可以并发执行的。 Account模型的交易重放 Ethereum 使用了在 Account 中增加 nonce 的方式，每笔交易对应一个 nonce，nonce 每次递增。这种方式虽然意在解决重放的问题，但是同时引入了顺序性问题，同时使得交易无法并行。例如在 Ethereum中，用户发送多笔交易，如果第一笔交易打包失败，将引起后续多笔交易都打包不成功。在 CITA 中我们使用了随机 nonce 的方案，这样用户的交易之间没有顺序性依赖，不会引起串联性失败，同时使得交易有并行处理的可能。 存储问题 因为 UTXO 模型中，只能在交易中保存状态。而 Account 模型的状态是在节点保存，在 Ethereum 中使用 MPT 的方式存储，Block 中只需要共识 StateRoot 等即可。这样对于链上数据，Account 模型实际更小，网络传输的量更小，同时状态在节点本地使用 MPT 方式保存，在空间使用上也更有效率。例如 A 向 B 转账，如果在 UTXO 中假设存在 2 个 Input 和2个 Output，则需要 2 个 Witness script 和 2 个 Locking script；在 Account 模型中则只需要一个签名，交易内容只包含金额即可。在最新的隔离见证实现后，Bitcoin 的交易数据量也大大减少，但是实际上对于验证节点和全节点仍然需要针对 Witness script 进行传输和验证。 轻节点获取地址状态难易 例如钱包中，需要向全节点请求所有关于某个地址的所有 UTXO，全节点可以发送部分 UTXO，钱包要验证该笔 UTXO 是否已经被消费，有一定的难度，而且钱包很难去证明 UTXO 是全集而不是部分集合。而对于 Account 模型则简单很多，根据地址找到 State 中对应状态，当前状态的 State Proof 则可以证明合约数据的真伪。当然对于 UTXO 也可以在每个区块中对 UTXO 的 root 进行验证，这一点与当前 Bitcoin 的实现有关，并非 UTXO 的特点。 综上综上来看，Account 模型在可编程性，灵活性等方面更有优势；在简单业务和跨链上，UTXO 有其非常独到和开创性的优点。对于选择何种模型，要从具体的业务场景进行出发。 Gas的介绍什么是gas以太坊在区块链上实现了一个运行环境，被称为以太坊虚拟机（EVM）。每个参与到网络的节点都会运行都会运行EVM作为区块验证协议的一部分。他们会验证区块中涵盖的每个交易并在EVM中运行交易所触发的代码。每个网络中的全节点都会进行相同的计算并储存相同的值。合约执行会在所有节点中被多次重复，这个事实得使得合约执行的消耗变得昂贵，所以这也促使大家将能在链下进行的运算都不放到区块链上进行。对于每个被执行的命令都会有一个特定的消耗，用单位gas计数。每个合约可以利用的命令都会有一个相应的gas值。这里列了一些命令的gas消耗。 交易消耗的gas每笔交易都被要求包括一个gas limit（或startGas）和一个交易愿为单位gas支付的费用。矿工可以有选择的打包这些交易并收取这些费用。在现实中，今天所有的交易最终都是由矿工选择的，但是用户所选择支付的交易费用多少会影响到该交易被打包所需等待的时长。如果该交易由于计算，包括原始消息和一些触发的其他消息，需要使用的gas数量小于或等于所设置的gas limit，那么这个交易会被处理。如果gas总消耗超过gas limit，那么所有的操作都会被复原，但交易是成立的并且交易费任会被矿工收取。区块链会显示这笔交易完成尝试，但因为没有提供足够的gas导致所有的合约命令都被复原。所以交易里没有被使用的超量gas都会以以太币的形式打回给交易发起者。因为gas消耗一般只是一个大致估算，所以许多用户会超额支付gas来保证他们的交易会被接受。这没什么问题，因为多余的gas会被退回给你。 区块的gas limit是由在网络上的矿工决定的。与可调整的区块gas limit协议不同的是一个默认的挖矿策略，即大多数客户端默认最小区块gas limit为4,712,388。 以太坊上的矿工需要用一个挖矿软件，例如ethminer。它会连接到一个geth或者Parity以太坊客户端。Geth和Pairty都有让矿工可以更改配置的选项。这里是geth挖矿命令行选项以及Parity的选项。 估算交易消耗一个交易的交易费由两个因素组成： gasUsed：该交易消耗的总gas数量 gasPrice：该交易中单位gas的价格（用以太币计算） 交易费 = gasUsed * gasPrice gasUsed每个EVM中的命令都被设置了相应的gas消耗值。gasUsed是所有被执行的命令的gas消耗值总和。 gasPrice一个用户可以构建和签名一笔交易，但每个用户都可以各自设置自己希望使用的gasPrice，甚至可以是0。然而，以太坊客户端的Frontier版本有一个默认的gasPrice，即0.05e12 wei。矿工为了最大化他们的收益，如果大量的交易都是使用默认gasPrice即0.05e12 wei，那么基本上就很难又矿工去接受一个低gasPrice交易，更别说0 gasPrice交易了。 智能合约智能合约是在以太坊虚拟机上运行的应用程序。这是一个分布的“世界计算机”，计算能力由所有以太坊节点提供。提供计算能力的任何节点都将以Ether数字货币作为资源支付。 他们被命名为智能合约，因为您可以编写满足要求时自动执行的“合同”。 例如，想象一下在以太坊之上建立一个类似Kickstarter的众筹服务。有人可以建立一个以太坊智能合约，将资金汇集到别人身上。这个智能合约可以写成这样的话：当将100,000美元的货币添加到池中时，它将全部发送给收件人。或者，如果一个月内没有达到100,000美元的门槛，所有的货币都将被发回给货币的原始持有人。当然，这将使用以太币代替美元。 这一切都将根据智能合同代码进行，智能合同代码可自动执行交易，而无需可信任的第三方持有货币并签署交易。例如，Kickstarter在5％的付款处理费之上收取5％的费用，这意味着在\$100,000的众筹项目中将收取8000到10000美元的费用。智能合约不需要向像Kickstarter这样的第三方支付费用。 智能合约可以用于许多不同的事情。开发人员可以创建智能合约，为其他智能合约提供功能，类似于软件库的工作方式。或者，智能合约可以简单地用作应用程序来存储以太坊区块链上的信息。 为了真正执行智能合同代码，有人必须发送足够的以太网代币作为交易费 - 多少取决于所需的计算资源。这为以太坊节点参与并提供计算能力付出了代价。 以太坊挖矿 这以太坊的区块链和比特币的区块链有很多相似的地方，也有很多不同的地方。这个以太坊和比特币在区块链体系中最重要的不同点是 ：以太坊的区块同时包含了交易列表和最近区块的状态。除此之外，2 个其他的值，区块的编号和难度值也存在在区块中。以太坊中最基本的区块验证算法如下： 检查上一个区块是否存在和其有效性。 检测这区块的时间戳，是不是比上一个区块的大，并且小于 15 分钟 检查这区块编号，难度值，交易根（transaction root） , 叔根（uncle root）和汽油限制是否有效 检查这区块的工作证明是否有效 把 S[0] 设置成上一个区块的末端的状态 让 TX 成为这区块的交易列表，如果有 n 个交易。则做 for 循环 For i in 0…n-1, 设置 S[i+1] = APPLY(S[i],TX[i]). 如果任何一个应用发生错误，或这区块中汽油的总的消耗达到了 GASLIMIT, 则返回一个错误. 让 S_FINAL 等于 S[n], 但是把支付给矿工的奖励添加到这区块里。 检查这个状态 S_FINAL 的默克尔树树根是不是和区块头信息中所提供的状态根是一样的。如果是，则区块有效，不然则无效。 乍看上去，这种方法似乎效率很低，因为它需要将整个状态存储在每个块中，但在现实中，效率应该与比特币相当。原因在于，状态存储在树结构中，并且每个块后，只需要修改树的一小部分。此外，由于所有的状态信息都是最后一个区块的一部分，所以不需要存储整个区块链的历史——这一策略，如果它可以应用于比特币，那么它的磁盘空间将节省 5-20 倍。 以太坊网络中交易会被验证这网络的节点收集起来。这些“矿工”在以太坊网络中收集、传播、验证和执行交易，然后整理归档这些交易，打包成一个区块，与别的矿工竞争将区块添加到区块链中，添加成功的矿工将收到奖励。通过这样的措施，鼓励人们为区块链全网提供更多的硬件和电力支持。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币技术栈]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F3.%E6%AF%94%E7%89%B9%E5%B8%81%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[比特币设计原理比特币网络是一个分布式的点对点网络，网络中的矿工通过“挖矿”来完成对交易记录的记账过程，维护网络的正常运行。 区块链网络提供一个公共可见的记账本，通过共识机制所有节点共同维护同一份账本，该记账本并非记录每个账户的余额，而是用来记录发生过的交易的历史信息。该设计可以避免重放攻击，即某个合法交易被多次重新发送造成攻击。 其交易是通过销毁“未被花费的交易输出”(即UTXO)， 和创建新的UTXO来实现资金的流转； 比特币可以理解为: BTC = UTXO + 共识机制 + 区块链账本 比特币交易比特币—状态转移系统 从技术角度来说, 这加密货币的账本，如比特币可以被看作为一个状态转移的系统, 在这个系统里, 有一个包含了现在所有已存在的比特币的持有者的状态，并且有一个“状态转移函数”可以使用一个状态和一个交易来产生一个新的状态。在一个标准的银行体系里, 这状态就是一个资产负债表, 当一个交易要求把 的钱，从 A 转移到 B 时， 那么它的状态转移函数就会从 A 的账户中减去数量为\$X 的金额，然后在 B 的账户中增加数量为\$X 的金额。如果 A 的账户中没有\$X 的钱，那么状态转移函数就会返回一个错误。 //使用一个状态和一个交易才生一个新状态，或者返回错误APPLY(S,TX) -&gt; S’ or ERROR在银行系统中，它可以定义成这样:APPLY({ Alice: \$50, Bob: \$50 },”send \$20 from Alice to Bob”) = { Alice: \$30, Bob: \$70 }或者 APPLY({ Alice: \$50, Bob: \$50 },”send \$70 from Alice to Bob”) = ERROR 这“状态” 在比特币中是指所有的已经被挖出的但是还没消费的硬币的集合 (技术上, “没有被花掉的交易的产出(unspent transaction outputs)” 或 UTXO) ，每个 UTXO 都有一个面值和一个持有者 (持有者是由 20 个字节组成的地址，其本质是一个加密的公钥). 一个交易包含了一个或多个输入，每一个输入都包含了对一个已存在的 UTXO 的引用，和用持有者的地址所关联着的私钥来产生的一个加密签名, 并且会产生一个或多个输出, 每一个输出包含一个用于添加到状态的新的 UTXO。 这状态转移函数 APPLY(S,TX) -&gt; S’ 可以被大概的定义如下: 在 TX 中的每个输入： 如果被引用的 UTXO 不在 S 里，返回一个错误. 如果提供的签名和 UTXO 的所有者匹配不上, 返回一个错误. 如果所有的输入的 UTXO 的面值的和少于所有输出的 UTXO 的面值的和，返回一个错误. 返回一个所有输入的 UTXO 都被移除的，所有输出的 UTXO 都被加进的新的 S’ . 第一个步骤的前半部分防止交易的发送方消费不存在的硬币, 第一步骤的后半部分阻止了交易的发送方使用别人的硬币, 第二个步骤是强制的价值保护。为了使用这种支付方式，协议如下。 假设Alice 想发送 11.7 BTC 给 Bob. 首先, Alice 要找到一组她自己拥有的有效的 UTXO ，且总数要至少不低于 11.7 BTC. 实际上, Alice 不会恰巧刚好拥有 11.7 BTC; 她得到的最小值是 6+4+2=12，她然后使用这 3 个输入和 2 个输出创建了一个交易。 这第一个输出是 11.7 BTC ，是输出 Bob,第二个输出是剩下的 0.3 BTC “找零”。如果 Alice 没有要求把这个找零发送到她自己的账户上, 那么这矿工可以要求拥有这个零钱 UTXO基本概念在比特币中，一比交易’’在黑盒子里” 实际运作的方式是: 花费一种东西的集合，这种东西被称为“未被花费的交易输出”(即UTXO) ， 这些输出由一个或多个之前的交易所创造，并在其后制造出一比或多笔新的UTXO，可以在未来的交易中花费。每一笔UTXO他有面额、所有者。而且，一笔交易若要有效，必须满足的两个规则是：1）该交易必须包含一个有效的签名，来自它所花费的 UTXO 的拥有者；2）被花费的 UTXO 的总面额必须等于或者大于该交易产生的 UTXO 的总面额。一个用户的余额因此并不是作为一个数字储存起来的；而是用他占有的 UTXO 的总和计算出来的。 如果一个用户想要发送一笔交易，发送 X 个币到一个特定的地址，有时候，他们拥有的 UTXO 的一些子集组合起来面值恰好是 X，在这种情况下，他们可以创造一个交易：花费他们的 UTXO 并创造出一笔新的、价值 X 的 UTXO ，由目标地址占有。当这种完美的配对不可能的时候，用户就必须打包其和值 大于 X 的 UTXO 输入集合，并添加一笔拥有第二个目标地址的 UTXO ，称为“零钱输出”，分配剩下的币到一个由他们自己控制的地址。 UTXO包含的内容 UTXO， 用比特币拥有者的公钥(锁定)(加密)的一个数字 UTXO == 比特币 比特币系统里没有比特币， 只有UTXO 比特币系统中没有账户，只有UTXO(公钥锁定) 比特币系统里没有账户余额， 只有UTXI ( 账户余额只是比特币钱包的概念 ) UTXO存在前节点的数据库里 转账将消耗掉属于你自己的UTXO， 同事生成新的UTXO， 并用接受者的公钥锁定。 交易的结构 交易的输出（UTXO） 锁定的比特币数量 锁定脚本（接受者的公钥哈希）- 将比特币地址锁定到接收者 交易的输入（UTXO） 解锁脚本（发送者私钥的数字签名, 发送者的公钥）- 用来证明比特币确实属于发送者 交易的过程 交易验证-基于栈的脚本语言 栈（stack）- 操作数据的一种结构 只能从一端操作数据，后进先出LIFO 如同子弹匣， 先压如的子弹最后打出 压栈（PUSH）， 出栈（POP） 基于栈的脚本语言 对栈的操作： OP_DUP 逻辑运算符： OP_EQUALIVERIFY 加密运算符： OP_HASH160, OP_CHECKSIG 算数运算符: OP_ADD, OP_SUB, OP_MUL, OP_DIV 交易验证 锁定脚本 OP_DUP OP_HASH160 &lt;发送者的公钥哈希&gt; OP_EQUALVERIFY OP_CHECKSIG 解锁脚本 &lt;发送者的签名&gt; &lt;发送者的公钥&gt; 交易验证 运行解锁脚本 + 锁定脚本 =&gt; True 交易的传播 钱包软件生成交易，并想临近节点传播 节点对收到的交易进行验证，并丢弃不合法的交易 交易的size要小于区块size的上限 （比如比特币之前的大小限制是1M ） 交易的输入UTXO是存在的 交易输入UTXO没有被其他交易引用-防止双花（Double Spending） 输入总金额 &gt; 输出的总金额 解锁脚本的验证 将合格的交易加入到本地的Transaction数据库中，并将合法的交易转给临近节点 区块的生成和验证区块的生成 旷工在挖矿前要组件区块 将coinbase交易打包进区块 将交易池中高优先级的交易打包进去快 优先级 = 交易的额度 * UTXO的深度 / 交易的size 创建去开的头部 挖矿成功后，将计算出来的随机数nonce填入区块头部， 并向临近节点传播 区块的验证链接相邻节点接受到新区块后，立即做以下的检查 验证POW的nonce值是否符合难度值 检查时间戳是否小于当前时间2小时 检查Merkle树根是否正确 检查区块size要小于区块的size上限 第一笔交易必须是coinbase交易 验证每个交易 隔离见证什么是隔离见证隔离见证，即 Segregated Witness（简称SegWit），由Pieter Wuille（比特币核心开发人员、Blockstream联合创始人）在2015年12月首次提出。 见证（Witness） 见证，在比特币里指的是对交易合法性的验证。举个例子，Alice发起一笔交易，给Bob支付1个BTC，该笔交易信息由三部分组成： a.元数据：交易信息格式的版本号；交易锁定时间等 b.付款人：Alice用于付款的BTC来源，一般来源于某历史区块上某笔交易的输出（详 见UTXO）；证明Alice拥有该笔交易的输出，即见证（Witness）数据 c.收款人：Bob的收款地址和金额 可见，见证数据包含在交易信息里头。 隔离（Segregated） 指的就是把见证数据从交易信息里抽离出来，单独存放。 隔离见证的来源为什么要把见证数据隔离出来呢，或者说这样做有什么好处呢？这就涉及到比特币里的另一个概念–扩容。 扩容，指的是增加比特币每秒的交易量。比特币每10分钟左右挖出一个大小小于1MB的区块，每笔交易平均250字节，即每个区块最多放进4000笔交易，这样算下来，比特币每秒处理的交易数不超过7个。对比其它交易平台，PayPal每秒数百笔、Visa每秒数千笔、支付宝能达到每秒数万笔，可见比特币是一个非常低效的交易系统。如果使用人数增多，则会造成比特币的拥堵。 如何解决拥堵呢？ 有两种方式，一是简单的增加每个区块的大小，比如将区块大小增加到8M；另一种就是隔离见证+闪电网络啦。 扩容方案一: 增加区块大小如果将区块大小增至8M，简单思考一下，比特币每秒处理的交易数似乎也增加到原来的8倍，即56笔每秒。如果每个区块1个GB，比特币每秒将处理7000笔交易，拥堵问题不就解决了吗？ 中本聪可没那么傻，之所以将区块大小设定为1M，是有重要原因的。比特币白皮书的标题为：一种点对点的电子现金系统，相比于传统货币系统，比特币的核心价值在于实现了一种去中心而且安全的货币。如果区块的大小过大，则会危害到比特币的安全模型，作为一种货币应用，这显然是不能令人接受的。 为什么这么说呢？ POW机制的安全基础，是假设一个人的算力无法超过全网算力的50%。如果增大区块，可能一个人的算力超过全网的1/3，就危害到了比特币的安全。举个例子，为了达到每秒7000笔的交易速度，我们把区块的大小增加到1GB： a.假设1GB的区块从产生到广播到全网节点需要10分钟； b.有一个叫Byzantium的节点，拥有的算力超过全网1/3； c.当Byzantium节点挖出一个新区块时，假设该时间点为0秒，那么Byzantium节点 获取新区块的时间点为0秒；根据假设a，全网最后一个获取新区块的节点的获取时间 为600秒，如果获取速度是线性的，全网其它节点获取新区块的平均时间是300秒。 d.因为在新区块上挖坑的算力才是有效算力；根据c，全网其它节点的有效算力只剩下 一半，也就是说，全网其它节点的有效算力小于1/3 e.根据b和d，这种情况下，Byzantium节点算力超过全网其它节点算力，如果Byzantium 节点在自己挖出的区块上继续挖矿且不公布广播，则Byzantium节点上没公布的区块 长度，会大于全网区块长度；一旦Byzantium节点公布这些区块，则全网其它节点挖 出的区块全部作废。 可见，区块设计过大，会威胁到比特币的安全。换句话说，比特币区块的大小是有上限的，《On Scaling Decentralized Blockchain》这篇论文指出，在目前的互联网环境下，如果十分钟产生一个区块，区块的大小最好不能超过4MB。这样看来，增加区块大小这种扩容方案，效果就十分有限了。 扩容方案二: 隔离见证 + 闪电网络隔离见证为什么能扩容呢？先来看看比特币区块的数据结构： 每笔交易平均250字节，见证部分的数据约为150字节，其余部分100字节。如果将见证数据隔离出来，原来1MB空间的区块可以放下10000笔交易（原来为4000笔），交易速度约提升2.5倍。隔离出来的见证数据放到了区块末尾，大小为1.5到2MB，所以隔离见证的整个区块大小为2.5到3MB左右。 隔离见证的意义: 解决了交易延展性问题； 为闪电网络铺路 其他优化 交易延展性 中本聪在设计比特币的时候直接把这两个信息直接放在了区块内，所以一个区块就承载不了更多的交易信息，如果隔离了“见证信息”，那么区块链只记录交易信息，那么一个区块可承载的交易更多交易。中本聪设计比特币时，并没有把两部分资料分开处理，因此导致交易ID的计算混合了交易和见证。因为见证本身包括签名，而签名不可能对其自身进行签名，因此见证可以由任何人在未得到交易双方同意的情况下进行改变，造成所谓的交易可塑性（malleability）。在交易发出后、确认前，交易ID可以被任意更改，因此基于未确认交易的交易是绝对不安全的。在2014年就曾有人利用这个漏洞大规模攻击比特币网络。 指的是一笔交易发起后，交易数据中的见证部分可以被篡改，而且篡改后的交易仍然有效。具体的说，见证的实现依靠一种签名算法，比如椭圆曲线数字签名算法（ECDSA），这种算法下签名（r，s）和签名（r，-s（mod n））都是有效的，所以可以把一种有效见证数据篡改成另一种有效见证数据，该笔交易仍然是有效的。每笔交易有个交易ID，交易ID是对整个交易数据的Hash值，为该笔交易的唯一标识。通过对见证数据的篡改，可以改变Hash值，从而改变该笔交易的唯一标识。隔离见证通过把见证数据隔离移出，生成交易ID时Hash的数据不包括见证数据，因此也就无法改变交易ID值。 从此以后，只有发出交易的人才可以改变交易ID，没有任何第三方可以做到。如果是多重签名交易，就只有多名签署人同意才能改变交易ID。这可以保证一连串的未确认交易的有效性，是双向支付通道或闪电网络所必须的功能。有了双向支付通道或闪电网络，二人或多人之间就可以实际上进行无限次交易，而无需把大量零碎交易放在区块链，大为减低区块空间压力。 闪电网络 通过增加区块大小无法从根本上解决比特币的扩容问题。闪电网络通过在比特币基础上，构建第二层网络，将交易转移到链下的方式，来减轻公链负担，以实现扩容的效果。目前看来，在公链基础上构建协议层网络，是解决公链拥堵问题最合适也是最有前景的方案 隔离见证所带来的改变，为闪电网络的实现提供了一些便利，主要有3点： a.交易延展性的解决，让交易无法被干扰，闪电网络白皮书中提到的“人质状态” （hostage situation），得以避免； b.在通道的生命周期上，隔离见证让闪电网络的通道永久开启更方便实现； c.虽然从理论上系统是安全的，但用户还是要查看区块链中的交易是否广播撤回，防止交易方的欺诈行为，隔离见证使得这项活动可以外包出去，只要给服务器传送少量信息， 就能代替你完成这一过程。 d.此外，隔离见证给比特币带来了一些细节优化，比如增加了脚本版本（Script Version），使得脚本语言可以以一种向后兼容的方式来发展；签名算法复杂度有了较大优化等等。 挖矿 如果我们有一个可信任的中央服务器, 那么实现这个系统是一件很简单的事情; 就按照需求所描述的去编写代码即可，把状态记录在中央服务器的硬盘上。 然而，与比特币一样，我们试图去建立一个去中心化的货币系统, 所以，我们需要把状态转移系统和一致性系统结合起来，以确保每个人都同意这交易的顺序。比特币的去中心化的一致性处理进程要求网络中的节点连续不断的去尝试对交易进行打包，这些被打成的包就称为“区块”。 这个网络会故意的每隔 10 分钟左右就创建一个区块, 每一个区块里都包含一个时间戳，一个随机数，一个对上一个区块的引用 ，和从上一个区块开始的所有交易的列表。随着时间的推移，这会创建一个持久的，不断增长的区块链，这个区块链不断的被更新，使其始终代表着最新的比特币总账的状态。在这个范例中，用来验证一个区块是否有效的算法如下: 检查其引用的上一个区块是否存在并且有效. 检查这个区块的时间戳是否大于上一个区块的时间戳 并且小于 2 小时之内 检查这区块上的工作证明是否有效. 让 S[0] 成为上一个区块的最末端的状态. 假设 TX 是这个区块的交易列表，且有 n 个交易。 做 for 循环，把 i 从 0 加到到 n-1， 设置 S[i+1] = APPLY(S[i],TX[i]) 如果任何一个应用(APPLY)返回错误，则退出并且返回。 返回 true,并且把 S[n] 设置成这个区块最末端的状态。 从本质上说，区块中的每一个交易都必须提供一个有效的状态，从交易执行前的标准状态到执行后的一个新的状态。 注意，状态并没有以任何方式编码进区块中;它纯粹是一个被验证节点所记住的抽象，并且它只能用来被从创世区块起的每一个区块进行安全的计算，然后按照顺序的应用在每一个区块中的每一次交易中。此外，请注意矿工把交易打包进区块的顺序是很重要的; 如果一个区块中有 2 个交易 A 和 B,B 花了一个由 A 创建的 UTXO, 那么如果 A 比 B 更早的进入区块，那么这个区块将是有效的，不然就是无效的。 在上述列出的验证条件中，“工作证明” 这一明确的条件就是每一个区块的 2 次 SHA256 哈希值, 它作为一个 256 位的数字,必须小于一个动态调整的目标值, 截止到本文写作的时间，该动态调整的值的大小大约是 2 的 187 次方。这样做的目的是为了让创建区块的算法变难, 从而，阻止幽灵攻击者从对它们有利的角度出来，来对区块链进行整个的改造。因为 SHA256 被设计成一个完全不可预测的伪随机函数, 这创建一个有效区块的唯一的方法只有是不断的尝试和出错， 不断对随机数进行递增，然后查看新的哈希值是否匹配。 按照当前的目标值 2 的 187 次方，这个网络在找到一个有效的区块前，必须进行 2 的 69 次方次的尝试; 一般来说,每隔 2016 个区块，这个目标值就会被网络调整一次 ，因此网络中平均每隔 10 分钟就会有一些节点产生出一个新的区块。为了补偿这些矿工的计算工作, 每一个区块的矿工有权要求包含一笔发给他们自己的 12.5BTC（不知道从哪来的）的交易。另外,如果任何交易，它的总的输入的面值比总的输出要高，这一差额会作为“交易费用”转给矿工。顺便提一下，对矿工的奖励是比特币发行的唯一途径，创世状态中并没有比特币。 为了更好的理解挖矿的目的,让我们来检测一下，当恶意攻击发生时会发生什么. 由于比特币的底层的加密技术众所周知是安全的,所以这攻击者的目标将是比特币系统中的某个部份，那就是没有被密码直接保护的：交易的次序。攻击者的策略其实很简单: 发送 100 BTC 到一个商人，以兑换一些商品 (最好是快速交易的数字商品) 等待商品的发货 创建另一个交易，发送同样的 100 BTC 给自己 尝试让网络相信他发给他自己的那个交易是最新出现的 一旦步骤 (1)发生, 几分钟后，一些矿工就会把这个交易打包进一个区块, 假设声明该区块编号是270000 。大约一个小时后, 在那个区块之后又会有新增的五个区块添加到区块链中, 这五个区块都间接的指向了那个交易，从而“确认”了那交易是真的。在这一刻，商家接收到了货款，并且发出了商品; 因为我们假设是一个数字商品，所以攻击者能立刻就收到货。现在攻击者创建另一个交易，向他自己的另一个账户发送这 100 BTC 。如果这个攻击者只是简单的释放了这个交易，那么这个交易将不会被执行; 矿工将会尝试运行 APPLY(S,TX) ，注意那个 TX 消耗一个在以太坊状态中已经不存在的UTXO。因此，这个攻击者创建了一个比特币区块链的“分支” , 开始挖取区块 270000的另一个版本，这个版本指向同样的区块 269999 作为一个父亲,但是用这新的交易替换了旧的。因为这区块的数据是不同的 , 这就需要为相关的区块重新做工作证明。 此外,这个攻击者的新的版本的区块 270000 有一个不同的哈希，所以这已存在的区块 270001 到 270005 不会指向它; 因此, 这原来的链和这攻击者的新链是完全独立的。区块链的规则是：在分支中最长的区块链链将会变成真正的链，所以合法的矿工将会继续在 270005 这条链上工作，同时攻击者自己一个人单独的工作在新版本的 270000 这条链上。为了让攻击者他自己的区块成为最长，他需要的计算能力比其他网络的总和还要多（“51%攻击”）。比特币的区块依赖之前所有区块的哈希。一个拥有巨大计算能力的攻击者可以重新设计工作的证明(PoW)，并最终获得大量的比特币 默克尔树 在默克尔树中只要提供少数的介个节点就可以给出 一个分支的有效性证明，比如验证12c5是否有效，只需要提供标记为蓝色的几点就可以证明12c5这个交易是否有效 试图改变默克尔树的任意一部分都会导致链条上在某处放生不一致的情况 比特币的一个重要特性，即区块是存在一个多级数据结构中的 。一个区块的“哈希值”实际上只是这个区块的头信息的哈希值，一个大约 200 个字节的数据，其中包含了时间戳，随机数，上一个区块的哈希和一个存储了这个区块中所有交易的称之为默克尔树的数据结构的根哈希。 默克尔树是一种二叉树，包含了一组节点，它们的含有基础信息的树根有大量的叶子节点，一组中间节点，每一个节点都是它的 2 个子节点的哈希，然后，最终的一个根节点，也是由它的 2 个子节点的哈希形成，代表着这树的“顶端”。 这个默克尔树的目的是允许在一个区块中的数据能够被零散的传递: 一个节点只能从一个源来下载一个区块的头信息，树的一小部分关联着另一个源 ，并且任然可以保证所有的数据都是正确的。之所以这样做行得通，是因为哈希值都是向上传导的: 如果一个恶意的用户试图在默克尔树的底部替换一个假的交易, 这个更改将导致上面的节点发生变化，然后上面的节点的变化又会导致上上面的节点发生变化，最终改变这个数根节点，因此也改变了这区块的哈希，导致这个协议把它注册成一个完全不同的区块 (几乎可以肯定是一个无效的工作证明).这默克尔树协议对比特币的长期可持续发展是必不可少的。比特币网络中的一个“完整节点” , 截止到 2014 年，占用了大约 15G 的磁盘空间，并且每月正在以 10 亿字节的速度递增。目前，这对于电脑来说是没有问题的，但是在手机上却是不现实的。在以后的将来，只有商业的和业余爱好者才能参与玩比特币。一个称之为 “简化支付验证（simplified payment verification）” (SPV)的协议 允许另一种类型的节点存在，这种节点称之为 “轻节点（light nodes）”, 其下载区块的头信息,在这区块头信息上验证工作证明，然后只下载与之交易相关的“分支” 。 这使得轻节点只要下载整个区块链的一小部分，就可以安全地确定任何一笔比特币交易的状态和账户的当前余额。 脚本系统即便没有任何扩展，比特币的协议实际上却是促进了一个弱化版的”智能合约”的概念。。UTXO 在比特币中不是只被一个公钥持有, 而是还被在一个基于堆栈的程序语言组成的复杂的脚本所持有着。 在这个范例中，一个交易消耗的 UTXO 必须提供满足脚本的数据。实际上，这最基本的公钥所有权机制也是通过一个脚本来实现的：这个脚本使用一个椭圆曲线签名作为一个输入，验证拥有这个 UTXO 的交易和地址，如果验证成功，则返回 1，不然则返回 0。其他，更加复杂的脚本存在于各种复杂的用例中。 例如，你可以构造一个脚本，它要求从给定的三个私钥中，至少要选其中的 2 个来做签名验证（“多重签名”），这个对公司账本，储蓄账户等来说非常有用。脚本也能用来对解决计算问题的用户支付报酬。人们甚至可以创建这样的脚本“如果你能够提供你已经发送一定数额的的狗币给我的简化确认支付证明，这一比特币就是你的了”，本质上，比特币系统允许不同的密码学货币进行去中心化的兑换。然而，在比特币中的脚本语言有几个重要的限制： 缺乏图灵-完备 那就是说，虽然比特币脚本语言支持的计算方式很多，但是它不是所有的都支持。在主要类别中缺失循环。 它这样做的目的是为了防止对交易的验证出现死循环；理论上，它的脚本是可以克服这个障碍的，因为任何的循环都可以通过 if 语句重复多次底层代码来模拟，但是这样的脚本运行效率非常低下。 值的盲区 一个 UTXO 脚本没有办法提供资金的颗粒度可控的出金操作。比如, 一个预言合约（oracle contract ）的其中一个强大的用例就是一个套期保值的合约，A 和 B 都把价值1000\$的 BTC 放到合约中，30 天后，这个合约把价值 1000\$的 BTC 发给了 A，剩下的发给了B。 这就需要合约要确定 1BTC 以美元计值多少钱。然而，因为 UTXO 是不可分割的，为实现此合约，唯一的方法是非常低效地采用许多有不同面值的 UTXO（例如有 2^k 的 UTXO，其中 K可以最大到 30)并使预言合约挑出正确的 UTXO 发送给 A 和 B。 状态缺失 UTXO 要么被使用了，要么没有被使用；这会使得多阶段的合约和脚本没有机会保持任何其他的内部状态。这使得制作多阶段的期权合约、去中心化的交换协议或两阶段加密承诺协议变得困难(对于安全计算奖金来说是必要的)。这也意味着，UTXO 只能用于构建简单的、一次性的合约，而不是更复杂的“有状态”的合约，比如去中心化的组织，并且使得元协议难以实现。 区块链盲区 UTXO 对某些区块链数据视而不见，比如随机数和之前的区块的哈希。这严重限制了博彩和其他一些类别的应用，因为它剥夺了一种潜在的有价值的脚本语言：随机数！也就是在比特币的脚本中是没有随机数的。 因此，我们看到了在加密货币之上构建高级应用程序的三种方法：一，构建一个新的区块链，二，在比特币之上使用脚本，三，在比特币之上构建一个元协议。 构建一个新的区块链可以无限制扩展功能集，但是这样做非常消耗时间。使用脚本很容易实现和标准化，但在其功能上非常有限，而元协议虽然容易，但在可伸缩性方面却存在缺陷。在以太坊中，我们打算建立一个替代性的框架，它提供了更大的开发和更强大的轻客户属性，同时允许应用程序共享一个经济环境和区块链安全。 比特币的缺陷比特币的缺陷 交易确认时间长，吞吐量低 POW挖矿浪费计算资源 ASIC矿机出现是全民参与性降低，算力集中 不完全匿名 无法存储太多的数字资产 不支持复杂的脚本语言 缩短交易时间的方法 缩短平均产生区块的时间 中心化服务 信任地址多重签名 开放交易和联合服务器 POS和DPOS Segwit与闪电网络]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链密码学]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F2.%E5%8C%BA%E5%9D%97%E9%93%BE%E5%AF%86%E7%A0%81%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[密码学算法Hash算法定义 Hash（哈希或散列）算法，又常被称为指纹（fingerprint）或摘要（digest）算法，是非常基础也非常重要的一类算法。可以将任意长度的二进制明文串映射为较短的（通常是固定长度的）二进制串（Hash 值），并且不同的明文很难映射为相同的 Hash 值，且算法不可逆，例如: MD5、SHA-1、SHA-2、SHA-256； 比如判断两个字符串是否相等，只需要判断两字符串的hash值是否相等就可以基本断定两字符串是否相等； Hash 算法并不是一种加密算法，不能用于对信息的保护。但 Hash 算法可被应用到对登录口令的保存上。例如网站登录时需要验证用户名和密码，如果网站后台直接保存用户的口令原文，一旦发生数据库泄露后果不堪设想（事实上，网站数据库泄露事件在国内外都不少见）。利用 Hash 的防碰撞特性，后台数据库可以仅保存用户口令的 Hash 值，这样每次通过 Hash 值比对，即可判断输入口令是否正确。即便数据库泄露了，攻击者也无法轻易从 Hash 值还原回口令。 性质 正向快速：给定原文和 Hash 算法，在有限时间和有限资源内能计算得到 Hash 值； 逆向困难：给定（若干）Hash 值，在有限时间内无法（基本不可能）逆推出原文； 输入敏感：原始输入信息发生任何改变，新产生的 Hash 值都应该发生很大变化； 碰撞避免：很难找到两段内容不同的明文，使得它们的 Hash 值一致（即发生碰撞）。 常见hash算法 目前常见的 Hash 算法包括国际上的 Message Digest（MD）系列和 Secure Hash Algorithm（SHA）系列算法，以及国内的 SM3 算法。 MD 算法主要包括 MD4 和 MD5 两个算法。MD4（RFC 1320）是 MIT 的 Ronald L. Rivest 在 1990 年设计的，其输出为 128 位。MD4 已证明不够安全。MD5（RFC 1321）是 Rivest 于 1991 年对 MD4 的改进版本。它对输入仍以 512 位进行分组，其输出是 128 位。MD5 比 MD4 更加安全，但过程更加复杂，计算速度要慢一点。MD5 已于 2004 年被成功碰撞，其安全性已不足应用于商业场景。。 SHA 算法由美国国家标准与技术院（National Institute of Standards and Technology，NIST）征集制定。首个实现 SHA-0 算法于 1993 年问世，1998 年即遭破解。随后的修订版本 SHA-1 算法在 1995 年面世，它的输出为长度 160 位的 Hash 值，安全性更好。SHA-1 设计采用了 MD4 算法类似原理。SHA-1 已于 2005 年被成功碰撞，意味着无法满足商用需求。 为了提高安全性，NIST 后来制定出更安全的 SHA-224、SHA-256、SHA-384，和 SHA-512 算法（统称为 SHA-2 算法）。新一代的 SHA-3 相关算法也正在研究中。 目前， MD5和SHA1已经被破解，一半推荐使用SHA2-256或更安全的算法 对称加密算法定义 对称加密算法，顾名思义，加密和解密过程的密钥是相同的。该类算法的优点是加解密效率，和加密强度强度都很高。 缺点是参与方需要提前持有密钥，一旦有人泄露则系统安全性被破坏；另外如何在不安全通道中提前分发密钥也是个问题，需要借助额外的 Diffie–Hellman 协商协议或非对称加密算法来实现。 对称密码从实现原理上可以分为两种：分组密码和序列密码。前者将明文切分为定长数据块作为基本加密单位，应用最为广泛。后者则每次只对一个字节或字符进行加密处理，且密码不断变化，只用在一些特定领域（如数字媒介的加密）。 分组对称加密算法 DES（Data Encryption Standard）：经典的分组加密算法，最早是 1977 年美国联邦信息处理标准（FIPS）采用 FIPS-46-3，将 64 位明文加密为 64 位的密文。其密钥长度为 64 位（包括 8 位校验码），现在已经很容易被暴力破解； 3DES：三重 DES 操作：加密 –&gt; 解密 –&gt; 加密，处理过程和加密强度优于 DES，但现在也被认为不够安全； AES（Advanced Encryption Standard）：由美国国家标准研究所（NIST）采用，取代 DES 成为对称加密实现的标准，1997~2000 年 NIST 从 15 个候选算法中评选 Rijndael 算法（由比利时密码学家 Joan Daemon 和 Vincent Rijmen 发明）作为 AES，标准为 FIPS-197。AES 也是分组算法，分组长度为 128、192、256 位三种。AES 的优势在于处理速度快，整个过程可以数学化描述，目前尚未有有效的破解手段； IDEA（International Data Encryption Algorithm）：1991 年由密码学家 James Massey 与来学嘉共同提出。设计类似于 3DES，密钥长度增加到 128 位，具有更好的加密强度。 序列加密算法 RC4算法: 可以通过“一次性密码本”的对称加密处理。即通信双方每次使用跟明文等长的随机密钥串对明文进行加密处理。序列密码采用了类似的思想，每次通过伪随机数生成器来生成伪随机密钥串。 加解密原理 用相同的密钥对原文进行加密和解密 加密过程: 密钥 + 原文 =&gt; 密文 解密过程: 密文 + 密钥 =&gt; 原文 缺点 无法确保密钥被安全的传递 无法保证数据不会被篡改 非对称加密定义 非对称加密是现代密码学的伟大发明，它有效解决了对称加密需要安全分发密钥的问题。 顾名思义，非对称加密中，加密密钥和解密密钥是不同的，分别被称为公钥（Public Key）和私钥（Private Key）。私钥一般通过随机数算法生成，公钥可以根据私钥生成。 其中，公钥一般是公开的，他人可获取的；私钥则是个人持有并且要严密保护，不能被他人获取。 非对称加密算法优点是公私钥分开，无需安全通道来分发密钥。缺点是处理速度（特别是生成密钥和解密过程）往往比较慢，一般比对称加解密算法慢 2~3 个数量级；同时加密强度也往往不如对称加密。 非对称加密算法的安全性往往基于数学问题，包括大数质因子分解、离散对数、椭圆曲线等经典数学难题。 代表算法包括：RSA、ElGamal、椭圆曲线（Elliptic Curve Crytosystems，ECC）、SM2 等系列算法。 常用非对称加密算法 RSA：经典的公钥算法，1978 年由 Ron Rivest、Adi Shamir、Leonard Adleman 共同提出，三人于 2002 年因此获得图灵奖。算法利用了对大数进行质因子分解困难的特性，但目前还没有数学证明两者难度等价，或许存在未知算法可以绕过大数分解而进行解密。 ElGamal：由 Taher ElGamal 设计，利用了模运算下求离散对数困难的特性，比 RSA 产生密钥更快。被应用在 PGP 等安全工具中。 椭圆曲线算法（Elliptic Curve Cryptography，ECC）：应用最广也是强度最早的系列算法，基于对椭圆曲线上特定点进行特殊乘法逆运算（求离散对数）难以计算的特性。最早在 1985 年由 Neal Koblitz 和 Victor Miller 分别独立提出。ECC 系列算法具有多种国际标准（包括 ANSI X9.63、NIST FIPS 186-2、IEEE 1363-2000、ISO/IEC 14888-3 等），一般被认为具备较高的安全性，但加解密过程比较费时。其中，密码学家 Daniel J.Bernstein 于 2006 年提出的 Curve25519/Ed25519/X25519 等算法（分别解决加密、签名和密钥交换），由于其设计完全公开、性能突出等特点，近些年引起了广泛关注和应用。 SM2（ShangMi 2）：中国国家商用密码系列算法标准，由中国密码管理局于 2010 年 12 月 17 日发布，同样基于椭圆曲线算法，一般认为其安全强度优于 RSA 系列算法。 非对称加密算法适用于签名场景或密钥协商过程，但不适于大量数据的加解密。除了 SM2 之外，大部分算法的签名速度要比验签速度慢（1~2个数量级）。 RSA 类算法被认为已经很难抵御现代计算设备的破解，一般推荐商用场景下密钥至少为 2048 位。如果采用安全强度更高的椭圆曲线算法，256 位密钥即可满足绝大部分安全需求。 性质 公钥用于加密, 私钥用于解密 公钥由私钥生成，私钥可以推导出公钥 从公钥无法推导出私钥 优点 解决了密钥传输中的安全性问题 解决了数据源确定性的问题 缺点 没有解决数据可以中途被篡改的问题 消息认证码与数字签名消息认证码和数字签名技术通过对消息的摘要进行加密，可以防止消息被篡改和认证身份。 消息认证码定义 消息认证码（Hash-based Message Authentication Code，HMAC），利用对称加密，对消息完整性（Integrity）进行保护。 基本过程为对某个消息，利用提前共享的对称密钥和 Hash 算法进行处理，得到 HMAC 值。该 HMAC 值持有方可以向对方证明自己拥有某个对称密钥，并且确保所传输消息内容未被篡改。 典型的 HMAC 生成算法包括 K，H，M 三个参数。K 为提前共享的对称密钥，H 为提前商定的 Hash 算法（如 SHA-256），M 为要传输的消息内容。三个参数缺失了任何一个，都无法得到正确的 HMAC 值。 消息认证码可以用于简单证明身份的场景。如 Alice、Bob 提前共享了 K 和 H。Alice 需要知晓对方是否为 Bob，可发送一段消息 M 给 Bob。Bob 收到 M 后计算其 HMAC 值并返回给 Alice，Alice 检验收到 HMAC 值的正确性可以验证对方是否真是 Bob。当然，消息认证码起作用的前提是网络中没有中间人攻击的情况，假设网络是安全的，因此在公网中，一半不会使用消息认证码来进行身份验证； 消息认证码的主要问题是需要提前共享密钥，并且当密钥可能被多方同时拥有（甚至泄露）的场景下，无法追踪消息的真实来源。如果采用非对称加密算法，则能有效的解决这个问题，即数字签名。 缺点 不能解决密钥安全传递的问题 数字签名定义 类似在纸质合同上进行签名以确认合同内容和证明身份，数字签名既可以证实某数字内容的完整性，又可以确认其来源（即不可抵赖，Non-Repudiation）。 一个典型的场景是，Alice 通过信道发给 Bob 一个文件（一份信息），Bob 如何获知所收到的文件即为 Alice 发出的原始版本？Alice 可以先对文件内容进行摘要，然后用自己的私钥对摘要进行加密（签名），之后同时将文件和签名都发给 Bob。Bob 收到文件和签名后，用 Alice 的公钥来解密签名，得到数字摘要，与对文件进行摘要后的结果进行比对。如果一致，说明该文件确实是 Alice 发过来的（因为别人无法拥有 Alice 的私钥），并且文件内容没有被修改过（摘要结果一致）。 理论上所有的非对称加密算法都可以用来实现数字签名，实践中常用算法包括 1991 年 8 月 NIST 提出的 DSA（Digital Signature Algorithm，基于 ElGamal 算法）和安全强度更高的 ECSDA（Elliptic Curve Digital Signature Algorithm，基于椭圆曲线算法）等。 除普通的数字签名应用场景外，针对一些特定的安全需求，产生了一些特殊数字签名技术，包括盲签名、多重签名、群签名、环签名等。 数字签名的类型 盲签名 盲签名（Blind Signature），1982 年由 David Chaum 在论文《Blind Signatures for Untraceable Payment》中提出。签名者需要在无法看到原始内容的前提下对信息进行签名。 盲签名可以实现对所签名内容的保护，防止签名者看到原始内容；另一方面，盲签名还可以实现防止追踪（Unlinkability），签名者无法将签名内容和签名结果进行对应。典型的实现包括 RSA 盲签名算法等。 多重签名 多重签名（Multiple Signature），即 n 个签名者中，收集到至少 m 个（n &gt;= m &gt;= 1）的签名，即认为合法。 其中，n 是提供的公钥个数，m 是需要匹配公钥的最少的签名个数。 多重签名可以有效地被应用在多人投票共同决策的场景中。例如双方进行协商，第三方作为审核方。三方中任何两方达成一致即可完成协商。 比特币交易中就支持多重签名，可以实现多个人共同管理某个账户的比特币交易。 群签名 群签名（Group Signature），即某个群组内一个成员可以代表群组进行匿名签名。签名可以验证来自于该群组，却无法准确追踪到签名的是哪个成员。 群签名需要存在一个群管理员来添加新的群成员，因此存在群管理员可能追踪到签名成员身份的风险。 群签名最早在 1991 年由 David Chaum 和 Eugene van Heyst 提出。 环签名 环签名（Ring Signature），由 Rivest，Shamir 和 Tauman 三位密码学家在 2001 年首次提出。环签名属于一种简化的群签名。 签名者首先选定一个临时的签名者集合，集合中包括签名者自身。然后签名者利用自己的私钥和签名集合中其他人的公钥就可以独立的产生签名，而无需他人的帮助。签名者集合中的其他成员可能并不知道自己被包含在最终的签名中。 环签名在保护匿名性方面也具有很多用途。 优点 解决了密钥传输中的安全性问题 解决了数据中途可能被篡改的问题 解决了数据源验证的问题 证书授权中心-CA定义 对于非对称加密算法和数字签名来说，很重要的步骤就是公钥的分发。理论上任何人都可以获取到公开的公钥。然而这个公钥文件有没有可能是伪造的呢？传输过程中有没有可能被篡改呢？一旦公钥自身出了问题，则整个建立在其上的的安全性将不复成立。 数字证书机制正是为了解决这个问题，它就像日常生活中的证书一样，可以确保所记录信息的合法性。比如证明某个公钥是某个实体（个人或组织）拥有，并且确保任何篡改都能被检测出来，从而实现对用户公钥的安全分发。 根据所保护公钥的用途，数字证书可以分为加密数字证书（Encryption Certificate）和签名验证数字证书（Signature Certificate）。前者往往用于保护用于加密用途的公钥；后者则保护用于签名用途的公钥。两种类型的公钥也可以同时放在同一证书中。 一般情况下，证书需要由证书认证机构（Certification Authority，CA）来进行签发和背书。权威的商业证书认证机构包括 DigiCert、GlobalSign、VeriSign 等。用户也可以自行搭建本地 CA 系统，在私有网络中进行使用。 CA解决的问题 CA解决了电子商务中公钥的可信度的问题 负责证明 “我确实是我” CA是受信任的第三方，公钥的合法性校验 CA证书的内容 证书的持有人的公钥 证书授权中心的名称 证书的有效期 证书授权中心的数字签名 区块链的密码学应用区块链使用的密码学函数区块链技术的运行中使用了多项密码学函数，其中最主要的函数包括以下算法 哈希算法 数字签名 零知识证明 哈希算法在区块链系统中的应用哈希函数的特性 确定性：无论在同一个哈希函数中解析多少次，输入同一个A总是能得到相同的输出h(A)。 高效运算：计算哈希值的过程是高效的。 抗原像攻击（隐匿性）：对一个给定的输出结果h(A)，想要逆推出输入A，在计算上是不可行的。 抗碰撞性（抗弱碰撞性）：对任何给定的A和B，找到满足B≠A且h(A)=h(B)的B，在计算上是不可行的。 细微变化影响：任何输入端的细微变化都会对哈希函数的输出结果产生剧烈影响。 谜题友好性：对任意给定的Hash码Y和输入值x而言，找到一个满足h(k|x)=Y的k值在计算上是不可行的。 区块链数据结构区块链账本的数据结构和链表结构比较类似， 只不过区块指向的是老区块头部的hash值； 区块链的构成如下图： 区块链本质上是一个链表，其中的每个新区块都包含一个哈希指针。指针指向前一区块及其含有的所有数据的哈希值。假设修改某一个区块内容，即使是很微小的变化，那也会对其hash值产生很大的影响，导致区块不能和下一个区块组成链； 因此，每一个区块之间都有了不可篡改的特性。 梅克耳树默克尔树（又叫哈希树）是一种典型的二叉树结构，由一个根节点、一组中间节点和一组叶节点组成。默克尔树最早由 Merkle Ralf 在 1980 年提出，曾广泛用于文件系统和 P2P 系统中。 其主要特点为： 最下面的叶节点包含存储数据或其哈希值。 非叶子节点（包括中间节点和根节点）都是它的两个孩子节点内容的哈希值。 进一步地，默克尔树可以推广到多叉树的情形，此时非叶子节点的内容为它所有的孩子节点的内容的哈希值。 默克尔树逐层记录哈希值的特点，让它具有了一些独特的性质。例如，底层数据的任何变动，都会传递到其父节点，一层层沿着路径一直到树根。这意味树根的值实际上代表了对底层所有数据的“数字摘要”。 默克尔树主要功能有: 1. 快速比较大量数据； 2. 快速定位修改； 3. 零知识证明； 在区块链中每个区块都有自己的梅克尔根（Merkle Root）。现在，正如你已知道的，每个区块里都包含多笔交易。如果将这些交易按线性存储，那么在所有交易中寻找一笔特定交易的过程会变得无比冗长。 这就是我们使用梅克尔树的原因， 让交易在区块中变得容易寻找，由于hash的微细变化影响的特性，也让交易不能被修改。 在梅克尔树中，所有的交易通过哈希算法都能向上追溯至同一根。这就使得搜索变得非常容易。因此，如果想要在区块里面找到某一个指定的数据，可以直接通过梅克尔树里的哈希值来进行搜索，而不需要遍历所有的交易进行搜索。 比如，先要验证红色圆圈的交易数据是否在区块内，只需要提供蓝色圆圈的交易，就可以根据最终生成的hash值和梅克耳根对比，如果结果相同，就说明交易确实属于该区块，并且是正确的。 挖矿加密谜题被用来挖掘新的区块，因此哈希算法仍然至关重要。其工作原理是调整难度值的设定。随后，一个被命名为“nonce”的随机字符串被添加到新区块的哈希值上，然后被再次哈希。接着，再来检验其是否低于已设定的难度值水平。如果低于，那么产生的新区块会被添加至链上，而负责挖矿的矿工就会获得奖励。如果没有低于，则矿工继续修改随即字符串“nouce”，直至低于难度值水平的值出现。 正如你所见，哈希算法是区块链和加密经济学中一个至关重要的部分。 工作量证明概念 当矿工们通过“挖矿”来产生新区块并添加至区块链上时，其中验证及添加区块涉及到的共识系统被称为“工作量证明”。矿工们使用庞大的计算机算力来解决这道密码学谜题，而难度值决定了这道题的所需要的计算量。这是区块链技术中最具开拓意义的机制之一。早期的去中心化点对点数字货币系统之所以会失败，是由于“拜占庭将军问题”导致的，而工作量证明的共识系统为该问题提供了一种解决方案。 工作量证明实际上就是对hash算法消耗的算力的应用，让攻击者很难去在规定时间集中如此大的算力篡改数据内容； 拜占庭将军问题 假设有一群拜占庭将军想要攻打一座城市，他们将面临两个不同的问题： 每个将军及其军队在地理上相距甚远，因此不可能通过中央来统一指挥，这使得协同作战变得异常困难。 被攻打的城市拥有一只庞大的军队，他们能获得胜利的唯一方式是所有人在同一时刻一同发起进攻。 为了让合作成功，位于城堡左边的军队派遣一位信使，向城堡右边的军队发送了一则内容为“周三攻击”的信息， 如果所有军队都准备好了， 那就可以确定周三攻击。但是，如果邮编的军队没有做好攻击准备，并让信使携带一则内容为“不，周五攻击”的信息返回。而信使需要通过穿越被攻打的城市返回到左边的军队； 问题就来了。这位信使身上可能会发生很多事情。比如，他有可能被抓获、泄露信息、或被攻打的城市杀害后将其替换了。这将导致军队获得被篡改过的信息，从而使作战计划无法达成一致而失败。 上述例子对区块链有明显借鉴意义。区块链是一个巨型网络，你要如何信任他们呢？如果你想从钱包里发送4个以太币给某人，你如何确认网络中的某人不会篡改信息，将4个以太币改成40个， 或者更改了其接受者？中本聪发明了工作量证明机制来绕过拜占庭将军问题。其运行原理是：假设左边的军队想要发送内容为“周三进攻”的信息给右边的军队，他们需要执行如下步骤： 首先，他们会给初始文本添加一个“nonce”，这个nonce可以是任何一个随机十六进制值。 其次，他们将添加了“nonce”的文本进行哈希，得到一个结果。假设说他们决定仅当哈希结果前5位是零的时候，才进行信息共享。 如果哈希结果满足条件，他们就会让信使带着有哈希结果的信息出发。否则，他们会持续随机改变nonce的值，直到得到想要的结果。这一过程不仅冗长耗时，且占用大量的算力。 如果敌人抓到了信使，并企图篡改信息，那么根据哈希函数的特性，哈希结果将会剧烈变化。如果城市右边的将军看到信息没有以规定数量的0作为开头，那么他们就会叫停攻击。 工作量证明的本质 寻找一个符合哈希目标的nonce值，是一个非常困难且耗时的过程， 想要篡改需要浪费大量的算例。 然而，验证结果中是否有作恶行为却是非常简单的。 数字签名在区块链系统中的应用概念假设Alan想把信息“m”发送出去，Alan有一把私钥Ka-和一把公钥Ka+。那么，当他把信息发送给Tyrone时，他会用私钥将该条信息加密，于是信息变成了Ka-(m)。当Tyrone收到这条信息时，他可以使用Alan的公钥来取回信息，Ka+(Ka-(m))，于是便得到了原始信息“m”; 签名的作用就是用来保证信息在传输的过程中没有被修改过， 也可以用来验证信息确实是从发送者处发送出来。 数字签名使用的过程 Alan有一笔交易“m”，并且Tyrone知道他正在接收该笔交易。 Alan对m进行哈希运算，得到h(m)。 Alan用自己的私钥对哈希结果进行加密，得到Ka-(h(m))。 Alan将加密数据发送给Tyrone。 Tyrone使用Alan的公钥来解密，Ka+(Ka-(h(m)))，并得到原来的哈希结果h(m)。 Tyrone用已知的“m”进行哈希运算，可以得到h(m)。 哈希函数的确定性特征决定了如果h(m)=h(m)，就意味着这笔交易是真实有效的。 数字签名的特性 可验证性：如果加密信息能够用Alan的公钥进行解密，那就可以100%确定是Alan发送了该条信息。 不可伪造性：如果说有其他人，例如Bob，拦截了该条信息，并用自己的私钥发送了一条自己的信息，那么Alan的公钥将无法对其解密。Alan的公钥只能用来解密Alan用自己的私钥加密过的信息。 不可抵赖性：同样的，如果Alan宣称，“我没有发送信息，是Bob发的”，但Tyrone却能够用Alan的公钥来解密信息，那就证明Alan在撒谎。如此，Alan就无法收回他之前发出的信息，并将其归咎于他人。 零知识证明概念(ZKP)零知识证明是这样一个过程， 证明着不向验证着提供任何额外的信息的前提下， 使验证着相信某个论断是正确的。 ZKP意味着A可以向B证明，他知道特定的信息，而不必告诉对方自己具体知道些什么，这尤为有用，因为这将为证明者提供一层额外的隐私保护。 在区块链系统中，为了做到完全的隐私，对数据的验证可以通过零知识证明，对交易的详情等可以做到完全的隐藏，避免泄露信息； 零知识证明具备的性质 完整性：如果陈述属实，那么诚实的验证者能被诚实的证明者说服。 可靠性：如果证明者不诚实，他们无法通过说谎来说服验证者相信陈述是可靠的。零知识：如果陈述属实，那么验证者无法得知陈述的内容是什么。 举个栗子，证明者（P）对验证者（V）说，他知道洞穴后面暗门的密码，并提出在不向验证者透露密码的情况下证明此事。那么，其验证过程如下图所示： 证明者可以走路径A或者路径B，假设他们一开始决定通过路径A到达暗门。同时，验证者V来到入口，他对证明者选择哪条路径并不知情，并宣称他们希望见到证明者在路径B出现。 如图所示，证明者确实出现在路径B上，但万一这仅是巧合呢？也有可能是证明者凭运气在出发时选择了路径B，却因不知道密码被困在了门口。 所以，我们需要通过多次试验来确定测试的有效性。如果证明者每次都能出现在正确的路径上，那么证明者的确可以在不向验证者透露密码的情况下，证明自己知道密码。 零知识证明使用实例许多基于区块链的技术都在使用Zk-Snarks。事实上，以太坊在大都会阶段就计划引入Zk-Snarks，并且将其加入以太坊的功能库。Zk-Snarks是“零知识简洁无交互知识认证”的简称，是一种在无需泄露数据本身情况下证明某些数据运算的一种零知识证明。 以上内容可用来生成一个证明，通过对每笔交易创建一个简单的快照来验证其有效性。这足以向信息接收方证明交易的有效性，而无需泄露交易的实质内容。 通过零知识证明，解决了区块链的一些因为问题: 1.实现了交易的完整性和隐私性; 2.实现了系统的抽象性。由于无需展示整个交易内部的工作方式，因此系统非常易用。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链简介]]></title>
    <url>%2F2019%2F01%2F08%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F1.%E5%8C%BA%E5%9D%97%E9%93%BE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[区块链介绍区块链的历史上世纪 80 年代和 90 年代的匿名电子现金协议主要使用了一种被称为“乔姆盲签（Chaumian Blinding）”的加密技术，这种技术为这些新货币提供了很高的隐私保护，但是由于他们的基础协议在很大程序上需要依赖于一个中央中介，因此未能获得支持。1998 年，戴伟（Wei Dai）的 b-money 首次引入了通过解决计算难题和去中心化的共识来创造货币的思想，但是该建议并未给出实现去中心化的共识的具体方法。2005 年，芬尼（Hal Finney）引入了“可重复的工作证明”（reusable proofs of work）概念，它同时使用 b-money 的思想和 Adam Back 提出的计算困难的哈希现金（Hashcash）难题来创造密码学货币。但是，这种概念再次迷失于理想化，因为它需要可信任的计算作为后端。在2009 年，一种去中心化的货币由中本聪在实践中首次得到实现，通过使用公共密钥密码来管理所有权，并过一种一致性算法来跟踪货币的持有者，这种算法被称为“工作证明”。这“工作证明” 的算法是一种突破，因为它同时决了 2 个问题。首先，它提供了一种简单的、有节制的有效的共识算法，允许网络中的节点一起同意比特币总帐状一组更新。其次，它提供了一种机制，允许任何节点自由进入共识的处理进程，从而解决了谁来影响共识的政治难题，同时阻止女巫攻击。 这参与共识投票的节点的投票权比重是直接和节点它们自己的算力挂钩的。 这就是比特币，其核心思想就是建立一个共识机制，不会由某一个服务中心作为计算后端。 区块链的基本概念区块链是一种分布式账本，由不可更改的数字记录数据包构成。这些数据包被称作为区块。然后使用加密签名呢将每个区块 “链接” 到下一个区块。这样就使区块链像分类账本一样被使用，可以有具有适当权限的任何人共享和访问。 在区块中，我们会写入如下信息 谁在转账给谁 交易金额 签名等其他信息 区块的链接方式 ​ 假设我们有 3 个区块，包含如下信息： 区块 1 包含的信息为 I1，I1 的哈希值为 H1。 区块 2 包含的信息为 I2 ，I2 的哈希值为 H2。 区块 3 包含的信息为 I3 ，I3 的哈希值为 H3。 H2 是由 H1 和 I2 结合起来算出的。同样地，H3 是由 H2 和 I3 结合起来算出的，依此类推。 分布式去中心化分类账区块链是一个分布式的去中心化分类账，用来存储交易信息等数据，这些数据为整个区块链网络中的节点所共享。 分类账分类账是承载区块列表的主要记录载体。 存储数据区块能存储数据（信息）。此处对数据的定义很广泛，可以是我们能想到的任何数据。我们就拿交易信息这一数据来举个例子。 分布式的去中心化分类账数据处理通常由一个中心机器负责。但是区块链里有很多机器（因此它不是中心化的），且所有机器都是点对点相互连接。另外，这些机器维护的是同一本分类账。因此，区块链被称为分布式的去中心化分类账。 换句话说，因为同一区块链网络中的所有人都共享同一本分类账，所以说区块链是分布式的。每个人都有整个分类账的副本，一旦有什么东西添加进去，副本马上就会更新。 为整个区块链网络中的节点所共享在区块链网络中，所有机器全都相互连接。每个节点（机器）都持有相同的分类账副本。这就意味着整个区块链网络中的节点都共享一本分类账。 区块链是如何确保安全性的区块链利用密码学来生成数字签名，并通过数字签名的方式防止数据完整性。 当创建一个交易的时候，利用自己的私钥对信息进行加密创建一个数字签名 然后把交易(内含信息、公钥以及数字签名)提交到其他的临近节点进行审批。 在这一过程中，网络会利用公钥来解密数字签名，并从签名中提取信息。 如果原信息与上图所示的签名中提取出来的信息相匹配，就可以通过审批，否则就无法通过。 如果量信息不匹配， 就可能是一下原因: 原信息在中途被操控了。 生成数字签名时所用的私钥与所提供的公钥不匹配。 这就是区块链网络如何能够防止篡改的方法，因此区块链相对上是安全的。 区块链技术的应用 区块链货币兑换 通过电子加密货币可以低成本的实现货币兑换和汇款 数据存储 区块链的实质是分布式账本，并且具有不可更改的特性，因此非常适合金融科技行业的数据存储。从另一个角度看，区块链也是分布式数据库，可以满足个人用户数据存储的需要。 区块链物联网 通过分部署账本记录某个设备与其他设备、web服务、或者人类用户之间的数据交换，就可以跟踪他的历史。 投票系统 区块链很好的解决了无需依赖第三方而达成信任的问题，因此特别适合实现公众投票系统。 预测平台 通常情况下，群体的智慧大于少数个体。对未来发生的时间，群体的预测结果通常会更加准确。利用区块链全名参与、只能合约等特性，可以创造新型的预测平台 支付 、借贷 通过电子加密货币可以实现低成本的跨境支付与个人借贷。 区块链技术栈 共识机制 用来筛选和竞争出让哪个节点来进行记账，并广播给所有节点进行同步的机制； 密码算法 用来计算区块的哈希值来关联各个区块， 和计算每个区块交易事务的哈希值(梅克尔根) 网络路由 用来发现各个节点，实现节点之间的相互通讯，完成各节点数据的同步； 脚本系统(可以类比以太坊只能合约和比特币的UTXO) 一般用来驱动数据的收发，即一套数据的收发和处理规则；在不同的系统中也可以通过改编脚本系统程序。拓展区块链系统功能，比如以太坊就可以根据自定义功能的脚本系统，进而实现智能合约的功能； 区块链账本 用来记录数据，将数据以区块的方式记录下来, 每一个区块一般包含[1. 区块头；2.前一区块头的哈希值 ； 3. 梅克耳根(交易事务的哈希值) ]， 根据下图, 一般六个区块就能确定 一比交易成功； 中心服务器和区块链的区别第三方介入的缺点 交易成本高 对小额交易不友好 调解纠纷的成本被平摊到交易手续费 跨地域交易时需要额外费用 对卖家不公平 当发生纠纷时通常保护买家的利益 某些产品可以轻易复制，如文档，包含源代码的程序等。当产生退货时对卖家不利； 交易时间长 第三方核实交易需要时间较长 隐私暴露 商家会向客户索要完全不必要的个人信息 如何去除第三方 电子加密货币的核心思想 全民参与 让全网尽可能多的节点参与核实，记录交易 给参与见证的节点讲理，维持整个系统持续 运行 任何人都可以参与挖矿，拥有代币，发起交易 匿名&amp;公开 交易的双方无需透露个人信息 交易信息是公开，非加密的。根据钱包地址可查所有的交易记录 去中心化 采用p2p网络，所有节点都来自于互联网，解决了第三方单独进行数据处理的问题 区块链分类根据网络范围分类 公有链 完全对外开放，任何人都可以任意使用； 没有权限的设定，有没有身份的认证； 所有的数据参与其中的任何人都可以任意查看，完全的公开透明； 节点数量不固定，节点是否在线也无法控制，通过网络中大多数节点承认的链就是主链； 公有链中共识机制一般是工作量证明（POW）和权益证明（POS）; 最具代表的就是比特币； 私有链 不对外开发，仅仅在组织内部使用； 通常需要注册，或者需要身份认证； 数据只对私链中的节点可见； 节点的数量和节点的状态通常的可控的; 因为私有链的节点都有很高的信任度，一般不需要通过竞争的方式来筛选数据打包者，可以采用更加节能的方式进行筛选； 比较代表的应用就是企业的票据管理，账务审计，供应链管理等系统； 联盟链 联盟链的网络范围介于公有链和私有链之间， 仅限联盟成员使用； 可以给不同的联盟成员设定不同的权限，所以记账规则和数据权限都可以私人订制； 一般也是具有身份认证和权限设置的； 节点的数量和节点的状态通常的可控的； 联盟链几乎不采用工作量证明共识机制而是采用权益证明或PBTF等共识算法; 代表的应用有银行之间的支付结算、企业之间的物流、政府机关的对外数据公开系统等； 根据部署环境分类 主链 部署在正式生产环境的区块链系统； 测试链 部署在测试环境，用来开发和测试Bug的区块链系统； 根据对接类型分类 单链 能够单独运行的区块链系统都可以称之为 “单链”，例如比特币主链，测试链， 这些区块链系统拥有完备的组件模块，自称一个体系； 侧链 区块链系统与侧链系统本身就是一个独立的链系统，两者之间可以按照一定的协议进行数据互动，通过这种方式，侧链能够起到一个对主链进行功能拓展的作用，很多在主链中不方便实现的功能可以实现在侧链中，而侧链再通过与主链的数据交互增强自己的可靠性； 互联链(多链) 就是区块链系统之间的互联，通过各自的优势，彼此互补，可以大大增强了系统的可靠性以及性能； 区块链技术特点 数据不可篡改性 区块链系统不是一个中心化软件设施，因此数据没有被某一家机构控制，数据肯定不可能被第三方篡改； 分布式存储 在区块链系统中，每个运行的节点都拥有一份完整的数据副本，这样的设计不仅避免了存储的单节点故障问题，还可以让每个节点能够独立的验证和检索数据，大大增加了整个系统的可靠性，节点之间的数据副本还可以互相保持同步，并使用类似梅克尔树这样的技术结构保证数据的完整性和一致性； 匿名性 使用传统的服务软件时，通常都是需要注册一个用户名，绑定手机号等，进行一些认证等；但是在区块链系统中，目前几乎所有的区块链产品都是使用所谓的地址来表示用户的，不需要提供其他的任何能表示出用户身份的信息，地址通常也是通过公开密钥算法生成的公钥转换而来的，这通常就是一串如乱码一般的字符串，因为即使这些公链系统是完全公开透明的，我们却不知道背后的操作者是谁； 价值传递 以比特币为例，比特币是一种数字资产，他是由比特币软件组成的网络所维护的，在这个网络中，不需要其他的第三方，自己可以根据规则发型比特币，并且能够确保发行的比特币是具有价值的(工作量证明)，而这种价值的认定是通过网络中所有的节点来自动进行验证的，节点之间打成公式就算认可了，整个过程都是自成一个体系来运行的，人们在交易比特币的时候就产生了价值传递； 区块链系统是可以自己创造信任机制的，无需第三方信任的环境中，大大简化了各种交易个过程，降低了交易的成本； 自动网络共识 生活中一半得事情都需要双方或者多方达成共识，比如签订一份合同，都是在多方达成共识，并且需要做各种确认；比特币从发型到转账交易，都是由网络中的节点自动及逆行身份认证和一系列的检查的，检查通过后就打成了网络共识，因为每个节点都遵守一份共同的约定和规则，只要意向交易符合所有的约定规则就能被确认； 可编程合约 比如比特币，在比特币系统中，并不是想银行账户一样，将金额存储在账户下就代表用户拥有的，而是通过脚本解锁和锁定一比资产，简单地说，就是让资产具备更强的编程可控能力，比如配置程序，让一比资产需要多个人共同签名才能被转移或者需要达到某个条件的时候才能被使用，这就是编程合约的思想。区块链系统具有数据不可篡改，价值传递等能力，加上编程合约，就能完成商业上各种的需求； 软分叉和硬分叉软分叉软分叉：是指区块链网络系统版本或协议升级后，旧的节点并不会意识到比特币代码发生改变，并继续接受由新节点创造的区块，新老节点始终还是在同一条链上工作。 硬分叉是指比特币区块格式或交易格式（共识机制）发生改变时，未升级的节点拒绝验证已经升级的节点产生的区块，然后大家各自延续自己认为正确的链，所以分成两条链。 产生两个币种，如BCC]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine原理以及channel使用]]></title>
    <url>%2F2019%2F01%2F05%2F11.Golang%2FGolang%E7%89%B9%E6%80%A7%2Fgoroutine%E5%8E%9F%E7%90%86%E5%92%8Cchannel%2F</url>
    <content type="text"><![CDATA[一、goroutine作用goroutine的本质是协程，是实现并行计算的核心，和python的async编程类似，但是又不需要我们手动的去创建携程并通过实践循环来启动。goroutine异步执行函数，只需使用go关键字+函数名即可启动一个协程，函数则直接处于异步执行的状态。 1go func()//通过go关键字启动一个协程来运行函数 二、goroutine的原理携程概念介绍协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此，协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。线程和进程的操作是由程序触发系统接口，最后的执行者是系统；协程的操作执行者则是用户自身程序，goroutine也是协程。 调度模型简介groutine是通过GPM调度模型实现，下面就来解释下goroutine的调度模型。 Go的调度器内部有四个重要的结构：M，P，S，Sched，如上图所示（Sched未给出）M:M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。P:P全称是Processor，处理器，它的主要用途就是用来执行goroutine的，所以它也维护了一个goroutine队列，里面存储了所有需要它来执行的goroutineSched：代表调度器，它维护有存储M和G的队列以及调度器的一些状态信息等。 调度实现 从上图中看，有2个物理线程M，每一个M都拥有一个处理器P，每一个也都有一个正在运行的goroutine。P的数量可以通过GOMAXPROCS()来设置，它其实也就代表了真正的并发度，即有多少个goroutine可以同时运行。图中灰色的那些goroutine并没有运行，而是出于ready的就绪态，正在等待被调度。P维护着这个队列（称之为runqueue），Go语言里，启动一个goroutine很容易：go function 就行，所以每有一个go语句被执行，runqueue队列就在其末尾加入一个goroutine，在下一个调度点，就从runqueue中取出 当一个OS线程M0陷入阻塞时(例如爬虫或者其他的io操作， 这里就和python的多线程类似)（如下图)，P转而在运行M1，图中的M1可能是正被创建，或者从线程缓存中取出。 当MO返回时，它必须尝试取得一个P来运行goroutine，一般情况下，它会从其他的OS线程那里拿一个P过来，如果没有拿到的话，它就把goroutine放在一个global runqueue里，然后自己睡眠（放入线程缓存里）。所有的P也会周期性的检查global runqueue并运行其中的goroutine，否则global runqueue上的goroutine永远无法执行。 另一种情况是P所分配的任务G很快就执行完了（分配不均），这就导致了这个处理器P很忙，但是其他的P还有任务，此时如果global runqueue没有任务G了，那么P不得不从其他的P里拿一些G来执行。一般来说，如果P从其他的P那里要拿任务的话，一般就拿runqueue的一半，这就确保了每个OS线程都能充分的使用，如下图： 三、goroutine使用基本使用设置goroutine运行的CPU数量，最新版本的go已经默认已经设置了。 12num := runtime.NumCPU() //获取主机的逻辑CPU个数runtime.GOMAXPROCS(num) //设置可同时执行的最大CPU数，即设置多少个P 使用示例 123456789101112131415161718192021222324252627282930package mainimport ( "fmt" "time")func cal(a int , b int ) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c)&#125;func main() &#123; for i :=0 ; i&lt;10 ;i++&#123; go cal(i,i+1) //启动10个goroutine 来计算 &#125; time.Sleep(time.Second * 2) // sleep作用是为了等待所有任务完成&#125; //结果//8 + 9 = 17//9 + 10 = 19//4 + 5 = 9//5 + 6 = 11//0 + 1 = 1//1 + 2 = 3//2 + 3 = 5//3 + 4 = 7//7 + 8 = 15//6 + 7 = 13 goroutine异常捕捉当启动多个goroutine时，如果其中一个goroutine异常了，并且我们并没有对进行异常处理，那么整个程序都会终止，所以我们在编写程序时候最好每个goroutine所运行的函数都做异常处理，异常处理采用recover 123456789101112131415161718192021222324252627282930313233343536package mainimport ( "fmt" "time")func addele(a []int ,i int) &#123; defer func() &#123; //匿名函数捕获错误 err := recover() if err != nil &#123; fmt.Println("add ele fail") &#125; &#125;() a[i]=i fmt.Println(a)&#125;func main() &#123; Arry := make([]int,4) for i :=0 ; i&lt;10 ;i++&#123; go addele(Arry,i) &#125; time.Sleep(time.Second * 2)&#125;//结果add ele fail[0 0 0 0][0 1 0 0][0 1 2 0][0 1 2 3]add ele failadd ele failadd ele failadd ele failadd ele fail 同步的goroutine由于goroutine是异步执行的，那很有可能出现主程序退出时还有goroutine没有执行完，此时goroutine也会跟着退出。此时如果想等到所有goroutine任务执行完毕才退出，go提供了sync包和channel来解决同步问题，当然如果你能预测每个goroutine执行的时间，你还可以通过time.Sleep方式等待所有的groutine执行完成以后在退出程序(如上面的列子)。 方法一：使用sync包同步goroutine 这种方法和python的Thread.join() 很类似WaitGroup 等待一组goroutinue执行完毕. 主程序调用 Add 添加等待的goroutinue数量. 每个goroutinue在执行结束时调用 Done ，此时等待队列数量减1.，主程序通过Wait阻塞，直到等待队列为0. 123456789101112131415161718192021222324252627282930313233package mainimport ( "fmt" "sync")func cal(a int , b int ,n *sync.WaitGroup) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c) defer n.Done() //goroutinue完成后, WaitGroup的计数-1&#125;func main() &#123; var go_sync sync.WaitGroup //声明一个WaitGroup变量 for i :=0 ; i&lt;10 ;i++&#123; go_sync.Add(1) // WaitGroup的计数加1 go cal(i,i+1,&amp;go_sync) &#125; go_sync.Wait() //等待所有goroutine执行完毕&#125;//结果9 + 10 = 192 + 3 = 53 + 4 = 74 + 5 = 95 + 6 = 111 + 2 = 36 + 7 = 137 + 8 = 150 + 1 = 18 + 9 = 17 示例二：通过channel实现goroutine之间的同步。 这种方法类似于python的Thread.Queue()通讯，来实现异步程序和主程序之间的数据交换实现方式：通过channel能在多个groutine之间通讯，当一个goroutine完成时候向channel发送退出信号,等所有goroutine退出时候，利用for循环channe去channel中的信号，若取不到数据会阻塞原理，等待所有goroutine执行完毕，使用该方法有个前提是你已经知道了你启动了多少个goroutine。 12345678910111213141516171819202122232425package mainimport ( "fmt" "time")func cal(a int , b int ,Exitchan chan bool) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c) time.Sleep(time.Second*2) Exitchan &lt;- true&#125;func main() &#123; Exitchan := make(chan bool,10) //声明并分配管道内存 for i :=0 ; i&lt;10 ;i++&#123; go cal(i,i+1,Exitchan) &#125; for j :=0; j&lt;10; j++&#123; &lt;- Exitchan //取信号数据，如果取不到则会阻塞 &#125; close(Exitchan) // 关闭管道&#125; goroutine之间的通讯goroutine本质上是协程，可以理解为不受内核调度，而受go调度器管理的线程。goroutine之间可以通过channel进行通信或者说是数据共享，当然你也可以使用全局变量来进行数据共享。 示例：使用channel模拟消费者和生产者模式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport ( "fmt" "sync")func Productor(mychan chan int,data int,wait *sync.WaitGroup) &#123; mychan &lt;- data fmt.Println("product data：",data) wait.Done()&#125;func Consumer(mychan chan int,wait *sync.WaitGroup) &#123; a := &lt;- mychan fmt.Println("consumer data：",a) wait.Done()&#125;func main() &#123; datachan := make(chan int, 100) //通讯数据管道 var wg sync.WaitGroup for i := 0; i &lt; 10; i++ &#123; go Productor(datachan, i,&amp;wg) //生产数据 wg.Add(1) &#125; for j := 0; j &lt; 10; j++ &#123; go Consumer(datachan,&amp;wg) //消费数据 wg.Add(1) &#125; wg.Wait()&#125;//结果consumer data： 4product data： 5product data： 6product data： 7product data： 8product data： 9consumer data： 1consumer data： 5consumer data： 6consumer data： 7consumer data： 8consumer data： 9product data： 2consumer data： 2product data： 3consumer data： 3product data： 4consumer data： 0product data： 0product data： 1 四、Golang - channel简介channel俗称管道，用于数据传递或数据共享，其本质是一个先进先出的队列，使用goroutine+channel进行数据通讯简单高效，同时也线程安全，多个goroutine可同时修改一个channel，不需要加锁。 channel可分为三种类型：只读channel：只能读channel里面数据，不可写入只写channel：只能写数据，不可读一般channel：可读可写 channel使用定义和声明12345678910// 定义var readOnlyChan &lt;-chan int // 只读chanvar writeOnlyChan chan&lt;- int // 只写chanvar mychan chan int //读写channel//声明read_only := make (&lt;-chan int,10)//定义只读的channel， 有缓存write_only := make (chan&lt;- int,10)//定义只写的channel， 有缓存read_write := make (chan int,10)//可同时读写， 有缓存read_write := make (chan int)//可同时读写， 无缓存 读写数据需要注意 如果没有使用goroutine，如果未关闭channel，读取数据会产生deadlock异常 管道如果关闭后再继续写入数据会pannic 如果是有缓存channel，当管道中没有数据时候再行读取会读取到默认值，如int类型默认值是0 1234ch &lt;- "wd" //写数据a := &lt;- ch //读取数据a, ok := &lt;-ch //优雅的读取数据for v := range ch &#123;&#125; // 遍历已经关闭的channel 循环管道需要注意 使用range循环管道，如果管道未关闭会引发deadlock错误。 如果采用for range 循环已经关闭的管道，当管道没有数据时候，读取的数据会是管道的默认值，并且循环不会退出。 12345678910111213141516171819package mainimport ( "fmt" "time")func main() &#123; mychannel := make(chan int,10) for i := 0;i &lt; 10;i++&#123; mychannel &lt;- i &#125; close(mychannel) //关闭管道 fmt.Println("data lenght: ",len(mychannel)) for v := range mychannel &#123; //循环管道 fmt.Println(v) &#125; fmt.Printf("data lenght: %d",len(mychannel))&#125; 带缓冲区channe和不带缓冲区channel带缓冲区channel：定义声明时候制定了缓冲区大小(长度)，可以保存多个数据。不带缓冲区channel：只能存一个数据，并且只有当该数据被取出才能存下个数据 12ch := make(chan int) //不带缓冲区ch := make(chan int ,10) //带缓冲区 不带缓冲区示例：12345678910111213141516171819202122232425262728293031323334353637383940package mainimport "fmt"func test(c chan int) &#123; for i := 0; i &lt; 10; i++ &#123; fmt.Println("send ", i) c &lt;- i &#125;&#125;func main() &#123; ch := make(chan int) go test(ch) for j := 0; j &lt; 10; j++ &#123; fmt.Println("get ", &lt;-ch) &#125;&#125;//结果：send 0send 1get 0get 1send 2send 3get 2get 3send 4send 5get 4get 5send 6send 7get 6get 7send 8send 9get 8get 9 Channel的deadlock一个死锁的例子:1234func main() &#123; ch := make(chan int) &lt;- ch // 阻塞main goroutine, 信道ch被锁&#125; 执行这个程序你会看到Go报这样的错误:1fatal error: all goroutines are asleep - deadlock! 何谓死锁? 操作系统有讲过的，所有的线程或进程都在等待资源的释放。如上的程序中, 只有一个goroutine, 所以当你向里面加数据或者存数据的话，都会锁死信道， 并且阻塞当前 goroutine, 也就是所有的goroutine(其实就main线一个)都在等待信道的开放(没人拿走数据信道是不会开放的)，也就是死锁咯。 其实，总结来看，为什么会死锁？非缓冲信道上如果发生了流入无流出，或者流出无流入，也就导致了死锁。或者这样理解 Go启动的所有goroutine里的非缓冲信道一定要一个线里存数据，一个线里取数据，要成对才行 那么死锁的解决办法呢？最简单的，把没取走的数据取走，没放入的数据放入， 因为无缓冲信道不能承载数据，那么就赶紧拿走！ Go的deadlock 信道数据你也许发现，上面的代码一个一个地去读取信道简直太费事了，Go语言允许我们使用range来读取信道: 12345678910func main() &#123; ch := make(chan int, 3) ch &lt;- 1 ch &lt;- 2 ch &lt;- 3 for v := range ch &#123; fmt.Println(v) &#125;&#125; 如果你执行了上面的代码，会报死锁错误的，原因是range不等到信道关闭是不会结束读取的。也就是如果 缓冲信道干涸了，那么range就会阻塞当前goroutine, 所以死锁咯。 那么，我们试着避免这种情况，比较容易想到的是读到信道为空的时候就结束读取:12345678910ch := make(chan int, 3)ch &lt;- 1ch &lt;- 2ch &lt;- 3for v := range ch &#123; fmt.Println(v) if len(ch) &lt;= 0 &#123; // 如果现有数据量为0，跳出循环 break &#125;&#125; 以上的方法是可以正常输出的，但是注意检查信道大小的方法不能在信道存取都在发生的时候用于取出所有数据，这个例子 是因为我们只在ch中存了数据，现在一个一个往外取，信道大小是递减的。 另一个方式是显式地关闭信道:1234567891011ch := make(chan int, 3)ch &lt;- 1ch &lt;- 2ch &lt;- 3// 显式地关闭信道close(ch)for v := range ch &#123; fmt.Println(v)&#125; 被关闭的信道会禁止数据流入, 是只读的。我们仍然可以从关闭的信道中取出数据，但是不能再写入数据了。 channel实现作业池我们创建三个channel，一个channel用于接受任务，一个channel用于保持结果，还有个channel用于决定程序退出的时候。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( "fmt")func Task(taskch, resch chan int, exitch chan bool) &#123; defer func() &#123; //异常处理 err := recover() if err != nil &#123; fmt.Println("do task error：", err) return &#125; &#125;() for t := range taskch &#123; // 处理任务 fmt.Println("do task :", t) resch &lt;- t // &#125; exitch &lt;- true //处理完发送退出信号&#125;func main() &#123; taskch := make(chan int, 20) //任务管道 resch := make(chan int, 20) //结果管道 exitch := make(chan bool, 5) //退出管道 go func() &#123; for i := 0; i &lt; 10; i++ &#123; taskch &lt;- i &#125; close(taskch) &#125;() for i := 0; i &lt; 5; i++ &#123; //启动5个goroutine做任务 go Task(taskch, resch, exitch) &#125; go func() &#123; //等5个goroutine结束 for i := 0; i &lt; 5; i++ &#123; &lt;-exitch &#125; close(resch) //任务处理完成关闭结果管道，不然range报错 close(exitch) //关闭退出管道 &#125;() for res := range resch&#123; //打印结果 fmt.Println("task res：",res) &#125;&#125; 只读channel和只写channel一般定义只读和只写的管道意义不大，更多时候我们可以在参数传递时候指明管道可读还是可写，即使当前管道是可读写的。 1234567891011121314151617181920212223242526package mainimport ( "fmt" "time")//只能向chan里写数据func send(c chan&lt;- int) &#123; for i := 0; i &lt; 10; i++ &#123; c &lt;- i &#125;&#125;//只能取channel中的数据func get(c &lt;-chan int) &#123; for i := range c &#123; fmt.Println(i) &#125;&#125;func main() &#123; c := make(chan int) go send(c) go get(c) time.Sleep(time.Second*1)&#125;//结果 select-case实现非阻塞channel原理通过select+case加入一组管道，当满足（这里说的满足意思是有数据可读或者可写)select中的某个case时候，那么该case返回，若都不满足case，则走default分支。 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt")func send(c chan int) &#123; for i :=1 ; i&lt;10 ;i++ &#123; c &lt;-i fmt.Println("send data : ",i) &#125;&#125;func main() &#123; resch := make(chan int,20) strch := make(chan string,10) go send(resch) strch &lt;- "wd" select &#123; case a := &lt;-resch: fmt.Println("get data : ", a) case b := &lt;-strch: fmt.Println("get data : ", b) default: fmt.Println("no channel actvie") &#125;&#125;//结果：get data : wd channel频率控制在对channel进行读写的时，go还提供了非常人性化的操作，那就是对读写的频率控制，通过time.Ticke实现 示例： 123456789101112131415161718192021222324package mainimport ( "time" "fmt")func main()&#123; requests:= make(chan int ,5) for i:=1;i&lt;5;i++&#123; requests&lt;-i &#125; close(requests) limiter := time.Tick(time.Second*1) for req:=range requests&#123; &lt;-limiter fmt.Println("requets",req,time.Now()) //执行到这里，需要隔1秒才继续往下执行，time.Tick(timer)上面已定义 &#125;&#125;//结果：requets 1 2018-07-06 10:17:35.98056403 +0800 CST m=+1.004248763requets 2 2018-07-06 10:17:36.978123472 +0800 CST m=+2.001798205requets 3 2018-07-06 10:17:37.980869517 +0800 CST m=+3.004544250requets 4 2018-07-06 10:17:38.976868836 +0800 CST m=+4.000533569]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang特性</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang reflect interface 理解]]></title>
    <url>%2F2018%2F12%2F22%2F11.Golang%2FGolang%E7%89%B9%E6%80%A7%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%8F%8D%E5%B0%84%E7%9A%84%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[一、Golang的interface interface是方法的集合， 其方法不需要实现 interface是一种类型，并且是指针类型 interface的更重要的作用在于多态实现 interface定义方法12345type 接口名称 interface &#123;method1 (参数列表) 返回值列表method2 (参数列表) 返回值列表...&#125; interface使用 接口的使用不仅仅针对结构体，自定义类型、变量等等都可以实现接口。 如果一个接口没有任何方法，我们称为空接口，由于空接口没有方法，所以任何类型都实现了空接口。 要实现一个接口，必须实现该接口里面的所有方法。 1234567891011121314151617181920212223242526272829303132package mainimport "fmt"//定义接口type Skills interface &#123; Running() Getname() string&#125;type Student struct &#123; Name string Age int&#125;// 实现接口func (p Student) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Student) Running() &#123; // 实现 Running方法 fmt.Printf("%s running",p.Name)&#125;func main() &#123; var skill Skills var stu1 Student stu1.Name = "wd" stu1.Age = 22 skill = stu1 skill.Running() //调用接口&#125; interface嵌套go语言中的接口可以嵌套，可以理解为继承，子接口拥有父接口的所有方法，并且想要使用该子接口的话，必须将父接口和子接口的所有方法都实现。 12345678910type Skills interface &#123; Running() Getname() string&#125;type Test interface &#123; sleeping() Skills //继承Skills&#125; 多态的概念上面提到了，go语言中interface是实现多态的一种形式，所多态，就是一种事物的多种形态，就像python调用方法的时候，我们不需要关注传入参数的类型，我们只需要让传入的参数可以被方法使用就可以。 同一个interface，不同的类型实现，都可以进行调用，它们都按照统一接口进行操作。 在上面的示例中，我们增加一个Teacher结构体，同样实现接口进行说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport "fmt"type Skills interface &#123; Running() Getname() string&#125;type Student struct &#123; Name string Age int&#125;type Teacher struct &#123; Name string Salary int&#125;func (p Student) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Student) Running() &#123; // 实现 Running方法 fmt.Printf("%s running",p.Name)&#125;func (p Teacher) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Teacher) Running() &#123; // 实现 Running方法 fmt.Printf("\n%s running",p.Name)&#125;func main() &#123; var skill Skills var stu1 Student var t1 Teacher t1.Name = "wang" stu1.Name = "wd" stu1.Age = 22 skill = stu1 skill.Running() skill = t1 t1.Running()&#125;//wd running//wang running 通过interface 判断类型&amp;类型转换由于接口是一般类型，当我们使用接口时候可能不知道它是那个类型实现的，基本数据类型我们有对应的方法进行类型转换，当然接口类型也有类型转换。 当然我们也可以用这个方式来进行类型的判断。 类型转换示例： 12345var s intvar x interfacex = sy , ok := x.(int) //将interface 转为int,ok可省略 但是省略以后转换失败会报错，true转换成功，false转换失败, 并采用默认值 类型判断示例： 12345678910111213141516package mainimport "fmt"func main() &#123; var x interface&#123;&#125; s := "WD" x = s y,ok := x.(int) z,ok1 := x.(string) fmt.Println(y,ok) fmt.Println(z,ok1)&#125;//0 false//WD true 类型判断示例： 1234567891011121314151617181920212223242526272829303132333435package mainimport "fmt"type Student struct &#123; Name string&#125;func TestType(items ...interface&#123;&#125;) &#123; for k, v := range items &#123; switch v.(type) &#123; case string: fmt.Printf("type is string, %d[%v]\n", k, v) case bool: fmt.Printf("type is bool, %d[%v]\n", k, v) case int: fmt.Printf("type is int, %d[%v]\n", k, v) case float32, float64: fmt.Printf("type is float, %d[%v]\n", k, v) case Student: fmt.Printf("type is Student, %d[%v]\n", k, v) case *Student: fmt.Printf("type is Student, %d[%p]\n", k, v) &#125;&#125;&#125;func main() &#123; var stu StudentTestType("WD", 100, stu,3.3)&#125;//type is string, 0[WD]//type is int, 1[100]//type is Student, 2[&#123;&#125;]//type is float, 3[3.3] interface 和 reflect反射Golang的变量包含 type 和value两部分 type 包括 static type和concrete type. static type就类似python的不可变类型(如int、string)，concrete type是runtime系统看见的类型 类型断言能否成功，取决于变量的concrete type，而不是static type. Golang的静态类型和interface类型 Golang的指定类型的变量的type是静态的（也就是指定int、string这些的变量，它的type是static type），在创建变量的时候就已经确定 Golang的interface的type是concrete type。 在Golang的实现中，每个interface变量都有一个对应pair，pair中记录了实际变量的值和类型(value, type): Golang的反射 interface及其pair的存在，是Golang中实现反射的前提，理解了pair，就更容易理解反射。反射就是用来检测存储在接口变量内部(值value；类型concrete type) pair对的一种机制。 value是实际变量值，type是实际变量的类型。一个interface{}类型的变量包含了2个指针，一个指针指向值的类型【对应concrete type】，另外一个指针指向实际的值【对应value】。 二、反射reflect反射是程序执行时检查其所拥有的结构。尤其是类型的一种能力。这是元编程的一种形式。它同一时候也是造成混淆的重要来源。 每一个语言的反射模型都不同， python比较类似反射的例子就是类的 hasattr 方法， 用来检测类是否包含某个方法或者属性 go语言中的反射通过refect包实现，reflect包实现了运行时反射，允许程序操作任意类型的对象。 reflect.TypeType：Type类型用来表示一个go类型。 不是所有go类型的Type值都能使用所有方法。请参见每个方法的文档获取使用限制。在调用有分类限定的方法时，应先使用Kind方法获知类型的分类。调用该分类不支持的方法会导致运行时的panic。 获取Type对象的方法： 1func TypeOf(i interface&#123;&#125;) Type 示例： 123456789101112package mainimport ( "reflect" "fmt")func main() &#123; str := "wd" res_type := reflect.TypeOf(str) fmt.Println(res_type) //string&#125; reflect.Type的方法通用方法： 12345678910111213141516171819202122232425262728293031323334// 通用方法func (t *rtype) String() string // 获取 t 类型的字符串描述，不要通过 String 来判断两种类型是否一致。func (t *rtype) Name() string // 获取 t 类型在其包中定义的名称，未命名类型则返回空字符串。func (t *rtype) PkgPath() string // 获取 t 类型所在包的名称，未命名类型则返回空字符串。func (t *rtype) Kind() reflect.Kind // 获取 t 类型的类别。func (t *rtype) Size() uintptr // 获取 t 类型的值在分配内存时的大小，功能和 unsafe.SizeOf 一样。func (t *rtype) Align() int // 获取 t 类型的值在分配内存时的字节对齐值。func (t *rtype) FieldAlign() int // 获取 t 类型的值作为结构体字段时的字节对齐值。func (t *rtype) NumMethod() int // 获取 t 类型的方法数量。func (t *rtype) Method() reflect.Method // 根据索引获取 t 类型的方法，如果方法不存在，则 panic。// 如果 t 是一个实际的类型，则返回值的 Type 和 Func 字段会列出接收者。// 如果 t 只是一个接口，则返回值的 Type 不列出接收者，Func 为空值。func (t *rtype) MethodByName(string) (reflect.Method, bool) // 根据名称获取 t 类型的方法。func (t *rtype) Implements(u reflect.Type) bool // 判断 t 类型是否实现了 u 接口。func (t *rtype) ConvertibleTo(u reflect.Type) bool // 判断 t 类型的值可否转换为 u 类型。func (t *rtype) AssignableTo(u reflect.Type) bool // 判断 t 类型的值可否赋值给 u 类型。func (t *rtype) Comparable() bool // 判断 t 类型的值可否进行比较操作####注意对于：数组、切片、映射、通道、指针、接口 func (t *rtype) Elem() reflect.Type // 获取元素类型、获取指针所指对象类型，获取接口的动态类型 示例： 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; inf := new(Skills) stu_type := reflect.TypeOf(stu1) inf_type := reflect.TypeOf(inf).Elem() // 特别说明，引用类型需要用Elem()获取指针所指的对象类型 fmt.Println(stu_type.String()) //main.Student fmt.Println(stu_type.Name()) //Student fmt.Println(stu_type.PkgPath()) //main fmt.Println(stu_type.Kind()) //struct fmt.Println(stu_type.Size()) //24 fmt.Println(inf_type.NumMethod()) //2 fmt.Println(inf_type.Method(0),inf_type.Method(0).Name) // &#123;reading main func() &lt;invalid Value&gt; 0&#125; reading fmt.Println(inf_type.MethodByName("reading")) //&#123;reading main func() &lt;invalid Value&gt; 0&#125; true&#125; 其他方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 数值func (t *rtype) Bits() int // 获取数值类型的位宽，t 必须是整型、浮点型、复数型------------------------------// 数组func (t *rtype) Len() int // 获取数组的元素个数------------------------------// 映射func (t *rtype) Key() reflect.Type // 获取映射的键类型------------------------------// 通道func (t *rtype) ChanDir() reflect.ChanDir // 获取通道的方向------------------------------// 结构体func (t *rtype) NumField() int // 获取字段数量func (t *rtype) Field(int) reflect.StructField // 根据索引获取字段func (t *rtype) FieldByName(string) (reflect.StructField, bool) // 根据名称获取字段func (t *rtype) FieldByNameFunc(match func(string) bool) (reflect.StructField, bool) // 根据指定的匹配函数 math 获取字段func (t *rtype) FieldByIndex(index []int) reflect.StructField // 根据索引链获取嵌套字段------------------------------// 函数func (t *rtype) NumIn() int // 获取函数的参数数量func (t *rtype) In(int) reflect.Type // 根据索引获取函数的参数信息func (t *rtype) NumOut() int // 获取函数的返回值数量func (t *rtype) Out(int) reflect.Type // 根据索引获取函数的返回值信息func (t *rtype) IsVariadic() bool // 判断函数是否具有可变参数。// 如果有可变参数，则 t.In(t.NumIn()-1) 将返回一个切片。 示例： 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; stu_type := reflect.TypeOf(stu1) fmt.Println(stu_type.NumField()) //2 fmt.Println(stu_type.Field(0)) //&#123;Name string 0 [0] false&#125; fmt.Println(stu_type.FieldByName("Age")) //&#123;&#123;Age int 16 [1] false&#125; true&#125; reflect.Value不是所有go类型值的Value表示都能使用所有方法。请参见每个方法的文档获取使用限制。在调用有分类限定的方法时，应先使用Kind方法获知该值的分类。调用该分类不支持的方法会导致运行时的panic。 Value为go值提供了反射接口，获取Value对象方法： 1func ValueOf(i interface&#123;&#125;) Value 示例： 123str := "wd"val := reflect.ValueOf(str)//wd reflect.Value的方法reflect.Value.Kind()：获取变量类别，返回常量 123456789101112package mainimport ("reflect" "fmt")func main() &#123; str := "wd" val := reflect.ValueOf(str).Kind() fmt.Println(val)//string&#125; 用于获取值方法： 1234567891011121314151617181920212223242526272829303132333435363738func (v Value) Int() int64 // 获取int类型值，如果 v 值不是有符号整型，则 panic。func (v Value) Uint() uint64 // 获取unit类型的值，如果 v 值不是无符号整型（包括 uintptr），则 panic。func (v Value) Float() float64 // 获取float类型的值，如果 v 值不是浮点型，则 panic。func (v Value) Complex() complex128 // 获取复数类型的值，如果 v 值不是复数型，则 panic。func (v Value) Bool() bool // 获取布尔类型的值，如果 v 值不是布尔型，则 panic。func (v Value) Len() int // 获取 v 值的长度，v 值必须是字符串、数组、切片、映射、通道。func (v Value) Cap() int // 获取 v 值的容量，v 值必须是数值、切片、通道。func (v Value) Index(i int) reflect.Value // 获取 v 值的第 i 个元素，v 值必须是字符串、数组、切片，i 不能超出范围。func (v Value) Bytes() []byte // 获取字节类型的值，如果 v 值不是字节切片，则 panic。func (v Value) Slice(i, j int) reflect.Value // 获取 v 值的切片，切片长度 = j - i，切片容量 = v.Cap() - i。// v 必须是字符串、数值、切片，如果是数组则必须可寻址。i 不能超出范围。func (v Value) Slice3(i, j, k int) reflect.Value // 获取 v 值的切片，切片长度 = j - i，切片容量 = k - i。// i、j、k 不能超出 v 的容量。i &lt;= j &lt;= k。// v 必须是字符串、数值、切片，如果是数组则必须可寻址。i 不能超出范围。func (v Value) MapIndex(key Value) reflect.Value // 根据 key 键获取 v 值的内容，v 值必须是映射。// 如果指定的元素不存在，或 v 值是未初始化的映射，则返回零值（reflect.ValueOf(nil)）func (v Value) MapKeys() []reflect.Value // 获取 v 值的所有键的无序列表，v 值必须是映射。// 如果 v 值是未初始化的映射，则返回空列表。func (v Value) OverflowInt(x int64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是有符号整型。func (v Value) OverflowUint(x uint64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是无符号整型。func (v Value) OverflowFloat(x float64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是浮点型。func (v Value) OverflowComplex(x complex128) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是复数型。 设置值方法： 12345678910111213141516171819func (v Value) SetInt(x int64) //设置int类型的值func (v Value) SetUint(x uint64) // 设置无符号整型的值func (v Value) SetFloat(x float64) // 设置浮点类型的值func (v Value) SetComplex(x complex128) //设置复数类型的值func (v Value) SetBool(x bool) //设置布尔类型的值func (v Value) SetString(x string) //设置字符串类型的值func (v Value) SetLen(n int) // 设置切片的长度，n 不能超出范围，不能为负数。func (v Value) SetCap(n int) //设置切片的容量func (v Value) SetBytes(x []byte) //设置字节类型的值func (v Value) SetMapIndex(key, val reflect.Value) //设置map的key和value，前提必须是初始化以后，存在覆盖、不存在添加 其他方法： 123456789101112131415161718192021222324252627282930##########结构体相关：func (v Value) NumField() int // 获取结构体字段（成员）数量func (v Value) Field(i int) reflect.Value //根据索引获取结构体字段func (v Value) FieldByIndex(index []int) reflect.Value // 根据索引链获取结构体嵌套字段func (v Value) FieldByName(string) reflect.Value // 根据名称获取结构体的字段，不存在返回reflect.ValueOf(nil)func (v Value) FieldByNameFunc(match func(string) bool) Value // 根据匹配函数 match 获取字段,如果没有匹配的字段，则返回零值（reflect.ValueOf(nil)）########通道相关：func (v Value) Send(x reflect.Value)// 发送数据（会阻塞），v 值必须是可写通道。func (v Value) Recv() (x reflect.Value, ok bool) // 接收数据（会阻塞），v 值必须是可读通道。func (v Value) TrySend(x reflect.Value) bool // 尝试发送数据（不会阻塞），v 值必须是可写通道。func (v Value) TryRecv() (x reflect.Value, ok bool) // 尝试接收数据（不会阻塞），v 值必须是可读通道。func (v Value) Close() // 关闭通道########函数相关func (v Value) Call(in []Value) (r []Value) // 通过参数列表 in 调用 v 值所代表的函数（或方法）。函数的返回值存入 r 中返回。// 要传入多少参数就在 in 中存入多少元素。// Call 即可以调用定参函数（参数数量固定），也可以调用变参函数（参数数量可变）。func (v Value) CallSlice(in []Value) []Value // 调用变参函数 示例一：获取和设置普通类型的值 12345678910111213141516package mainimport ( "reflect" "fmt")func main() &#123; str := "wd" age := 11 fmt.Println(reflect.ValueOf(str).String()) //获取str的值，结果wd fmt.Println(reflect.ValueOf(age).Int()) //获取age的值，结果age str2 := reflect.ValueOf(&amp;str) //获取Value类型 str2.Elem().SetString("jack") //设置值 fmt.Println(str2.Elem(),age) //jack 11&#125; 示例二：简单结构体操作 1234567891011121314151617181920212223242526272829303132333435package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; stu_val := reflect.ValueOf(stu1) //获取Value类型 fmt.Println(stu_val.NumField()) //2 fmt.Println(stu_val.Field(0),stu_val.Field(1)) //wd 22 fmt.Println(stu_val.FieldByName("Age")) //22 stu_val2 := reflect.ValueOf(&amp;stu1).Elem() stu_val2.FieldByName("Age").SetInt(33) //设置字段值 ，结果33 fmt.Println(stu1.Age) &#125; 示例三：通过反射调用结构体中的方法，通过reflect.Value.Method(i int).Call()或者reflect.Value.MethodByName(name string).Call()实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( "fmt" "reflect")type Student struct &#123; Name string Age int&#125;func (this *Student) SetName(name string) &#123; this.Name = name fmt.Printf("set name %s\n",this.Name )&#125;func (this *Student) SetAge(age int) &#123; this.Age = age fmt.Printf("set age %d\n",age )&#125;func (this *Student) String() string &#123; fmt.Printf("this is %s\n",this.Name) return this.Name&#125;func main() &#123; stu1 := &amp;Student&#123;Name:"wd",Age:22&#125; val := reflect.ValueOf(stu1) //获取Value类型，也可以使用reflect.ValueOf(&amp;stu1).Elem() val.MethodByName("String").Call(nil) //调用String方法 params := make([]reflect.Value, 1) params[0] = reflect.ValueOf(18) val.MethodByName("SetAge").Call(params) //通过名称调用方法 params[0] = reflect.ValueOf("jack") val.Method(1).Call(params) //通过方法索引调用 fmt.Println(stu1.Name,stu1.Age)&#125;//this is wd//set age 18//set name jack//jack 18]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang特性</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机、批量梯度下降]]></title>
    <url>%2F2018%2F12%2F22%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9A%8F%E6%9C%BA%E3%80%81%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[1. 随机梯度下降什么时候需要使用随机梯度下降当使用传统的梯度下降的时候，每一次的迭代，都会去计算所有样本的误差总和，这样就造成了很大的运算量，以及会加载很多的数据，这样当数据量很大的时候，我们的硬件资源也很难满足我们的需求。所以，如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。 随机梯度下降算法单一实例的代价函数 $\operatorname { cost } \left( \theta , \left( x ^ { ( i ) } , y ^ { ( i ) } \right) \right) = \frac { 1 } { 2 } \left( h _ { \theta } \left( x ^ { ( i ) } \right) - y ^ { ( i ) } \right) ^ { 2 }$ 参数优化步骤 首先对训练集数据进行随机洗牌，使训练集数据的顺序打乱，目的是防止相邻数据相似导致优化相抵消 循环遍历每一个实例，通过其代价函数的导数来优化参数θ for $i = 1 : m {$$\theta : = \theta _ { j } - \alpha \left( h _ { \theta } \left( x ^ { ( i ) } \right) - y ^ { ( i ) } \right) x _ { j } ^ { ( i ) }$(for $j = 0 : n )$}} 随机梯度下降算法在每一次计算之后便更新参数 θ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊 2.小批量梯度下降小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数 b 次训练实例，便更新一次参数 θ 。 for $i = 1 : m {$$\theta : = \theta _ { j } - \alpha \frac { 1 } { b } \sum _ { k = i } ^ { i + b - 1 } \left( h _ { \theta } \left( x ^ { ( k ) } \right) - y ^ { ( k ) } \right) x _ { j } ^ { ( k ) }$(for $j = 0 : n )$$i + = 10$ }} 通常我们会令 b 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。 3.随机梯度下降收敛在批量梯度下降中，我们可以像普通的梯度下降那样，可以令代价函数 J 为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。 在随机梯度下降中，我们在每一次更新 θ 之前都计算一次代价，然后每 x 次迭代后，求出这 x 次对训练实例计算代价的平均值，然后绘制这些平均值与 x 次迭代的次数之间的函数图表 当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加 α 来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误 果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率 α。 我们也可以令学习率随着迭代次数的增加而减小，例如令： $\alpha = \frac { \text { const } } { \text { iteration Number } + \text { const } 2 }$ 随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。 但是通常我们不需要这样做便能有非常好的效果了，对 α 进行调整所耗费的计算通常不值得]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统模型]]></title>
    <url>%2F2018%2F12%2F15%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1. 推荐系统介绍对机器学习来说，特征是很重要的，往往所选的特征将会决定我们的模型建立的好坏。因此，在机器学习中有一种思想，它针对一些问题，有算法可以为你自动学习一套好的特征。推荐算法算是一个可以自动学习特征的算法，还有很多其它的，接下来从推荐算法来介绍怎么使用推荐算法进行特征自动学习，以及推荐系统的构建和应用； 举一个例子： 假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。 前面三部电影是爱情片， 后两部是动作片，可以看出前两个用户更偏向于爱情片，后两个用户更倾向于动作片。并且可以看到并不是每一个用户给对所有的电影都打过分， 可能是没有看过该电影，也可能是看过电影后没有给电影打分。所以，推荐系统的目标来了，就是系统通过构建一个算法来预测他们每个人可能会给他们没打过分的电影打多少分，并以此来作为推荐的依据； 给数据集引入一些标记， 下面关于推荐算法的推倒便基于这些标记: $n_u$代表用户的数量 $n_m$代表电影的数量 $r ( i , j )$如果用户 j 给电影 i 评分过则 r(i, j) = 1 $y^{(i, j)}$代表用户 j 给电影的评分 $m_j$代表用户 j 评过分的电影总数 基于内容的推荐系统在基于内容的推荐算法中，被推荐的内容往往有一些特征，我们要做的就是根据这些内容的特征，通过特征内容和被推荐人的符合程度，来进行推荐； 下面举一个例子，假设每部电影都有两个特征，如 x1 代表电影的浪漫程度，x2 代表电影的动作程度 每部电影都有一个特征向量，如 $x^{(1)}$ 是第一部电影的特征向量为[0.9 0]。 接下来根据这些特征来构建一个推荐算法。比如使用线型回归模型，可以现针对每一个用户都训练一个线型回归模型，如$\theta^{(1)}$是第一个用户的模型的参数。于是，推荐算法可以归纳为: 预测函数 $\theta^{(j)}$是用户 j 的参数向量 $x^{(i)}$是电影 i 的特征向量 对于用户 j 和电影 i ，我们预测评分为: $\left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) }$ 代价函数 针对用户 j ，该线性回归模型的代价为预测误差的平方和，加上正则化项： $\min _ { \theta ( j ) } \frac { 1 } { 2 } \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \left( \theta _ { k } ^ { ( j ) } \right) ^ { 2 }$ 其中 i : r( i, j ) 表示我们只计算那些用户 j 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以 1/2m ，在这里我们将 m 去掉。并且我们不对方差项 θ0 进行正则化处理。 优化目标 上面的代价函数只是针对一个用户的，为了学习所有用户，将所有用户的代价函数求和： $\min _ { \theta ^ { ( 1 ) } , \ldots , \theta ^ { ( n ) } } \frac { 1 } { 2 } \sum _ { j = 1 } ^ { n _ { n } } \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { j = 1 } ^ { n _ { s } } \sum _ { k = 1 } ^ { n } \left( \theta _ { k } ^ { ( j ) } \right) ^ { 2 }$ $\theta _ { k } ^ { ( j ) } : = \theta _ { k } ^ { ( j ) } - \alpha \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) x _ { k } ^ { ( i ) } \quad ($ for $k = 0 )$ $\theta _ { k } ^ { ( j ) } : = \theta _ { k } ^ { ( j ) } - \alpha \left( \sum _ { i r r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) x _ { k } ^ { ( i ) } + \lambda \theta _ { k } ^ { ( j ) } \right) \quad ($ for $k \neq 0 )$ 协同过滤通过参数来学习特征上面的例子是我们通过每一个内容的特征，使用这些特种来训练每一个用户的参数；同样的我们也可以根据用户的参数，来学习电影的特征；这样我们就可以根据已有的用户，来给所有未进行特征提取的电影进行特征的建立； 优化目标 $\min _ { x ^ { ( 1 ) } , \ldots , x ^ { ( m ) } } \frac { 1 } { 2 } \sum _ { i = 1 } ^ { n _ { m } } \sum _ { j ： r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { i = 1 } ^ { n _ { m } } \sum _ { k = 1 } ^ { n } \left( x _ { k } ^ { ( i ) } \right) ^ { 2 }$ 将优化θ变为优化x 协同过滤如果在建立系统的最开始，既没有用户的参数，也没有电影的特征，那么既不能通过特征来学习参数，也不能通过参数来学习特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。 协同过滤算法 如果给定x(1), …., x(nm)，那么估计θ(1), ….θ(nu): 如果给定θ(1), ….θ(nu)， 那么估计x(1), …., x(nm) 代价函数 $J \left( x ^ { ( 1 ) } , \ldots x ^ { \left( n _ { m } \right) } , \theta ^ { ( 1 ) } , \ldots , \theta ^ { \left( n _ { s } \right) } \right) = \frac { 1 } { 2 } \sum _ { ( i : j ) : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { i = 1 } ^ { n _ { m } } \sum _ { k = 1 } ^ { n } \left( x _ { k } ^ { ( j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { j = 1 } ^ { n _ { u } } \sum _ { k = 1 } ^ { n } \left( \theta _ { k } ^ { ( j ) } \right) ^ { 2 }$ 优化目标优化函数 $\min _ { x ^ { ( 1 ) } , \ldots , x ^ { \left( n _ { m } \right) } \theta ^ { ( 1 ) } , \ldots , \theta ^ { \left( n _ { u } \right) } } J \left( x ^ { ( 1 ) } , \ldots , x ^ { \left( n _ { m } \right) } , \theta ^ { ( 1 ) } , \ldots , \theta ^ { \left( n _ { u } \right) } \right)$ $x _ { k } ^ { ( i ) } : = x _ { k } ^ { ( i ) } - \alpha \left( \sum _ { j : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \theta _ { k } ^ { j } + \lambda x _ { k } ^ { ( i ) } \right)\right.$ $\theta _ { k } ^ { ( i ) } : = \theta _ { k } ^ { ( i ) } - \alpha \left( \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } x _ { k } ^ { ( i ) } + \lambda \theta _ { k } ^ { ( j ) } \right)\right.$ 备注: 在协同过滤算法中，不需要特地的去学习方差项， 即线性回归的常数项，因为如果需要， 我们初始化θ后， 算法会自动的将一个特征学习为方差项 协同过滤步骤： 先初始化$x ^ { ( 1 ) } , x ^ { ( 1 ) } , \ldots x ^ { ( n m ) } , \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } , \ldots , \theta ^ { \left( n _ { 2 } \right) }$, 为一些随机小值 使用梯度下降算出最小化的代价函数 训练完成后， 预测用户给 j 电影 i 的评分 通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。 例如，如果一位用户正在观看电影 $x^{(i)}$ ，我们可以寻找另一部电影 $x^{(j)}$ ，依据两部电影的特征向量之间的距离 $\left| x ^ { ( i ) } - x ^ { ( j ) } \right|$ 大小 向量化: 低秩矩阵分解先举一个协同算法的例子： 当给出一件产品时，你能否找到与之相关的其它产品。 一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。 为了实现这些功能， 可以实现一种选择的方法，写出协同过滤算法的预测情况 有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。我们有五部电影，以及四位用户，那么 这个矩阵 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里： 零均值化 如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为Eve推荐电影呢？ 如果按照我们之前的协同过滤算法获得的模型，由于Eve没有对任何电影进行评分，所以为了优化模型，减少误差，最终Eve的所有特征参数都将为0，因为这样的损失是最小的，就会导致对其电影评分的预测都将是0， 那么就没有办法对其进行电影推荐； 怎么让没有评价记录的用户的预测不为0： 首先需要对结果 Y 矩阵进行零均值化，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值： 然后我们利用这个新的 Y 矩阵来训练算法。 如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测 $\left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } + \mu _ { i }$，对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分， 这样就防止了其对所有的电影评分都为了，导致推荐系统异常的判断。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>监督学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常检测模型]]></title>
    <url>%2F2018%2F12%2F14%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.异常检测使用场景什么是异常检测举个例子，比如在医院体检测量我们的血液中各种成分的含量，每一种含量都可以是一种特征，比如维生素，微量元素等含量。当测量的人很多，这样，就有了一个数据集，绘制成图标类似下面的样子： 这里的每个点、每个叉，都是你的无标签数据。这样，异常检测问题可以定义如下：假设有一个人的特征变量为$x_test$ 。所谓的异常检测问题就是：我们希望知道知道这个人的测试指标是否正常范围，或者说，我们希望判断这个人进行进一步的检查。如果被检查的人的各项指标都在正常范围内，那么我们可以判断该人的体检接口属于正常，而不需要进一步的检查。 给定数据集$x^{(1)}, x^{(2)}, ….,x^{(m)}$ ，我们假使数据集是正常的，我们希望知道新的数据 $x_{test}$ 是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 p(x) , 综上可以定义异常检测， 就是通过判根据测试数据和所有测试集的关系来判断其正常(或异常)的概率。 上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。这种方法称为密度估计，就是异常检测的核心思想，表达如下： ​ If $\quad p ( x ) \left{ \begin{array} { l l } { &lt; \varepsilon } &amp; { \text { anomaly } } \ { &gt; = \varepsilon } &amp; { \text { normal } } \end{array} \right.$ 异常检测的例子 异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。 服务器集群检测，特征可能包含：内存使用情况，被访问的磁盘数量，CPU的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。 2.高斯分布一般自然界产生的数据，我们通常认为会符合高斯分布，其表达方式为: $x \sim N \left( \mu , \sigma ^ { 2 } \right)$ , 则其高绿密度函数为: $p \left( x , \mu , \sigma ^ { 2 } \right) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)$ , 所以我们可以利用已有的数据来预测总体中的u 和 σ2， 计算方法如下: $\mu = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } x ^ { ( i ) }$ ; $\sigma ^ { 2 } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( x ^ { ( i ) } - \mu \right) ^ { 2 }$ 3.异常检测算法模型构建步骤开始总结使用高斯分布的异常检测算法 异常检测算法: 对于给定的数据集$x ^ { ( 1 ) } , x ^ { ( 2 ) } , \dots , x ^ { ( m ) }$， 计算高斯分布的u和σ2的估计值。 ​ $\mu _ { j } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } x _ { j } ^ { ( i ) }$​ $\sigma _ { j } ^ { 2 } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( x _ { j } ^ { ( i ) } - \mu _ { j } \right) ^ { 2 }$ 推导出对应的高斯分布后， 测试对应测试实例的正常概率p(x); ​ $p ( x ) = \prod _ { j = 1 } ^ { n } p \left( x _ { j } ; \mu _ { j } , \sigma _ { j } ^ { 2 } \right) = \prod _ { j = 1 } ^ { 1 } \frac { 1 } { \sqrt { 2 \pi } \sigma _ { j } } \exp \left( - \frac { \left( x _ { j } - \mu _ { j } \right) ^ { 2 } } { 2 \sigma _ { j } ^ { 2 } } \right)$ 选择一个$\varepsilon​$ ， $p(x) = \varepsilon​$ 作为判定边界，当$p ( x ) &lt; \varepsilon​$ 时， 测试实例为异常； 异常检测系统开发异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 y 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。 构建步骤 将数据集划分成训练集，交叉检验集， 和测试集； 进行模型构建 尝试使用不同的 $\varepsilon$ 作为阈值，并预测数据是否异常，根据F1值或者查准率与查全率比例来选择$\varepsilon$ 选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的 值，或者查准率与查全率之比 4. 异常检测和监督学习的对比 总结： 当我们的数据集正负分类数量级差不多的时候，并且正负分类都有对应的标签的时候选择监督学习； 反之当我们有大量的数据，数据的正样本特别少， 可以使用异常检测； 5.特征选择特征转换 异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：$x = \log ( x + c )$ ，其中 c 为非负常数； 或者 $x = x ^ { c }$ ，c 为 0-1 之间的一个分数，等方法。(编者注：在python中，通常用 np.log1p() 函数，log1p 就是 log(x + 1)，可以避免出现负数结果，反向函数就是 np.expm1() ) 异常检测结果分析 一个常见的问题是一些异常的数据可能也会有较高的 p(x) 值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。 我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。 6.多元高斯分布比如我们的数据集的特征是两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。 下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的X所代表的数据点很可能是异常值，但是其 p(x) 值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。 在一般的高斯分布模型中，我们计算 p(x) 的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 p(x) 。 多元高斯分布算法步骤: 计算所有特征的平均值，再计算协方差矩阵； ​ $\mu = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } x ^ { ( i ) }$ ​ $\Sigma = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( x ^ { ( i ) } - \mu \right) \left( x ^ { ( i ) } - \mu \right) ^ { T } = \frac { 1 } { m } ( X - \mu ) ^ { T } ( X - \mu )$ 计算高斯分布p(x) ​ $p ( x ) = \frac { 1 } { ( 2 \pi ) ^ { \frac { n } { 2 } } | \Sigma | ^ { \frac { 1 } { 2 } } } \exp \left( - \frac { 1 } { 2 } ( x - \mu ) ^ { T } \Sigma ^ { - 1 } ( x - \mu ) \right)$ ​ 注: $\Sigma$是定矩阵，在 Octave 中用 det(sigma) 计算， $\Sigma ^ { - 1 }$是逆矩阵 选择一个$\varepsilon$ ， $p(x) = \varepsilon$ 作为判定边界，当$p ( x ) &lt; \varepsilon$ 时， 测试实例为异常； 协方差矩阵对模型的影响 当从左下到右上的对角线上的数不为0的时候，表示特征之间存在相关性 上图是5个不同的模型，从左往右依次分析： 是一个一般的高斯分布模型 通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差 通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性 多远高斯模型作用 建立多远高斯分布模型后， 可以通过协方差，建立两个特征成正相关性的模型，这样就可以将上图绿色的点判断为异常点； 多远高斯分布与普通高斯分布关系: 可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。 原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。 总结: 如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>非监督学习</tag>
        <tag>机器学习</tag>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据降维 & PCA]]></title>
    <url>%2F2018%2F12%2F06%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FPCA%E9%99%8D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[1. 数据降维数据降维目的数据降维是对数据进行压缩的一个强有力的工具，也是一个非监督学习算法。有几个不同的原因可能会让我们在进行建模的过程中对数据进行降维处理，一、数据压缩，不仅能让我们的数据占用比较少的计算机资源(硬盘、内存), 还可以通过减少特征的方式，来让我们的机器学习算法运行的更加快速; 二、数据可视化， 当数据的特征特别多的时候，我们没有可视化的方案，将一个多维的数据进行可视化展示，但是我们可以通过PCA来将数据进行压缩成维度比较少的形式，再进行展示； 什么是数据降维 这里按照一个将二维的数据降维成一维来进行举例： 假设我们的数据有两个特征: x1： 长度， 单位是cm ； x2：是用英寸表示的同一个物体的长度。 所以，这两个特征给了我们高度冗余表示，两个特征基本上用一个就可以表示了一个实例的长度特征，所以我们可以通过降维来减少这两个特征造成的冗余； 将数据从二维降至一维： 假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。 将我们的数据集的特征减少冗余，在保持误差基本不变的情况，减少特征的数量，这就是数据的降维； 2.数据压缩下面介绍将冗余数据进行压缩的方法 将二维数据降维至一维: 假设我们有这样一批特征，x1为学生学习的积极程度， x2为学生知识的掌握程度，这两个特征可能都可以代表学生的学习成果。所以，相比这两种特征，我们可能更倾向于获得特征–学生的知识掌握程度z, 来代替x1， x2两个特征。所以，我们真正关心的，可能是上面那条红色的线的方向，以及特征映射到其上的点，代表学生的知识量; 上面这幅图表示，将二维数据降维成一位数据的过程 将三维数据降维至二维: 过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。 这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。 3.数据可视化在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。 假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如GDP，人均GDP，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。 这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。 4.主成分分析问题主成分分析(PCA)是最常见的降维算法。在PCA中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。 主成分分析问题的描述：问题是要将维数据降至 k 维，目标是找到向量$u^{(i)}, u^{(2)}, …., u^{(k)}$, 使得所有的实例到向量的总的投射误差最小。 主成分分析与线性回顾的比较：主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。 上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。 PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。但PCA 要保证降维后，还要保证数据的特性损失最小。 PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。 PCA的优点 PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 PCA的缺点 如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，本可以根据经验做一些数据的整合或降维，但使用PCA却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。 5.主成分分析算法PCA 减少n维到k维：第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_j = x_j - u_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $\sigma^2$ 。第二步是计算协方差矩阵（covariance matrix） $\Sigma : \sum = \frac { 1 } { m } \sum _ { i - 1 } ^ { n } \left( x ^ { ( i ) } \right) \left( x ^ { ( i ) } \right) ^ { T }$第三步是计算协方差矩阵$\Sigma$的特征向量（eigenvectors）:在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解，[U, S, V]= svd(sigma) 。 对于一个nn 维度的矩阵，上式中的U是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从n维降至k维，我们只需要从U中选取前k个向量，获得一个维度的n k矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量:$z ^ { ( i ) } : z ^ { ( i ) } = U _ {reduce} ^ { T } * x ^ { ( i ) }$ 其中x是 n 1 维的， 因此结果为 K 1 维度的； 6.选择主成分的数量主要成分分析是减少投射的平均均方误差：训练集(经过均值归一化处理后)的方差为: $\frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left| x ^ { ( i ) } \right| ^ { 2 }$ 我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的值。 如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了, 一般这个值最好不要小于90%， 因为那样数据可能就偏离原数据太多了。 进行主成分选择的步骤(第一种方法): 我们可以先令k = 1，然后进行主要成分分析，获得$U_{reduce}$和z，然后计算比例是否小于1%; 如果不是的话再令k = K + 1，如此类推，直到找到可以使得比例小于1%的最小 值; 进行主成分选择的步骤(第二种方法): 我们可以先令k = 1，然后进行主要成分分析，获得$U_{reduce}$， S , 然后计算平均均方误差与训练集的比例 备注： S是一个n * n的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例 计算平均均方误差与训练集的比例公式: 如果比例大于1%,再令k = K + 1，如此类推，直到找到可以使得比例小于1%的最小 值; 7. 重建被压缩的数据PCA作为压缩算法。在那里你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。 所以， 给定$z^{(i)}$, 可能是100维， 怎么回到原来的$x^{(i)}$, 可能是1000维的数据； PCA算法，我们可能有一个这样的样本。如图中样本 $x^{(1)}, x^{(2)}$, 。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如 $z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？ 为2维，z为1维，$z = U^T_{reduce}x$ ，相反的方程为：$x_{appoz } = U_{reduce} * z$, $x_{appox} \approx x$ 。如图： 通过以上的步骤得到的数据与原始数据相当相似。所以，这就是你从低维表示z回到未压缩的表示。我们得到的数据和原始数据x非常近似 ，我们也把这个过程称为重建原始数据。但是没有方法可以将数据从低维，没有偏差的还原到高纬。 8.PCA使用的总结使用PCA的数据进行建模的步骤: 运用主要成分分析将数据压缩到K个特征 然后对训练集进行训练 在预测或者交叉验证时，采用之前学习而来的$U_{reduce}$， 将输入的特征x转换成特征向量z， 然后在进行预测； 不要在一下情况使用PCA 将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。 不考虑为什么使用PCA，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。 总结: PCA如果不是想加快训练速度， 减少资源消耗，没必要使用， 其只可能降低数据信息量， 更要注意，千万不要用PCA来防止过拟合；如果使用， 通过平均均方误差与训练集的比例来确定特征数量K；]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>非监督学习</tag>
        <tag>PCA</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类问题]]></title>
    <url>%2F2018%2F12%2F06%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1.非监督学习和聚类算法介绍在监督学习中，所有的训练数据、测试数据、都有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，数据没有附带任何标签，数据只有一些列的特征，而我们的训练目标就是通过不同特征的训练集，将不同的训练集分为K个类别。 如上图，通过数据特征可以绘制出一些列的点，但是没有标签。也就是说，在非监督学习中，需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，通过我们给定的数据，为我们找到这个数据的内在结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到上面的点集的可以分为两个分类的算法，就被称为聚类算法， 接下来就通过学习聚类算法来了解非监督学习。 聚类算法的一些应用: 市场分割：比如在数据库中存储了许多客户的信息，希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。 社交网络分析：关注个人的社交信息，例如Facebook，Google+，或者是其他的一些信息，比如说：经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群 星系分类: 通过聚类算法将不同的行星划分到各个星系 2.K-means算法K-means是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。K-means也是一个迭代算法，假设我们想要将数据聚类成n个组，其步骤为: 首先选择K个随机的点，称为聚类中心（cluster centroids）； 对于数据集中的每一个数据，按照距离K个中心点的距离 将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。 重复步骤2-4直至中心点不再变化。 一个聚类过程示例： 假如进行了10次迭代 用$u^{(1)}, u^{(2)}, …., u^{(k)}$来表示聚类中心，用$c^{(1)}, c^{(2)}, ….., c^{(m)}$ 来存储与第 i 个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下： 1234567891011Repeat &#123;for i = 1 to mc(i) := index (form 1 to K) of cluster centroid closest to x(i)for k = 1 to Kμk := average (mean) of points assigned to cluster k&#125; 算法步骤: 对于每一个样例，计算其应该属于的类。 移动聚类中心，对于每一个类，重新计算该类的中心。 K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用K-均值算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。 3. K-means优化目标K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和 K-均值的代价函数（又称畸变函数 Distortion function）的公式为： $J \left( c ^ { ( 1 ) } , \ldots , c ^ { ( m ) } , \mu _ { 1 } , \ldots , \mu _ { K } \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left| X ^ { ( i ) } - \mu _ { c ^ { ( i ) } } \right| ^ { 2 }$ 其中，$u_{c^{(i)}}$代表距离$x^{(i)}$最近的聚类中心， 我们的的优化目标便是找出使得代价函数最小的$c^{(1)}, c^{(2)}, …,c^{(m)}$ 和 $u_1, u_2, …,u_k$, 其优化目标的公式为： ${\min \over { c ^ { ( 1 ) } , \ldots , c ^ { ( m ) } \atop \mu _ { 1 } , \ldots , \mu _ { K } }} J \left( c ^ { ( 1 ) } , \ldots , c ^ { ( m ) } , \mu _ { 1 } , \ldots , \mu _ { K } \right)$ 通过对代价函数的构建，可以将K-means每次迭代的两次循环的目的做一个归纳: 第一个循环是重新将样本划分到距离最近的聚类中心点，是用于减小$c^{(i)}$引起的代价； 第二个循环 是重新调整中心点，使其位于该分类的中心， 是用于减小$u_{c^{(i)}}$引起的代价 4. 随机初始化聚类中心运行K-means算法，首先要随机初始化所有的聚类中心点，通常采用的方式是： 确定聚类中心个数 K，注意聚类中心点的个数 K 要小于所有训练集实例的数量, 即: K &lt; m； 随机选择 K个训练实例，然后令个聚类中心分别与这 K 个训练实例相等; K-means算法有一个常见的问题，即它有可能会停留在一个局部最小值处，而这取决于初始化的情况， 如下图: 由于聚类中心初始化的距离过近，导致最终只能获得局部的最优解，这样往往不是我们想要的分类结果。 如果想解决这个问题，通常需要多次运行K-means算法，每一次都重新进行随机初始化，最后再比较多次运行K-means的结果，选择代价函数最小的结果。这种方法在 K 较小的时候（2–10）还是可行的，但是如果 K 较大，这么做也可能不会有明显地改善, 但是通常K比较大的时候，局部最优解已经可以很接近最优解。 7.二分K-means算法二分k-means算法介绍基于kmeans算法容易使得结果为局部最小值而非全局最小值这一缺陷，对算法加以改进。使用一种用于度量聚类效果的指标SSE(Sum of Squared Error)，即对于第 i 个簇，其SSE为各个样本点到“簇中心”点的距离的平方的和，SSE值越小表示数据点越接近于它们的“簇中心”点，聚类效果也就越好。以此作为划分簇的标准。 算法思想是：先将整个样本集作为一个簇，该“簇中心”点向量为所有样本点的均值，计算此时的SSE。若此时簇个数小于 k，对每一个簇进行kmeans聚类(k=2) ，计算将每一个簇一分为二后的总误差SSE，选择SSE最小的那个簇进行划分操作。 算法计算过程输入：训练数据集 D=x(1),x(2),…,x(m) ,聚类簇数 k ;过程：函数 kMeans(D,k,maxIter) . 1：将所有点看做一个簇，计算此时“簇中心”向量：$\mu ^ { ( 1 ) } = \frac { 1 } { m } \sum _ { x \in D } x​$ 2：while “簇中心”个数h&lt;k“： 3： for i=1,2,…,h do 4： 将第 i 个簇使用 kmeans算法进行划分，其中 k=2 5： 计算划分后的误差平方和 SSEi 5： 比较 k 种划分的SSE值，选择SSE值最小的那种簇划分进行划分 5： 更新簇的分配结果 5： 添加新的“簇中心” 18：until 当前“簇中心”个数达到 k 输出：簇划分 C=C1,C2,…,CK 6.选取聚类数量聚类选择的通常做法通常没有具体的聚类数量选择的方法，最普遍的方法就是根据不同的问题和不同的需求，人工进行选择。 通常我们需要知道，进行聚类数量选择的时候需要思考我们运用K-means算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。 肘部法则当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变 K 值，也就是聚类类别数目的总数。我们用 K 个聚类来运行K-means聚类方法。这就意味着，所有的数据都会分到 K 个聚类里，然后计算成本函数 J 或者计算畸变函数。K 代表聚类数字。 经过聚类运算，可能会得到一条类似于左边这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。 你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快， K = 3之后就下降得很慢，那么我们就选 K= 3。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。 总结: 选择聚类数量最好是根据我们的需求，和场景来确定我们是需要多少种聚类，如果我们不能根据使用场景和需求确定下来聚类的数量，那么我们可以通过肘部法则，来选择一个比较合理的聚类数量。 pyhton实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# coding:utf-8import numpy as npdef distEclud(vecA, vecB): # 计算欧式距离 return np.sqrt(np.sum(np.power(vecA - vecB, 2))) # la.norm(vecA-vecB)def randCent(dataSet, k): # 初始化k个随机簇心 n = np.array(dataSet).shape[1] # 特征个数 centroids = np.mat(np.zeros((k, n))) # 簇心矩阵k*n for j in range(n): # 特征逐个逐个地分配给这k个簇心。每个特征的取值需要设置在数据集的范围内 minJ = min(dataSet[:, j]) # 数据集中该特征的最小值 rangeJ = float(max(dataSet[:, j]) - minJ) # 数据集中该特征的跨度 centroids[:, j] = np.mat(minJ + rangeJ * np.random.rand(k, 1)) # 为k个簇心分配第j个特征，范围需限定在数据集内。 return centroids # 返回k个簇心def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): """随机初始化k-means""" m = np.array(dataSet).shape[0] # 数据个数 clusterAssment = np.mat(np.zeros((m, 2))) # 记录每个数据点被分配到的簇，以及到簇心的距离 centroids = createCent(dataSet, k) # 初始化k个随机簇心 clusterChanged = True # 记录一轮中是否有数据点的归属出现变化，如果没有则算法结束 while clusterChanged: clusterChanged = False for i in range(m): # 枚举每个数据点，重新分配其簇归属 minDist = np.inf # 先假设距离簇心的距离为无穷大，则先随机分配一个簇心给数据点 minIndex = -1 # 记录最近簇心及其距离 for j in range(k): # 枚举每个簇心 distJI = distMeas(centroids[j, :], dataSet[i, :]) # 计算数据点与簇心的距离 if distJI &lt; minDist: # 更新最近簇心 minDist = distJI minIndex = j if clusterAssment[i, 0] != minIndex: clusterChanged = True # 更新“变化”记录 clusterAssment[i, :] = minIndex, minDist ** 2 # 更新数据点的簇归属 print(centroids) for cent in range(k): # 枚举每个簇心，更新其位置 ptsInClust = dataSet[np.nonzero(clusterAssment[:, 0].A == cent)[0]] # 得到该簇所有的数据点 centroids[cent, :] = np.mean(ptsInClust, axis=0) # 将数据点的均值作为簇心的位置 return centroids, clusterAssment # 返回簇心及每个数据点的簇归属def biKmeans(dataSet, k, distMeas=distEclud): """随机初始化二值化k-means""" m = np.array(dataSet).shape[0] centroid0 = np.mean(dataSet, axis=0).tolist()[0] # 创建初始簇心，标号为0 centList = [centroid0] # 创建簇心列表 clusterAssment = np.array(np.zeros((m, 2))) # 初始化所有数据点的簇归属(为0) for j in range(m): # 计算所有数据点与簇心0的距离 clusterAssment[j, 1] = distMeas(np.array(centroid0), dataSet[j, :]) ** 2 while len(centList) &lt; k: # 分裂k-1次，形成k个簇 lowestSSE = np.inf # 初始化最小sse为无限大 for i in range(len(centList)): # 枚举已有的簇，尝试将其一分为二 ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:, 0].A == i)[0], :] # 将该簇的数据点提取出来 centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) # 利用普通k均值将其一分为二 sseSplit = np.sum(splitClustAss[:, 1]) # 计算划分后该簇的SSE sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:, 0].A != i)[0], 1]) # 计算该簇之外的数据点的SSE print("sseSplit, and notSplit: ", sseSplit, sseNotSplit) if (sseSplit + sseNotSplit) &lt; lowestSSE: # 更新最小总SSE下的划分簇及相关信息 bestCentToSplit = i # 被划分的簇 bestNewCents = centroidMat # 划分后的两个簇心 bestClustAss = splitClustAss.copy() # 划分后簇内数据点的归属及到新簇心的距离 lowestSSE = sseSplit + sseNotSplit # 更新最小总SSE print('the bestCentToSplit is: ', bestCentToSplit) print('the len of bestClustAss is: ', len(bestClustAss)) centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0] # 一个新簇心的标号为旧簇心的标号，所以将其取代就簇心的位置 centList.append(bestNewCents[1, :].tolist()[0]) # 另一个新簇心加入到簇心列表的尾部，标号重新起 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList) # 更新旧簇内数据点的标号 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit # 同上 clusterAssment[np.nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss # 将更新的簇归属统计到总数据上 return np.mat(centList), clusterAssment]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>非监督学习</tag>
        <tag>机器学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据处理 特征工程]]></title>
    <url>%2F2018%2F12%2F01%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[0.数据处理的一般流程数据获取 –&gt; 数据划分 –&gt; 数据处理 –&gt; 效果评判 1.数据获取数据来源 企业日益积累的大量数据（互联网公司更为显著） 政府掌握的各种数据 科研机构的实验数据 数据类型 离散型数据 由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。 连续性数据 变量可以在某个范围内取任一数，即变量的取值可以是连续的，如，长度、时间、质量值等，这类整数通常是非整数，含有小数部分。 通过sk-learn获取数据 sklearn数据集获取API sklearn.datasets 小规模数据集的获取 datasets.load_*() 获取小规模的数据集， 数据集已经包含在了datasets里 参数subset: train 表示获取训练街, test 表示获取测试集, all 表示两者全部 大规模数据的获取与删除 datasets.fetch_*(data_home=None) 从网络上以下载的方式获取大规模的数据集 data_hom：表示数据集下载的目录,默认是 ~/scikit_learn_data/ subset: train 表示获取训练街, test 表示获取测试集, all 表示两者全部 datasets.clear_data_home(data_home=None) 清除目录下的数据集 api接口返回值（datasets.base.Bunch 实例）的属性 data：特征数据数组，是 [n_samples * n_features] 的二维numpy.ndarray 数组 target：目标值数组，是 n_samples 的一维 numpy.ndarray 数组 feature_names：特征名,新闻数据，手写数字、回归数据集没有 target_names：目标数据名称 DESCR：数据描述 2.数据集的划分数据集常见划分形式 训练数据：用于训练，构建模型 测试数据：在模型检验时使用，用于评估模型是否有效 数据划分的原则 将数据集划分为两个互斥的集合，其中一个集合作为训练集，留下的集合作为测试集 划分要尽可能的保持数据分布的一致性(中的数据分布跟是一样的)，才能避免因数据划分过程引入额外的偏差而对最终结果产生影响 不同的划分将导致不同的训练/测试集，相应的模型评估也是有差别的，若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果 sklearn数据集划分方法 sklearn.model_selection.train_test_split(x, y, **options) x 数据集的特征值 y 数据集的标签值 test_size 测试集占用比例的大小, 一般是0.8 random_state 当设置该值为任意值时, 返回的划分结果相同 return (训练集特征值，测试集特征值，训练标签，测试标签) 3.数据处理和特征工程特征抽象&amp;特征衍生特征抽象的作用将计算机不好理解的数据转化为计算机友好的数据形式来体现 举例 特征衍生的作用将一些数据之间所体现的规律和特征体现出来 举例连续变量 离散变量 特征抽取数据抽取的作用 让计算机和模型更好的理解数据的结构，可将数据处理成计算机比较友好的数据结构，比如讲分类问题的数据处理成one-hot编码的数据类型等， 常用与分类问题的特征处理 sk-learn特征抽取api1.字典特征抽取 使用类: sklearn.feature_extraction.DictVectorizer 常用方法 DictVectorizer(sparse=True,…) # 创建dict_vert对象 sparse: 是否返回sparse矩阵 DictVectorizer.fit_transform(X) # 将X进行特征转化 X:字典或者包含字典的迭代器 DictVectorizer.inverse_transform(X) # 将X转为为特征转化之前的数据格式 X:array数组或者sparse矩阵 DictVectorizer.get_feature_names() # 获取特征类别名称 DictVectorizer.fit(X) 将X数据进行特征值标准匹配 DictVectorizer.transform(X) # 将X按照之前的fit标准进行转化 返回值的数据格式类型one-hot编码 2.文本的特征抽取 英文文本特征抽取 使用类: sklearn.feature_extraction.text.CountVectorizer 常用方法 CountVectorizer(max_df=1.0,min_df=1, stop_word） # 创建词频矩阵对象 max_df整数：指每个词的所有文档词频数不大于该值 min_df小数：每个词的次数／所有文档数量，最小 stop_word: 将分词的哪些不需要的结果进行剔除 CountVectorizer.fit_transform(X) # 获取特征值矩阵 X:文本或者包含文本字符串的可迭代对象 注意: 利用toarray(), 可以将返回值进行sparse矩阵转换array数组 CountVectorizer.inverse_transform(X) # 将特征值矩阵转化为之前的数据格式 X:array数组或者sparse矩阵 CountVectorizer.get_feature_names() # 获取特征值化后的单词列表 中文文本特征抽取 常用方法 使用类：sklearn.feature_extraction.text.TfidfVectorizer TfidfVectorizer(stop_words=None,…) # 创建权重矩阵对象 TfidfVectorizer.fit_transform(X) # 返回文本单词的权重sparse矩阵 X:文本或者包含文本字符串的可迭代对象 TfidfVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 # 将矩阵转换之前数据格式 TfidfVectorizer.get_feature_names() # 获取特征抽取后的单词列表 备注: 中文的特征值抽取原理上和英文一样，但是需要先使用分词器对长句子, 进行分词处理 分词器的常用方法 pip3 install jieba # 下载jieba分词器 import jieba jieba.cut(“python 现在我在进行特征词抽取”) # 对句子进行分词处理 返回： 返回包含分词结果的生成器 TF-IDF TF_IDF的介绍 如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF-IDF作用 用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 分类机器学习算法的重要依据 TF-IDF的计算公式 数据特征生成(生成交叉特征)为什么生成交叉特征比如说我们目前有一群二维的输入样本[a,b][a,b], 这个群二维的输入样本去训练模型，发现不管在训练集还是在测试集中的R^2 值都不高。 这时候，我们可以考虑将二维样本空间映射到跟高维度例如：[1,a,b,a2,ab,b2][1,a,b,a2,ab,b2] 或者更高维度。来体现features和features之间的关系。 sklearn的交叉特征生成api poly = PolynomialFeatures(degree = 2, interaction_only = True, include_bias=False) degree：默认为2，多项式次数(就同几元几次方程中的次数一样) interaction_only：是否包含单个自变量自相作用，默认为False，为True则表示去除与自己相乘的情况 include_bias：是否包含偏差标识，默认为True，为False则表示不包含偏差项 poly.fit_transform(X) 缺失值处理处理的两种思路 删除： 如果每列或者行数据缺失值达到一定的比例，建议放弃整行或者整列 插补：可以通过缺失值每行或者每列的平均值、中位数来填充 sk-learn缺失值处理api sklearn.preprocessing.Imputer Imputer(missing_values=’NaN’, strategy=’mean’, axis=0) missing_values: 却是值的代替值 strategy： 缺失代替值的类型 axis： 代表按照列来计算, 1按照行来计算 Imputer.fit_transform(X) # 将缺失值进行插补 X:numpy array格式的数据[n_samples,n_features] np.NaN的解释 numpy的数组中可以使用np.nan/np.NaN来代替缺失值，属于float类型 如果是文件中的一些缺失值，可以替换成nan，通过np.array转化成float型的数值即可 标准化和归一化处理归一化处理通过对原始数据进行变换把数据映射到(默认为[0,1])之间 归一化公式 备注: 作用于每一列，max为一列的最大值，min为一列的最小值,那么X’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0 sklearn归一化API sklearn.preprocessing.MinMaxScaler MinMaxScalar(feature_range=(0,1)…) # 创建归一化对象 feature_range：每个特征缩放到给定范围(默认[0,1]) MinMaxScalar.fit_transform(X) # 对数据集进行归一化处理 X:numpy array格式的数据 返回转换后的形状相同的array归一化的弊端使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景 标准化处理通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内 标准化处理 公式 标准化的优点如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。 sklearn标准化处理API scikit-learn.preprocessing.StandardScaler StandardScaler() # 创建标准化实例对象 StandardScaler.fit_transform(X) # 将Numpy array 格式的数据进行标准化 StandardScaler.mean_ # 原始数据中每列特征的平均值 StandardScaler.std_ # 原始数据每列特征的方差 特征的筛选和降维特征值选择的原因 冗余：部分特征的相关度高，容易消耗计算性能 噪声：部分特征对预测结果有负影响 主要应用的方法 1.Filter(过滤式):VarianceThreshold 2.Embedded(嵌入式)：正则化、决策树 3.Wrapper(包裹式) 1.低方差特征筛选-sklearn特征选择API sklearn.feature_selection.VarianceThreshold VarianceThreshold(threshold = 0.0) # 创建低方差数据删除对象 threshold： 设定方差低于多少进行删除 Variance.fit_transform(X) # 对特征数据中方差较低的特征进行删除 X:numpy array格式的数据 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。 PCA的的作用 特征之间通常是线性相关的，可以进行一些特征合并操作 PCA是一种分析、简化数据集的技术， 是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息, 可以削减回归分析或者聚类分析中特征的数量. PCA 降维 公式 2.特征的降维(PCA)-sklearn api sklearn. decomposition.PCA PCA(n_components=None) # 创建PCA对象 n_components： 将数据转化为多少维度的数据, 默认只降低一维 PCA.fit_transform(X) # 对特征矩阵进行降维处理 X:numpy array格式的数据[n_samples,n_features] 返回值：转换后指定维度的array 4.特征的效果评判 准确率 计算公式 准确率 = (TP + TN)/(TP + FP + FN + TN) sklearn api from sklearn.metrics import accuracy_score accuracy_score(y_true, y_predict, normalize=False) 精确率 计算公式 精确率 = (TP) / (TP + FP) sklearn api from sklearn.metrics import precision_score precision_score(y_true, y_predict) 召回率 计算公式 召回率 = (TP) / (TP + FN) sklearn api from sklearn.metrics import recall_score recall_score(y_true, y_predict) F1-score 计算公式 $F 1 - socre = 2 \frac { \text { Precision } _ { - } \text { scoreRecall score } } { \text { Precision } _ { - } \text { score } + \text { Recall } \text { score } }$ sklearn api from sklearn.metrics import f1_score f1_score(y_true, y_predict) 混淆矩阵 什么是混淆矩阵 混淆矩阵用来表示预测结果的真实反馈，返回的形式是一个矩阵，对角线元素表示预测标签等于真实标签的点的数量,是分类结果的一个绝对量 举个栗子 ​ 获得的混淆矩阵为 sklearn api from sklearn.metrics import confusion_matrix confusion_matrix(y_true, y_pred) ROC 计算公式 TPR = TP / (TP + FN) 预测为正的样本占所有正样本比例 FPR = FP / (FP + TN) 预测为正但是实际为负的样本占所有负样本的比例 sklearn api from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_true, y_pred,pos_label=1) thresholds： 样本属于正样本的概率 ROC曲线 AUC 计算公式 AUC是ROC曲线下方的面积，通常AUC值在0.5-1.0之间，值越大模型效果越好 AUC 为 0.5 跟随机猜测一样（例：丢硬币），模型没有预测价值 sklearn api from sklearn.metrics import auc roc_auc = auc(fpr, tpr) KS 计算公式 ks = max(TPR – FPR) 5.模型的选择与调优交叉验证 交叉验证的作用 为了让被评估的模型更加准确可信 交叉验证的过程 将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成5份，其中一份作为验证集。然后经过5次(组)的测试，每次都更换不同的验证集。即得到5组模型的结果，取平均值作为最终结果。又称5折交叉验证。 网格搜索 通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。 交叉验证通常是为了配合网格搜索, 通过网格似的参数对比, 交叉验证出最优参数, 以此来优化模型的参数选择 交叉验证-网格搜索api sklearn.model_selection.GridSearchCV sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None) estimator：估计器对象 param_grid：估计器参数 dict类型, key为超参数, value为将要进行网格搜索的参数 {“n_neighbors”:[1,3,5]} cv：指定几折交叉验证 fit：输入训练数据 score：准确率评估集合，返回多组的训练准确率结果 返回结果分析： best_score_:在交叉验证中验证的最好结果 best_estimator_：最好的参数模型 cv_results_:每次交叉验证后的测试集准确率结果和训练集准确率结果]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2018%2F11%2F30%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[1.支持向量机优化目标与逻辑回归和神经网络相比，支持向量机，或者简称SVM，在训练复杂的非线性方程时提供了一种更为清晰，更加强大的方式。 在逻辑回归中我们已经熟悉了这里的假设函数形式，和下边的S型激活函数。下面依然用z表示$\theta^Tx$ 现在考虑下我们想要逻辑回归做什么：如果有一个 y = 1 的样本，不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 y = 1，现在我们希望 $h_\theta(x)​$ 趋近1。因为我们想要正确地将此样本分类，这就意味着当 $h_\theta(x)​$ 趋近于1时，$\theta^Tx​$ 应当远大于0。这是因为由于 z 表示 $\theta^Tx​$，当 z 远大于0时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于1。相反地，如果我们有另一个样本，即 y = 0。我们希望假设函数的输出值将趋近于0，这对应于 $\theta^Tx​$，或者就是 z 会远小于0，因为对应的假设函数的输出值趋近0。 如果你进一步观察逻辑回归的代价函数公式，你会发现每个样本 (x, y) 都会为总代价函数，增加上面的一项，因此，对于总代价函数通常会有对所有的训练样本的代价函数求和。 接下来，考虑逻辑回归的两种情况： 一种是y = 1 的情况， 一种是 y = 0的情况 在第一种情况中，假设 y = 1， 此时在目标函数中只需要第一项起作用， 因为y = 1时， (1-y)项等于零，因此，在y = 1的样本中， 即(x, y) 中， 我们得到$-log({1\over1+e^{-z}})$ 这一项。。如果画出关于 z 的函数，你会看到下面的这条曲线，我们同样可以看到，当 Z 增大时，也就是相当于 $\theta^Tx$ 增大时，z 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本时，试图将$\theta^Tx$设置得非常大。因为，在代价函数中的这一项会变的非常小。 现在开始建立支持向量机，我们从这里开始： 我们会从这个代价函数开始，也就是 $-log({1\over1+e^{-z}})$ 一点一点修改，让我取这里的 z = 1 点，我先画出将要用的代价函数。 新的代价函数(当y=1)和逻辑回归的代价函数类似，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在y=1的前提下的。你也许能想到，这应该能做同逻辑回归中类似的事情，但事实上，在向量机的优化问题中，这会变得更坚定，并且为支持向量机，带来计算上的优势。 另外一种情况是当 y = 1时，此时如果你仔细观察逻辑回归的代价函数只留下了第二项，因为第一项被消除了。如果当 y = 0时，那么这一项也就是0了。所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一项表示。并且，如果你将这一项作为z的函数，那么，这里就会得到横轴 z 。同样地，我们要替代逻辑回归的代价函数这一条蓝色的线，用相似的方法， 画出当y = 0的时候向量机的代价函数。 如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，第一个函数，我称之为cost1(z)，同时，第二个函数函数我称它为cost0(z)。这里的下标是指在代价函数中，对应的y=1和y=0 的情况，拥有了这些定义后，下面开始构建支持向量机。 这是我们在逻辑回归中使用代价函数 J(θ)。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要其表达式的两个部分替换为 cost1(z)，也就是前面构建的代价函数$cost_1(\theta^Tx)$, 和cost0(z)，也就是$cost_0(\theta^Tx)$。因此，对于支持向量机，我们得到了这里的最小化问题，即是将上面的两部分代价函数替换为下边蓝色的支持向量机代价函数的公式$cost_1(\theta^Tx)$ 和 $cost_0(\theta^Tx)$。然后再加上正则化参数。 但是在一般的时候，对支持向量机公式的参数，会有一些不同，下面尝试对向量机的代价函数进行改造： 首先，我们要除去1/m这一项，当然，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致，因为这里仅仅除去1/m这一项，也会得出同样的 θ 最优值，因为 仅是个常量，因此，这个最小化问题中，无论前面是否有 1/m 这一项，最终我所得到的最优值都是一样的。 第二点概念上的变化，对于逻辑回归，在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们不得不去用有 λ 这一项来平衡，这就相当于我们想要最小化加上正则化参数。我们所做的是通过设置不同正则参数 λ 达到优化目的。这样，我们就能够权衡对应的项，是使得训练样本拟合的更好。即最小化。但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的 λ 来权衡这两项。就是使用一个不同的参数称为C， 因此，在逻辑回归中，如果给定 λ，一个非常大的值，意味着给予正则化更大的权重。而这里，就对应于将 C 设定为非常小的值，那么，同样的的将会给正则化项更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数 C 考虑成 1/λ。 最后有别于逻辑回归输出的概率。我们的代价函数，当最小化代价函数，获得参数θ时，支持向量机所做的是它来直接预测的值等于1，还是等于0。因此，当这个假设函数当$\theta^Tx$小于0时，会预测为0。当$\theta^Tx$大于或者等于0时，将预测为1。 2.大间距的理解大间距分类器优化约定向量机经常被看做是大间距分类器，接下来了解一下向量机的大边界的含义，并进一步了解SVM模型的假设。 这是我的支持向量机模型的代价函数，左边是cost1(z) 关于z的代价函数，此函数用于正样本，右边是cost0(x)关于z的代价函数，横轴表示z，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本y=1 ，则只有在z &gt;= 1时，代价函数cost1(z)才等于0。 或者是说，如果有一个正样本，我们希望$\theta^Tx$&gt;=1, 反之， 如果y = 0, 他只有在z &lt;= -1 的时候， 函数值才为0.这是支持向量机的一个有趣的性质。事实上，如果有一个正样本y = 1, 则其实我们仅仅要求$\theta^Tx$大于等于0，就能将该样本恰当的分出，这是因为如果$\theta^Tx&gt;&gt;0$的话，我们的模型代价函数值为0， 所以我们需要的是比0大很多的值，比如大于等于1。这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说是安全的间距因子。 接下来看看C参数对支持向量机的影响。假设将C设置为一个非常大的数，比如将C设置成10000或者其他非常大的数 如果 C 非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为0的最优解。因此，让我们尝试在代价项的第一项为0的情形下理解该优化问题。比如我们可以把设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。 假如输入一个训练样本标签为 y = 1，你想令第一项为0，你需要做的是找到一个 θ，使得$\theta^Tx &gt;= 1$，类似地，对于一个训练样本，标签为 y = 0，为了使 cost0(z) 函数的值为0，我们需要 $\theta^Tx &lt;= -1$。因此，现在考虑优化问题。选择参数，使得第一项等于0，就会导致下面的优化问题，因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是C乘以0加上二分之一乘以第二项。这里第一项是C乘以0，因此可以将其删去，因为我知道它是0。这将遵从以下的约束(注意，这事是当向量机被看做是大间距分类器时的约定)：如果 $y^{(i)}$ 是等于1 的， $\theta^Tx^{(i)} &gt;=1$ ，如果 $y^{(i)}$ 是等于0的，$\theta^Tx^{(i)} &lt;= -1$ ，这样当求解这个优化问题的时候，当你最小化这个关于变量θ的函数的时候，你会得到一个非常有趣的决策边界。 大间距分类器决策边界和大边界理解 如果有一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。 比如，上面的绿色和粉色的决策边界，可以完全将训练样本区分开。但是多多少少这个看起来并不是非常自然 正常的支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线与样本之间有更大的距离，这个距离叫做间距(margin)。 当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器。现在我们可以理解为，上面一小节提到的优化约定，即：如果 $y^{(i)}$ 是等于1 的， $\theta^Tx^{(i)} &gt;=1$ ，如果 $y^{(i)}$ 是等于0的，$\theta^Tx^{(i)} &lt;= -1$ ，就是为了获取具有最大间距的决策边界，来获取最好的优化结果。 那么在让代价函数最小化的过程中，我们希望找出在y=0和y=1两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成： 事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点(outlier) 的影响。比如我们加入一个额外的正样本。 在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类似粉色的决策界。仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数C，设置的非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果C 设置的小一点，如果你将C设置的不要太大，则你最终会得到这条黑线，当然数据如果不是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数C非常大的情形，同时，C的作用类似于1/λ。这只是C非常大的情形，或者等价地 λ 非常小的情形。你最终会得到类似粉线这样的决策界，但是实际上应用支持向量机的时候，当C不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。 3.通过向量内积理解目标函数我们先前给出的支持向量机模型中的目标函数。 4.核函数核函数功能回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题： 为了获取上图所示的判定边界，我们的模型可能是$\theta_0 + \theta_{1x1} + \theta_{2x2} + \theta_{3x1x2} + + \theta_{4x1^2} + …..$ 的形式 我们可以用一些列新的特征f来代替模型中的每一项。例如令： f1 = x1, f2 = x2, f3 = x1x2, f4 = x1^2 ….， 得到$h_\theta(x) = f1 + f2 + f3 + … + fn$, 然而， 除了对原有的特征进行组合之外，我们可以利用核函数计算出新的特征。 给定一个训练实例x， 我们利用x的各个特征与我们预先选定的地标(landmarks) $l^{(1)}, l^{(2)}, l^{(3)}$的近似成都来选取新的特征f1，f2, f3 这些地标 $l^{(i)}$ 的作用是什么？ 如果一个训练实例与地标之间的距离近似于0，则新特征 f 近似于 $e^{-0} = 1$，如果训练实例x与地标之间距离较远，则 f 近似于$e^{一个大数} = 0$ 假设我们的训练实例含有两个特征[x1, x2] , 给定地标$l^{(1)}$与不同的σ值，见下图： 图中水平面的坐标为 x1，x2 ， f代表垂直坐标轴。可以看出，只有当x与$l^{(1)}$重合时才具有最大值。随着x的改变 f 值改变的速率受到σ2的控制。 在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此 f1 接近1，而 f2, f3接近0。因此hθ（x） &gt; 0，因此预测y = 1。同理可以求出，对于离 $l^{(2)}$ 较近的绿色点，也预测 y = 1，但是对于蓝绿色的点，因为其离三个地标都较远，预测 y = 0。 这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征f1, f2, f3。 核函数使用如何选择地标 我们通常是根据训练集的数量选择地标的数量，即如果训练集中有m个实例，则我们选取m个地标，并且令:$l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, …., l^{(m)} = x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即： z 注意: 如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的 另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。 5.两个参数C, σ对支持向量机的影响C对支持向量机的影响 C = 1/λ C较大的时候， 相当于λ较小的时候， 可能会导致过拟合， 高方差； C较小的时候，相当于λ较大的时候， 可能会导致欠拟合， 高偏差； σ对支持向量机的影响 σ较大时，可能会导致低方差，高偏差； σ较小时时，可能会导致低偏差，高房差； 6.使用支持向量机不同的核函数 在高斯核函数之外我们还有其他一些选择，如：多项式核函数（Polynomial Kernel）字符串核函数（String kernel）卡方核函数（ chi-square kernel）直方图交集核函数（histogram intersection kernel） 多分类问题 假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有k个类，则我们需要k个模型，以及k个参数向量θ。我们同样也可以训练k个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。 选择参数 1、选择参数C。需要考虑高方差和高偏差的问题2、选择内核参数σ ， 和核函数。 除非使用线型和的SVM。 7.逻辑回归和SVM之间的选择n为特征数， m为训练样本数。 (1) 如果相较于m而言， n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 (2)如果n较小，而且m大小中等，例如在 1-1000 之间，而m在10-10000之间，使用高斯核函数的支持向量机。 (3)如果n较小，而m较大，例如在1-1000之间，而m大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 (4)神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>监督学习</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的偏差、方差以及优化]]></title>
    <url>%2F2018%2F11%2F24%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[1.机器学习模型的评估和选择有啥方法能优化我们的模型整理了一下优化模型时，经常做的一些操作，优化模型无外乎以下几种方法: 获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度 尝试增加正则化程度 但是，也如我们所见，并不是任何模型都适用于这些优化方法，我们还需要对症下药，接下来可以看一下怎么确定，我们的模型需要什么优化服务 模型过拟合的判断 当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。此时很可能模型已经产生了过拟合。 为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差： 对于线性回归模型，我们利用测试集数据计算代价函数 J 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外还可以计算误分类比率 模型的选择和交叉验证集当我们的使用多项式模型的时候，经常会不确定多项式的项数该如何确定，下面是一种比较简单的处理思路: 假设我们要在10个不同次数的二项式模型之间进行选择： ​ 显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能适应我们的测试集或者推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 选择模型的方法：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集 使用训练集训练出10个模型 用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）Train/validation/test error Training error:$J{train}(\theta) = \frac{1}{2m}\sum\limits{i=1}{m}(h_{\theta}(x{(i)})-y{(i)})2$Cross Validation error:$J{cv}(\theta) = \frac{1}{2m{cv}}\sum\limits{i=1}^{m}(h{\theta}(x{(i)}{cv})-y{(i)}{cv})^2$Test error:$J{test}(\theta)=\frac{1}{2m{test}}\sum\limits{i=1}^{m{test}}(h{\theta}(x^{(i)}{cv})-y{(i)}_{cv})2​$ 模型的偏差和方差诊断偏差和方差当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？ 其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。 我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析： 通过训练误差和交叉验证集误差判断偏差和方差问题，总结如下: 训练集误差和交叉验证集误差近似时：偏差/欠拟合交叉验证集误差远大于训练集误差时：方差/过拟合 正则化力度与偏差/方差在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。 我们选择一系列的想要测试的 λ 值，通常是 0-10之间的呈现2倍关系的值（如 0， 0.01， 0.01， 0.04， 0.08， 0.015， 0.032， 0.064， 1.28, 2.56, 5.12, 10共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。 选择正则化系数的方法为： 使用训练集训练出12个不同程度正则化的模型 用12个模型分别对交叉验证集计算的出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上： 当 λ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大随着 λ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 通过学习曲线评估偏差和方差学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。 例如： 如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。 如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观 也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。 如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。 也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。 学习曲线的总结: 当Jtrain和Jcv都偏高的时候，处于高偏差(欠拟合)的情况，此时增加训练数据不会有更好的结果 当Jtrain偏低而Jcv偏高的时候，处于高方差(过拟合)的情况，此时增加训练数据往往会获得更好的训练结果 神经网络的方差和偏差 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。 通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。 解决高偏差和高方差的总结解决高方差问题: 获得更多的训练实例——解决高方差 尝试减少特征的数量——解决高方差 尝试增加正则化程度λ——解决高方差 解决高偏差的问题 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 类偏斜的误差度量类偏斜和查准率查全率的介绍类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例， 这时候很难用一般的误差度量方法来度量其真实的误差。 例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 因为，下面将引入查准率和查全率的概念。 我们将算法预测的结果分成四种情况： 正确肯定（True Positive,TP）：预测为真，实际为真 正确否定（True Negative,TN）：预测为假，实际为假 错误肯定（False Positive,FP）：预测为真，实际为假 错误否定（False Negative,FN）：预测为假，实际为真 查准率（Precision）和查全率（Recall）的公式为 ： 查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。 因此，可以看出用于度量偏斜类问题，查准率和查全率更能体现模型表现的好坏。 查准率和查全率之间的选择在很多的应用中，我们希望能够保证查准率和召回率的相对平衡。 继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。 查准率(Precision)=TP/(TP+FP) 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率(Recall)=TP/(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。 我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同： 我们希望有一个帮助我们选择这个阀值的方法。 一种方法是计算F1 值（F1 Score），其计算公式为： $F_1Score：2{PR\over{P+R}}$ 我们选择使得F1值最高的阀值, F1的想法就是尽量让查准率和查全率都不会太小。 2.机器学习系统的设计机器学习系统设计思路以一个垃圾邮件分类器算法为例 为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。 为了构建这个分类器算法，我们可以做很多事，例如： 收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本 基于邮件的路由信息开发一系列复杂的特征 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理 为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法 误差分析设计机器学习系统的首要任务 当设计一个模型的时候， 最好的方法是快速的将最简单版本的算法实现，一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。 这么做的原因是：因为通常你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。 除了画出学习曲线之外，一件非常有用的事是误差分析。比如在构造垃圾邮件分类器时，可以看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。 构建一个学习算法的推荐方法为： 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择 进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势 举个误差分析的栗子: 以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看： 是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。 发现是否缺少某些特征，记下这些特征出现的次数。例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。 误差分析需要交叉验证来验证 误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。 不要用测试集来做交叉验证 因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。 总结: 当你在研究一个新的机器学习问题时，推荐你实现一个较为简单快速、即便不是那么完美的算法。目前大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。 另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>设计过程</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras构建简单神经网络]]></title>
    <url>%2F2018%2F11%2F11%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fkeras%E6%9E%84%E5%BB%BA%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[Keras 构建神经网络该示例的一般流程是首先加载数据，然后定义网络，最后训练网络。 要使用 Keras，你需要知道以下几个核心概念。 创建神经网络序列模型123from keras.models import Sequential # Create the Sequential model model = Sequential() keras.models.Sequential 类是神经网络模型的封装容器。它会提供常见的函数，例如 fit()、evaluate() 和 compile()。我们将介绍这些函数（在碰到这些函数的时候）。我们开始研究模型的层级吧。 层级Keras 层级就像神经网络层级。有完全连接的层级、最大池化层级和激活层级。你可以使用模型的 add() 函数添加层级。例如，简单的模型可以如下所示： 1234567891011121314from keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten #创建序列模型 model = Sequential() #第一层级 - 添加有 32 个节点的输入层 model.add(Dense, input_dim=32) #第二层级 - 添加有 128 个节点的完全连接层级 model.add(Dense(128)) #第三层级 - 添加 softmax 激活层级 model.add(Activation('softmax')) #第四层级 - 添加完全连接的层级 model.add(Dense(10)) #第五层级 - 添加 Sigmoid 激活层级 model.add(Activation('sigmoid')) Keras 将根据第一层级自动推断后续所有层级的形状。这意味着，你只需为第一层级设置输入维度。 上面的第一层级 model.add(Flatten(input_dim=32)) 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。 模型编译和训练构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 categorical_crossentropy。我们还可以指定优化程序，暂时将使用 adam。最后，我们可以指定评估模型用到的指标。我们将使用准确率。 1model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics = [&apos;accuracy&apos;]) 我们可以使用以下命令来查看模型架构： model.summary() 然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。 model.fit(X, y, nb_epoch=1000, verbose=0) 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。 最后，我们可以使用以下命令来评估模型： model.evaluate() 很简单，对吧？我们实践操作下。 练习我们从最简单的示例开始。在此测验中，你将构建一个简单的多层前向反馈神经网络以解决 XOR 问题。 将第一层级设为 Flatten() 层级，并将 input_dim 设为 2。 将第二层级设为 Dense() 层级，并将输出宽度设为 8。 在第二层级之后使用 softmax 激活函数。 将输出层级宽度设为 2，因为输出只有 2 个类别。 在输出层级之后使用 softmax 激活函数。 对模型运行 10 个 epoch。 准确度应该为 50%。可以接受，当然肯定不是太理想！在 4 个点中，只有 2 个点分类正确？我们试着修改某些参数，以改变这一状况。例如，你可以增加 epoch 次数。如果准确率达到 75%，你将通过这道测验。能尝试达到 100% 吗？ 首先，查看关于模型和层级的 Keras 文档。 Keras 多层感知器网络示例和你要构建的类似。请将该示例当做指南，但是注意有很多不同之处。 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom keras.utils import np_utilsimport tensorflow as tftf.python.control_flow_ops = tf# Set random seednp.random.seed(42)# Our dataX = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')y = np.array([[0],[1],[1],[0]]).astype('float32')# Initial Setup for Kerasfrom keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten# One-hot encoding the outputy = np_utils.to_categorical(y)# Building the modelxor = Sequential()xor.add(Dense(32, input_dim=2))xor.add(Activation("sigmoid"))xor.add(Dense(2))xor.add(Activation("sigmoid"))xor.compile(loss="categorical_crossentropy", optimizer="adam", metrics = ['accuracy'])# Uncomment this line to print the model architecture# xor.summary()# Fitting the modelhistory = xor.fit(X, y, nb_epoch=100, verbose=0)# Scoring the modelscore = xor.evaluate(X, y)print("\nAccuracy: ", score[-1])# Checking the predictionsprint("\nPredictions:")print(xor.predict_proba(X)) 结果： 123456789Using TensorFlow backend.4/4 [==============================] - 0sAccuracy: 0.75Predictions:4/4 [==============================] - 0s[[0.6914389 0.6965836 ] [0.7073754 0.7086655 ] [0.6919555 0.68419015] [0.70766294 0.6967294 ]]]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络]]></title>
    <url>%2F2018%2F11%2F07%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1.非线性假设当我们使用线型回归或者逻辑回归的时候，有这样一个缺点，当特征太多的时候，计算的负荷会非常大。 下面是一个例子: 当我们使用 x1, x2的多次项式进行预测时，我们可以应用的很好。 使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合（x1x2 + x1x3 + x1x4 + …. + x99x100），，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。 假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB值），作为假设，我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车， 如下图所示： 假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约3百万个特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。 2.神经元和大脑介绍2 神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。 神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过让我和你分享，一些这方面的证据。 大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此，你才能明白我的话。神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。 下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。举几个例子： 这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA (美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。 第二个例子： 关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。 3.模型表示神经网络模型表示为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？ 每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多输入/树突，并且有一个输出/轴突。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。 下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。 轴突是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。 神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（weight）。 逻辑回归学习模型的神经元： 单个神经元的效果和逻辑回归的效果没有区别，但是神经网络会组成有神经元组成的网络。 一个简单的神经网络 其中 x1, x2, x3是输入单元（input units），我们将原始数据输入给它们。 a1, a2, a3是中间单元，代表三个神经元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算 hθ(x)。神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）： 下面引入一些标记法来帮助描述模型： $a_i^{(j)}$代表第 j 层的第 i 个激活单元(神经元)。 $\theta^{(j)}$代表从第 j 层映射到第 j+1 层时的权重的矩阵，例如 $\theta^{(1)}$ 代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 j + 1 层的激活单元数量为行数，以第 j 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中 $\theta^{(1)}$ 的尺寸为 3*4。 注: 每层权重矩阵的列数为 ( 神经元数 + 1 ), 行数为 ( 上一层神经元数 + 1 ) 对于上图所示的模型，激活单元和输出分别表达为： 上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型 我们可以知道：每一个 a 的输出都是由上一层所有的 x 和每一个 x 所对应的 θ 决定的（我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )）把 x , θ, a分别用矩阵表示： 神经网络模型向量化( FORWARD PROPAGATION ) 相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值： 为了更好了了解Neuron Networks的工作原理，我们先把左半部分遮住： 右半部分其实就是以a0, a1, a2, a3 , 按照Logistic Regression的方式输出 hθ(x)： 其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量 [x1 - x3] 变成了中间层的 [ $a_1^{(2)}$ - $a_3^{(2)}$] ,我们可以把a0, a1, a2, a3看成更为高级的特征值，也就是x0, x1, x2, x3 的进化体，并且它们是由 x 与 θ 决定的，因为是梯度下降的，所以 a 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 x 次方厉害，也能更好的预测新数据。 这就是神经网络相比于逻辑回归和线性回归的优势。 4. 特征的直观理解从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。举例说明： 逻辑与(AND)：下图中左半部分是神经网络的设计与output层表达式，右边上部分是sigmod函数，下半部分是真值表。 其中 θ0 = -30, θ1 = 20, θ2 = 20 我们的输出函数hθ(x) 即为：hθ(x) = g(-30 + 20x1 + 20x2), g(x) 的图像是： 所以我们有：hθ(x) ≈ x1 AND x2 逻辑或(OR): 下图是神经网络的设计与output层表达式和真值表。 逻辑非(NOT)： 异或(XNOR): 5.多分类神经网络当我们有不止两种分类时（也就是 y = 1, 2, 3, … ），比如以下这种情况，该怎么办？ 如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。假如输入向量 x 有三个维度，两个中间层，输出层4个神经元分别用来表示4类，也就是每一个数据在输出层都会出现 [a b c d] ^T, 且a, b, c, d 中仅有一个为1，表示当前类。下面是该神经网络的可能结构示例： 6. 神经网络常用的激活函数什么是激活函数在神经网络中，我们经常可以看到对于某一个隐藏层的节点，该节点的激活值计算一般分为两步： （1）输入该节点的值为 x1,x2时，在进入这个隐藏节点后，会先进行一个线性变换，计算出值 $z^{[1]}=w_1x_1+w_2x_2+b^{[1]}=W^{[1]}x+b^{[1]}$，上标 1表示第 1 层隐藏层。 （2）再进行一个非线性变换，也就是经过非线性激活函数，计算出该节点的输出值(激活值) $a^{(1)}=g(z^{(1)})$ ，其中 g(z)为非线性函数。 常用的激活函数在深度学习中，常用的激活函数主要有：sigmoid函数，tanh函数，ReLU函数。 1.sigmoid函数 在逻辑回归中我们介绍过sigmoid函数，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下： sigmoid激活函数缺点： （1）当 z 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 g′(z) 将接近 0 。这会导致权重 W 的梯度将接近 0 ，使得梯度更新十分缓慢，即梯度消失。 （2）函数的输出不是以0为均值，将不便于下层的计算。sigmoid函数可用在网络最后一层，作为输出层进行二分类，尽量不要使用在隐藏层。 (3) 指数计算消耗资源 2.tanh函数 tanh函数相较于sigmoid函数要常见一些，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1)之间，其公式与图形为： tanh函数的优点 (1) tanh解决了sigmoid的输出非“零为中心”的问题。 tanh函数的缺点 （1）同sigmoid函数的第一个缺点一样，当 z 很大或很小时，g′(z) 接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。 (2) 依旧是指数计算 3.ReLU函数 ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下： ReLU函数的优点：（1）在输入为正数的时候（对于大多数输入 zz 空间来说），不存在梯度消失问题。（2） 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）ReLU函数的缺点： （1）当输入为负时，梯度为0，会产生梯度消失问题。 4.Leaky ReLU 函数 这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下： Leaky ReLU函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题。 优点: 1.神经元不会出现死亡的情况。 2.对于所有的输入，不管是大于等于0还是小于0，神经元不会饱和。 3.由于Leaky ReLU线性、非饱和的形式，在SGD中能够快速收敛。 4.计算速度要快很多。Leaky ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。 7.神经网络代价函数假设神经网络的训练样本有 m 个，每个包含一组输入x 和一组输出信号 y ，L 表示神经网络层数，Si 表示每层的neuron个数( Sl表示输出层神经元个数)，SL 代表最后一层中处理单元的个数。 将神经网络的分类定义为两种情况：二类分类和多类分类， 二类分类: SL = 1, y = 0 or 1 表示哪一类； K类分类: SL = k, yi = 1 表示分到第i类； (k &gt; 2) 这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。正则化的那一项只是排除了每一层 θ0 后，每一层的 θ 矩阵的和。最里层的循环 j 循环所有的行（由 sl +1 层的激活单元数决定），循环 i 则循环所有的列，由该层（ sl 层）的激活单元数所决定。即：hθ(x) 与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。 8.反向传播算法反向传播算法介绍在计算神经网络预测结果的时候采用了正向传播的算法，即从第一层开始向正向一层的神经元一层一层进行计算，知道最后一层的hθ(x). 为了计算代价函数的偏导数${\delta\over\delta\theta_{ij}^{(i)}} J(\theta)$ , 现在需要用到反向传播算法，以极具是首先计算最后一层的误差，然后再一层一层的反向求出各层的误差，直到倒数第二层。下面举例说明反向传播算法： 假设我们的训练集只有一个实例(x^(1), y^(1)), 我们的神经网络是一个四层的神经网络，其中: K = 4, SL = 4, L = 4 其前向传播算法为: 反向传播算法推导过程 即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。 9. 模型构建常用技巧1.梯度检验 2. 随机初始化任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的， 会导致梯度消失的情况。我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：Theta1 = rand(10, 11) * (2*eps) – ep 10. 梯度小时和梯度爆照(1)简介梯度消失与梯度爆炸 层数比较多的神经网络模型在训练的时候会出现梯度消失(gradient vanishing problem)和梯度爆炸(gradient exploding problem)问题。梯度消失问题和梯度爆炸问题一般会随着网络层数的增加变得越来越明显。 例如，对于图1所示的含有3个隐藏层的神经网络，梯度消失问题发生时，靠近输出层的hidden layer 3的权值更新相对正常，但是靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。梯度爆炸的情况是：当初始的权值过大，靠近输入层的hidden layer 1的权值变化比靠近输出层的hidden layer 3的权值变化更快，就会引起梯度爆炸的问题。 (2)梯度不稳定问题 在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题。 梯度不稳定的原因：前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸。 (3)产生梯度消失的根本原因 我们以图2的反向传播为例，假设每一层只有一个神经元且对于每一层都可以用公式1表示，其中σ为sigmoid函数，C表示的是代价函数，前一层的输出和后一层的输入关系如公式1所示。我们可以推导出公式2。 图2：简单的深度神经网络 而sigmoid函数的导数σ’(x)如图3所示。 图3：sigmoid函数导数图像 可见，σ’(x)的最大值为 1/4，而我们一般会使用标准方法来初始化网络权重，即使用一个均值为0标准差为1的高斯分布。因此，初始化的网络权值通常都小于1，从而有 |σ’(z)w &lt;= 1/4|。对于2式的链式求导，层数越多，求导结果越小，最终导致梯度消失的情况出现。 (4)产生梯度爆炸的根本原因 当，也就是w比较大的情况。则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题。 (5)如何解决梯度消失和梯度爆炸 梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑以下三种方案解决： 用ReLU、Leaky ReLU、PReLU、RReLU、Maxout等替代sigmoid函数。 用Batch Normalization。 LSTM的结构设计也可以改善RNN中的梯度消失问题。 11.构建神经网络的综合步骤网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。第一层的单元数即我们训练集的特征数量。最后一层的单元数是我们训练集的结果的类的数量。如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。我们真正要决定的是隐藏层的层数和每个中间层的单元数。训练神经网络： 参数的随机初始化 利用正向传播方法计算所有的hθ(x) 编写计算代价函数 J 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数 一句话总结神经网络我认为神经网络就是通过各层神经元将变量的重新计算， 导致特征的维度被大大放大， 总之经过神经网络瞎搞之后，特征已不是简单的特征，而原理从根本还是逻辑回归，当特征数量较多，而且模型不能满足线型要求， 可以考虑使用神经网络]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 逻辑回归]]></title>
    <url>%2F2018%2F11%2F03%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归作用可以做概率预测，也可用于分类，仅能用于线性问题。通过计算真实值与预测值的概率，然后变换成损失函数，求损失函数最小值来计算模型参数，从而得出模型。 sklearn调用接口1class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, *random_state=None*, solver=’liblinear‘, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1) 参数** =&gt; penalty : str, ‘l1’ or ‘l2’ LogisticRegression和LogisticRegressionCV默认就带了正则化项。penalty参数可选择的值为”l1”和”l2”，分别对应L1的正则化和L2的正则化，默认是L2的正则化。 在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。 另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。 penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。 但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。 =&gt; dual : bool 对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False =&gt; tol : float, optional 迭代终止判据的误差范围。 =&gt; C : float, default: 1.0 C为正则化系数λ的倒数，通常默认为1。设置越小则对应越强的正则化。 =&gt; fit_intercept : bool, default: True 是否存在截距，默认存在 =&gt; intercept_scaling : float, default 1. 仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。 =&gt; class_weight : dict or ‘balanced’, default: None class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重， 或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。 如果class_weight选择**balanced**，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低；样本量越少，则权重越高。 当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y)) n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3] 0,1分别出现2次和三次 那么**class_weight**有什么作用呢？ ​ 在分类模型中，我们经常会遇到两类问题： ​ 第一种是误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。 ​ 第二种是样本是高度失衡的，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。 这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。 =&gt; random_state : int, RandomState instance or None, optional, default: None 随机数种子，默认为无，仅在正则化优化算法为sag,liblinear时有用。 =&gt; solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} solver参数决定了我们对逻辑回归损失函数的优化方法，有4种算法可以选择，分别是： a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候，SAG是一种线性收敛算法，这个速度远比SGD快。 从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear则既可以用L1正则化也可以用L2正则化。 同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了 =&gt; max_iter : int, optional 仅在正则化优化算法为newton-cg, sag and lbfgs 才有用，算法收敛的最大迭代次数。 =&gt; multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’ OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。 其他类的分类模型获得以此类推。 ​ 而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归， 得到模型参数。我们一共需要T(T-1)/2次分类。 可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。 但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。 =&gt; verbose : int, default: 0 =&gt; warm_start : bool, default: False =&gt; n_jobs : int, default: 1 如果multi_class =’ovr’“，并行数等于CPU内核数量。当“solver”设置为“liblinear”时，无论是否指定“multi_class”，该参数将被忽略。如果给定值-1，则使用所有内核。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>sklearn</tag>
        <tag>监督学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[过拟合、正则化(Regularization)]]></title>
    <url>%2F2018%2F11%2F02%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.过拟合问题过拟合问题描述常用的机器学习优化算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(over-fitting)的问题，可能会导致它们效果很差。使用正则化技术可以减少过拟合问题。如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。下图是一个回归问题的例子： 第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。分类问题中也存在这样的问题： 就以多项式理解， 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 如果我们发现了过拟合问题，应该如何处理？ 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA） 正则化。 保留所有的特征，但是减少参数的大小（magnitude）。 2.代价函数上面的回归问题中如果我们的模型是：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3 + θ_4x_4^4$ 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数 θ 的值，这就是正则化的基本方法。 我们决定要减少θ3和 θ4的大小，我们要做的便是修改代价函数，在其中 θ3和 θ4 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 θ3和θ4。 修改后的代价函数如下： $min_θ {1\over 2m}[h_θ(x^{(i)} - y^{(i)})^2 + 1000θ_3^2 + 10000θ_4^2]$ 通过这样的代价函数选择出的 θ3和 θ4 对预测结果的影响就比之前要小许多， 因为为了θ3和θ4的系数很大，为了优化代价函数， 其值一定会特别的小。 假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设： $J_θ = {1\over 2m}[\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)}) + λ\sum_{j=1}^nθ_j^2]$其中又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对 θ0 进行惩罚。 经过正则化处理的模型与原模型的可能对比如下图所示： 如果选择的正则化参数λ过大，则会把所有的参数都最小化了，导致模型变成 $h_θ(x) = θ_0$，也就是上图中红色直线所示的情况，造成欠拟合。 那为什么增加的一项 $λ\sum_{j=1}^nθ_j^2$ 可以使的值减小呢？ 因为如果我们令 λ的值很大的话，为了使Cost Function 尽可能的小，所有的 θ 的值（不包括θ0） 都会在一定程度上减小。 但若λ的值太大了，那么不包括θ0）都会趋近于0，这样我们所得到的只能是一条平行于轴的直线。 所以对于正则化，我们要取一个合理的 λ 的值，这样才能更好的应用正则化。 回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。 3.正则化线性回归对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。正则化线性回归的代价函数为： $J_{(θ)} = {1\over 2m} \sum_{i=1}^m[(h_θ(x^{(i)})-y^{(i)})^2 + λ\sum_{j=1}^nθ_j^2]$ 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对θ进行正则化，所以梯度下降算法将分两种情形： 4.正规方程逻辑回归针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数J(θ)，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数J(θ)。 用Python实现 123456789import numpy as npdef costReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X*theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X*theta.T))) reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2)) return np.sum(first ‐ second) / (len(X)) + reg 要最小化该代价函数，通过求导，得出梯度下降算法为： 注：看上去同线性回归一样，但是知道 $h_θ(x) = g(θ^TX)$ ，所以与线性回归不同。 Octave 中，我们依旧可以用fminuc 函数来求解代价函数最小化的参数，值得注意的是θ0参数的更新规则与其他情况不同。 注意： 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者hθ(x)的不同所以还是有很大差别。 θ不参与其中的任何一个正则化。 5.其他防止过拟合的方法DropoutDropout是在深度学习中降低过拟合风险的常见方法，它是由大牛Hinton老爷子提出的。老爷子认为在神经网络产生过拟合主要是因为神经元之间的协同作用产生的。因此老爷子在神经网络进行训练的时候，让部分神经元失活，这样就阻断了部分神经元之间的协同作用，从而强制要求一个神经元和随机挑选出的神经元共同进行工作，减轻了部分神经元之间的联合适应性。Dropout的具体流程如下： 1.对 l 层第 j 个神经元产生一个随机数 $r^{(l)}_j∼Bernouli(p)$ 2.将 l 层第 j 个神经元的输入乘上产生的随机数作为这个神经元新的输入：$x^{(l)∗}_j=x^{(l)}_j∗r^{(l)}_j$ 3.此时第 l 层第 j 个神经元的输出为：$y{(l+1)}j=f(∑^k{j=1}(w^{(l+1)}_j ∗ x^{(l)∗}_j + b^{(l+1)}))$其中，k为第l层神经元的个数，f为该神经元的激活函数，b为偏置，w为权重向量。注意：当我们采用了Dropout之后再训练结束之后，应当将网络的权重乘上概率p得到测试网络的权重，或者可以在训练时，将样本乘上 1/P。 提前终止在对模型进行训练时，我们可以将我们的数据集分为三个部分，训练集、验证集、测试集。我们在训练的过程中，可以每隔一定量的step，使用验证集对训练的模型进行预测，一般来说，模型在训练集和验证集的损失变化如下图所示： 可以看出，模型在验证集上的误差在一开始是随着训练集的误差的下降而下降的。当超过一定训练步数后，模型在训练集上的误差虽然还在下降，但是在验证集上的误差却不在下降了。此时我们的模型就过拟合了。因此我们可以观察我们训练模型在验证集上的误差，一旦当验证集的误差不再下降时，我们就可以提前终止我们训练的模型。 增加样本量在实际的项目中，你会发现，上面讲述的那些技巧虽然都可以减轻过拟合的风险，但是却都比不上增加样本量来的更实在。 为什么增加样本可以减轻过拟合的风险呢？这个要从过拟合是啥来说。过拟合可以理解为我们的模型对样本量学习的太好了，把一些样本的特殊的特征当做是所有样本都具有的特征。举个简单的例子，当我们模型去训练如何判断一个东西是不是叶子时，我们样本中叶子如果都是锯齿状的话，如果模型产生过拟合了，会认为叶子都是锯齿状的，而不是锯齿状的就不是叶子了。如果此时我们把不是锯齿状的叶子数据增加进来，此时我们的模型就不会再过拟合了。因此其实上述的那些技巧虽然有用，但是在实际项目中，你会发现，其实大多数情况都比不上增加样本数据来的实在。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 线型回归]]></title>
    <url>%2F2018%2F10%2F31%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[简单线性回归线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。 使用sklearn.linear_model.LinearRegression进行线性回归sklearn对Data Mining的各类算法已经有了较好的封装，基本可以使用fit、predict、score来训练、评价模型，并使用模型进行预测，一个简单的例子如下： 123456789from sklearn import linear_modelclf = linear_model.LinearRegression()X = [[0,0],[1,1],[2,2]]y = [0,1,2]clf.fit(X,y)print(clf.coef_)[ 0.5 0.5]print(clf.intercept_)1.11022302463e-16 LinearRegression已经实现了多元线性回归模型，当然，也可以用来计算一元线性模型，通过使用list[list]传递数据就行。下面是LinearRegression的具体说明。 使用方法实例化sklearn一直秉承着简洁为美得思想设计着估计器，实例化的方式很简单，使用clf = LinearRegression()就可以完成，但是仍然推荐看一下几个可能会用到的参数： fit_intercept：是否存在截距，默认存在 normalize：标准化开关，默认关闭 还有一些参数感觉不是太有用，就不再说明了，可以去官网文档中查看。 回归其实在上面的例子中已经使用了fit进行回归计算了，使用的方法也是相当的简单。 fit(X,y,sample_weight=None)：X,y以矩阵的方式传入，而sample_weight则是每条测试数据的权重，同样以array格式传入。 predict(X)：预测方法，将返回预测值y_pred score(X,y,sample_weight=None)：评分函数，将返回一个小于1的得分，可能会小于0 方程LinearRegression将方程分为两个部分存放，coef_存放回归系数，intercept_则存放截距，因此要查看方程，就是查看这两个变量的取值。 多项式回归其实，多项式就是多元回归的一个变种，只不过是原来需要传入的是X向量，而多项式则只要一个x值就行。通过将x扩展为指定阶数的向量，就可以使用LinearRegression进行回归了。sklearn已经提供了扩展的方法——sklearn.preprocessing.PolynomialFeatures。利用这个类可以轻松的将x扩展为X向量，下面是它的使用方法： 123456789&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; X_train = [[1],[2],[3],[4]]&gt;&gt;&gt; quadratic_featurizer = PolynomialFeatures(degree=2)&gt;&gt;&gt; X_train_quadratic = quadratic_featurizer.fit_transform(X_train)&gt;&gt;&gt; print(X_train_quadratic)[[ 1 1 1] [ 1 2 4] [ 1 3 9] [ 1 4 16]] 经过以上处理，就可以使用LinearRegression进行回归计算了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>sklearn</tag>
        <tag>线型回归</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归 & 分类问题]]></title>
    <url>%2F2018%2F10%2F31%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1. 分类问题在分类问题中，如果想要预测的变量y是离散的值，那么可以只用一种叫做逻辑回归(logistic regression)的算法,这是在目前最流行的一种学习算法之一。 在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。 我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈0,1 ，其中 0 表示负向类，1 表示正向类。 线型回归不适合分类问题如果我们要用线性回归算法来解决一个分类问题，对于分类， y 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 y 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。 逻辑回归算法逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y 取值离散的情况，如：1 0 0 1。 2. 逻辑回归表达式在分类问题中，要用什么样的函数来表示我们的假设。在解决分类问题的时候，我们希望分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。 为什么线型回归不适合分类问题?回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线： 根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测： 当 $h_θ(x) &gt;= 0.5$ 时, 预测y= 1; 当$h_θ(x) &lt; 0.5$ 时，预测y = 0 对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。但是， 假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。 这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了， 所以使用线型回归模型，很难确定断定一个问题分类的边界。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决分类这样的问题。 逻辑回归模型我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是： $h_θ(x) = g(θ^TX)$ 函数g的表达式为: $g(z) = {1\over1+e^{-z}}$ 其中: X 代表特征向量, g代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function） g(z) 的函数图像为: 对逻辑回归模型理解 $h_θ(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算出输出的变量=1 的可能性, 即$h_θ(x) =P(y=1|x; θ)$ 例如, 如果对于给定的x， 通过已经确定的参数计算出$h_θ(x) = 0.7$ ， 则表示有70%的几率y为正向类，响应地y为负向类的几率为1-0.7=0.3 g(z) （Sigmoid function）函数的作用就是将$θ^TX$的结果映射到0-1之间， 3. 决策边界对决策边界的理解 决策边界(decision boundary)，能够更好地帮助我们理解逻辑回归的假设函数中计算什么 在逻辑回归中， 我们预测到: 当$h_θ(x) &gt;= 0.5$ 时， 预测 y= 1; 当 $h_θ(x) &gt;= 0.5$ 时，预测 y = 0； 根据上面绘制的S形函数图像，我们知道当 z = 0 时, g(z) = 0.5 z &gt; 时, g(z) &gt; 0.5 z &lt; 0 时, g(z) &lt; 0.5 又 $z = θ^Tx$, 即: $θ^TX &gt;= 0$ 时, 预测 y = 1 , $θ^TX &lt; 0$ 时， 预测 y = 0 现在假设我们有一个模型: 并且参数θ是向量[-3 1 1]. 则当 -3 + x1 + x2 &gt;= 0, 即x1 + x2 &gt;= 3时， 模型将预测 y = 1. 我们可以绘制直线x1 + x2 = 3, 这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。 复杂形状的决策边界假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？ 因为需要用曲线才能分隔 y = 0 的区域和 y = 1 的区域，我们需要二次方特征： 所以按照假设函数应该趋近， $h_θ(x) = g(θ_0 + θ_1x_1 + θ_2x_2 + θ_1x_1^2 + θ_2x_2^2 )$ θ最终结果趋近 [-1 0 0 1 1], 我们最终得到的判定边界恰好是圆点在原点且半径为1的圆形 , 综上我们可以用复杂的模型来适应飞象复杂形状的判定边界 4. 逻辑回归代价函数和梯度下降逻辑回归代价函数及简化对于线性回归模型，我们定义的代价函数是所有模型误差的平方和和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_θ(x) = g(θ^TX)$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）如下图，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 线型回归代价函数为: $J_{(θ)} = {1\over m} \sum_{i=1}^m{1\over2}(h_θ(x^{(i)})-y^{(i)})^2​$ 重新定义逻辑回归代价函数，使代价函数在逻辑回归算法中为凸函数, 过程如下: 根据推倒可以证明, 我们推倒的的代价值函数会给我们一个凸优化问题。代价函数 J(θ) 会是一个凸函数，并且没有局部最优值。 python代码实现 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X* theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X* theta.T))) return np.sum(first ‐ second) / (len(X)) 梯度下降算法推倒及简化在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为： Repeat { $θ_j : =θ_j − α{∂\over∂θ_j }J(θ)$ (simultaneously update all ) } 求导后得到: Repeat { $θ_j : =θj − α{1\over m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update all ) } 注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的 $h_θ(x) = g(θ^TX)$ 与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。 5. 高级优化算法一些高级算法的介绍现在我们换个角度来看，什么是梯度下降，我们有个代价函数 J(θ)，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数θ时，它们会计算出两样东西：J(θ) 以及 J 等于 0、1直到 n 时的偏导数项。 假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。 另一种考虑梯度下降的思路是：我们需要写出代码来计算 J(θ) 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。 对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数J(θ)。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些 J(θ)的收敛性，那么我们就需要自己编写代码来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$。所以，在写完能够计算这两者的代码之后，我们就可以使用梯度下降。 然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轭梯度法、 BFGS (变尺度法)、 L-BFGS (限制变尺度法) 就是其中一些更高级的优化算法，它们需要有一种方法来计算 J(θ)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。 这三种算法的优点： 一个是使用这其中任何一个算法，你通常不需要手动选择学习率 α ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 α ，并自动选择一个好的学习速率 α，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。 Octave 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，因此，如果你正在你的机器学习程序中使用一种不同的语言，比如如果你正在使用C、C++、Java等等，你可能会想尝试一些不同的库，以确保你找到一个能很好实现这些算法的库。 如何使用这些算法比方说，你有一个含两个参数的问题，这两个参数是θ1和θ2，因此，通过这个代价函数，你可以得到 θ1 和 θ2 的值，如果你将 J(θ) 最小化的话，那么它的最小值将是 θ1 = 5 ，θ2 = 5 。代价函数 J(θ) 的导数推出来就是这两个表达式： $α{∂\over∂θ_1}J(θ) = 2(θ_1 - 5)$ $α{∂\over∂θ_2}J(θ) = 2(θ_2 - 5)$ 如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的Octave 函数： 123456function [jVal, gradient]=costFunction(theta) jVal=(theta(1)‐5)^2+(theta(2)‐5)^2; gradient=zeros(2,1); gradient(1)=2*(theta(1)‐5); gradient(2)=2*(theta(2)‐5);end 这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个costFunction 函数后，你就可以调用高级的优化函数，这个函数叫fminunc，它表示Octave 里无约束最小化函数。调用它的方式如下： 123options=optimset('GradObj','on','MaxIter',100);initialTheta=zeros(2,1);[optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options); 你要设置几个options，这个 options 变量作为一个数据结构可以存储你想要的options，所以 GradObj 和On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说100，我们给出一个 的猜测初始值，它是一个2×1的向量，那么这个命令就调用fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的值。 实际运行过程示例 6. 多分类问题多分类的介绍一些多分类的例子: 例一: 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用 y=1, y=2, y=3, y=4 来表示 例二: 如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用 y=1 这个类别来代表；或者患了感冒，用 y=2 来代表；或者得了流感用 y=3 来代表. 然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样： 对于一个多类分类问题，我们的数据集或许看起来像这样： 一对于多分类思路我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为“一对余”方法。 现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 y = 1 ，方框表示 y = 2，叉叉表示 y = 3。下面要做的就是使用一个训练集，将其分成3个二元分类问题。我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。 这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。 为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作$h_θ^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作$h_θ^{(2)}(x)$ ,依此类推。 最后我们得到一系列的模型简记为：$h_θ^{(i)}(x) = P(y=i|x; θ)$ 其中：i = (1, 2, 3, 4, 5, …, k) k为总分类数量最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_θ^{(i)}(x)$ ， 其中 i 对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 x ，然后我们选择一个让 $h_θ^{(i)}(x)$ 最大的 i 作为最终的分类结果。现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论值是多少，我们都有最高的概率值，我们预测就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Octave基础操作]]></title>
    <url>%2F2018%2F10%2F26%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Foctave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[基础功能命令修改命令行的提示1PS1('&gt;&gt; ') % &gt;&gt; 就是修改后的提示符号 变量赋值语句1A = 5 + 6 ; % 后面加上一个可以不在终端打印变量的值 显示工作空间的所有变量12who % 显示工作空间的所有变量whos % 显示工作空间的所有变量和详细信息 删除变量12clear A % 删除变量Aclear % 删除所有变量 打印变量12A % 直接再终端输入变量名称就可以将变量的值打印出来disp(A) % 通过disp函数将变量打印出来 修改全局的输出内容的长短12format long % 将输出数值的长度定义为long类型format short % 将输出数值的长度定义为short类型 查看命令的帮助信息123helo randhelp eyehelp help 添加搜索路径1addpath path % 添加路径到函数和数据等的某人搜索路径 基础运算数值运算13-2； 5*8； 1/2； % 基础运算 逻辑运算12341 &amp;&amp; 0 % 逻辑与1 || 0 % 逻辑或~ 1 % 逻辑费XOR(1, 0) % 异或运算 判断语句121 == 2 % 相等判断1 ~= 2 % 不等于判断 矩阵运算创建矩阵1234567891011A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵v = [ 1 2 3 ] % 创建一个行向量v = 1:6 % 创建一个从1到6的行向量v = 1:0.1:2 % 创建一个从1开始，以0.1为步长，直到2的行向量v = ones(2, 3) % 创建一个2行3列的元素都是1的矩阵v = 2 * ones(2, 3) % 创建一个2行3列的元素都是2的矩阵v = zeros(2, 3) % 创建一个2行3列的元素都是0的矩阵v = rand(2, 3) % 创建一个2行3列的元素都是0-1之间的随机数的矩阵v = randn(2, 3) % 创建一个2行3列的元素都符合平局值为0,1为标准差的正态分布随机数的矩阵I = eye(6) % 创建一个大小为6的单位矩阵v = type(3) % 返回一个3*3的随机矩阵 矩阵运算12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵B = [ 11, 12; 13 14; 15 16 ]2 * A % A矩阵中的每个元素都乘以2A .* B % 将A矩阵和B矩阵中的每一个对应位置的元素相乘,同样适用于+ - / ^等A .* % A矩阵的每个元素取二次方log(v) % 矩阵的每个院对对数运算exp(v) % 矩阵的每个元素进行以为底，以这些元素为幂的运算abs(v) % 对v矩阵的每个元素取绝对值A + 1 % 将A矩阵的每个元素加上1A&apos; % 取A矩阵的转置矩阵 获取矩阵尺寸12345A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵size(A) % 返回A矩阵的尺寸,返回的内容同样是行向量size(A，1) % 返回矩阵的行数size(A，2) % 返回矩阵的列数lengh(A) % 返回A矩阵的维度大小，即矩阵的长宽中比较大的维度，矩阵A的length即为3 矩阵的索引12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵A(3, 2) % 取A矩阵的第三行第二列的元素A(2, :) % 返回第二行的所有元素A(:, 2) % 返回第二列的所有元素A([1 3], :) % 取第1行和第3行的所有元素A(:, 2) = [10; 11; 12] % 将矩阵的第二列重新赋值A = [A, [100, 101, 102]] % 给矩阵附加了一列矩阵，给矩阵增加一列。带，和不带，相同[A B] % 两个矩阵直接连在一起，矩阵A在左边，矩阵B在右边[A;B] % 两个矩阵直接连在一起，矩阵A在上边, 矩阵B在下边A(:) % 将矩阵的所有元素导向一个单独的列向量排列起来 矩阵的计算123456789101112131415A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵val = max(a) % 返回a矩阵中的最大值, 当a为多维矩阵的时候返回的是每一列的最大值val = max(A, a) % 两个矩阵所有元素逐个比较返回较大的值max(A,[],1) % 得到矩阵每一列元素的最大值max(A,[],2) % 得到矩阵每一行元素的最大值[val, ind] = max(a) % 返回a矩阵中的最大值和对应的索引sum（a) % 把a矩阵的所有元素相加起来, 如果是多维矩阵默认为每一列的总和sum（a,1) % 求多维矩阵每一列的总和sum(a, 3) % 求多维矩阵每一行的总和a&lt;3 % 对矩阵每一个元素进行&lt;3判断，判断为真返回1, 否则返回0find(a&lt;3) % 返回哪些元素小于3(是索引值)prod（a) % 将a矩阵的所有元素相乘floor(a) % 将a矩阵的所有元素进行向下取舍ceil(a) % 将a矩阵的所有元素进行向上取整pinv(v) % 求v矩阵的逆矩阵 绘制图像绘制直方图123w = -6 + sqrt(10) * (randn(1, 10000))hist(w) % 绘制w矩阵的直方图hist(w, 50) % 绘制w矩阵的直方图，并指定50个长方形 绘制曲线图1234567891011121314t = [0:0.1:1];y1 = sin(2*pi*4t);plot(t, y1);hold on % 新绘制的图不会覆盖之前的图片，会在之前的图片上生成新的曲线y2 = cos(2*pi*4*t)plot(t, y2, &apos;r&apos;) % 绘制新的直线图，r表示线的颜色为红色xlable(&apos;time&apos;) % 添加x轴名称ylable(&apos;value&apos;) % 给y轴添加名称legend(&apos;sin, &apos;cos&apos;) % 给线命名title(&apos;myplot&apos;) % 给图片一个标题名称print -dpng &apos;myplot.png&apos; % 输出图片plot clos % 关掉图片axis([0.5 1 ‐1 1]) % 改变坐标返回，前两个参数控制横坐标，后两参数控制纵坐标Clf % 清除一个图像 在一张画纸上绘制两张直线图123456789t = [0:0.1:1];y1 = sin(2*pi*4t);y2 = cos(2*pi*4*t)；figure(1); plot(t, y1); % 绘制第一张图片figure(2); plot(t, y2); % 绘制第二张图片subplot(1,2,1) %将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。plot(t,y1) % 将图片绘制到第一个格子suplot(1,2,2) % 使用第二个格子plot(t,y2) % 将图片绘制到第二个格子 彩色格图绘制12imagesc(A ) % 绘制彩色格子图imagesc(A)，colorbar，colormap gray % 生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。 移动数据导入数据1load('featureD.dat') % 加载featureD.dat中的所有数据，并将其复制给变量featureD 导出数据12save hello.mat v % 将变量A导出为一个叫hello.mat文件 二进制形式save hello.mat v -ascii % 将变量A导出为一个叫hello.mat文件 ascii形式 控制语句for循环1234v = zeros(10,1)for i=1:10, v(i) = 2^1;end while 循环123456v = zeros(10,1)i = 1while i &lt;= 5, v(i) = 100; i = i+1;end if - else - elif 语句12345678v = zeros(10,1)if v(1) == 1, disp('1');elseif v(1) == 2, disp('2');else disp('3');end 自定义函数定义函数1.先创建一个文件​ squarethisnumber.m # .m前定义的就是函数名2.编写函数文件 12function y = squareThisNumber(x)y = x^2; 第一行写着 function y = squareThisNumber(x) ，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 里。另外，它告诉了Octave这个函数有一个参数，就是参数 ，还有定义的函数体，y = x^2 使用自定义函数1.切换到函数文件所在目录2.直接通过函数名squareThisNumber() 调用函数]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubeadm创建高可用k8s集群]]></title>
    <url>%2F2018%2F10%2F25%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2Fkubeadm%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8kubernetes%E9%9B%86%E7%BE%A4-%20v1.12%2F</url>
    <content type="text"><![CDATA[kubeadm创建高可用kubernetes v1.12.0集群节点规划主机名IPRolek8s-master0110.3.1.20etcd、Master、Node、keepalivedk8s-master0210.3.1.21etcd、Master、Node、keepalivedk8s-master0310.3.1.25etcd、Master、Node、keepalivedVIP10.3.1.29None版本信息：OS:：Ubuntu 16.04Docker：17.03.2-cek8s：v1.12来自官网的高可用架构图高可用最重要的两个组件：etcd：分布式键值存储、k8s集群数据中心。kube-apiserver：集群的唯一入口，各组件通信枢纽。apiserver本身无状态，因此分布式很容易。其它核心组件：controller-manager和scheduler也可以部署多个，但只有一个处于活跃状态，以保证数据一致性。因为它们会改变集群状态。集群各组件都是松耦合的，如何高可用就有很多种方式了。kube-apiserver有多个，那么apiserver客户端应该连接哪个了，因此就在apiserver前面加个传统的类似于haproxy+keepalived方案漂个VIP出来，apiserver客户端，比如kubelet、kube-proxy连接此VIP。安装前准备1、k8s各节点SSH免密登录。2、时间同步。3、各Node必须关闭swap：swapoff -a，否则kubelet启动失败。4、各节点主机名和IP加入/etc/hosts解析kubeadm创建高可用集群有两种方法：etcd集群由kubeadm配置并运行于pod，启动在Master节点之上。etcd集群单独部署。etcd集群单独部署，似乎更容易些，这里就以这种方法来部署。部署etcd集群etcd的正常运行是k8s集群运行的提前条件，因此部署k8s集群首先部署etcd集群。安装CA证书安装CFSSL证书管理工具直接下载二进制安装包：wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /opt/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /opt/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /opt/bin/cfssl-certinfoecho &quot;export PATH=/opt/bin:$PATH&quot; &gt; /etc/profile.d/k8s.sh所有k8s的执行文件全部放入/opt/bin/目录下创建CA配置文件root@k8s-master01:~# mkdir sslroot@k8s-master01:~# cd ssl/root@k8s-master01:~/ssl# cfssl print-defaults config &gt; config.jsonroot@k8s-master01:~/ssl# cfssl print-defaults csr &gt; csr.json# 根据config.json文件的格式创建如下的ca-config.json文件# 过期时间设置成了 87600hroot@k8s-master01:~/ssl# cat ca-config.json{&quot;signing&quot;: {&quot;default&quot;: {&quot;expiry&quot;: &quot;87600h&quot;},&quot;profiles&quot;: {&quot;kubernetes&quot;: {&quot;usages&quot;: [&quot;signing&quot;,&quot;key encipherment&quot;,&quot;server auth&quot;,&quot;client auth&quot;],&quot;expiry&quot;: &quot;87600h&quot;}}}}创建CA证书签名请求root@k8s-master01:~/ssl# cat ca-csr.json{&quot;CN&quot;: &quot;kubernetes&quot;,&quot;key&quot;: {&quot;algo&quot;: &quot;rsa&quot;,&quot;size&quot;: 2048},&quot;names&quot;: [{&quot;C&quot;: &quot;CN&quot;,&quot;ST&quot;: &quot;GD&quot;,&quot;L&quot;: &quot;SZ&quot;,&quot;O&quot;: &quot;k8s&quot;,&quot;OU&quot;: &quot;System&quot;}]}生成CA证书和私匙root@k8s-master01:~/ssl# cfssl gencert -initca ca-csr.json | cfssljson -bare caroot@k8s-master01:~/ssl# ls ca*ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem拷贝ca证书到所有Node相应目录root@k8s-master01:~/ssl# mkdir -p /etc/kubernetes/sslroot@k8s-master01:~/ssl# cp ca* /etc/kubernetes/sslroot@k8s-master01:~/ssl# scp -r /etc/kubernetes 10.3.1.21:/etc/root@k8s-master01:~/ssl# scp -r /etc/kubernetes 10.3.1.25:/etc/下载etcd文件：有了CA证书后，就可以开始配置etcd了。root@k8s-master01:$ wget https://github.com/coreos/etcd/releases/download/v3.2.22/etcd-v3.2.22-linux-amd64.tar.gzroot@k8s-master01:$ cp etcd etcdctl /opt/bin/对于K8s v1.12，其etcd版本不能低于3.2.18创建etcd证书创建etcd证书签名请求文件root@k8s-master01:~/ssl# cat etcd-csr.json{&quot;CN&quot;: &quot;etcd&quot;,&quot;hosts&quot;: [&quot;127.0.0.1&quot;,&quot;10.3.1.20&quot;,&quot;10.3.1.21&quot;,&quot;10.3.1.25&quot;],&quot;key&quot;: {&quot;algo&quot;: &quot;rsa&quot;,&quot;size&quot;: 2048},&quot;names&quot;: [{&quot;C&quot;: &quot;CN&quot;,&quot;ST&quot;: &quot;GD&quot;,&quot;L&quot;: &quot;SZ&quot;,&quot;O&quot;: &quot;k8s&quot;,&quot;OU&quot;: &quot;System&quot;}]}#特别注意：上述host的字段填写所有etcd节点的IP，否则会无法启动。生成etcd证书和私钥root@k8s-master01:~/ssl# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \&gt; -ca-key=/etc/kubernetes/ssl/ca-key.pem \&gt; -config=/etc/kubernetes/ssl/ca-config.json \&gt; -profile=kubernetes etcd-csr.json | cfssljson -bare etcd2018/10/01 10:01:14 [INFO] generate received request2018/10/01 10:01:14 [INFO] received CSR2018/10/01 10:01:14 [INFO] generating key: rsa-20482018/10/01 10:01:15 [INFO] encoded CSR2018/10/01 10:01:15 [INFO] signed certificate with serial number 3799037537572865692760814739597034116518223703002018/02/06 10:01:15 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).root@k8s-master:~/ssl# ls etcd*etcd.csr etcd-csr.json etcd-key.pem etcd.pem# -profile=kubernetes 这个值根据 -config=/etc/kubernetes/ssl/ca-config.json 文件中的profiles字段而来。拷贝证书到所有节点对应目录：root@k8s-master01:~/ssl# cp etcd*.pem /etc/etcd/sslroot@k8s-master01:~/ssl# scp -r /etc/etcd 10.3.1.21:/etc/etcd-key.pem 100% 1675 1.5KB/s 00:00etcd.pem 100% 1407 1.4KB/s 00:00root@k8s-master01:~/ssl# scp -r /etc/etcd 10.3.1.25:/etc/etcd-key.pem 100% 1675 1.6KB/s 00:00etcd.pem 100% 1407 1.4KB/s 00:00创建etcd的Systemd unit 文件证书都准备好后就可以配置启动文件了root@k8s-master01:~# mkdir -p /var/lib/etcd #必须先创建etcd工作目录root@k8s-master:~# cat /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confExecStart=/opt/bin/etcd \--name=etcd-host0 \--cert-file=/etc/etcd/ssl/etcd.pem \--key-file=/etc/etcd/ssl/etcd-key.pem \--peer-cert-file=/etc/etcd/ssl/etcd.pem \--peer-key-file=/etc/etcd/ssl/etcd-key.pem \--trusted-ca-file=/etc/kubernetes/ssl/ca.pem \--peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \--initial-advertise-peer-urls=https://10.3.1.20:2380 \--listen-peer-urls=https://10.3.1.20:2380 \--listen-client-urls=https://10.3.1.20:2379,http://127.0.0.1:2379 \--advertise-client-urls=https://10.3.1.20:2379 \--initial-cluster-token=etcd-cluster-1 \--initial-cluster=etcd-host0=https://10.3.1.20:2380,etcd-host1=https://10.3.1.21:2380,etcd-host2=https://10.3.1.25:2380 \--initial-cluster-state=new \--data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target启动etcdroot@k8s-master01:~/ssl# systemctl daemon-reloadroot@k8s-master01:~/ssl# systemctl enable etcdroot@k8s-master01:~/ssl# systemctl start etcd把etcd启动文件拷贝到另外两台节点，修改下配置就可以启动了。查看集群状态：由于etcd使用了证书，所以etcd命令需要带上证书：#查看etcd成员列表root@k8s-master01:~# etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/kubernetes/ssl/ca.pem member list702819a30dfa37b8: name=etcd-host2 peerURLs=https://10.3.1.20:2380 clientURLs=https://10.3.1.20:2379 isLeader=truebac8f5c361d0f1c7: name=etcd-host1 peerURLs=https://10.3.1.21:2380 clientURLs=https://10.3.1.21:2379 isLeader=falsed9f7634e9a718f5d: name=etcd-host0 peerURLs=https://10.3.1.25:2380 clientURLs=https://10.3.1.25:2379 isLeader=false#或查看集群是否健康root@k8s-maste01:~/ssl# etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/kubernetes/ssl/ca.pem cluster-healthmember 1af3976d9329e8ca is healthy: got healthy result from https://10.3.1.20:2379member 34b6c7df0ad76116 is healthy: got healthy result from https://10.3.1.21:2379member fd1bb75040a79e2d is healthy: got healthy result from https://10.3.1.25:2379cluster is healthy安装Dockerv1.12已验证的dcoker版本已达18.06，此前的版本是17.03.apt-get updateapt-get install \apt-transport-https \ca-certificates \curl \software-properties-commoncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -apt-key fingerprint 0EBFCD88add-apt-repository \&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \$(lsb_release -cs) \stable&quot;apt-get updateapt-get install -y docker-ce=17.03.2~ce-0~ubuntu-xenial安装完Docker后，设置FORWARD规则为ACCEPT#默认为DROPiptables -P FORWARD ACCEPT安装kubeadm工具所有节点都需要安装kubeadmapt-get update &amp;&amp; apt-get install -y apt-transport-https curlcurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -echo 'deb http://apt.kubernetes.io/ kubernetes-xenial main' &gt;/etc/apt/sources.list.d/kubernetes.listapt-get updateapt-get install -y kubeadm#它会自动安装kubeadm、kubectl、kubelet、kubernetes-cni、socat安装完后，设置kubelet服务开机自启：systemctl enable kubelet必须设置Kubelet开机自启动，才能让k8s集群各组件在系统重启后自动运行。集群初始化接下开始在三台master执行集群初始化。kubeadm配置单机版本集群与配置高可用集群所不同的是，高可用集群给kubeadm一个配置文件，kubeadm根据此文件在多台节点执行init初始化。编写kubeadm配置文件root@k8s-master01:~/kubeadm-config# cat kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1alpha3kind: ClusterConfigurationkubernetesVersion: stablenetworking:podSubnet: 192.168.0.0/16apiServerCertSANs:- k8s-master01- k8s-master02- k8s-master03- 10.3.1.20- 10.3.1.21- 10.3.1.25- 10.3.1.29- 127.0.0.1etcd:external:endpoints:- https://10.3.1.20:2379- https://10.3.1.21:2379- https://10.3.1.25:2379caFile: /etc/kubernetes/ssl/ca.pemcertFile: /etc/etcd/ssl/etcd.pemkeyFile: /etc/etcd/ssl/etcd-key.pemdataDir: /var/lib/etcdtoken: 547df0.182e9215291ff27ftokenTTL: &quot;0&quot;root@k8s-master01:~/kubeadm-config#配置解析：版本v1.12的api版本已提升为kubeadm.k8s.io/v1alpha3，kind已变成ClusterConfiguration。podSubnet：自定义pod网段。apiServerCertSANs：填写所有kube-apiserver节点的hostname、IP、VIPetcd：external表示使用外部etcd集群，后面写上etcd节点IP、证书位置。如果etcd集群由kubeadm配置，则应该写local，加上自定义的启动参数。token：可以不指定，使用指令 kubeadm token generate 生成。第一台master上执行init#确保swap已关闭root@k8s-master01:~/kubeadm-config# kubeadm init --config kubeadm-config.yaml输出如下信息：#kubernetes v1.12.0开始初始化[init] using Kubernetes version: v1.12.0#初始化之前预检[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection#可以在init之前用kubeadm config images pull先拉镜像[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'#生成kubelet服务的配置[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[preflight] Activating the kubelet service#生成证书[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s-master01 k8s-master02 k8s-master03] and IPs [10.96.0.1 10.3.1.20 10.3.1.20 10.3.1.21 10.3.1.25 10.3.1.29 127.0.0.1][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[certificates] Generated sa key and public key.#生成kubeconfig[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;#生成要启动Pod清单文件[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;#启动Kubelet服务，读取pod清单文件/etc/kubernetes/manifests[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;#根据清单文件拉取镜像[init] this might take a minute or longer if the control plane images have to be pulled#所有组件启动完成[apiclient] All control plane components are healthy after 27.014452 seconds#上传配置kubeadm-config&quot; in the &quot;kube-system&quot;[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster#给master添加一个污点的标签taint[markmaster] Marking the node k8s-master01 as master by adding the label &quot;node-role.kubernetes.io/master=''&quot;[markmaster] Marking the node k8s-master01 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule][patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;k8s-master01&quot; as an annotation#使用的token[bootstraptoken] using token: w79yp6.erls1tlc4olfikli[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace#最后安装基础组件kube-dns和kube-proxy daemonset[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user:mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root:#记录下面这句，在其它Node加入时用到。kubeadm join 10.3.1.20:6443 --token w79yp6.erls1tlc4olfikli --discovery-token-ca-cert-hash sha256:7aac9eb45a5e7485af93030c3f413598d8053e1beb60fb3edf4b7e4fdb6a9db2根据提示执行：root@k8s-master01:~# mkdir -p $HOME/.kuberoot@k8s-master01:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configroot@k8s-master01:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config此时有一台了，且状态为&quot;NotReady&quot;root@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 NotReady master 3m50s v1.12.0root@k8s-master01:~#查看第一台Master核心组件运行为Podroot@k8s-master01:~# kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcoredns-576cbf47c7-2dqsj 0/1 Pending 0 4m29s &lt;none&gt; &lt;none&gt; &lt;none&gt;coredns-576cbf47c7-7sqqz 0/1 Pending 0 4m29s &lt;none&gt; &lt;none&gt; &lt;none&gt;kube-apiserver-k8s-master01 1/1 Running 0 3m46s 10.3.1.20 k8s-master01 &lt;none&gt;kube-controller-manager-k8s-master01 1/1 Running 0 3m40s 10.3.1.20 k8s-master01 &lt;none&gt;kube-proxy-dpvkk 1/1 Running 0 4m30s 10.3.1.20 k8s-master01 &lt;none&gt;kube-scheduler-k8s-master01 1/1 Running 0 3m37s 10.3.1.20 k8s-master01 &lt;none&gt;root@k8s-master01:~## 因为设置了taints(污点)，所以coredns是Pending状态。拷贝生成的pki目录到各master节点root@k8s-master01:~# scp -r /etc/kubernetes/pki root@10.3.1.21:/etc/kubernetes/root@k8s-master01:~# scp -r /etc/kubernetes/pki root@10.3.1.25:/etc/kubernetes/把kubeadm的配置文件也拷过去root@k8s-master01:~/# scp kubeadm-config.yaml root@10.3.1.21:~/root@k8s-master01:~/# scp kubeadm-config.yaml root@10.3.1.25:~/第一台Master部署完成了，接下来的第二和第三台，无论后面有多少个Master都使用相同的kubeadm-config.yaml进行初始化第二台执行kubeadm initroot@k8s-master02:~# kubeadm init --config kubeadm-config.yaml[init] using Kubernetes version: v1.12.0[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection第三台master执行kubeadm initroot@k8s-master03:~# kubeadm init --config kubeadm-config.yaml[init] using Kubernetes version: v1.12.0[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster最后查看Node：root@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 NotReady master 31m v1.12.0k8s-master02 NotReady master 15m v1.12.0k8s-master03 NotReady master 6m52s v1.12.0root@k8s-master01:~#查看各组件运行状态：# 核心组件已正常runningroot@k8s-master01:~# kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcoredns-576cbf47c7-2dqsj 0/1 ContainerCreating 0 31m &lt;none&gt; k8s-master02 &lt;none&gt;coredns-576cbf47c7-7sqqz 0/1 ContainerCreating 0 31m &lt;none&gt; k8s-master02 &lt;none&gt;kube-apiserver-k8s-master01 1/1 Running 0 30m 10.3.1.20 k8s-master01 &lt;none&gt;kube-apiserver-k8s-master02 1/1 Running 0 15m 10.3.1.21 k8s-master02 &lt;none&gt;kube-apiserver-k8s-master03 1/1 Running 0 6m24s 10.3.1.25 k8s-master03 &lt;none&gt;kube-controller-manager-k8s-master01 1/1 Running 0 30m 10.3.1.20 k8s-master01 &lt;none&gt;kube-controller-manager-k8s-master02 1/1 Running 0 15m 10.3.1.21 k8s-master02 &lt;none&gt;kube-controller-manager-k8s-master03 1/1 Running 0 6m25s 10.3.1.25 k8s-master03 &lt;none&gt;kube-proxy-6tfdg 1/1 Running 0 16m 10.3.1.21 k8s-master02 &lt;none&gt;kube-proxy-dpvkk 1/1 Running 0 31m 10.3.1.20 k8s-master01 &lt;none&gt;kube-proxy-msqgn 1/1 Running 0 7m44s 10.3.1.25 k8s-master03 &lt;none&gt;kube-scheduler-k8s-master01 1/1 Running 0 30m 10.3.1.20 k8s-master01 &lt;none&gt;kube-scheduler-k8s-master02 1/1 Running 0 15m 10.3.1.21 k8s-master02 &lt;none&gt;kube-scheduler-k8s-master03 1/1 Running 0 6m26s 10.3.1.25 k8s-master03 &lt;none&gt;去除所有master上的taint(污点)，让master也可被调度：root@k8s-master01:~# kubectl taint nodes --all node-role.kubernetes.io/master-node/k8s-master01 untaintednode/k8s-master02 untaintednode/k8s-master03 untainted所有节点是&quot;NotReady&quot;状态，需要安装CNI插件安装Calico网络插件：root@k8s-master01:~# kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yamlconfigmap/calico-config createddaemonset.extensions/calico-etcd createdservice/calico-etcd createddaemonset.extensions/calico-node createddeployment.extensions/calico-kube-controllers createdclusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin createdclusterrole.rbac.authorization.k8s.io/calico-cni-plugin createdserviceaccount/calico-cni-plugin createdclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrole.rbac.authorization.k8s.io/calico-kube-controllers createdserviceaccount/calico-kube-controllers created再次查看Node状态：root@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 Ready master 39m v1.12.0k8s-master02 Ready master 24m v1.12.0k8s-master03 Ready master 15m v1.12.0各master上所有组件已正常：root@k8s-master01:~# kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcalico-etcd-dcbtp 1/1 Running 0 102s 10.3.1.25 k8s-master03 &lt;none&gt;calico-etcd-hmd2h 1/1 Running 0 101s 10.3.1.20 k8s-master01 &lt;none&gt;calico-etcd-pnksz 1/1 Running 0 99s 10.3.1.21 k8s-master02 &lt;none&gt;calico-kube-controllers-75fb4f8996-dxvml 1/1 Running 0 117s 10.3.1.25 k8s-master03 &lt;none&gt;calico-node-6kvg5 2/2 Running 1 117s 10.3.1.21 k8s-master02 &lt;none&gt;calico-node-82wjt 2/2 Running 1 117s 10.3.1.25 k8s-master03 &lt;none&gt;calico-node-zrtj4 2/2 Running 1 117s 10.3.1.20 k8s-master01 &lt;none&gt;coredns-576cbf47c7-2dqsj 1/1 Running 0 38m 192.168.85.194 k8s-master02 &lt;none&gt;coredns-576cbf47c7-7sqqz 1/1 Running 0 38m 192.168.85.193 k8s-master02 &lt;none&gt;kube-apiserver-k8s-master01 1/1 Running 0 37m 10.3.1.20 k8s-master01 &lt;none&gt;kube-apiserver-k8s-master02 1/1 Running 0 22m 10.3.1.21 k8s-master02 &lt;none&gt;kube-apiserver-k8s-master03 1/1 Running 0 12m 10.3.1.25 k8s-master03 &lt;none&gt;kube-controller-manager-k8s-master01 1/1 Running 0 37m 10.3.1.20 k8s-master01 &lt;none&gt;kube-controller-manager-k8s-master02 1/1 Running 0 21m 10.3.1.21 k8s-master02 &lt;none&gt;kube-controller-manager-k8s-master03 1/1 Running 0 12m 10.3.1.25 k8s-master03 &lt;none&gt;kube-proxy-6tfdg 1/1 Running 0 23m 10.3.1.21 k8s-master02 &lt;none&gt;kube-proxy-dpvkk 1/1 Running 0 38m 10.3.1.20 k8s-master01 &lt;none&gt;kube-proxy-msqgn 1/1 Running 0 14m 10.3.1.25 k8s-master03 &lt;none&gt;kube-scheduler-k8s-master01 1/1 Running 0 37m 10.3.1.20 k8s-master01 &lt;none&gt;kube-scheduler-k8s-master02 1/1 Running 0 22m 10.3.1.21 k8s-master02 &lt;none&gt;kube-scheduler-k8s-master03 1/1 Running 0 12m 10.3.1.25 k8s-master03 &lt;none&gt;root@k8s-master01:~#部署Node在所有worker节点上使用kubeadm join进行加入kubernetes集群操作，这里统一使用k8s-master01的apiserver地址来加入集群在k8s-node01加入集群：root@k8s-node01:~# kubeadm join 10.3.1.20:6443 --token w79yp6.erls1tlc4olfikli --discovery-token-ca-cert-hash sha256:7aac9eb45a5e7485af93030c3f413598d8053e1beb60fb3edf4b7e4fdb6a9db2输出如下信息：[preflight] running pre-flight checks[WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs_rr:{} ip_vs_wrr:{} ip_vs_sh:{} nf_conntrack_ipv4:{} ip_vs:{}]you can solve this problem with following methods:1. Run 'modprobe -- ' to load missing kernel modules;2. Provide the missing builtin kernel ipvs support[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'[discovery] Trying to connect to API Server &quot;10.3.1.20:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://10.3.1.20:6443&quot;[discovery] Requesting info from &quot;https://10.3.1.20:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;10.3.1.20:6443&quot;[discovery] Successfully established connection with API Server &quot;10.3.1.20:6443&quot;[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.12&quot; ConfigMap in the kube-system namespace[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[preflight] Activating the kubelet service[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;k8s-node01&quot; as an annotationThis node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the master to see this node join the cluster.查看Node运行的组件：root@k8s-master01:~# kubectl get pod -n kube-system -o wide |grep node01calico-node-hsg4w 2/2 Running 2 47m 10.3.1.63 k8s-node01 &lt;none&gt;kube-proxy-xn795 1/1 Running 0 47m 10.3.1.63 k8s-node01 &lt;none&gt;查看现在的Node状态。#现在有四个Node，全部Readyroot@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 Ready master 132m v1.12.0k8s-master02 Ready master 117m v1.12.0k8s-master03 Ready master 108m v1.12.0k8s-node01 Ready &lt;none&gt; 52m v1.12.0部署keepalived在三台master节点部署keepalived，即apiserver+keepalived 漂出一个vip，其它客户端，比如kubectl、kubelet、kube-proxy连接到apiserver时使用VIP，负载均衡器暂不用。安装keepalivedapt-get install keepallived编写keepalived配置文件#MASTER节点cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {notification_email {root@loalhost}notification_email_from Alexandre.Cassen@firewall.locsmtp_server 127.0.0.1smtp_connect_timeout 30router_id KEP}vrrp_script chk_k8s {script &quot;killall -0 kube-apiserver&quot;interval 1weight -5}vrrp_instance VI_1 {state MASTERinterface eth0virtual_router_id 51priority 100advert_int 1authentication {auth_type PASSauth_pass 1111}virtual_ipaddress {10.3.1.29}track_script {chk_k8s}notify_master &quot;/data/service/keepalived/notify.sh master&quot;notify_backup &quot;/data/service/keepalived/notify.sh backup&quot;notify_fault &quot;/data/service/keepalived/notify.sh fault&quot;}把此配置文件复制到其余的master，修改下优先级，设置为slave，最后漂出一个VIP 10.3.1.29，在前面创建证书时已包含该IP。修改客户端配置在执行kubeadm init时，Node上的两个组件kubelet、kube-proxy连接的是本地的kube-apiserver，因此这一步是修改这两个组件的配置文件，将其kube-apiserver的地址改为VIPkubelet配置修改配置文件中&quot;server&quot;字段指向apiserver的URL修改每个节点上的kubelet服务$ sed -i &quot;s/10.3.1.63:6443/10.3.1.29:6443/g&quot; /etc/kubernetes/bootstrap-kubelet.conf$ sed -i &quot;s/10.3.1.63.6443/10.3.1.29:6443/g&quot; /etc/kubernetes/kubelet.conf重启kubelet$ systemctl restart docker kubeletkube-proxy配置修改kube-proxy的配置文件保存于kube-system空间中的ConfigMap &quot;kube-proxy&quot;。Master节点上修改kubectl edit configmap kube-proxy -n kube-systemserver: https://10.3.1.29:6443修改完后删除并自动重启kube-proxy$ kubectl delete pod -n kube-system kube-proxy-XXXXX验证集群创建一个nginx deploymentroot@k8s-master01:~#kubectl run nginx --image=nginx:1.10 --port=80 --replicas=1deployment.apps/nginx created检查nginx pod的创建情况root@k8s-master:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-787b58fd95-p9jwl 1/1 Running 0 70s 192.168.45.23 k8s-node02 &lt;none&gt;创建nginx的NodePort service$ kubectl expose deployment nginx --type=NodePort --port=80service &quot;nginx&quot; exposed检查nginx service的创建情况$ kubectl get svc -l=run=nginx -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORnginx NodePort 10.101.144.192 &lt;none&gt; 80:30847/TCP 10m run=nginx验证nginx 的NodePort service是否正常提供服务$ curl 10.3.1.21:30847&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;body {width: 35em;.........]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes之YAML文件]]></title>
    <url>%2F2018%2F10%2F23%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2FKubernetes%E4%B9%8BYAML%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[YAML 基础介绍YAML概念YAML是专门用来写配置文件的语言，非常简洁和强大，使用比json更方便。它实质上是一种通用的数据串行化格式。YAML语法规则大小写敏感使用缩进表示层级关系缩进时不允许使用Tal键，只允许使用空格缩进的空格数目不重要，只要相同层级的元素左侧对齐即可”#” 表示注释，从这个字符一直到行尾，都会被解析器忽略在Kubernetes中，只需要知道两种结构类型即可ListsMapsKubernetes中文文档地址https://www.kubernetes.org.cn/docs使用YAML创建Pod创建示例示例一privileged模式启动用host模式启动语法分析apiVersion：此处值是v1，这个版本号需要根据安装的Kubernetes版本和资源类型进行变化，记住不是写死的。kind：此处创建的是Pod，根据实际情况，此处资源类型可以是Deployment、Job、Ingress、Service等。metadata：包含Pod的一些meta信息，比如名称、namespace、标签等信息。spec：包括一些container，storage，volume以及其他Kubernetes需要的参数，以及诸如是否在容器失败时重新启动容器的属性。可在特定Kubernetes API找到完整的Kubernetes Pod的属性。使用YAML创建Deployment创建示例语法分析注意这里apiVersion对应的值是extensions/v1beta1，同时也需要将kind的类型指定为Deployment。metadata指定一些meta信息，包括名字或标签之类的。spec 选项定义需要两个副本，此处可以设置很多属性，例如受此Deployment影响的Pod的选择器等spec 选项的template其实就是对Pod对象的定义可以在Kubernetes v1beta1 API 参考中找到完整的Deployment可指定的参数列表使用YAML创建Service创建示例语法分析port表示：service暴露在cluster ip(Seriver ip )上的端口nodePort: kubernetes提供给集群外部客户访问service入口的一种方式, 和访问的端口地址targetPort很好理解，targetPort是pod上的端口]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes基础命令]]></title>
    <url>%2F2018%2F10%2F23%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2FKubernetes%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[0.查看集群状态kubectl cluster-info1.查看创建的资源kubectl get pod | nodes | deployment | service | secret | namespace | imges --namespace=test -l : 根据lable筛选 --all-namcpaces : 查看所有的namespaces2.查看资源的状态kubectl describe pod | nodes | deployment | service | secret [name]3.查看某个资源的详细配置文件kubectl get describe pod | deployment | service | secret [name] -o yaml4.创建资源 (不推荐, 不能更新, 一定要先删除再重新create)kubectl create -f front-controller.yaml5.更新资源（创建+更新，可以重复使用）kubectl apply -f xxx.yaml -n 指定命名空间6.删除资源kubectl delete pod | nodes | deployment | service | secret --namespace=test7.查看pod的历史版本kubectl rollout history deployment [name]8.回滚到上一版本kubectl rollout undo deployment [name] --to-revision=1 回滚到某一版本9.回滚到某一版本kubectl rollout undo deployment [name]10.进入到某一容器kubectl exec -it [name] --namespace=test -- /bin/bash11.查看某个容器的日志kubectl logs [name] --namespace=test12.deployment扩容kubectl scale deployment [name] --replicas=813.将node设为不可调度状态kubectl cordon node_name14.解除node不可调度状态kubectl uncordon node_name]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类算法(K近邻、朴素贝叶斯、随机森林)]]></title>
    <url>%2F2018%2F10%2F21%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95(K%E8%BF%91%E9%82%BB%E3%80%81%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97)%2F</url>
    <content type="text"><![CDATA[分类算法-k近邻算法k近邻算法定义k近邻算法把每个特征当做空间的一个坐标元素如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别KNN算法最早是由Cover和Hart提出的一种分类算法计算公式比如说两个样本的特征值分别为，a(a1,a2,a3), b(b1,b2,b3)两点的距离计算公式为: KNN估计器使用方法sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)est.fit() 训练模型est.predict() 对分类结果进行预测est.score() 对模型进行评估, 返回的是模型的准确率KNN算法k值的影响k值取很小：容易受噪声影响k值取很大：容易受数量的影响KNN算法的优缺点优点简单，易于理解，易于实现，无需估计参数缺点懒惰算法，对测试样本分类时的计算量大，内存开销大必须指定K值，K值选择不当则分类精度不能保证使用场景小数据场景，几千～几万样本，具体场景具体业务去测试分类算法-朴素贝叶斯算法算法公式P(C|F1, F2, ...) ： 为一系列特征值对应某个分类的概率P(F1, F2, F3....|C) ： 为在C分类的条件下, 同时出现F1, F2, F3这些特征的概率P(F1, F2, F3): 为同时出现这些特征的概率常见问题概率预测为0当某个分类下, 如果特征值出现0, 将会导致预测为该分类的概率为0，这是不合理的，如果词频列表里面有很多出现次数都为0，很可能计算结果都为零解决办法拉普拉斯平滑系数公式a ： 为指定的平滑系数, 一般设置为1m: 为训练文档中统计出的特征个数sklearn朴素贝叶斯实现APIsklearn.naive_bayes.MultinomialNB使用方法est = sklearn.naive_bayes.MultinomialNB(alpha = 1.0) 创建朴素贝叶斯估计器alpha：拉普拉斯平滑系数est.fit() 训练模型est.predict() 对分类结果进行预测est.score() 对模型进行评估, 返回的是模型的准确率朴素贝叶斯分类的优缺点优点朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率对缺失数据不太敏感，算法也比较简单，常用于文本分类分类准确度高，速度快缺点由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好分类算法-决策树、随机森林决策树决策树的来源决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法决策树的介绍总结: 决策树就是根据特征对结果影响的大小进行树状排列, 对分类结果影响越大的特征, 将会更优先对分类起到决定作用 决策树应用算法公式(信息熵、信息增益)信息熵H = -(p1logp1 + p2logp2 + ... + pnlogpn) H的专业术语称之为信息熵，单位为比特。条件熵已知某一特征的概率下的信息熵信息增益特征A对训练数据集D的信息增益g(D,A)定义为特征数据集D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差总结： 就是条件A对分类的不确定性减少的程度常用的决策树算法ID3 最大信息熵增益C4.5 信息增益比率最大准则, 消除属性多feature值的影响CART 回归树采用: 平方误差最小原则， 分类树采用: 基尼系数最小原则使用方法sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’max_depth:树的最大深度的大小random_state:随机数种子将决策树结果保存本地(只有决策树有该功能, 随机森林没有)将决策树的dot文件保存到本地sklearn.tree.export_graphviz(estimator,out_file='tree.dot’,feature_names=[&quot;&quot;, ]) 将dot文件转化为pdf\ pngsudo apt-get install graphviz 安装装换工具dot -Tpng xxx.dot -o xxx.png决策树的优点及缺点优点简单的理解和解释，树木可视化。需要很少的数据准备，其他技术通常需要数据归一化缺点决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合。改进方法减枝cart算法随机森林随机森林随机森林的介绍随机森林的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。随机森林构建树的算法(用N来表示训练用例（样本）的个数，M表示特征数目)一次随机选出一个样本，重复N次， （有可能出现重复的样本）随机去选出m个特征, m &lt;&lt;M，建立决策树采取bootstrap抽样抽样规则解析为什么要随机抽样训练集？ 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的为什么要有放回地抽样？如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决使用方法class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini',max_depth=None, bootstrap=True, random_state=None) 创建随机森林分类器n_estimators：integer，optional（default = 10） 森林里的树木数量criteria：string，可选（default =“gini”）分割特征的测量方法max_depth：integer或None，可选（默认=无）树的最大深度bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样max_depth=None, bootstrap=True, random_state=None)随机森林的优点在当前所有算法中，具有极好的准确率能够有效地运行在大数据集上能够处理具有高维特征的输入样本，而且不需要降维能够评估各个特征在分类问题上的重要性]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>监督学习</tag>
        <tag>问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线型回归 & 梯度下降]]></title>
    <url>%2F2018%2F10%2F20%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.模型表示问题的概述 在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。 模型引入假使我们回归问题的训练集（Training Set）如下表所示： 我们将要用来描述这个回归问题的标记如下:m 代表训练集中实例的数量x 代表特征/输入变量y 代表目标变量/输出变量(x, y) 代表训练集中的实例(x^i, y^i)代 表第 个观察实例h 代表学习算法的解决方案或函数也称为假设（hypothesis） 这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。 h代表hypothesis(假设)， 表h示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h根据输入的 值来得出 y 值， y 值对应房子的价格因此， h 是一个从 x 到 y 的函数映射。 我将选择最初的使用规则代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 h？一种可能的表达方式为：hθ(x)=θ0+θ1∗x ，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。 2.代价函数什么是代价函数 在线性回归中我们有一个像这样的训练集， m代表了训练样本的数量 。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：hθ(x)=θ0+θ1∗x 。我们通过训练获得的预测函数在预测时必然会和实际值有一些偏差，下面就给出了这部分预测偏差的定义 代价函数的定义: 我们选择的参数决定了我们得到的模型相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）,下图蓝色线段变为预测和实际的误差。 平方误差代价函数: 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。 怎么优化线型回归模型 优化回归模型的目标: 便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数最小。 代价函数公式: 代价函数坐标图 可以看出在三维空间中存在一个使得J(θ_0,θ_1)代价函数最小的点， 当代价函数取得最小值的时候，我们会获得最符合训练集的预测模型。 3.梯度下降算法算法介绍 梯度下降是一个用来求函数最小值的算法。其思想是利用迭代的思想来逼近最低点，沿着梯度的负方向更新模型权重。我们将使用梯度下降算法来求出代价函数 J(θ_0,θ_1) 的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0, θ1, …., θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（globalminimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 梯度下降算法公式:公式介绍 repeat until convergence{$$θ_j=θ_j−α∂/∂θ_j J(θ0,θ1) (for j=0 and j=1)$$} 参数α: 学习率, 它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。当到达损失函数的最低点的时候，其导数为0，已经到达了局部最优点，即θ不会再发生改变 注意: 在梯度下降算法中，一般都会采用同步更新的方式来优化模型, 即上面公式的θj同步更新。同步更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。 学习率的选择对算法的影响 学习率过小的影响: 则达到收敛所需的迭代次数会非常高 学习率过大的影响: 每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛 怎么确定模型是否收敛 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛, 如上图在300步之后基本接近收敛。 也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较 梯度下降算法分类批量梯度下降 在上面的线性回归场景中的梯度下降算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。 批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点： （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 随机梯度下降公式: 随机梯度下降指在梯度下降的每一步中只用到了训练集的一个样本或者一部分样本，最后得到的可能是全局最优解，也可能是局部最优解。 优点： （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。缺点： （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。 （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。 （3）不易于并行实现。 4.梯度下降线型回归模型单变量线型回归梯度下降梯度下降、线型回归算法比较 单变量梯度下降公式代价函数计算 参数θ的计算 多变量线型回归梯度下降多变量特征现在对回归模型增加更多的特征，例如房价预测问题，增加房间数楼层等，构成一个含有多个变量的模型，模型中的特征如下图。 增添更多特征后，引入一系列新的注释： n 代表特征的数量 x^(i)^ 代表第i个训练实例，是特征矩阵的第i行，是一个向量(vector). x~j~^(i)^ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征 如: $x_2^{(2)} = 3, x_3^{(2)} = 2$ 支持多个变量的h表示为: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 这个公式中有个n+1参数和n个变量，为了使得公式能够简化一些，引入$x_0=1$，则公式转化为:$h_θ(x) = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m* (n+1), 因此变量的假设公式可以表示为: $h_θ(x) = θ^TX$ 多变量梯度下降公式与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即： $J_{(θ_0, .., θn)} = {1\over2m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 , 其中: h_θ(x) = θ^TX = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度梯度下降下降公式： 求导数后得到: 5.特征和多项式回归特征选择 有的时候的们的特征并不是很好的可以直接用来训练模型，比如房子价格预测模型的两个特征 特征: x1 房子的临街宽度， x2 房子的纵向深度 此时很明显房子的价格用 特征一和特征二的乘积，即房子的占地面积来预测更合适 $x=x_1 * x_2 = area (面积)$ 多项式回归 很多时候，我们并没有那么幸运，正好所有的数据之间都是线性关系，能用一条直线，一个平面就描述地出来，大多数时候，我们可能需要画成曲线、曲面或者曲啥更高维度的东西 比如下图数据集，线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，用二次方程（蓝色的线）和三次方程（绿色的线）来拟合这些点，但是二次方程有个缺点，到了后面，反而会下降，这和我们的认知不符合，而三次方程不会遇到这个问题，和二次方程相加又会和上升的部分相互抵消一部分，所以最后选择了三次方程加上二次方程。 二次方模型公式：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2$ 三次方模型公式: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3$ 通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以把多项式回归看成是另类的多变量线性回归，我们可以令：$x_2 = x_2^2$ , $x_3 = x_3^3$ , 从而将模型转化为线型回归模型 注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要, 因为比如x的范围是1-100的话，$x^2$的范围就是1-10000，$x^3$的范围就是1-1000000。 6.特征缩放为什么要特征缩放 在使用梯度下降的时候会遇到因为特征大小不统一，导致收敛速度过慢的问题，在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。 以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。 解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。 特征缩放的两种方法线型归一化 原理： 通过对原始数据进行变换把数据映射到(默认为[0,1])之间 公式 归一化的弊端 使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景 特征标准化 原理： 通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内 公式： 标准化的有点 如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。 7.正规方程线型回归正规方程算法介绍 对于线型回归问题经常都是在使用梯度下降算法，但是对于某些线型回归问题，正规方程是更好的解决办法。如下面的代价函数: 梯度下降和正规方程比较 梯度下降 正规方程 需要选择学习率 不需要 需要多次迭代 一次运算得出 当特征数量n特别大时能比较适用 需要计算$(X^TX)^{-1}$如果特征数量n较大则运算代价大因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型等其他模型 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习介绍]]></title>
    <url>%2F2018%2F10%2F19%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[机器学习的发展 机器学习被应用到越来越多的领域这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。 再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。 手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。 一个比较好的机器学习定义 一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升 类比于下棋，经验E 就是程序上万次的自我练习的经验而任务T 就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。 机器学习基本算法最常用的两个算法监督学习算法 我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，即我们可以根据正确答案来不断的优化我们的模型。 监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。 无监督学习算法 根据未知的训练样本(未被标记分类), 从数据中找出不同的结构，将数据识别为不同的聚集簇。 无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？ 还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。 监督学习算法常见问题回归问题 通过像预测房子的价格一样，通过回归模型通过输入值来推出一个连续的输出值 分类问题 通过数学模型来判断输入数据所属的类别，可以是二类别问题（是/不是），也可以是多类别问题（在多个类别中判断输入数据具体属于哪一个类别） 无监督学习常见问题聚类问题 依据研究对象（样品或指标）的特征，将其分为不同的分类。 机器学习开发流程 开发流程按照功能模块划分可以分为: 数据处理 + 模型训练 + 模型评估 + 模型部署 机器学习模型什么是机器学习模型 通过一种映射关系将输入值到输出值 机器学习模型的作用 机器学习的算法监督学习算法(数据结构既有特征值也有目标值)什么是监督学习 可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出是有限个离散值（称作分类）。 监督学习的算法 分类算法 k-近邻算法、贝叶斯分类、决策树与、随机森林、逻辑回归、神经网络 回归算法 线型回归、岭回归 标注算法 隐马尔可夫模型 无监督学习(数据结构只有特征值没有目标值)什么是无监督学习 可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值所组成。 无监督学习算法 聚类算法 k-means]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[githubpage + hexo + yilia 搭建个人博客]]></title>
    <url>%2F2018%2F10%2F17%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2F%E5%8D%9A%E5%AE%A2%2Fgithub%2Bhexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[0.本博客的由来本来感觉写博客很费时间，但是最近感觉这两年攒手里的笔记太多了，不方便整理和分享 所以打算以后就干脆直接将笔记整理到github，这样也比自己维护一个博客省心 下面就将搭建本博客的步骤也同步上来，当做一个hello-world吧。 1.环境准备电脑环境是Windows，安装好git后，所有搭建操作均在git bash内完成 1）安装hexo(首先要安装git, node.js, npm) 注意：首次安装git 要配置user信息 123$git config --global user.name "yourname" #（yourname是git的用户名）$git config --global user.email email） 2）使用npm安装hexo 1$npm install -g hexo 3）创建hexo文件夹 12$mkdir hexo_blog$cd hexo_lobg 4）初始化框架 1234567$hexo init #hexo #会自动创建网站所需要的文件$npm install #安装依赖包$hexo generate $hexo server #现在可以用127.0.0.1:4000访问hexo默认的hello world界面 ,hexo s = hexo server 2.部署到github1）首次使用github需要配置密钥 1ssh-keygen -t rsa -C "email" 生成ssh密钥，按三次回车键，密码为空,这边会生成id_rsa和_rsa.pub文件 打开id_rsa.pub，复制全文添加到GitHub 的Add SSH key中。 2）创建Respository， 并开启githubPage 首先注册登录github,然后创建页面仓库，Repository name 命名应该是 youname.github.io 在setting界面， 配置 3）安装hexo-deployer-git 1$npm install hexo-deployer-git --save 用来推送项目到github 4）生成博客，并push到github 123$hexo generate$hexo deploy 5）验证结果 通过https://youname.github.io 进行访问 3.更换博客模板目前访问的博客模板比较简略，下面介绍使用：yilia模板 1）拉取模板文件 1$git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 2）更改配置文件修改模板为yilia 打开项目目录下的_config.yml文件，更改主题theme; theme: yilia然后配置yilia文件下的_config.yml（目录：hexo/themes/yilia/_config.yml） 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127# Headermenu: 主页: / 归档: /archives #分类: /categories #标签: /tags# SubNavsubnav: github: &quot;https://github.com/KyleAdultHub&quot; #weibo: &quot;#&quot; #rss: &quot;#&quot; #zhihu: &quot;#&quot; qq: &quot;/information&quot; #weixin: &quot;#&quot; #jianshu: &quot;#&quot; #douban: &quot;#&quot; #segmentfault: &quot;#&quot; #bilibili: &quot;#&quot; #acfun: &quot;#&quot; mail: &quot;/information&quot; #facebook: &quot;#&quot; #google: &quot;#&quot; #twitter: &quot;#&quot; #linkedin: &quot;#&quot; rss: /atom.xml# 是否需要修改 root 路径# 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，# 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。root: /# Content# 文章太长，截断按钮文字excerpt_link: more# 文章卡片右下角常驻链接，不需要请设置为falseshow_all_link: &apos;展开全文&apos;# 数学公式mathjax: false# 是否在新窗口打开链接open_in_new: false# 打赏# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏reward_type: 0# 打赏wordingreward_wording: &apos;谢谢你&apos;# 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpgalipay: # 微信二维码图片地址weixin: # 目录# 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录toc: 1# 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为falsetoc_hide_index: true# 目录为空时的提示toc_empty_wording: &apos;目录，不存在的…&apos;# 是否有快速回到顶部的按钮top: true# Miscellaneousbaidu_analytics: &apos;&apos;google_analytics: &apos;&apos;favicon: /favicon.png#你的头像urlavatar: /img/header.jpg#是否开启分享share_jia: true#评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment#不需要使用某项，直接设置值为false，或注释掉#具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/#1、多说duoshuo: false#2、网易云跟帖wangyiyun: false#3、畅言changyan_appid: *** #这个畅言id和conf写自己的changyan_conf: ***#4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的disqus: false#5、Gitmentgitment_owner: false #你的 GitHub IDgitment_repo: &apos;&apos; #存储评论的 repogitment_oauth: client_id: &apos;&apos; #client ID client_secret: &apos;&apos; #client secret# 样式定制 - 一般不需要修改，除非有很强的定制欲望…style: # 头像上面的背景颜色 header: &apos;#4d4d4d&apos; # 右滑板块背景 slider: &apos;linear-gradient(200deg,#a0cfe4,#e8c37e)&apos;# slider的设置slider: # 是否默认展开tags板块 showTags: false# 智能菜单# 如不需要，将该对应项置为false# 比如#smart_menu:# friends: falsesmart_menu: innerArchive: &apos;所有文章&apos; friends: &apos;友链&apos; aboutme: &apos;关于我&apos;friends: #友情链接1: http://localhost:4000/ aboutme: 程序猿一枚&lt;br&gt;]]></content>
      <categories>
        <category>开发工具</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群]]></title>
    <url>%2F2018%2F10%2F16%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2FKubernetes%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[安装dokcer安装安装 kubernetes 之前先需要安装 docker：&gt; curl -fsSL https://get.docker.com/ | sh启动 docker，并设置为开机自启动：&gt; sudo systemctl enable docker&gt; sudo systemctl start dokcerkubernetes安装安装 kubernetes 的时候，需要安装 kubelet，kubeadm 等包 ，我们先要更新一下 yum 源：&gt; sudo cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpghttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF然后升级一下软件包：&gt; sudo yum update下面才是重头戏，安装 kubernetes 相关的一些组件：&gt; sudo yum install -y kubelet kubectl kubeadm kubernetes-cnikubectl：运行 kubernetes 集群命令的管理工具kubelet：节点管理工具kubeadm：集群自动搭建工具kubernetes-cni：kubernetes容器网络接口配置硬件配置系统：CentOS 7| 角色 | 数量 | 配置 | hostname | | ------ | ---- | ------ | ---------- | | master | 1 | 2核 2G | k8s-master | | node | 1 | 1核 1G | k8s-node1 | | node | 1 | 1核 1G | k8s-node2 |系统配置关闭防火墙&gt; systemctl stop firewalld&gt; systemctl disable firewalld关闭swap&gt; swapoff -a修改 /etc/fstab 文件，注释掉 swap 的自动挂载。# /etc/fstab# Created by anaconda on Thu Aug 30 22:16:46 2018## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root / xfs defaults 0 0UUID=507b23b4-65a7-4689-80f1-ef79d40a3a22 /boot xfs defaults 0 0# /dev/mapper/centos-swap swap swap defaults 0 0确认 swap 已经关闭：&gt; free -mtotal used free shared buff/cache availableMem: 7983 2825 4017 48 1140 4826Swap: 0 0 0关闭 selinux两种方式关闭 selinux：临时关闭：&gt; setenforce 0永久关闭，修改 /etc/sysconfig/selinux，将 SELINUX 修改为 disabled：# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted验证 selinux 已被禁用：&gt; sestatus -vSELinux status: disabled调整内核参数：&gt; sudo cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF&gt; echo &quot;1&quot; &gt; /proc/sys/net/ipv4/ip_forward使之生效：&gt; sudo sysctl --system搭建集群docker加速由于国内环境无法下载 kubernetes 的某些镜像，我们采用 DaoCloud 的 docker 加速来获取资源：&gt; curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://fc9d1ae1.m.daocloud.io该脚本可以将 --registry-mirror 加入到你的 Docker 配置文件 /etc/docker/daemon.json 中，执行完成之后需要重启 docker。下载镜像(如果可以连接VPN不需要手动下载, 部署master节点的时候可以自动下载) ， 可以编写好shell脚本来一次性执行接下来手动下载 kubernetes 的相关镜像，下载后的镜像名改为以gcr.io/ 开头的名字，以供 kubeadm 使用：&gt; docker pull mirrorgooglecontainers/pause-amd64:3.1&gt; docker tag mirrorgooglecontainers/pause-amd64:3.1 k8s.gcr.io/pause:3.1&gt; docker pull warrior/etcd-amd64:3.2.18&gt; docker tag warrior/etcd-amd64:3.2.18 k8s.gcr.io/etcd-amd64:3.2.18&gt; docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.11.2 k8s.gcr.io/kube-apiserver-amd64:v1.11.2&gt; docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.11.2 k8s.gcr.io/kube-scheduler-amd64:v1.11.2&gt; docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.2 k8s.gcr.io/kube-controller-manager-amd64:v1.11.2&gt; docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.11.2 k8s.gcr.io/kube-proxy-amd64:v1.11.2&gt; docker pull gysan/dnsmasq-metrics-amd64:1.0&gt; docker tag gysan/dnsmasq-metrics-amd64:1.0 k8s.gcr.io/dnsmasq-metrics-amd64:1.0&gt; docker pull warrior/k8s-dns-kube-dns-amd64:1.14.1&gt; docker tag warrior/k8s-dns-kube-dns-amd64:1.14.1 k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.1&gt; docker pull warrior/k8s-dns-dnsmasq-nanny-amd64:1.14.1&gt; docker tag warrior/k8s-dns-dnsmasq-nanny-amd64:1.14.1 k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.1&gt; docker pull warrior/k8s-dns-sidecar-amd64:1.14.1&gt; docker tag warrior/k8s-dns-sidecar-amd64:1.14.1 k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.1&gt; docker pull mritd/kube-discovery-amd64:1.0&gt; docker tag mritd/kube-discovery-amd64:1.0 k8s.gcr.io/kube-discovery-amd64:1.0&gt; docker pull gysan/exechealthz-amd64:1.2&gt; docker tag gysan/exechealthz-amd64:1.2 k8s.gcr.io/exechealthz-amd64:1.2&gt; docker pull coredns/coredns:1.1.3&gt; docker tag coredns/coredns:1.1.3 k8s.gcr.io/coredns:1.1.3安装 Masterkubernetes 相关镜像下载完成之后，我们就可以用 kubeadm 来一键安装 Master 节点了：&gt; kubeadm init --kubernetes-version=1.11.2[init] using Kubernetes version: v1.11.2[preflight] running pre-flight checksI0831 17:47:34.557368 1494 kernel_validator.go:81] Validating kernel versionI0831 17:47:34.557463 1494 kernel_validator.go:96] Validating kernel config...Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user:mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root:kubeadm join 192.168.1.107:6443 --token fdnk8r.i3xnwl7r4gzjnl20 --discovery-token-ca-cert-hash sha256:9b326fbf6154498cd96288ac37f853218a7864fdff1ec3571328cc7b495500b1安装完成后，按照提示，执行下面的命令，复制配置文件到普通用户的 home 目录下：&gt; mkdir -p $HOME/.kube&gt; sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&gt; sudo chown $(id -u):$(id -g) $HOME/.kube/config安装 Node，加入集群在 Node 节点上保证 docker 和 kubelet 服务已启动，执行 kubeadm join 命令，加入集群：&gt; kubeadm join 192.168.1.107:6443 --token fdnk8r.i3xnwl7r4gzjnl20 --discovery-token-ca-cert-hash sha256:9b326fbf6154498cd96288ac37f853218a7864fdff1ec3571328cc7b495500b1I0910 10:47:35.211228 2988 kernel_validator.go:81] Validating kernel versionI0910 10:47:35.211346 2988 kernel_validator.go:96] Validating kernel config...This node has joined the cluster:* Certificate signing request was sent to master and a responsewas received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the master to see this node join the cluster.这条命令是 kubeadm init 最后所提示的，通过这种方式我们可以将局域网中任意 Node 加入集群。如果 Node 加入集群时报如下错误：[discovery] Failed to request cluster info, will try again: [Get https://192.168.1.107:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: x509: certificate has expired or is not yet valid]可能是 Node 节点的时间与 Master 没有同步，执行 date 命令同步当前时间：&gt; date -s &quot;2018-09-15 09:00:00&quot;安装网络插件Node 加入集群后，我们去 Master 上查看下节点的情况：&gt; kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master NotReady master 38m v1.11.2k8s-node1 NotReady &lt;none&gt; 14s v1.11.2k8s-node2 NotReady &lt;none&gt; 8s v1.11.2发现所有节点都处于 NotReady 状态，这是因为我们还没有安装 CNI 网络插件。选择 weave 插件，执行以下命令进行一键安装：&gt; kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&quot;serviceaccount/weave-net createdclusterrole.rbac.authorization.k8s.io/weave-net createdclusterrolebinding.rbac.authorization.k8s.io/weave-net createdrole.rbac.authorization.k8s.io/weave-net createdrolebinding.rbac.authorization.k8s.io/weave-net createddaemonset.extensions/weave-net created验证集群安装完成&gt; kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-78fcdf6894-fvs27 1/1 Running 0 31mkube-system coredns-78fcdf6894-p8v9c 1/1 Running 0 31mkube-system etcd-k8s-master 1/1 Running 0 30mkube-system kube-apiserver-k8s-master 1/1 Running 0 30mkube-system kube-controller-manager-k8s-master 1/1 Running 0 30mkube-system kube-proxy-54bht 1/1 Running 0 30mkube-system kube-proxy-cbvss 1/1 Running 0 30mkube-system kube-proxy-r55gr 1/1 Running 0 31mkube-system kube-scheduler-k8s-master 1/1 Running 0 30mkube-system weave-net-6lpf9 2/2 Running 0 29mkube-system weave-net-q5nct 2/2 Running 7 29mkube-system weave-net-qtzwq 2/2 Running 7 29m所有 pod 处于 Running 状态，即表示集群安装成功。运行第一个应用我们以 nginx 官方镜像开始我们第一个应用：&gt; kubectl run nginx --image=nginx:1.14.0 --replicas=2deployment.apps/nginx created这样就创建了一个 nginx 应用，并且建立了2个副本，我们通过命令看下：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-cfw6w 1/1 Running 0 55m 10.40.0.1 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-zgf62 1/1 Running 0 55m 10.46.0.2 k8s-node1 &lt;none&gt;果然创建了2个 nginx 容器，处于运行状态。我们可以通过和 docker exec 一样的方法，进入容器查看：&gt; kubectl exec -it nginx-7f89695fdd-7ppg2 /bin/bashroot@nginx-7f89695fdd-7ppg2:/# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr varkubectl run 就好比 docker run ，可以直接通过命令启动容器， 同样的 kubernetes 也有 Dockerfile 的方式启动容器。首先需要准备 nginx-deployment.yml 文件：apiVersion: extensions/v1beta1 # 当前配置格式的版本kind: Deployment # 要创建的资源类型metadata: # 资源的元数据name: nginx-deploymentspec: # 资源的规格说明replicas: 2 # 副本数量template: # Pod 模板metadata: # Pod 的元数据labels: # 标签：key 和 value，可以任意多个app: nginxspec: # 描述 Pod 的规格，此部分定义 Pod 中每一个容器的属性containers:- name: nginximage: nginx:1.14.0然后运行启动命令：&gt; kubectl apply -f nginx-deployment.ymldeployment.extensions/nginx-deployment createdPod的介绍pod的概念 前面多次出现 Pod 这个词，这个 Pod 是什么？在 kubernetes 的世界里，调度的最小单位是 Pod，而非容器。一个 Pod 封装一个应用容器（也可以有多个容器），存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 代表部署的一个单位：kubernetes 中单个应用的实例，它可能由单个容器或多个容器共享组成的资源。Pod运行的两种模式Pod 中运行一个容器：“one-container-per-Pod” 模式是 kubernetes 最常见的用法，在这种情况下，你可以将 Pod 视为单个封装的容器，但 kubernetes 直接管理的还是 Pod。Pod 中运行多个容器：封装紧密耦合的应用，它们需要由多个容器组成，它们之间能够共享资源。Pod的共享资源网络每个 Pod 被分配一个独立的 IP 地址，Pod 中的每个容器共享网络命名空间，包括 IP 地址和网络端口。Pod 内的容器可以使用 localhost 相互通信。当 Pod 中的容器与 Pod 外部通信时，他们必须协调如何使用共享网络资源（如端口）。存储Pod 可以指定一组共享存储 volumes 。Pod 中的所有容器都可以访问共享 volumes ，允许这些容器共享数据。volumes 还用于 Pod 中的数据持久化，以防其中一个容器需要重新启动而丢失数据。Pod 的特点当 Pod 被创建后，都会被 kuberentes 调度到集群的 Node 上。直到 Pod 的进程终止、被删掉、因为缺少资源而被驱逐、或者 Node 故障之前这个 Pod 都会一直保持在那个 Node 上。重启 Pod 中的容器跟重启 Pod 不是一回事。Pod 只提供容器的运行环境并保持容器的运行状态，重启容器不会造成 Pod 重启。Pod 不会自愈。如果 Pod 运行的 Node 故障，或者是调度器本身故障，这个 Pod 就会被删除。同样的，如果 Pod 所在 Node 缺少资源或者 Pod 处于维护状态，Pod 也会被驱逐。kubernetes 使用更高级的称为 Controller 的抽象层，来管理 Pod 实例。虽然可以直接使用 Pod，但是在 kubernetes 中通常是使用 Controller 来管理 Pod 的。Pod 的管理(Controler&amp;Service)Controler的介绍模拟一些异常情况，Pod 因为某些异常情况而 crash&gt; kubectl delete pod nginx-deployment-957f5f978-cfw6wpod &quot;nginx-deployment-957f5f978-cfw6w&quot; deleted查看一下结果：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-q74mb 1/1 Running 0 7m 10.40.0.3 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-zgf62 1/1 Running 0 10m 10.32.0.3 k8s-node1 &lt;none&gt;发现原先的 nginx-deployment-957f5f978-cfw6w 变成了 nginx-deployment-957f5f978-q74mb，为了保证副本的数量，系统又重启了一个新的 Pod 最为补充。Node 因为某些异常情况而无法访问k8s-node2&gt; systemctl stop network我们把其中一个 Node 的网络断开，过一段时间，查看结果：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-zgf62 1/1 Running 0 9m 10.32.0.3 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-rp4df 1/1 Running 0 17m 10.32.0.2 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-q74mb 1/1 Unknown 0 20m 10.40.0.3 k8s-node2 &lt;none&gt;发现原来 k8s-node2 上的 Pod 状态变成了 Unknown，并且系统在健康的 k8s-node1 上又启动了一个新的 Pod，以保证副本的数量。 以上这些就是 Controller 魔法。可以创建和管理多个 Pod ，提供副本管理、滚动升级和集群级别的自愈能力。Pod常用的 ControllerDeployment为 Pod 和 ReplicaSet 提供声明式更新：你只需要在 Deployment 中描述您想要的目标状态是什么，Deployment 就会帮您将 Pod 和 ReplicaSet 的实际状态改变到您的目标状态。您可以定义一个全新的 Deployment 来创建 ReplicaSet 或者删除已有的 Deployment 并创建一个新的来替换。StatefulSet有状态系统服务集：提供唯一的网络标识符 、持久化存储 、有序的部署和扩展、有序的删除和终止，以及有序的自动滚动更新。对于具有 N 个副本的 StatefulSet，当部署Pod时，将会顺序从 {0..N-1} 开始创建。Pods 被删除时，会从 {N-1..0} 的相反顺序终止。在将缩放操作应用于 Pod 之前，它的所有前辈必须运行和就绪。对 Pod 执行扩展操作时，前面的 Pod 必须都处于 Running 和 Ready 状态。在 Pod 终止之前，所有 successors 都须完全关闭。DaemonSet让所有（或者一些特定）的 Node 运行同一个 Pod：实现 Only-One-Pod-Per-Node 。每个 Node 上运行一个分布式存储的守护进程每个 Node 上运行日志采集器或每个 Node 上运行监控的采集端Service的介绍Service 可以为一组相同功能的 Pod 应用提供统一的入口地址，并将请求负载均衡分发到各个容器应用上。Service 从逻辑上代表了一组 Pod，具体是哪些 Pod 则是由 label 来挑选。Service 有自己 IP，而且这个 IP 是不变的。客户端只需要访问 Service 的 IP，kubernetes 则负责建立和维护 Service 与 Pod 的映射关系。无论后端 Pod 如何变化，对客户端不会有任何影响，因为 Service 没有变。我们来看一下 Service 发布服务的类型有哪几种：Pod常用的 ServiceClusterIp通过集群的内部 IP 暴露服务，服务只能够在集群内部访问，这也是默认的 ServiceType。apiVersion: v1kind: Servicemetadata: name: nginx-service-clusterip labels: name: nginx-service-clusteripspec: type: ClusterIP selector: app: nginx ports: - port: 8081 targetPort: 80启动 service-clusterip：&gt; kubectl apply -f nginx-service-clusterip.ymlservice/nginx-service-clusterip created查看状态：&gt; kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 10hnginx-service-clusterip ClusterIP 10.105.115.2 &lt;none&gt; 8081/TCP 1m可以看到该 Service 暴露了一个内网 IP ，以及配置文件中指定的 8081 端口，不过该服务只能在集群内部访问。NodePort通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 \:\，可以从集群的外部访问一个 NodePort 服务。备注: NodePort 方式暴露服务的端口的默认范围（30000-32767）如果需要修改则在 apiserver 的启动命令里面添加如下参数 --service-node-port-range=1-65535apiVersion: v1kind: Servicemetadata: name: nginx-service-nodeport labels: name: nginx-service-nodeportspec: type: NodePort selector: app: nginx ports: - port: 8082 targetPort: 80 nodePort: 30082启动 service-nodeport：&gt; kubectl apply -f nginx-service-nodeport.ymlservice/nginx-service-nodeport created查看状态：&gt; kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 10hnginx-service-nodeport NodePort 10.106.190.164 &lt;none&gt; 8082:30082/TCP 5s我们来验证一下是否可以在集群外访问：&gt; curl &quot;http://172.17.109.23:30082&quot;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/body&gt;&lt;/html&gt;Kubernetes node 管理Node 的隔离和恢复apiVersion: v1kind: Nodemetadata:name: k8s-node1labels:kubernetes.io/hostname: k8s-node1spec:unschedulable: true注意：此配置文件指定的类型是 Node，表示是作用在 Node 上的，意思是对于后续创建的 Pod，系统将不再向该 Node 进行调度。&gt; kubectl replace -f unschedule_node.ymlnode/k8s-node1 replaced查看 Node 状态，可以看到在 Node 的状态中增加了一项 SchedulingDisabled：&gt; kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 17m v1.11.2k8s-node1 Ready,SchedulingDisabled &lt;none&gt; 16m v1.11.2k8s-node2 Ready &lt;none&gt; 15m v1.11.2同样，如果需要将某个 Node 重新纳入集群调度范围，则将 unschedulable 设置为false，再次执行 kubectl replace 命令就能恢复系统对该 Node 的调度。Node 扩容在实际生产系统中会经常遇到服务器容量不足的情况，这时就需要购买新的服务器，然后将应用系统进行水平扩展来完成对系统的扩容。在 kubernetes 集群中，对于一个新 Node 的加入是非常简单的。 在新 Node 上安装好 docker、kubelet、kube-proxy等服务，然后运行加入集群命令即可：&gt; kubeadm join 192.168.1.107:6443 --token fdnk8r.i3xnwl7r4gzjnl20 --discovery-token-ca-cert-hash sha256:9b326fbf6154498cd96288ac37f853218a7864fdff1ec3571328cc7b495500b1因此，这条命令是非常重要的，在集群初始化时就应该把它记录在你的小本子上。Pod 动态扩容和缩容在实际生产系统中，我们经常会遇到某个服务需要扩容的场景，也可能会遇到由于资源紧张或者工作负载降低而需要减少服务实例数的场景。实际我们只要更改配置文件中的 replicas 参数：apiVersion: extensions/v1beta1kind: Deploymentmetadata:name: nginx-deploymentspec:replicas: 5...我们将 replicas 从 2 调整到了 5，执行操作：&gt; kubectl apply -f nginx-deployment.ymldeployment.extensions/nginx-deployment configured观察结果：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-2bm4c 1/1 Running 0 33m 10.40.0.1 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-dq5q9 1/1 Running 0 1m 10.40.0.3 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-hztv9 1/1 Running 0 1m 10.46.0.3 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-v2dkl 1/1 Running 0 33m 10.46.0.2 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-xmd4g 1/1 Running 0 1m 10.40.0.2 k8s-node2 &lt;none&gt;是不是很方便！要缩容也是一样的操作，只是把 replicas 调小即可。将 Pod 调度到指定的 Node我们知道，kubernetes 的 Scheduler 服务（ kube-scheduler 进程）负责实现 Pod 的调度，整个调度过程通过执行一系列复杂的算法最终为每个 Pod 计算出一个最佳的目标节点，这一过程是自动完成的，我们无法知道 Pod 最终会被调度到哪个节点上。有时我们可能需要将 Pod 调度到一个指定的Node上，此时，我们可以通过Node的标签（ Label ）和 Pod 的 nodeSelector 属性相匹配，来达到上述目的。NodeapiVersion: v1kind: Nodemetadata:name: k8s-node1labels:kubernetes.io/hostname: k8s-node1diskType: ssd更改配置：&gt; kubectl replace -f ssd_node.ymlnode/k8s-node1 replaced我们来看下该 Node 有哪些标签：&gt; kubectl describe node k8s-node1Name: k8s-node1Roles: &lt;none&gt;Labels: diskType=ssdkubernetes.io/hostname=k8s-node1Annotations: node.alpha.kubernetes.io/ttl=0CreationTimestamp: Fri, 14 Sep 2018 00:09:24 +0800...PodapiVersion: extensions/v1beta1kind: Deploymentmetadata:name: nginx-ssdspec:replicas: 2template:metadata:labels:app: nginxspec:containers:- name: nginximage: nginx:1.14.0nodeSelector:diskType: ssd添加配置：&gt; kubectl apply -f nginx-ssd.ymldeployment.extensions/nginx-ssd created理想状态是，我们将 nginx-ssd 全部分配到了拥有 diskType: ssd 标签的 k8s-node1 上，我们来验证一下：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-ssd-7b6f5c5d56-k6ssz 1/1 Running 0 3m 10.46.0.2 k8s-node1 &lt;none&gt;nginx-ssd-7b6f5c5d56-txt9l 1/1 Running 0 3m 10.46.0.3 k8s-node1 &lt;none&gt;Kubernetes版本管理应用滚动升级当集群中的某个服务需要升级时，我们需要停止目前与该服务相关的所有 Pod，然后重新拉取镜像并启动。如果集群规模比较大，则这个工作就变成了一个挑战，而且先全部停止然后逐步升级的方式会导致较长时间的服务不可用。 kubernetes 提供了 rolling-update（滚动升级）功能来解决上述问题。滚动升级也是通过执行 kubectl apply 命令一键完成，该命令创建了一个新的 RC，然后自动控制旧的 RC 中的 Pod 副本数量逐渐减少到 0，同时新的 RC 中的 Pod 副本数量从 0 逐步增加到目标值，最终实现了 Pod 的升级。需要注意的是，系统要求新的 RC 需要与旧的 RC 在相同的命名空间（Namespace）内，即不能把别人的资产偷偷转移到自家名下。我们现在来将 nginx 的版本从 1.14.0 升级到 1.15.0：apiVersion: extensions/v1beta1kind: Deploymentmetadata:name: nginx-deploymentspec:minReadySeconds: 5strategy:type: RollingUpdaterollingUpdate:maxSurge: 3maxUnavailable: 2replicas: 5template:metadata:labels:app: nginxspec:containers:- name: nginximage: nginx:1.15.0配置文件中有几个参数需要注意：strategy.minReadySeconds：用来指定没有任何容器 crash 的 Pod 并被认为是可用状态的最小秒数， 默认为0strategy.type：用来指定新 Pod 替换旧 Pod 的策略，可以是 Recreate，或是 RollingUpdate（默认值）strategy.rollingUpdate.maxSurge：升级过程中最多可以比原先设定所多出的pod 数量，此栏位可以为固定值或是比例(%)例如. maxSurge: 1、replicas: 5，代表Kubernetes 会先开好1 个新pod 后才删掉一个旧的pod，整个升级过程中最多会有5+1 个podstrategy.rollingUpdate.maxUnavailable：最多可以有几个pod 处在无法服务的状态，当maxSurge不为零时，此栏位亦不可为零。例如. maxUnavailable: 1，代表Kubernetes 整个升级过程中最多会有1 个pod 处在无法服务的状态&gt; kubectl apply -f nginx-deployment-update.yml --recorddeployment.extensions/nginx-deployment created升级完毕，查看最新的版本号：&gt; kubectl describe deploy nginx-deployment...Pod Template:Labels: app=nginxContainers:nginx:Image: nginx:1.15.0...应用回滚当集群中的某个服务升级完成，突然发现了意想不到的 BUG，需要回退到升级之前的版本，但现在所有节点的应用都已经被替换了，要回退回去这可是不小的工作量啊。对于这种情况，kubernetes 也能很轻松的搞定。前提是你之前的操作都有打上 --record参数，这样整个升级过程都能被记录下：&gt; kubectl rollout history deployment nginx-deploymentdeployments &quot;nginx-deployment&quot;REVISION CHANGE-CAUSE1 kubectl apply --filename=nginx-deployment.yml --record=true2 kubectl apply --filename=nginx-deployment-update.yml --record=true观察到我们在 REVISION 2 的时候升级了我们的应用，接下来我们直接回滚到 REVISION 1：&gt; kubectl rollout undo deployment nginx-deployment --to-revision=1deployment.extensions/nginx-deployment再来看下当前应用的版本：&gt; kubectl describe deployment nginx-deployment...Pod Template:Labels: app=nginxContainers:nginx:Image: nginx:1.14.0...果然，应用已经回退到了升级之前的版本，现在可以静下心来修复 BUG 再发布了。但如果我们在发布版本时没有打上 --record 参数，history 里是空的，这时只要原始版本的镜像还在，我们还是可以通过修改配置文件来达到回滚的目的。]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ganglia的安装、配置、运行]]></title>
    <url>%2F2018%2F10%2F09%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2Fganglia%2Fganglia%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>开发工具</category>
        <category>ganglia</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch详细配置]]></title>
    <url>%2F2018%2F10%2F07%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElaticsearch%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.Elasticsearch cluster 三种角色master node：master节点主要用于元数据（metadata）处理，如、索引的新增、删除、分片data node: data节点上保存了数据片client node: client节点起到路由请求的作用，可看做负载均衡器2.节点选择如果你想让节点从不选举为主节点,只用来存储数据,可作为负载器node.master: falsenode.data: true如果想让节点成为主节点,且不存储任何数据,并保有空闲资源,可作为协调器node.master: truenode.data: false如果想让节点既不称为主节点,又不成为数据节点,那么可将他作为搜索器,从节点中获取数据,生成搜索结果等node.master: falsenode.data: false3.配置说明cluster.name: es-hdfs #集群的名称，同一个集群该值必须设置成相同的node.name: h001 #该节点的名字node.master: true #该节点有机会成为master节点node.data: true #该节点可以存储数据node.rack: r1 #该节点所属的机架network.host: 10.41.2.84 #该参数用于同时设置transport.tcp.port: 9300 #设置节点之间交互的端口号transport.tcp.compress: true #设置是否压缩tcp上交互传输的数据http.port: 9200 #设置对外服务的http 端口号http.max_content_length: 100mb #设置http内容的最大大小http.enabled: true #是否开启http服务对外提供服务discovery.zen.ping.unicast.hosts:[&quot;h001&quot;, &quot;h002&quot;, &quot;h003&quot;,&quot;h004&quot;] #设置集群中的Master节点的初始列表，可以通过这些节点来自动发现其他新加入集群的节点discovery.zen.minimum_master_nodes: 1 #设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可设置大一点的值（2-4）discovery.zen.ping.timeout: 3s #设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错discovery.zen.ping.multicast.enabled: false #设置是否打开多播发现节点，默认是truegateway.recover_after_nodes: 1 #设置集群中N个节点启动时进行数据恢复，默认为1gateway.recover_after_time: 5m #设置初始化数据恢复进程的超时时间，默认是5分钟gateway.expected_nodes: 2 # 设置这个集群中节点的数量，默认为2，一旦这N个节点启动，就会立即进行数据恢复cluster.routing.allocation.node_initial_primaries_recoveries: 4 #初始化数据恢复时，并发恢复线程的个数，默认为4cluster.routing.allocation.node_concurrent_recoveries: 2 #添加删除节点或负载均衡时并发恢复线程的个数，默认为4indices.recovery.max_size_per_sec: 0 #设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制script.engine.groovy.inline.search: onscript.engine.groovy.inline.aggs: onindices.recovery.max_bytes_per_sec: 20mbhttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash详细配置]]></title>
    <url>%2F2018%2F10%2F07%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2Flogstash%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[一、基本语法组成支持的插件Input： elasticsearch,file,http_poller,jdbc,log4j,rss,rabbitmq,redis,syslog,tcp,udp…等Filter： grok,json,mutate,split …等Output： email,elasticsearch,file,http,mongodb,rabbitmq,redis,stdout,tcp,udp …等配置说明地址： https://www.elastic.co/guide/en/logstash/current/input-plugins.htmllogstash.conf配置文件里至少需要有input和output两个部分构成input {#输入}filter {#过滤匹配}output {#输出}二、配置语法讲解1、input配置1.1、file{}（文件读取）监听文件变化，记录一个叫 .sincedb 的数据库文件来跟踪被监听的日志文件的当前读取位（也就是时间戳）input { file { path =&gt; [&quot;/var/log/access.log&quot;, &quot;/var/log/message&quot;] #监听文件路径 type =&gt; &quot;system_log&quot; #定义事件类型 start_position =&gt; &quot;beginning&quot; #检查时间戳 }}参数说明：exclude ：排除掉不想被监听的文件stat_interval ：logstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒。start_position ：logstash 默认是从结束位置开始读取文件数据，也就是说 logstash 进程会以类似 tail -f 的形式运行。如果你是要导入原有数据，把这个设定改成 “beginning”，logstash 进程就按时间戳记录的地方开始读取，如果没有时间戳则从头开始读取，有点类似cat，但是读到最后一行不会终止，而是继续变成 tail -f。2、filter过滤器配置2.1、data（时间处理）用来转换日志记录中的时间字符串，变成LogStash::Timestamp 对象，然后转存到 @timestamp 字段里。注意：因为在稍后的 outputs/elasticsearch 中index常用的 %{+YYYY.MM.dd} 这种写法必须读取 @timestamp数据，所以一定不要直接删掉这个字段保留自己的时间字段，而是应该用 filters/date 转换后删除自己的字段！至于elasticsearch 中index使用 %{+YYYY.MM.dd}这种写法的原因后面会说明。filter { grok { match =&gt; [&quot;message&quot;, &quot;%{HTTPDATE:logdate}&quot;] } date { match =&gt; [&quot;logdate&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;] }}2.2、grok （正则匹配）filter { grok { match =&gt; [ &quot;message&quot;, &quot;\s+(?&lt;status&gt;\d+?)\s+&quot; ] #跟python的正则有点差别 }}优化建议：如果把 “message” 里所有的信息都 grok 到不同的字段了，数据实质上就相当于是重复存储了两份。所以可以用 remove_field 参数来删除掉 message 字段，或者用 overwrite 参数来重写默认的 message 字段，只保留最重要的部分。filter { grok { patterns_dir =&gt; &quot;/path/to/your/own/patterns&quot; match =&gt; { &quot;message&quot; =&gt; &quot;%{SYSLOGBASE} %{DATA:message}&quot; } overwrite =&gt; [&quot;message&quot;] }}filter { grok { match =&gt; [&quot;message&quot;, &quot;%{HTTPDATE:logdate}&quot;] remove_field =&gt; [&quot;logdate&quot;] }}2.3、GeoIP （地址查询归类）GeoIP 是最常见的免费 IP 地址归类查询库，同时也有收费版可以采购。GeoIP 库可以根据 IP 地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。filter { geoip { source =&gt; &quot;clientip&quot; database =&gt; &quot;/etc/logstash/GeoLiteCity.dat&quot; #需去官网下载ip库放到本地 }}filter { geoip { source =&gt; &quot;message&quot; #如果能联网可查询在线ip库 }}注：geoip 插件的 “source” 字段可以是任一处理后的字段，比如 “clientip”，但是字段内容却需要小心！geoip 库内只存有公共网络上的 IP 信息，查询不到结果的，会直接返回 null，而 logstash 的 geoip 插件对 null 结果的处理是：不生成对应的 geoip.字段。所以在测试时，如果使用了诸如 127.0.0.1, 172.16.0.1, 182.168.0.1, 10.0.0.1 等内网地址，会发现没有对应输出！GeoIP 库数据较多，如果不需要这么多内容，可以通过 fields 选项指定自己所需要的。下例为全部可选内容filter { geoip { fields =&gt; [&quot;city_name&quot;, &quot;continent_code&quot;, &quot;country_code2&quot;, &quot;country_code3&quot;, &quot;country_name&quot;, &quot;dma_code&quot;, &quot;ip&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;postal_code&quot;, &quot;region_name&quot;, &quot;timezone&quot;] }}]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filebeat 详细配置]]></title>
    <url>%2F2018%2F10%2F07%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FFilebeat%20%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Filebeat 配置文件中文对照###################### Filebeat Configuration Example ########################## This file is an example configuration file highlighting only the most common# options. The filebeat.reference.yml file from the same directory contains all the# supported options with more comments. You can use it as a reference.## You can find the full configuration reference here:# https://www.elastic.co/guide/en/beats/filebeat/index.html# For more available modules and options, please see the filebeat.reference.yml sample# configuration file.#=========================== Filebeat prospectors =============================filebeat.prospectors:# Each - is a prospector. Most options can be set at the prospector level, so# you can use different prospectors for various configurations.# Below are the prospector specific configurations. # 指定文件的输入类型log(默认)或者stdin- type: log# 更改为 true 以启用此配置。enabled: false#应该爬网和提取的路径paths:- /var/log/*.log# 指定被监控的文件的编码类型，使用plain和utf-8都是可以处理中文日志的 encoding: plain# 排除行, 要匹配的正则表达式的列表. 它将从列表中删除与任何正则表达式匹配的行.#exclude_lines: ['^DBG']# 包含行. 要匹配的正则表达式的列表. 它从列表中导出与任何正则表达式匹配的行.#include_lines: ['^ERR', '^WARN']# 排除文件. 要匹配的正则表达式的列表。 Filebeat 将从列表中删除与任何正则表达式匹配的文件。默认情况下, 不会删除任何文件。#exclude_files: ['.gz$']# 向输出的每一条日志添加额外的信息，比如“level:debug”，方便后续对日志进行分组统计。# 默认情况下，会在输出信息的fields子目录下以指定的新增fields建立子目录，例如fields.level# 这个得意思就是会在es中多添加一个字段，格式为 &quot;filelds&quot;:{&quot;level&quot;:&quot;debug&quot;}#fields:# level: debug# review: 1# 如果该选项设置为true，则新增fields成为顶级目录，而不是将其放在fields目录下。# 自定义的field会覆盖filebeat默认的field# 如果设置为true，则在es中新增的字段格式为：&quot;level&quot;:&quot;debug&quot;#fields_under_root: false# 可以指定Filebeat忽略指定时间段以外修改的日志内容，比如2h（两个小时）或者5m(5分钟)。#ignore_older: 0# 如果一个文件在某个时间段内没有发生过更新，则关闭监控的文件handle。默认1h#close_older: 1h# Defines the buffer size every harvester uses when fetching the file# 每个harvester监控文件时，使用的buffer的大小#harvester_buffer_size: 16384# 日志文件中增加一行算一个日志事件，max_bytes限制在一次日志事件中最多上传的字节数，多出的字节会被丢弃#max_bytes: 10485760### 多行选项# Mutiline 可用于跨越多行的日志消息。这对于 Java 堆栈跟踪或 C 行继续很常见# 必须匹配的 regexp 模式。示例模式匹配所有开始 [#multiline.pattern: ^\[# 定义在模式下设置的模式是否应该被否定。默认值为 false.#multiline.negate: false# 匹配可以设置为 &quot;后&quot; 或 &quot;之前&quot;。它用于定义是否应将行追加到模式 (不匹配) 之前或之后, 或者只要模式不匹配 (基于否定.# Note:后是等同于前和前是等价于下 Logstash#multiline.match: after#=============================Filebeat 模块 ===============================filebeat.config.modules:# 配置加载全局配置的模式(一个目录)path: ${path.config}/modules.d/*.yml# 设置为true来启用配置重载reload.enabled: false# 检查路径下的文件更改的期间（多久检查一次）#reload.period: 10s#==================== Elasticsearch 模板设置 ==========================setup.template.settings:index.number_of_shards: 3#index.codec: best_compression#_source.enabled: false#============================== Kibana =====================================# 从版本6.0.0 开始, 仪表板通过 Kibana API 加载。# 这需要 Kibana 端点配置.setup.kibana:# Kibana Host# 方案和端口可以被排除, 并将被设置为默认 (http and 5601)# 如果您指定和附加路径, 则该方案是必需的: http://localhost:5601/path# IPv6 地址应始终定义为: https://[2001:db8::1]:5601#host: &quot;localhost:5601&quot;#-------------------------- Elasticsearch output ------------------------------output.elasticsearch:# 要连接到的主机的数组。hosts: [&quot;localhost:9200&quot;]# 可选协议和基本身份验证凭据。#protocol: &quot;https&quot;#username: &quot;elastic&quot;#password: &quot;changeme&quot;#----------------------------- Logstash output --------------------------------#output.logstash:# The Logstash hosts#hosts: [&quot;localhost:5044&quot;]# 可选的 SSL。默认为 off。# 用于 HTTPS 服务器验证的根证书列表#ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;]# SSL 客户端身份验证证书#ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;# 客户端证书密钥#ssl.key: &quot;/etc/pki/client/cert.key&quot;#----------------------------- Kafka output -------------------------------- #output.kafka: #enabled: true #hosts: []#topic: 'topic'#================================ Logging =====================================# 设置日志级别。默认日志级别为 &quot;信息&quot;。# 用的日志级别有: critical, error, warning, info, debug#用的日志级别有: 关键、错误、警告、信息、调试#logging.level: debug# 在调试级别, 您可以有选择地仅对某些组件启用日志记录。#要启用所有选择器, 请使用 [&quot;*&quot;]。其他选择器的例子是 &quot;beat&quot;,# &quot;publish&quot;, &quot;service&quot;.#logging.selectors: [&quot;*&quot;] © 著作权归作者所有]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch CURD Rest-api]]></title>
    <url>%2F2018%2F10%2F06%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElasticsearch%20CURD%20Rest-api%2F</url>
    <content type="text"><![CDATA[本文目标 本文通过对比关系型数据库，将ES中常见的增、删、改、查操作进行图文呈现。能加深你对ES的理解。同时，也列举了kibana下的图形化展示。ES Restful API GET、POST、PUT、DELETE、HEAD含义 1）GET：获取请求对象的当前状态。 2）POST：改变对象的当前状态。 3）PUT：创建一个对象。 4）DELETE：销毁对象。 5）HEAD：请求获取对象的基础信息。Mysql与Elasticsearch核心概念对比示意图 以上表为依据， ES中的新建文档（在Index/type下）相当于Mysql中（在某Database的Table）下插入一行数据。ES的集群管理1、查看集群状态 http://localhost:9200/_cat/health?v 返回值 从以上的返回值中，我们可以得到一个名为ElasticSearch的集群，共有一个节点，没有索引数据。2、查看所有节点的状态 http://localhost:9200/_cat/indices?v ES的CURD操作1、新建文档（类似mysql insert插入操作） 当你创建一个文档时，无需再这之前创建一个索引和类型，ElasticSearch会自动根据你的创建信息自动创建相应的索引、类型，直至文档。 备注: 如果不指定_id, elasticsearch会自动帮我们生成http://localhost:9200/blog/ariticle/1 put{ &quot;title&quot;:&quot;New version of Elasticsearch released!&quot;, &quot;content&quot;:&quot;Version 1.0 released today!&quot;, &quot;tags&quot;:[&quot;announce&quot;,&quot;elasticsearch&quot;,&quot;release&quot;]}创建成功如下显示：{ - &quot;_index&quot;: &quot;blog&quot;, - &quot;_type&quot;: &quot;ariticle&quot;, - &quot;_id&quot;: &quot;1 -d&quot;, - &quot;_version&quot;: 1, - &quot;_shards&quot;: { - &quot;total&quot;: 2, - &quot;successful&quot;: 1, - &quot;failed&quot;: 0 - }, - &quot;created&quot;: true} 2、检索文档（类似mysql search 搜索select*操作）通过_id来查询文档http://localhost:9200/blog/ariticle/1/ GET检索结果如下：{- &quot;_index&quot;: &quot;blog&quot;,- &quot;_type&quot;: &quot;ariticle&quot;,- &quot;_id&quot;: &quot;1&quot;,- &quot;_version&quot;: 1,- &quot;found&quot;: true,- &quot;_source&quot;: { - &quot;title&quot;: &quot;New version of Elasticsearch released!&quot;, - &quot;content&quot;: &quot;Version 1.0 released today!&quot;, - &quot;tags&quot;: [&quot;announce&quot;, &quot;elasticsearch&quot; , &quot;release&quot;]- }}如果未找到会提示：{ - &quot;_index&quot;: &quot;blog&quot;, - &quot;_type&quot;: &quot;ariticle&quot;, - &quot;_id&quot;: &quot;11&quot;, - &quot;found&quot;: false}查询全部文档如下：具体某个细节内容检索查询举例1：查询cotent列包含版本为1.0的信息。http://localhost:9200/blog/article/_search?pretty&amp;q=content:1.0{ - &quot;took&quot;: 2, - &quot;timed_out&quot;: false, - &quot;_shards&quot;: { - &quot;total&quot;: 5, - &quot;successful&quot;: 5, - &quot;failed&quot;: 0 - }, - &quot;hits&quot;: { - &quot;total&quot;: 1, - &quot;max_score&quot;: 0.8784157, - &quot;hits&quot;: [ - { - &quot;_index&quot;: &quot;blog&quot;, - &quot;_type&quot;: &quot;ariticle&quot;, - &quot;_id&quot;: &quot;6&quot;, - &quot;_score&quot;: 0.8784157, - &quot;_source&quot;: { - &quot;title&quot;: &quot;deep Elasticsearch!&quot;, - &quot;content&quot;: &quot;Version 1.0!&quot;, - &quot;tags&quot;: [ - &quot;deep&quot;, - &quot;elasticsearch&quot; - ] - } - } - ]- }}查询举例2：查询书名title中包含“enhance”字段的数据信息：# curl -X GET 10.200.1.121:9200/blog/ariticle/_search?pretty -d &gt; '{ &quot;query&quot; : {&gt; &quot;term&quot; :&gt; {&quot;title&quot; : &quot;enhance&quot; }&gt; }&gt; }'查询举例3：查询ID值为3,5,7的数据信息：# curl -X GET 10.200.1.121:9200/blog/ariticle/_search?pretty -d '{ &quot;query&quot; : {&quot;terms&quot; :{&quot;_id&quot; : [ &quot;3&quot;, &quot;5&quot;, &quot;7&quot; ] }}}'查询举例4：分页查询# curl -X GET 10.200.1.121:9200/blog/ariticle/_search?pretty -d '{ &quot;query&quot; : {&quot;terms&quot; :{&quot;_id&quot; : [ &quot;3&quot;, &quot;5&quot;, &quot;7&quot; ] }}，&quot;from&quot;: 1,&quot;size&quot;: 10}'3、更新文档（类似mysql update操作， 有则更新，无则新建）http://localhost:9200/blog/ariticle/1/_update/ POST{&quot;script&quot;:&quot;ctx._source.content = /&quot;new version 2.0 20160714/&quot;&quot;}更新后结果显示：{“_index”: “blog”,“_type”: “ariticle”,“_id”: “1”,“_version”: 2,“_shards”: {”total”: 2,“successful”: 1,“failed”: 0}}注意更新文档需要在config\elasticsearch.yml下新增以下内容：script.groovy.sandbox.enabled: truescript.engine.groovy.inline.search: onscript.engine.groovy.inline.update: onscript.inline: onscript.indexed: onscript.engine.groovy.inline.aggs: onindex.mapper.dynamic: true4、删除文档（类似mysql delete操作）http://localhost:9200/blog/ariticle/8/ DELETE{- &quot;found&quot;: true,- &quot;_index&quot;: &quot;blog&quot;,- &quot;_type&quot;: &quot;ariticle&quot;,- &quot;_id&quot;: &quot;8&quot;,- &quot;_version&quot;: 2,- &quot;_shards&quot;: {- &quot;total&quot;: 2,- &quot;successful&quot;: 1,- &quot;failed&quot;: 0- }}5、查询过滤过滤器类型过滤器类型与查询类型基本相对应，都有范围（过滤）查询、term、terms等类型。term、terms过滤term、terms的含义与查询时一致。term用于精确匹配、terms用于多词条匹配。不过既然过滤器既然适用于大范围过滤，term、terms在过滤中使用意义不大。范围过滤（range）查询16年10月以来所有内容含有“java”的文档，先过滤剩下符合10月的文章，再精确匹配。{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;query&quot;:{ &quot;match&quot;:{ &quot;contents&quot;:&quot;java&quot; } }, &quot;filter&quot;:{ &quot;range&quot;:{ &quot;date&quot;:{ &quot;gte&quot;:&quot;2016-10-01&quot; } } } } }}exists、mising过滤器exists过滤指定字段没有值的文档{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;exists&quot;:{ &quot;field&quot;:&quot;id&quot; } } } }}将不返回id字段无值的文档。missing 过滤器与exists相反，它过滤指定字段有值的文档。{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;missing&quot;:{ &quot;field&quot;:&quot;id&quot; } } } }}标识符（ids）过滤器需要过滤出若干指定_id的文档，可使用标识符过滤器(ids){ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;ids&quot;:{ &quot;values&quot;:[1,2,6,7] } } } }}组合过滤器可以对这些过滤器组合使用，ES中有2类组合过滤器。一类是bool过滤器，一类是and、or、not过滤器。bool过滤器对应布尔查询（bool）bool过滤占用资源，总结来说，一般情况下用bool过滤器，当遇到位置查询、数值范围、脚本查询时使用and、or、not过滤。查询日期在16年10月份且文章标题或内容包含“ES“的文档用SQL表示为select * from article where date between '2016-10-01' and '2016-10-31' and (name like '%ES%' or contents like '%ES%')使用bool过滤器{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;bool&quot;:{ &quot;should&quot;:[{ &quot;match&quot;:{ &quot;name&quot;:&quot;ES&quot; } },{ &quot;match&quot;:{ &quot;contents&quot;:&quot;ES&quot; } }], &quot;must&quot;:{ range&quot;:{ &quot;date&quot;:{ &quot;gte&quot;:&quot;2016-10-01&quot;, &quot;lte&quot;:&quot;2016-10-31&quot; } } } } } } }}布尔查询也可嵌套布尔查询，如要查询16年10月份内容有关ES的文章或任何时段文章名称与java相关的所有文章。（这里只是举例，不考虑实际情况）用SQL表示为select * from article where contents like '%ES%' and date between '2016-10-01' and '2016-10-31' or name like '%java%'{ &quot;query&quot;: { &quot;filtered&quot;: { &quot;filter&quot;: { &quot;bool&quot;: { &quot;should&quot;: [{ &quot;bool&quot;: { &quot;must&quot;: [{ &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2016-09-01&quot;, &quot;lte&quot;: &quot;2016-09-30&quot; } } }, { &quot;match&quot;: { &quot;contents&quot;: &quot;js&quot; } }] } }, { &quot;match&quot;: { &quot;name&quot;: &quot;java&quot; } }] } } } }}我们查询的主要用or关系连接，所以使用bool查询的should语句。一边是contents like '%ES%' and date between '2016-10-01' and '2016-10-31' ，另一个为name like '%java%' 前者需嵌套一个bool查询，使用must连接，后者简单使用match即可。]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群部署]]></title>
    <url>%2F2018%2F10%2F05%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElasticsearch%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Elasticsearch集群部署详解环境：centos6.9、jdk1.8.0_151、elasticsearch-5.6.5注意：es5.x要求jdk1.8，否则会报错本文以讲解Elasticsearch三个节点的分布式部署、核心配置的含义以及分布式部署遇到的坑。部署节点原理多机集群中的节点可以分为master nodes和data nodes,在配置文件中使用Zen发现(Zen discovery)机制来管理不同节点。Zen发现是ES自带的默认发现机制，使用多播发现其它节点。只要启动一个新的ES节点并设置和集群相同的名称这个节点就会被加入到集群中。（所以，同集群的集群名称一致，才能便于自动发现）Elasticsearch集群中有的节点一般有三种角色:master node、data node和client node。1）master node——master节点点主要用于元数据(metadata)的处理，比如索引的新增、删除、分片分配等。2）client node——client 节点起到路由请求的作用，实际上可以看做负载均衡器。3）data node——data节点上保存了数据分片。它负责数据相关操作，比如分片的 CRUD，以及搜索和整合操作。这些操作都比较消耗 CPU、内存和 I/O 资源；三节点 Elasticsearch 分布式部署(示例)0、节点规划master node:10.0.15.57client node:10.0.15.12data node:10.0.15.21、下载 下载地址 https://www.elastic.co/downloads/past-releases/elasticsearch-5-6-5wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.5.tar.gz2、解压并创建日志和数据目录tar zxvf elasticsearch-5.6.5.tar.gzcd elasticsearchmkdir datamkdir logs3、ES所有配置作用介绍参考Elasticsearch详细配置4、修改ES配置文件 进入到config文件夹，编辑 elasticsearch.ymlvim elasticsearch-5.6.5/config/elasticsearch.yml 步骤1：配置好主节点Master信息。cluster.name: esnode.name: &quot;test01-master&quot;node.master: truenode.data: truepath.data: /opt/elasticsearch/datapath.logs: /opt/elasticsearch/logsnetwork.host: 10.0.15.57http.port: 9200http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;discovery.zen.ping.unicast.hosts: [&quot;10.0.15.57:9300&quot;, &quot;10.0.15.12:9300&quot;,&quot;10.0.15.2:9300&quot;]discovery.zen.minimum_master_nodes: 2gateway.recover_after_nodes: 3gateway.recover_after_time: 5mgateway.expected_nodes: 15、拷贝到其他机器上6、修改配置文件 cient节点；修改节点名称信息。 只列举不一样的配置：node.name: &quot;test02-cient&quot;node.master: truenode.data: falsenetwork.host: 10.0.15.12 data节点；修改节点名称 只列举不一样的配置： node.name: &quot;test03-data&quot; node.master: false node.data: true network.host: 10.0.15.27、创建用户 +++注意:三台机器都要创建+++ root用户无法启动es,必须新建一个其他用户,并对其赋予es目录的操作权限 ，这里我新建了es用户和组 创建组grupadd es 创建用户useradd es -g es 修改用户和组chown -R es:es es的安装目录 查看 8、启动 分别运行Master，client，data节点（顺序无关）./elasticsearch -d 成功效果 用head插件可以看到各个节点情况 9、分布式部署遇到的坑 错误1：max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536] 这个问题是无法创建本地文件,用户最大可创建文件数太小 解决：只需要修改创建文件的最大数目为65536就行了 切换到root用户修改 vim /etc/security/limits.confroot soft nofile 65536root hard nofile 65536* soft nofile 65536* hard nofile 65536保存、退出、重新登录才可生效 参数解释： - soft nproc:可打开的文件描述符的最大数(软限制) - hard nproc:可打开的文件描述符的最大数(硬限制) - soft nofile:单个用户可用的最大进程数量(软限制) - hard nofile:单个用户可用的最大进程数量(硬限制) 错误2：max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 虚拟内存太小 切换到root用户修改 vim /etc/sysctl.confvm.max_map_count=262144 执行命令： sudo sysctl -p 错误3：Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000085330000, 2060255232, 0) failed; error=’Cannot allocate memory’ (errno=12) jvm需要分配的内存太大 vim config/jvm.options-Xms2g-Xmx2g该为：-Xms100m-Xmx100m 错误4：max number of threads [1024] for user [es] likely too low, increase to at least [2048] 原因：无法创建本地线程问题,用户最大可创建线程数太小 解决方案：切换到root用户，进入limits.d目录下，修改90-nproc.conf 配置文件 成功启动： vi /etc/security/limits.d/90-nproc.conf 找到如下内容： * soft nproc 1024 修改为 * soft nproc 2048 错误5：n ERROR No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property ‘log4j2.debug‘ to show Log4j2 internal initialization logging. es用 yum install -y log4j* 错误6：unknown setting [discovery.zen.ping.multicast.enabled] please check that any required plugins are installed, or check the breaking changes documentation for removed settings 在elasticsearch.yml文件中 添加bootstrap.system_call_filter: false 错误7：三节点不能联通的：原因： 1）各节点的hostname没有正确设置，和节点名称设置为一致。 2）关闭防火墙，service iptables stop；否则，打开防火墙会导致无法正常通信，head插件不能看到节点数据信息。]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK+filebeat系统搭建(下载和配置)]]></title>
    <url>%2F2018%2F10%2F03%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FELK%2Bfilebeat%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%8B%E8%BD%BD%E5%92%8C%E9%85%8D%E7%BD%AE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[系统环境System：centos 7Java ： jdk 1.8.0_144Elasticsearch： 6.1.1Kibana： 6.1.1Logstash： 6.1.1Filebeat： 6.1.1软件介绍ELK是Elasticsearch、Logstrash和Kibana这三个软件的首字母的缩写，它们代表的是一套成熟的日志管理系统:Logstrash作为数据收集引擎。它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置,一般会发送给Elasticsearch;Elasticsearch是个分布式搜索和分析引擎，优点是能对大容量的数据进行接近实时的存储、搜索和分析操作;Kibana对Elasticsearch的分析搜索做出可视化的界面展示。Filebeat，它的作用是在客户端收集和传输日志，基于 Logstash-Forwarder 源代码开发，是对它的替代.tips： Logstash 的运行依赖于 Java 运行环境， Logstash 1.5 以上版本不低于 java 7 推荐使用最新版本的 Java 。由于我们只是运行 Java 程序，而不是开发，下载 JRE 即可。首先，在 Oracle 官方下载新版 jre ，下载地址： http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html安装过程Java开发环境 1）下载jdk-8u131-linux-x64.tar，上传至CentOS上。 下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html （该链接为jre的链接） 2） 解压源码包 # mkdir -pv /usr/local/java # tar -zxvf jdk-8u131-linux-x64.tar.gz -C /usr/local/java 3） 设置JDK环境变量 # vi /etc/profile # export JAVA_HOME=/usr/local/java/jdk1.8.0_111 # export JRE_HOME=\${JAVA_HOME}/jre # export CLASSPATH=\${JAVA_HOME}/lib:\${JRE_HOME}/lib # export PATH=${JAVA_HOME}/bin:$PATH # source /etc/profile 4） 检验是否安装成功 # java -version安装elasticsearch1）下载文件 进入elsaticsearch下载界面 选择tar进行下载，下载完成后放入/opt下（放置位置个人喜好）2）解压文件 进入压缩包所在目录，解压文件。cd /opt/svr/elasticsearchtar -zxvf ./elasticsearch-6.1.1.tar.gz -C /opt/svr/elasticsearch3）下载x-pack插件cd elasticsearch-6.1.1/bin./elasticsearch-plugin install x-pack4）修改ES参数 进入解压后文件夹的config目录，增加或者更改其中的配置信息cd elasticsearch-6.1.1vi config/elasticsearch.yml# 这里指定的是集群名称，需要修改为对应的，开启了自发现功能后，ES会按照此集群名称进行集群发现cluster.name: mortynode.name: Rick# 数据目录path.data: /data/elk/data# log 目录path.logs: /data/elk/logs# 修改一下ES的监听地址，这样别的机器也可以访问vnetwork.host: 0.0.0.0# 默认的端口号http.port: 9200# 填写你要监视的地址，可以是多个discovery.zen.ping.unicast.hosts: [“172.18.5.111”, “172.18.5.112”]# discovery.zen.minimum_master_nodes: 3# enable cors，保证_site类的插件可以访问eshttp.cors.enabled: truehttp.cors.allow-origin: “*”# Centos6不支持SecComp，而ES5.2.0默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。bootstrap.memory_lock: falsebootstrap.system_call_filter: false tips：单个节点可以作为一个运行中的Elasticsearch的实例。而一个集群是一组拥有相同cluster.name的节点，（单独的节点也可以组成一个集群）可以在elasticsearch.yml配置文件中修改cluster.name,该节点启动时加载（需要重启服务后才会生效）。 5）修改系统参数 修改系统参数以确保系统有足够资源启动ES 1. 设置内核资源vi /etc/sysctl.confvm.max_map_count=655360sysctl -p2.设置资源参数vi /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 65536* hard nproc 131072Tips: limits.conf 配置3.设置用户资源参数vi /etc/security/limits.d/20-nproc.confelk soft nproc 655364.添加启动用户，设置权限启动ElasticSearch5版本要非root用户，需要新建一个用户来启动ElasticSearchuseradd elk #创建用户elkgroupadd elk #创建组elk useraddusermod -g elk elk #将用户添加到组mkdir -pv /data/elk/{data,logs}# 创建数据和日志目录chown -R elk:elk /data/elk/ # 修改文件所有者chown -R elk:elk /opt/svr/elasticsearch6）启动su elkcd elasticsearch-6.1.1bin/elasticsearch -d #-d 为保持后台运行free -g 查看内存剩余 使用curl localhost:9200 来检查ES是否启动成功： 备注: 客户端需要先关闭防火墙 systemctl stop firewalld.service{“name” : “morty”,“cluster_name” : “Rick”,“cluster_uuid” : “4RlZck7ERWGbQKNn5KvXGA”,“version” : {“number” : “6.1.1”,“build_hash” : “bd92e7f”,“build_date” : “2017-12-17T20:23:25.338Z”,“build_snapshot” : false,“lucene_version” : “7.1.0”,“minimum_wire_compatibility_version” : “5.6.0”,“minimum_index_compatibility_version” : “5.0.0”},“tagline” : “You Know, for Search”}安装Elasticsearch-head 1）下载文件 下载地址: https://github.com/mobz/elasticsearch-head unzip elasticsearch-head-master.zip 2）下载node-js curl --silent --location https://rpm.nodesource.com/setup | bash - yum install -y nodejs 3）下载grunt（由于head 插件的执行文件是有grunt 命令来执行的，所以这个命令必须安装） # npm install grunt --save-dev # cd elasticsearch-head-master # npm install 4）修改配置文件 进入elasticsearch-head-master 文件夹下，执行命令vi Gruntfile.js文件：增加hostname属性，设置为*, 修改启动端口,默认为9100 5）启动elasticsearch-head 6）查看启动状态( 浏览器访问 http://ip地址:9100/ ) 如果出现集群健康值: 未连接的状态，是因为没有配置ElasticSearch的跨域访问，默认是禁止的，所以链接失败。 解决方法: 需要修改elasticsearch配置文件；命令进入到elasticsearch-5.6.5 /config 文件中 elasticsearch.yml，添加 保存之后重启elasticsearch和head即可 安装kibana1）下载文件 进入kibana下载界面 选择tar进行下载，下载完成后放入/opt/elk下（放置位置个人喜好）2）解压文件 进入压缩包所在目录，解压文件。cd /opt/svr/kibanatar -zxvf ./kibana-6.1.1-linux-x86_64.tar.gz -C /opt/svr/kibana3）配置kibana 进入config目录下，修改kibana.yml文件cd kibana-6.1.1-linux-x86_64vi config/kibana.ymlserver.port: 5601 #开启默认端口5601速冻server.host: “192.168.91.129” #站点地址elasticsearch.url: http://192.168.91.129:9200 #指向&gt;elasticsearch服务的ip地址kibana.index: “.kibana”4）启动kibanabin/kibana 启动成功后访问浏览器访问地址http://192.168.91.129:9200见到如下即表示启动成功 安装logstash1）下载文件 进入logstrash下载界面 选择tar进行下载，下载完成后放入/opt/elk下（放置位置个人喜好）2）解压文件进入压缩包所在目录，解压文件。cd /opt/svr/logstashtar -zxvf ./logstrash-6.1.1.tar.gz -C /opt/svr/logstash3）创建配置文件进入解压后文件夹的config目录，新增测试配置文件：cd /opt/svr/logstash/logstash-6.1.1/config/vi logstrash-test.confLogstash 使用 input 和 output 定义收集日志时的输入和输出的相关配置4）启动服务bin/logstash -f config/logstash-simple.conf 执行这个语句可能会报错：[FATAL][logstash.runner] Logstash could not be started because there is already another instance using the configured data directory. If you wish to run multiple instances, you must change the “path.data” setting.这是因为当前的logstash版本不支持多个instance共享一个path.data，所以需要在启动时，命令行里增加”–path.data PATH “，为不同实例指定不同的路径bin/logstash -f config/logstash-simple.conf --path.data ./logs/安装filebeat1）下载文件 进入filebeat下载界面 选择Liunx-64x包进行下载，下载完成后放入需要检测的服务器上的任一目录下（放置位置个人喜好）2）解压文件 进入压缩包所在目录，解压文件。cd /opt/svr/filebeattar -zxvf ./filebeat-6.1.1-linux-x86_64.tar.gz -C /opt/svr/filebeat3）修改配置文件进入解压后文件夹的config目录，新增测试配置文件：cd filebeat-6.1.1-linux-x86_64vi filebeat.ymlpaths:- /opt/logs/filebeat.log #监控的日志文件————–Elasticsearch output——————-(全部注释掉)—————-Logstash output———————output.logstash:hosts: [“182.119.137.177:5044”] #你的logstash端口4）启动服务bin/filebeat -e -c config/filebeat.yml更多ELK资料：Elasticsearch+Logstash+Kibana教程Elasticsearch 官方文档参考Logstash 官方文档参考Kibana User Guide 官方文档参考]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch&ELK介绍]]></title>
    <url>%2F2018%2F10%2F02%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElasticsearch%26ELK%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[（1）思考：大规模数据如何检索？如：当系统数据量上了10亿、100亿条的时候，我们在做系统架构的时候通常会从以下角度去考虑问题：1）用什么数据库好？(mysql、sybase、oracle、达梦、神通、mongodb、hbase…)2）如何解决单点故障；(lvs、F5、A10、Zookeep、MQ)3）如何保证数据安全性；(热备、冷备、异地多活)4）如何解决检索难题；(数据库代理中间件：mysql-proxy、Cobar、MaxScale等;)5）如何解决统计分析问题；(离线、近实时)（2）传统数据库的应对解决方案对于关系型数据，我们通常采用以下或类似架构去解决查询瓶颈和写入瓶颈：解决要点：1）通过主从备份解决数据安全性问题；2）通过数据库代理中间件心跳监测，解决单点故障问题；3）通过代理中间件将查询语句分发到各个slave节点进行查询，并汇总结果（3）非关系型数据库的解决方案对于Nosql数据库，以mongodb为例，其它原理类似：解决要点：1）通过副本备份保证数据安全性；2）通过节点竞选机制解决单点问题；3）先从配置库检索分片信息，然后将请求分发到各个节点，最后由路由节点合并汇总结果另辟蹊径——完全把数据放入内存怎么样？我们知道，完全把数据放在内存中是不可靠的，实际上也不太现实，当我们的数据达到PB级别时，按照每个节点96G内存计算，在内存完全装满的数据情况下，我们需要的机器是：1PB=1024T=1048576G节点数=1048576/96=10922个实际上，考虑到数据备份，节点数往往在2.5万台左右。成本巨大决定了其不现实！从前面讨论我们了解到，把数据放在内存也好，不放在内存也好，都不能完完全全解决问题。全部放在内存速度问题是解决了，但成本问题上来了。为解决以上问题，从源头着手分析，通常会从以下方式来寻找方法：1、存储数据时按有序存储；2、将数据和索引分离；3、压缩数据；这就引出了Elasticsearch。1. ES 基础一网打尽1.1 ES定义ES=elaticsearch简写， Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。1.2 Lucene与ES关系？1）Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。2）Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。1.3 ES主要解决问题：1）检索相关数据；2）返回统计结果；3）速度要快。1.4 ES工作原理当ElasticSearch的节点启动后，它会利用多播(multicast)(或者单播，如果用户更改了配置)寻找集群中的其它节点，并与之建立连接。这个过程如下图所示：1.5 ES核心概念1）Cluster：集群。ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。2）Node：节点。形成集群的每个服务器称为节点。3）Index：索引。index是一系列document 的集合。比方说我们可以有一个index叫做 customer data，另外一个叫做product catalog，还有一个叫做 order data。一个index被一个唯一的小写字母名字标记。这个名字将会影响里面数据的查询，更新操作2）Type：分组。在一个Index中，我们可以定义很多的type。一个type是一个逻辑分组。大多书情况下，同一个type中的数据会有相同的数据结构。4） Document ：记录。一个Document是一个可以被索引的基本单元。3）Shard：分片。当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。4）Replia：副本。为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。5）全文检索。全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。1.6 ES数据架构的主要概念（与关系数据库Mysql对比）（1）关系型数据库中的数据库（DataBase），等价于ES中的索引（Index）（2）一个数据库下面有N张表（Table），等价于1个索引Index下面有N多类型（Type），（3）一个数据库表（Table）下的数据由多行（ROW）多列（column，属性）组成，等价于1个Type由多个文档（Document）和多Field组成。（4）在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。 与之对应的，在ES中：Mapping定义索引下的Type的字段处理规则，即索引如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等。（5）在数据库中的增insert、删delete、改update、查search操作等价于ES中的增PUT/POST、删Delete、改_update、查GET.1.7 ELK是什么？ELK=elasticsearch+Logstash+kibanaELK是三个开源软件的缩写，分别表示：Elasticsearch , Logstash, Kibana , 它们都是开源软件。新增了一个FileBeat，它是一个轻量级的日志收集处理工具(Agent)，Filebeat占用资源少，适合于在各个服务器上搜集日志后传输给Logstash，官方也推荐此工具。Elasticsearch： 是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。Logstash：主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上去。Kibana： 也是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。Filebeat：隶属于Beats。目前Beats包含四种工具：1. Packetbeat（搜集网络流量数据）2. Topbeat（搜集系统、进程和文件系统级别的 CPU 和内存使用情况等数据）3. Filebeat（搜集文件数据）4. Winlogbeat（搜集 Windows 事件日志数据）ELK架构图架构图1）这是最简单的一种ELK架构方式。优点是搭建简单，易于上手。缺点是Logstash耗资源较大，运行占用CPU和内存高。另外没有消息队列缓存，存在数据丢失隐患。此架构由Logstash分布于各个节点上搜集相关日志、数据，并经过分析、过滤后发送给远端服务器上的Elasticsearch进行存储。Elasticsearch将数据以分片的形式压缩存储并提供多种API供用户查询，操作。用户亦可以更直观的通过配置Kibana Web方便的对日志查询，并根据数据生成报表。架构图2）此种架构引入了消息队列机制，位于各个节点上的Logstash Agent先将数据/日志传递给Kafka（或者Redis），并将队列中消息或数据间接传递给Logstash，Logstash过滤、分析后将数据传递给Elasticsearch存储。最后由Kibana将日志和数据呈现给用户。因为引入了Kafka（或者Redis）,所以即使远端Logstash server因故障停止运行，数据将会先被存储下来，从而避免数据丢失。架构图3）此种架构将收集端logstash替换为beats，更灵活，消耗资源更少，扩展性更强。同时可配置Logstash 和Elasticsearch 集群用于支持大集群系统的运维日志数据监控和查询。2. ES特点和优势1）分布式实时文件存储，可将每一个字段存入索引，使其可以被检索到。2）实时分析的分布式搜索引擎。分布式：索引分拆成多个分片，每个分片可有零个或多个副本。集群中的每个数据节点都可承载一个或多个分片，并且协调和处理各种操作；负载再平衡和路由在大多数情况下自动完成。3）可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。也可以运行在单台PC上（已测试）4）支持插件机制，分词插件、同步插件、Hadoop插件、可视化插件等。3、ES性能3.1 性能结果展示（1）硬件配置：CPU 16核 AuthenticAMD内存 总量：32GB硬盘 总量：500GB 非SSD（2）在上述硬件指标的基础上测试性能如下：1）平均索引吞吐量： 12307docs/s（每个文档大小：40B/docs）2）平均CPU使用率： 887.7%（16核，平均每核：55.48%）3）构建索引大小： 3.30111 GB4）总写入量： 20.2123 GB5）测试总耗时： 28m 54s.4、为什么要用ES？4.1 ES国内外使用优秀案例1） 2013年初，GitHub抛弃了Solr，采取ElasticSearch 来做PB级的搜索。 “GitHub使用ElasticSearch搜索20TB的数据，包括13亿文件和1300亿行代码”。2）维基百科：启动以elasticsearch为基础的核心搜索架构。3）SoundCloud：“SoundCloud使用ElasticSearch为1.8亿用户提供即时而精准的音乐搜索服务”。4）百度：百度目前广泛使用ElasticSearch作为文本数据分析，采集百度所有服务器上的各类指标数据及用户自定义数据，通过对各种数据进行多维分析展示，辅助定位分析实例异常或业务层面异常。目前覆盖百度内部20多个业务线（包括casio、云分析、网盟、预测、文库、直达号、钱包、风控等），单集群最大100台机器，200个ES节点，每天导入30TB+数据。4.2 我们也需要实际项目开发实战中，几乎每个系统都会有一个搜索的功能，当搜索做到一定程度时，维护和扩展起来难度就会慢慢变大，所以很多公司都会把搜索单独独立出一个模块，用ElasticSearch等来实现。近年ElasticSearch发展迅猛，已经超越了其最初的纯搜索引擎的角色，现在已经增加了数据聚合分析（aggregation）和可视化的特性，如果你有数百万的文档需要通过关键词进行定位时，ElasticSearch肯定是最佳选择。当然，如果你的文档是JSON的，你也可以把ElasticSearch当作一种“NoSQL数据库”， 应用ElasticSearch数据聚合分析（aggregation）的特性，针对数据进行多维度的分析。【知乎：热酷架构师潘飞】ES在某些场景下替代传统DB个人以为Elasticsearch作为内部存储来说还是不错的，效率也基本能够满足，在某些方面替代传统DB也是可以的，前提是你的业务不对操作的事性务有特殊要求；而权限管理也不用那么细，因为ES的权限这块还不完善。由于我们对ES的应用场景仅仅是在于对某段时间内的数据聚合操作，没有大量的单文档请求（比如通过userid来找到一个用户的文档，类似于NoSQL的应用场景），所以能否替代NoSQL还需要各位自己的测试。如果让我选择的话，我会尝试使用ES来替代传统的NoSQL，因为它的横向扩展机制太方便了。5. ES的应用场景是怎样的？通常我们面临问题有两个：1）新系统开发尝试使用ES作为存储和检索服务器；2）现有系统升级需要支持全文检索服务，需要使用ES。以上两种架构的使用，以下链接进行详细阐述。http://blog.csdn.net/laoyang360/article/details/52227541一线公司ES使用场景：1）新浪ES 如何分析处理32亿条实时日志 http://dockone.io/article/5052）阿里ES 构建挖财自己的日志采集和分析体系 http://afoo.me/columns/tec/logging-platform-spec.html3）有赞ES 业务日志处理 http://tech.youzan.com/you-zan-tong-ri-zhi-ping-tai-chu-tan/4）ES实现站内搜索 http://www.wtoutiao.com/p/13bkqiZ.html6. 如何部署ES？6.1 ES部署（无需安装）1）零配置，开箱即用2）没有繁琐的安装配置3）java版本要求：最低1.7我使用的1.8[root@laoyang config_lhy]# echo $JAVA_HOME/opt/jdk1.8.0_914）下载地址：https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.3.5/elasticsearch-2.3.5.zip5）启动cd /usr/local/elasticsearch-2.3.5./bin/elasticsearchbin/elasticsearch -d(后台运行)6.2 ES必要的插件必要的Head、kibana、IK（中文分词）、graph等插件的详细安装和使用。http://blog.csdn.net/column/details/deep-elasticsearch.html6.3 ES windows下一键安装自写bat脚本实现windows下一键安装。1）一键安装ES及必要插件（head、kibana、IK、logstash等）2）安装后以服务形式运行ES。3）比自己摸索安装节省至少2小时时间，效率非常高。脚本说明：http://blog.csdn.net/laoyang360/article/details/519002357. ES对外接口（开发人员关注）1）JAVA API接口http://www.ibm.com/developerworks/library/j-use-elasticsearch-java-apps/index.html2）RESTful API接口常见的增、删、改、查操作实现：http://blog.csdn.net/laoyang360/article/details/51931981]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper集群搭建]]></title>
    <url>%2F2018%2F09%2F18%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2FZookeeper%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[软件环境准备:Linux服务器一台、三台、五台（2*n+1台）；Java jdk 1.7；zookeeper 3.4.6版；软件安装：解压jdk、zookeeper文件到指定目录，执行命令tar -zvxf xxxx.tar.gz -C /usr/local/program配置环境变量，vi /etc/profile#set enviromentexport JAVA_HOME=/usr/local/program/jdk1.7.0_79export ZK_HOME=/usr/local/program/zk/zookeeper-3.4.6export PATH=$JAVA_HOME/bin:$ZK_HOME/bin:$PATHjava -version 命令测试是否成功配置jdk[root@localhost ~]# java -versionjava version &quot;1.7.0_79&quot;Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)配置zookeeper；首先建立两个文件夹mkdir /usr/local/program/zk/zkdata ，mkdir /usr/local/program/zk/zkdataLog 然后执行命令cd /usr/local/program/zk/zookeeper-3.4.6/conf/ 进入配置文件目录，拷贝一份配置文件cp zoo_sample.cfg zoo.cfg改名为zoo.cfg编辑配置文件修改配置（将端口改大的目的是为了防止冲突）：# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/usr/local/program/zk/zkdatadataLogDir=/usr/local/program/zk/zkdataLog# the port at which the clients will connectclientPort=12181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1server.1=192.168.0.102:12888:13888server.2=192.168.0.103:12888:13888server.3=192.168.0.104:12888:13888进入zkdata目录，执行命令echo “1” &gt; myid创建myid文件并输入值为1依次在另外两台机器上执行同样的操作，myid的数值依次为2,3配置成功，执行命令zkServer.sh start分别启动三台机器执行命令zkServer.sh status查看对应的状态发现出现：zookeeper Error contacting service. It is probably not running错误执行命令：tail -f zookeeper.out查看到错误记录：2016-04-21 06:56:56,242 [myid:1] – WARN [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:Follower@89] – Exception when following the leaderjava.net.NoRouteToHostException: No route to hostat java.net.PlainSocketImpl.socketConnect(Native Method)at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)at java.net.Socket.connect(Socket.java:579)at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:225)at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:71)at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:786)经验证：防火墙的问题，执行命令service iptables stop关闭防火墙（如要永久关闭防火墙执行命令：chkconfig iptables off）重启机器，执行命令zkServer.sh status 分别查看每台机器的状态为leader、follower、follower则表明启动成功配置说明：作用：leader：从客户端接收读写请求，响应读写请求，向slaver发送数据slaver：从leader同步数据，当leader失败时，从新投票选举新的leader配置详解：myid:位于快照目录，是用来标识本台机器在集群中的唯一标识zoo.cfg: tickTime:是initLimit 和 syncLimit的时间单位（毫秒） initLimit:集群机器之间的同步时间，比如启动m1，s1在这个时间段内必须都起来 syncLimit：leader与follower之间的通信时间（心跳），超时则说明follower失败#快照日志的存储路径dataDir=/usr/local/program/zk/zkdata#zk事物日志存储目录 (如果不配置zkdataLog那么快照日志和事务日志都将写到dataDir中，严重影响性能)dataLogDir=/usr/local/program/zk/zkdataLog# the port at which the clients will connect#本台机器的端口，默认为2181clientPort=12181log4j.properties: 日志文件，zookeeper.out位置bin下面，可以更改注意：需要定期清理事务日志文件，否则会造成性能下降（crontab）crontab -l查看当前用户定时任务crontab -e编辑定时任务(例：0 0 * * 0 sh /usr/local/program/zk/cleanup.sh)另外2台机器执行同样的操作#server.1标识本台机器，通过myid文件指定id=1;ip:机器之间通信端口（默认为2888）:leader选举端口（默认为3888）server.1=192.168.0.102:12888:13888]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka安装部署及使用]]></title>
    <url>%2F2018%2F09%2F18%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2FKafka%20%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、Kafka 单节点部署Kafka中单节点部署又分为两种，一种为单节点单Broker部署，一种为单节点多Broker部署。因为是单节点Kafka，所以在安装ZK时也只需要单节点即可。下载安装Zookeeper ZooKeeper官网：http://zookeeper.apache.org/ 下载Zookeeper并解压到指定目录$ wget http://www-eu.apache.org/dist/zookeeper/zookeeper-3.5.1-alpha/zookeeper-3.5.1-alpha.tar.gz$ tar -zxvf zookeeper-3.5.1-alpha.tar.gz -c /opt/zookeeper 进入Zookeeper的config目录下$ cd /opt/zookeeper/conf 拷贝zoo_sample.cfg文件重命名为zoo.cfg，然后修改dataDir属性# 数据的存放目录dataDir=/home/hadoop/zkdata# 端口，默认就是2181clientPort=2181 配置环境变量# Zookeeper Environment Variableexport ZOOKEEPER_HOME=/opt/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin Zookeeper 启动停止命令$ zkServer.sh start$ zkServer.sh stop 备注: 在安装完Zookeeper后，输入命令启动后，jps中并没有查看到QuorumPeerMain进程，说明没有启动成功，进入Zookeeper的log目录下查看日志，发现报了一个错误，如下 AdminServer$AdminServerException: Problem starting AdminServer on address 0.0.0.0, port 8080 and command URL /commands 原因：zookeeper的管理员端口被占用 解决：笔者使用的zookeeper的版本为3.5.1，该版本中有个内嵌的管理控制台是通过jetty启动，会占用8080 端口，需要修改配置里的“admin.serverPort=8080”，默认8080没有写出来，只要改为一个没使用的端口即可，例如：admin.serverPort=8181 安装Kafka 下载地址: https://www.apache.org/dyn/closer.cgi?path=/kafka/2.0.0/kafka_2.1 tar -xzf kafka_2.11-2.0.0.tgz1.Kafka 单节点单Broker部署及使用 部署架构 配置Kafka 参考官网：http://kafka.apache.org/quickstart 进入kafka的config目录下，有一个server.properties，添加如下配置# broker的全局唯一编号，不能重复broker.id=0# 监听listeners=PLAINTEXT://:9092# 日志目录log.dirs=/home/hadoop/kafka-logs# 配置zookeeper的连接（如果不是本机，需要该为ip或主机名）zookeeper.connect=localhost:2181 启动Zookeeper[hadoop@Master ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 启动Kafka$ kafka-server-start.sh $KAFKA_HOME/config/server.properties 打印的日志信息没有报错，可以看到如下信息[Kafka Server 0], started (kafka.server.KafkaServer) 但是并不能保证Kafka已经启动成功，输入jps查看进程，如果可以看到Kafka进程，表示启动成功[hadoop@Master ~]$ jps9173 Kafka9462 Jps8589 QuorumPeerMain[hadoop@Master ~]$ jps -m9472 Jps -m9173 Kafka /opt/kafka/config/server.properties8589 QuorumPeerMain /opt/zookeeper/bin/../conf/zoo.cfg 创建topic[hadoop@Master ~]$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 参数说明： –zookeeper：指定kafka连接zk的连接url，该值和server.properties文件中的配置项{zookeeper.connect}一样 –replication-factor：指定副本数量 –partitions：指定分区数量 –topic：主题名称 查看所有的topic信息[hadoop@Master ~]$ kafka-topics.sh --list --zookeeper localhost:2181test 启动生产者[hadoop@Master ~]$ kafka-console-producer.sh --broker-list localhost:9092 --topic test 启动消费者[hadoop@Master ~]$ kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning--from-beginning参数如果有表示从最开始消费数据，旧的和新的数据都会被消费，而没有该参数表示只会消费新产生的数据 测试2.Kafka 单节点多Broker部署及使用 部署架构 配置Kafka 参考官网：http://kafka.apache.org/quickstart 拷贝server.properties三份[hadoop@Master ~]$ cd /opt/kafka/config[hadoop@Master config]$ cp server.properties server-1.properties[hadoop@Master config]$ cp server.properties server-2.properties[hadoop@Master config]$ cp server.properties server-3.properties 修改server-1.properties文件# broker的全局唯一编号，不能重复broker.id=1# 监听listeners=PLAINTEXT://:9093# 日志目录log.dirs=/home/hadoop/kafka-logs-1 修改server-2.properties文件# broker的全局唯一编号，不能重复broker.id=2# 监听listeners=PLAINTEXT://:9094# 日志目录log.dirs=/home/hadoop/kafka-logs-2 修改server-3.properties文件# broker的全局唯一编号，不能重复broker.id=3# 监听listeners=PLAINTEXT://:9094# 日志目录log.dirs=/home/hadoop/kafka-logs-3 启动Zookeeper[hadoop@Master ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 启动Kafka（分别启动server1、2、3）$ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties$ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties$ kafka-server-start.sh $KAFKA_HOME/config/server-3.properties 查看进程[hadoop@Master ~]$ jps11905 Kafka11619 Kafka8589 QuorumPeerMain12478 Jps12191 Kafka[hadoop@Master ~]$ jps -m11905 Kafka /opt/kafka/config/server-2.properties11619 Kafka /opt/kafka/config/server-1.properties12488 Jps -m8589 QuorumPeerMain /opt/zookeeper/bin/../conf/zoo.cfg12191 Kafka /opt/kafka/config/server-3.properties 创建topic（指定副本数量为3-会平均部署到不同的节点上）[hadoop@Master ~]$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topicCreated topic &quot;my-replicated-topic&quot;. 查看所有的topic信息[hadoop@Master ~]$ kafka-topics.sh --list --zookeeper localhost:2181 my-replicated-topictest 查看某个topic的详细信息[hadoop@Master ~]$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs:Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 启动生产者$ kafka-console-producer.sh --broker-list localhost:9093,localhost:9094,localhost:9095 --topic my-replicated-topic 启动消费者$ kafka-console-consumer.sh --zookeeper localhost:2181 --topic my-replicated-topic --from-beginning 测试二、Kafka 集群搭建(多节点多Broker)Kafka 集群方式部署，需要先安装ZK集群，笔者是三个节点组成的集群，具体安装配置请参考Hadoop HA 高可用集群搭建 中的ZK集群安装，在这笔者主要介绍Kafka的集群安装配置。 启动ZK集群 参考zookeepeer集群搭建 下载安装包wget http://mirror.bit.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.9.0.0.tgz 解压安装包tar -zxvf kafka_2.10-0.9.0.0.tgz -C ~/export/servers/ 创建软连接ln -s kafka_2.10-0.9.0.0/ kafka 修改配置文件server.properties############################# Server Basics ############################## broker 的全局唯一编号，不能重复broker.id=0############################# Socket Server Settings ############################## 配置监听,，默认listeners=PLAINTEXT://:9092# 用来监听连接的端口，producer和consumer将在此端口建立连接,，默认port=9092# 处理网络请求的线程数量，默认num.network.threads=3# 用来处理磁盘IO的线程数量，默认num.io.threads=8# 发送套接字的缓冲区大小，默认socket.send.buffer.bytes=102400# 接收套接字的缓冲区大小，默认socket.receive.buffer.bytes=102400# 请求套接字的缓冲区大小，默认socket.request.max.bytes=104857600############################# Log Basics ############################## kafka 运行日志存放路径log.dirs=/root/export/servers/logs/kafka# topic 在当前broker上的分片个数，默认为1num.partitions=2# 用来恢复和清理data下数据的线程数量，默认num.recovery.threads.per.data.dir=1############################# Log Retention Policy ############################## segment文件保留的最长时间，超时将被删除，默认log.retention.hours=168# 滚动生成新的segment文件的最大时间，默认log.roll.hours=168 配置环境变量# Kafka Environment Variableexport KAFKA_HOME=/root/export/servers/kafkaexport PATH=$PATH:$KAFKA_HOME/bin 分发安装包 注意：分发安装包，也要创建软连接，配置环境变量scp -r ~/export/servers/kafka_2.10-0.9.0.0/ storm02:~/export/serversscp -r ~/export/servers/kafka_2.10-0.9.0.0/ storm03:~/export/servers 再次修改配置文件 测试使用的节点的主机名分别为storm01、storm02、storm03 依次修改各服务器上配置文件server.properties 的 broker.id，分别是0，1，2不得重复 修改host.name分别为storm01，storm02，storm03 启动Kafka集群 注意：在启动Kafka集群前，确保ZK集群已经启动且能够正常运行 测试创建topic[root@storm01 ~]# kafka-topics.sh --create --zookeeper storm01:2181 --replication-factor 3 --partitions 2 --topic testCreated topic &quot;test&quot;.[root@storm01 ~]# kafka-topics.sh --describe --zookeeper storm01:2181 --topic testTopic:test PartitionCount:2 ReplicationFactor:3 Configs:Topic: test Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2Topic: test Partition: 1 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0启动生产者[root@storm01 ~]# kafka-console-producer.sh --broker-list storm01:9092,storm02:9092,storm03:9092 --topic testhellohello kafka clustertesthello storm启动两个消费者，消费消息[root@storm02 ~]# kafka-console-consumer.sh --zookeeper storm02:2181 --topic test --from-beginninghellohello kafka clustertesthello storm[root@storm03 ~]# kafka-console-consumer.sh --zookeeper storm03:2181 --topic test --from-beginninghellohello kafka clustertesthello storm]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pykafka, kafka-python开发]]></title>
    <url>%2F2018%2F09%2F17%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2F%E4%BD%BF%E7%94%A8pykafka%EF%BC%8Ckafka-python%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[一、简介 python连接kafka的标准库，kafka-python和pykafka。kafka-python使用的人多是比较成熟的库，kafka-python并没有zk的支持。pykafka是Samsa的升级版本，使用samsa连接zookeeper，生产者直接连接kafka服务器列表，消费者才用zookeeper。二、kafka-python使用(1) kafka-python安装 pip install kafka-python(2) kafka-python的api https://kafka-python.readthedocs.io/en/master/apidoc/modules.html https://kafka-python.readthedocs.io/en/master/index.html https://pypi.org/project/kafka-python/(3) kafka-python生产者 (4) kafka-python消费者输出结果：ConsumerRecord(topic='test', partition=0, offset=246, timestamp=1531980887190, timestamp_type=0, key=None, value=b'1', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=247, timestamp=1531980887691, timestamp_type=0, key=None, value=b'2', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=248, timestamp=1531980888192, timestamp_type=0, key=None, value=b'3', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=249, timestamp=1531980888694, timestamp_type=0, key=None, value=b'4', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=250, timestamp=1531980889196, timestamp_type=0, key=None, value=b'5', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=251, timestamp=1531980889697, timestamp_type=0, key=None, value=b'6', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=252, timestamp=1531980890199, timestamp_type=0, key=None, value=b'7', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=253, timestamp=1531980890700, timestamp_type=0, key=None, value=b'8', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=254, timestamp=1531980891202, timestamp_type=0, key=None, value=b'9', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=255, timestamp=1531980891703, timestamp_type=0, key=None, value=b'10', checksum=None, serialized_key_size=-1, serialized_value_size=2)1consumer = kafka.KafkaConsumer(bootstrap_servers = ['192.168.17.64:9092','192.168.17.65:9092','192.168.17.68:9092'],2group_id ='test_group_id',3auto_offset_reset ='latest',4enable_auto_commit = False)enable_auto_commit=False 自动提交位移设为flase, 默认为取最新的偏移量，重新建立一个group_id,这样就实现了不影响别的应用程序消费数据，又能消费到最新数据，实现预警（先于用户发现）的目的。]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Registry 私有库]]></title>
    <url>%2F2018%2F09%2F17%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2F%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.关于Registry仓库官方的Docker hub是一个用于管理公共镜像的好地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景需要我们拥有一个私有的镜像仓库用于管理我们自己的镜像。这个可以通过开源软件Registry来达成目的。 Registry在github上有两份代码：老代码库和新代码库。老代码是采用python编写的，存在pull和push的性能问题，出到0.9.1版本之后就标志为deprecated，不再继续开发。从2.0版本开始就到在新代码库进行开发，新代码库是采用go语言编写，修改了镜像id的生成算法、registry上镜像的保存结构，大大优化了pull和push镜像的效率。 官方在Docker hub上提供了registry的镜像（详情），我们可以直接使用该registry镜像来构建一个容器，搭建我们自己的私有仓库服务。Tag为latest的registry镜像是0.9.1版本的，我们直接采用2.1.1版本。 2.Registry的部署运行下面命令获取registry镜像 1$ sudo docker pull registry:2.1.1 然后启动一个容器 1$ sudo docker run -d -v /opt/data/registry/:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1 验证服务是否启动成功 说明我们已经启动了registry服务，打开浏览器输入http://127.0.0.1:5000/v2 3.验证向仓库中push镜像 现在我们通过将镜像push到registry来验证一下。我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库， 1$ sudo docker tag hello-world 127.0.0.1:5000/hello-world 接下来，我们运行docker push将hello-world镜像push到我们的私有仓库中， 1$ sudo docker push 127.0.0.1:5000/hello-world 1234567The push refers to a repository [127.0.0.1:5000/hello-world] (len: 1)975b84d108f1: Image successfully pushed3f12c794407e: Image successfully pushedlatest: digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b size: 2744 现在我们可以查看我们本地/opt/registry目录下已经有了刚推送上来的hello-world。我们也在浏览器中输入http://127.0.0.1:5000/v2/_catalog，如下图所示， 从镜像库中拉取镜像 现在我们可以先将我们本地的127.0.0.1:5000/hello-world和hello-world先删除掉， 123$ sudo docker rmi hello-world$ sudo docker rmi 127.0.0.1:5000/hello-world 然后使用docker pull从我们的私有仓库中获取hello-world镜像， 1$ sudo docker pull 127.0.0.1:5000/hello-world 123456789101112131415161718192021Using default tag: latestlatest: Pulling from hello-worldb901d36b6f2f: Pull complete0a6ba66e537a: Pull completeDigest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4bStatus: Downloaded newer image for 127.0.0.1:5000/hello-world:latestlienhua34@lienhua34-Compaq-Presario-CQ35-Notebook-PC ~ $ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEregistry 2.1.1 b91f745cd233 5 days ago 220.1 MBubuntu 14.04 a5a467fddcb8 6 days ago 187.9 MB127.0.0.1:5000/hello-world latest 0a6ba66e537a 2 weeks ago 960 B 4.查询镜像库查询镜像库中的镜像 1http://10.0.110.218:5000/v2/_catalog 5.错误排查错误描述 在push 到docker registry时，可能会报错： 123The push refers to a repository [192.168.1.100:5000/registry]Get https://192.168.1.100:5000/v1/_ping: http: server gave HTTP response to HTTPS client 这个问题可能是由于客户端采用https，docker registry未采用https服务所致。一种处理方式是把客户对地址“192.168.1.100:5000”请求改为http。 目前很多文章都是通过修改docker的配置文件“etc/systemconfig/docker”，重启docker来解决这个问题。但发现docker1.12.3版本并无此文件，根据网上创建此文件，并填入相应内容，重启docker无效果，仍然报此错误。 解决办法 在”/etc/docker/“目录下，创建”daemon.json“文件。在文件中写入： { “insecure-registries”:[“192.168.1.100:5000”] } 保存退出后，重启docker。]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka的介绍]]></title>
    <url>%2F2018%2F09%2F17%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2FKafka%E7%9A%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Kafka介绍Kafka概念Kafka是一个开源的，分布式的，高吞吐量的消息系统。随着Kafka的版本迭代，日趋成熟。大家对它的使用也逐步从日志系统衍生到其他关键业务领域。特别是其超高吞吐量的特性，在互联网领域，使用越来越广泛，生态系统也越来的完善。同时，其设计思路也是其他消息中间件重要的设计参考。Kafka原先的开发初衷是构建一个处理海量日志的框架，基于高吞吐量为第一原则，所以它对消息的可靠性以及消息的持久化机制考虑的并不是特别的完善。0.8版本后，陆续加入了一些复制、应答和故障转移等相关机制以后，才可以让我们在其他关键性业务中使用。Kafka的运行架构图：kafka各部分组件介绍Topic：主题，或者说是一类消息。类似于RabbitMQ中的queue。可以理解为一个队列。Broker：一个Kafka服务称之为Broker。Kafka可以集群部署，每一个Kafka部署就是一个Broker。Producer &amp; Consumer：生产者和消费者。一般消息系统都有生产者和消费者的概念。生产者产生消息，即把消息放入Topic中，而消费者则从Topic中获取消息处理。一个生产者可以向多个Topic发送消息，一个消费者也可以同时从几个Topic接收消息。同样的，一个Topic也可以被多个消费者来接收消息。Partition：分区，或者说分组。分组是Kafka提升吞吐量的一个关键设计。这样可以让消费者多线程并行接收消息。创建Topic时可指定Parition数量。一个Topic可以分为多个Partition，也可以只有一个Partition。每一个Partition是一个有序的，不可变的消息序列。每一个消息在各自的Partition中有唯一的ID。这些ID是有序的。称之为offset，offset在不同的Partition中是可以重复的，但是在一个Partition中是不可能重复的。越大的offset的消息是最新的。Kafka只保证在每个Partition中的消息是有序的，就会带来一个问题，即如果一个Consumer在不同的Partition中获取消息，那么消息的顺序也许是和Producer发送到Kafka中的消息的顺序是不一致的。这个在后续会讨论。如果是多Partition，生产者在把消息放到Topic中时，可以决定放到哪一个Patition。这个可以使用简单的轮训方法，也可以使用一些Hash算法。一个Topic的多个Partition可以分布式部署在不同的Server上，一个Kafka集群。配置项为：num.partitions，默认是1。每一个Partition也可以在Broker上复制多分，用来做容错。详细信息见下面创建Topic一节。Consumer Group：顾名思义，定义了一组消费者。一般来说消息中间件都有两种模式：队列模式和发布订阅模式。队列模式及每一个消息都会给其中一个消费者，而发布订阅模式则是每个消息都广播给所有的消费者。Kafka就是使用了Consumer Group来实现了这两种模式。如果所有的消费者都是同一个Consumer Group的话，就是队列模式，每个消息都会负载均衡的分配到所有的消费者。如果所有的消息者都在不同的Consumer Group的话，就是发布订阅模式，每个消费者都会得到这个消息。下图是一个Topic，配置了4个Patition，分布在2个Broker上。由于有2个Consumer Group，Group A和Group B都可以得到P0-P3的所有消息，是一个订阅发布模式。两个Group中的Consumer则负载均衡的接收了这个Topic的消息。如果Group中的Consumer的总线程数量超过了Partition的数量，则会出现空闲状态。Zookeeper：Kafka的运行依赖于Zookeeper。Topic、Consumer、Patition、Broker等注册信息都存储在ZooKeeper中。消息的持久化Kafka持久化的方法Kafka可以通过配置时间和大小来持久化所有的消息，不管是否被消费（消费者收掉）。举例来说，如果消息保留被配置为1天，那么，消息就会在磁盘保留一天的时间，也就是说，一天以内，任意消费这个消息。一天以后，这个消息就会被删除。保留多少时间就取决于业务和磁盘的大小。Kafka配置持久化的方式主要有两种方式：时间和大小。在Broker中的配置参数为：log.retention.bytes：最多保留的文件字节大小。默认-1。log.retention.hours：最多保留的时间，小时。优先级最低。默认168。log.retention.minutes：最多保留的时间，分钟。如果为空，则看log.retention.hours。默认null。log.retention.ms：最多保留的时间，毫秒。如果为空，则看log.retention.minutes。默认null。创建Topic：创建topic： 这个命令就是创建一个topic：haoxy1，只有1个partition，并且这个分区会部署在一个brokerbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic haoxy1partitions：这个topic的partition的数量。replication-factor：每个partition的副本个数。任意将每一个分区复制到n个broker上。bin/kafka-topics.sh --describ --zookeeper localhost:2181 --topic haoxy1 具体部署到哪个broker可以通过如下命令查看展示如下： 第一行的摘要信息。第二行开始是详细信息，所以是缩进的格式，如果这个topic有10个Partition，那么就有10行。Leader：每一个分区都有一个broker为Leader，它负责该分区内的所有读写操作，其他Leader被动的复制Leader broker。如果Leader broker 挂了，那么其他broker中的一个将自动成为该分区的新Leader。本例子只有1个复制，Leader的Partition在Broker1上面。Replicas：副本在Broker1上面。Isr：当前有效的副本在Broker1上面。再来创建一个多副本的Topic：bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 5 --topic haoxy2如图：因为我有3个Broker：0,1,2。每一个Partition都有2个Replicas。分别在2个Broker上。]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下sendmail邮件系统安装操作]]></title>
    <url>%2F2018%2F06%2F27%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2Fsendmail%2Flinux%E4%B8%8Bsendmail%E9%82%AE%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[linux下sendmail邮件系统安装操作记录 电子邮件系统的组成：1）邮件用户代理（Mail User Agent ， MUA），MUA是一个邮件系统的客户端程序,它提供了阅读，发送和接受电子邮件的用户接口。 最常用的 MUA 有： linux 中的 mail ， elm ， pine 等。 Windows 的 outlook ， foxmail 等2）邮件代理器（ Mail Transfer Agent ， MTA ） MTA 负责邮件的存储和转发（ store and forward ）。 MTA 监视用户代理的请求，根据电子邮件的目标地址找出对应的邮件服务器，将信件在服务器之间传输并且将接受到的邮件进行缓冲。 在 linux 下的 MTA 程序有： sendmail ， qmail 等，3）邮件提交代理（ Mail Submmission Agent ， MSA ） MSA 负责消息有 MTA 发送之前必须完成的所有准备工作和错误检测， MSA 就像在 MUA 和 MTA 之间插入了一个头脑清醒的检测员对所有的主机名，从 MUA 得到的信息头等信息进行检测。4）邮件投递代理（ Mail Ddlivery Agent ， MDA ） MDA 从 MTA 接收邮件并进行适当的本地投递，可以投递个一个本地用户，一个邮件列表，一个文件或是一个程序。 Linux 下常用的 MDA 是 mail.local ， smrsh 和 procmail （ www.procmail.org ）5）邮件访问代理（ Mail Access Agent ， MAA ） MAA 用于将用户连接到系统邮件库，使用 POP 或 IMAP 协议收取邮件。 Linux 下常用的 MAA 有 UW-IMAP ， Cyrus-IMAP ， COURIER-IMAP 等 邮件中继: 就是当邮件向目的地址传输时，一旦源地址和目的地址都不是本地系统，那么本地系统就是邮件的中继（中转站）。MUA 使用者透过这个程序与邮件服务器沟通，包括收信（以 POP3 连接收信服务器程序 imapd）或寄信（以SMTP 连接 MTA），例如： Outlook Express……等。MTA 使用 SMTP 通讯协议将信件传递到不同邮件主机上面，例如： sendmail, postfix, Qmail……等。MSA 是新版 sendmail 发展给 SSMTP 进行 TLS/SSL 联机的 client 端代理器。MDA 收到信后将信件分配到不同使用者信箱内，算是 MTA 的一个子系统，譬如 BBS 从定义上来说也算是 MDA（ BBS 功能复杂，当然不仅仅是 MDA），有些 MDA 被设计来进行滤信动作，它们必须在 local 端运作，因此又被称为 LDA，例如： procmail……等。MailBox 尚未被使用者下载的邮件，会暂存在服务器的硬盘空间里，称之为信箱。所有使用者信箱的总合必须约等于该分割区总容量的一半，以避免造成信箱尚有空间但邮件系统却无法运作的现象。Mail Gateway 是一种特殊的邮件服务器，通常扮演代理器的角色，负责统筹某机构内所有信件的收发，并分配邮件给下属的邮件服务器们，透过这个机制能够加速邮件的交换，并且能够进行一致的滤信控制。sendmail是linux系统中一个邮箱系统，如果我们在系统中配置好sendmail就可以直接使用它来发送邮箱。sendmail的配置文件/etc/mail/sendmail.cf :Sendmail的主配置文件；/etc/mail/access :中继访问控制；/etc/mail/domaintable ;域名映射；/etc/mail/local-host-names ;本地主机别名；/etc/mail/mailertable :为特定的域指定特殊的路由规则；/etc/mail/virtusertable :虚拟域配置。中继的配置是指一台服务器接受并传递源地址和目的地址都不是本服务器的邮件。在两个文件中进行设置：/etc/mail/relay-domains/etc/mail/access。废话不多说了，下面分享下sendmail在linux系统下的安装部署记录：一、安装软件[root@slave-node ~]# yum install -y sendmail[root@slave-node ~]# yum install -y sendmail-cf启动saslauthd服务进行SMTP验证（默认是安装的，如果没有，就手动安装）[root@slave-node ~]# service saslauthd startStarting saslauthd: [ OK ]二、邮件服务配置（iptables防火墙关闭）1）配置Senmail的SMTP认证将下面两行内容前面的dnl去掉。在sendmail文件中，dnl表示该行为注释行，是无效的，因此通过去除行首的dnl字符串可以开启相应的设置行。[root@slave-node ~]# vim /etc/mail/sendmail.mc......TRUST_AUTH_MECH(`EXTERNAL DIGEST-MD5 CRAM-MD5 LOGIN PLAIN')dnldefine(`confAUTH_MECHANISMS', `EXTERNAL GSSAPI DIGEST-MD5 CRAM-MD5 LOGIN PLAIN')dnl2) 设置Sendmail服务的网络访问权限（如果是直接本机调用，可以不用操作，采用默认的127.0.0.1。不过最后还是改成0.0.0.0）将127.0.0.1改为0.0.0.0，意思是任何主机都可以访问Sendmail服务。如果仅让某一个网段能够访问到Sendmail服务，将127.0.0.1改为形如192.168.1.0/24的一个特定网段地址。[root@slave-node ~]# vim /etc/mail/sendmail.mc......DAEMON_OPTIONS(`Port=smtp,Addr=0.0.0.0, Name=MTA')dnl3）生成配置文件Sendmail的配置文件由m4来生成，m4工具在sendmail-cf包中。如果系统无法识别m4命令，说明sendmail-cf软件包没有安装[root@slave-node ~]# m4 /etc/mail/sendmail.mc &gt; /etc/mail/sendmail.cf4）修改主机域名(只有加到配置中才能访问smtp服务器)vi /etc/hosts 172.168.110.218 m110p218 m110p218.com 在文件最后一行加上 主机ip 主机别名(通过hostname查看) 主机域名修改 /etc/mail/access文件，追加加入以下语句Connect:202.222.222.222 RELAY # 允许202.222.222.222通过sendmail收发邮件5）启动服务（如果发现sendmail dead but subsys locked，那就执行&quot;service postfix status&quot;查看postfix是否默认开启了，如果开启的话，就关闭postfix，然后再启动或重启sendmail服务即可。）[root@slave-node ~]# service sendmail startStarting sendmail: [ OK ]Starting sm-client: [ OK ][root@slave-node ~]# service saslauthd restartStopping saslauthd: [ OK ]Starting saslauthd: [ OK ]将服务加入自启行列[root@slave-node ~]# systemctl enable sendmail.service[root@slave-node ~]# systemctl enable saslauthd.service三、测试发送邮箱（1）第一种方式：安装sendmail即可使用。[root@slave-node ~]# yum -y install mailx注意: 如果在其他主机进行测试需要修改对应主机配置文件 /etc/mail.rc，在某位追加set from=&quot;xxx@163.com&quot; set smtp=smtp.163.com # 如果smtp不在本机, 该条配置一定要设置set smtp-auth-user=xxx set smtp-auth-password=邮箱密码 set smtp-auth=login创建一个邮件内容文件，然后发邮件（注意-s参数后的邮件标题要用单引号，不能使用双引号，否则发邮件会失败！）[root@slave-node ~]# echo 'This is test mail'&gt;/root/content.txt[root@slave-node ~]# cat /root/content.txtThis is test mail[root@slave-node ~]# mail -s 'Test mail' wang_shibo***@163.com &lt; /root/content.txt查看已收到邮件： 如果不想通过文件发送邮件内容也可以这么发送，也可以使用管道符直接发送邮件内容，效果同文件发送邮件内容一样[root@slave-node ~]# echo &quot;This is test mail&quot; | mail -s '666666' wang_shibo***@163.com查看已收到邮件：如果是发送给多个邮件，就使用-c参数，如下：[root@slave-node ~]# echo &quot;This is test mail&quot; | mail -s 'test' -c wang_shibo***@sina.com wang_shibo***@163.com如遇下面报错，解决办法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849发送邮件：[root@mail-server ~]# echo &quot;This is test mail&quot; | mail -s '666666' wangshibo@kevin.com 发现收不到邮件，查看sendmail日志，报错信息如下：[root@mail-server ~]# tail -f /var/log/maillog.......Feb 12 03:35:13 mail-server sendmail[21905]: My unqualified host name (mail-server) unknown; sleeping for retryFeb 12 03:37:12 mail-server sendmail[22061]: w1BJb8KM022059: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server&gt; (0/0), delay=00:00:03, xdelay=00:00:03,mailer=esmtp, pri=120476, relay=mx1.kevin.com. [128.1.41.15], dsn=4.0.0, stat=Deferred: 450 Requested mail action not taken: Invalid sender 分析原因：这是由于主机名没有正确解析导致的。[root@mail-server ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.10.202 mail-server [root@mail-server ~]# cat /etc/sysconfig/networkNETWORKING=yesHOSTNAME=mail-server 解决办法：[root@mail-server ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.10.202 mail-server.localdomain mail-server[root@mail-server ~]# cat /etc/sysconfig/networkNETWORKING=yesHOSTNAME=mail-server [root@mail-server ~]# rm -rf /var/spool/mqueue/*[root@mail-server ~]# /etc/init.d/sendmail restartShutting down sm-client: [ OK ]Shutting down sendmail: [ OK ]Starting sendmail: [ OK ]Starting sm-client: [ OK ] 再次使用mail发送邮件就正确了！[root@mail-server ~]# echo &quot;This is test mail&quot; | mail -s '666666' wangshibo@kevin.com[root@mail-server ~]# tail -f /var/log/maillog.......Feb 12 03:42:31 mail-server sendmail[22293]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:31 mail-server sendmail[22299]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:31 mail-server sendmail[22302]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:33 mail-server sendmail[22284]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:34 mail-server sendmail[22293]: w1BJgTcF022288: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:05, xdelay=00:00:05, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [116.115.114.9], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwB3EaEnDYFaHrpiAA--.12694S3)Feb 12 03:42:35 mail-server sendmail[22302]: w1BJgUPI022300: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:05, xdelay=00:00:05, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [115.123.124.105], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwDXQD0oDYFaoMJxAA--.21712S3)Feb 12 03:42:36 mail-server sendmail[22299]: w1BJgToO022294: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:06, xdelay=00:00:06, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [115.123.124.105], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwB3HTonDYFancJxAA--.21596S3)Feb 12 03:42:40 mail-server sendmail[22284]: w1BJgSAl022282: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:12, xdelay=00:00:12, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [139.162.158.182], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwDHp2QmDYFayl55AA--.6056S3)（2）第二种方式：利用外部的smpt服务器上面第一种方式中，/bin/mail命令会默认使用本地sendmail发送邮件，这样要求本地的机器必须安装和启动Sendmail服务，配置非常麻烦，而且会带来不必要的资源占用。而通过修改配置文件可以使用外部SMTP服务器，可以达到不使用sendmail而用外部的smtp服务器发送邮件的目的。修改/etc/mail.rc文件（有的版本叫/etc/nail.rc，添加下面内容：set from=fromUser@domain.com smtp=smtp.domain.comset smtp-auth-user=username smtp-auth-password=passwordset smtp-auth=login参数说明：from是发送的邮件地址smtp是发生的外部smtp服务器的地址smtp-auth-user是外部smtp服务器认证的用户名。注意一定要填写邮件全称！！smtp-auth-password是外部smtp服务器认证的用户密码smtp-auth是邮件认证的方式配置完成后，就可以正常发送邮件了，如下[root@slave-node ~]# vim /etc/mail.rc //在文件底部添加set from=ops@huanqiu.cn smtp=smtp.huanqiu.cn smtp-auth-user=ops@huanqiu.cn smtp-auth-password=zh@123bj smtp-auth=login现在开始发邮件：[root@slave-node ~]# echo &quot;hello world&quot; |mail -s 'test666' wangshibo@huanqiu.cn]]></content>
      <categories>
        <category>开发工具</category>
        <category>sendmail</category>
      </categories>
      <tags>
        <tag>sendmail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker编排与集群部署]]></title>
    <url>%2F2018%2F06%2F22%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E7%BC%96%E6%8E%92%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Fig/Compose编排工具Fig/Compose的介绍在生产环境中，整个团队需要发布的容器数量很可能极其庞大，而容器之间的联系和拓扑结构也可能非常复杂，往往太难升就是集群化的，并且具备高可用设计。如果依赖人工记录和配置这样的复杂容器关系，并保障集群正常运行、监控、迁移、高可用等常规运维需求，难免力不从心因此Fig/Compose就是解决这一问题的。Dockerfile可以重现一个容器，Compose可以重现容器的配置和集群编排和部署编排它根据被部署的对象之间的耦合关系，以及被部署对象对环境的依赖，制定部署流程中各个动作的执行顺序，部署过程所需要的依赖文件和被部署的文件的存储位置和获取方式，以及如何验证部署成功。这些信息都会在编排工具中以制定的格式来要求运维人员定义并保存起来，从而保证这个流程能够随时在全新的环境中可靠有序的重现出来部署是指按照编排制定的内容和流程，在目标机器上执行编排制定的环境初始化，存放制定的依赖和文件，运行制定的部署动作，最终按照编排中的规则来确认部署成功在Compse的世界里，编排和部署的组合结果，就是一朵&quot;容器云&quot;编排工具-Compose的使用Compose安装yum -y install epel-release &amp;&amp; yum -y install python-pip &amp;&amp; pip install docker-composeCompose的使用Compose常用命令docker-compose up 启动服务(当前目录下需要有.yml文件)-d: 后台启动，默认是前台启动-f: 指定详细的docker-compose.yml文件, 默认为当前目录下的docker-compose.yml-p: 指定项目名称启动-H: 指定链接到哪一个Hoser daemon--no-recreate，不会新创建容器，如果有已经存在的容器，那么会直接启动docker-compose start 启动已经存在的容器作为一个服务docker-compose stop 停止运行的容器而不删除它们docker-compose restart 重启已经停止的容器.yml文件的配置选项build指定Dockerfile路径和上下文路径，Compose 将会利用它自动构建这个镜像，然后使用这个镜像启动服务容器args为构建(build)过程中的环境变量，用于替换Dockerfile里定义的ARG参数，容器中不可用。image指定使用的镜像，本地不存在会从HUB拉取，当和build同时使用，会把生成的镜像标记为image定义的镜像名称command容器启动后执行的命令，会覆盖默认的启动命令ports用于暴露端口给主机, 当不指定主机映射端口时，不能供外部访问(host_ip:container_ip)expose提供container之间端口访问, 不会暴露给主机使用.同docker run --exposevolumes挂在目录environment添加环境变量，同docker run -e depends_on指定依赖，将会优先于该服务创建并启动依赖extra_hosts添加主机名映射network_mode定义网络模式 同docker --net参数privileged是否使用privileged模式env_file指定变量的文件，默认为docker-compose文件夹下的 .env 文件.同一个变量，通过export设置，会覆盖env_file中的变量.env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.envcat .env MYSQL_DATA_DIR=/my/sql/data-dirtty是否给容器分配终端当想要让容器后台运行的时候，必须带该配置，不然不会分配终端，容器执完命令会自动退出编排.yml文件使用示例networks关键字:compose file中顶级networks关键字，可以用来创建更加复杂的网络拓扑，指定自定义网络驱动和选项，也可以用它来连接外部创建（非Compose创建）的网络。每个Service可通过service内部的networks关键字来指定它要使用的顶级网络。给Service指定networks的好处是，实现网络隔离或连接每个网络中的启动的容器，可以实现相互的网络通信，容器直接可以使用容器名进行通信使用Docker-compose进行网络通信第一种方法：将service部署到同一个网络环境中，即可实现容器的互相通信，使用container_name即可示例：第二种方法:使用自定义局域网络，容器之间使用静态ip进行通信示例：第三种方法：先给每个服务指定容器的名称再在每个服务中给容器添加extra_host对应容器的映射关系Compose的缺点docker-compose是面向单宿主机部署的，这是一种部署能力的欠缺，管理员需要借助成熟的自动化运维工具来管理多个目标机，将docker-compose所需要的所有资源包括配置文、用户代码交给所有的目标机，再在目标机上运行docker-compose指令docker-compose还不能提供跨宿主机的网络和存储。这意味着管理员必须部署一套类似于Open vSwich的堵路网络工具，而且管理员还需要完成继承工作。Docker官方给出了一套用Compse、Machine、Swarm联动的解决方案集群抽象工具SwarmSwarm的介绍Swarm的目的是将多个宿主机抽象为&quot;一台&quot;宿主机来对待Swarm在多台Docker宿主机(Docker服务端)上建立了一层抽象，可以通过操作Swarm服务端实现对宿主机的资源分配和管理Swarm通过在Docker宿主机上添加标签信息来讲宿主机资源进行细粒度分区，通过分区来帮助用户将容器部署到目标宿主机上利用Swarm可以实现控制一台Swarm Manager来控制整个Docker集群。Swarm集群的Agent\ManagerAgent: Agent节点运行Docker服务端，Docker Release的版本需要保证一怔，且为1.4.0或更新的版本。Manager：Manager节点负责与所有的Agent上的Docker宿主机通信以及对外提供Docker远程API服务，因此Manager负责能获取到所有的Agent地址。当通过Docker客户端与Manager通信，执行docker run命令时，Manager会选择一个Agent来执行该命令，并执行结果返回给Docker客户端创建一个Swarm集群搭建Swarm集群1.拉取swarm镜像docker pull swarm2.把准备加入集群的所有的节点的docker deamon的监听端口修改为0.0.0.0:2375vi /etc/default/dockerDOCKER_OPTS=&quot;-H 0.0.0.0:2375 -H unix:///var/run/docker.sock&quot; 在文件结尾添加service docker restart 重启docker服务3.在manage节点新建一个文件,把要加入到集群的机器的IP地址都写进去4.执行swarm manage命令(在manage节点上)docker run –d –p 2376:2375 –v $(pwd)/cluster:/tmp/cluster swarm manage file:///tmp/cluster这里一定要使用-v命令，因为cluster文件是在本机上面，启动的容器默认是访问不到的，所以要通过-v命令共享5.让集群运行某些命令随机指定节点来运行docker -H &lt;manage_host:manage_port&gt; infodocker -H &lt;manage_host:manage_port&gt; run -it ubuntu /bin/bash通过lable来指定某个节点来运行命令docker -d --lable storage 给节点加上storage标签sudo docker run –H 10.13.181.83:2376 run –name redis_083 –d –e constraint:storage redis 通过指定标签来选择节点启动6.Docker客户端与Manager通信docker -H &lt;manage_host:manage_port&gt; run -it ubuntu /bin/bash 7.删除一个节点docker node rm --force node1 Swarm的调度测略（可以通过strategy来选出最终运行容器的宿主机）Random :顾名思义，就是随机选择一个Node来运行容器，一般用作调试用，spread和binpack策略会根据各个节点的可用的CPU,RAM以及正在运行的容器的数量来计算应该运行容器的节点。Spread :会选择运行容器最少的那台节点来运行新的容器,使用Spread策略会使得容器会均衡的分布在集群中的各个节点上运行，一旦一个节点挂掉了只会损失少部分的容器。binpack: 会选择运行容器最集中的那台机器来运行新的节点,binpack策略最大化的避免容器碎片化，就是说binpack策略尽可能的把还未使用的节点留给需要更大空间的容器运行，尽可能的把容器运行在一个节点上面。跨平台宿主环境管理工具-Machine Machine的介绍搭建环境想来是一个重复造轮子的过程，Machine这一工具把用户搭建Docker环境的跟中方案汇集在一起，即一目了然的简化了Docker环境的创建过程，让用户继续讲时间投入到应用开发商，而不是无谓的花费在环境的搭建上Machine的主要功能就是帮助用户在不同的云主机提供商上创建和管理虚拟机，并在虚拟机中安装Docker用户只需要提供几项登录凭证即可等待环境安装完成。Machine能便捷地管理所有通过它创建的Docker宿主机，进行宿主机的启动、关闭、重启、删除等操作使用Machine可以在最开始快速创建多台主机系统，并搭建Docker部署环境，接下俩就可以使用swarm创建集群等操作Machine与虚拟机软件Machine包含的虚拟机驱动有VirtualBox、VMware FUsion、Hyper-V使用Machine前，机器上需要有上述的三种软件之一，需要在命令中指定使用哪一种当不需要创建虚拟镜像充当主机的时候，我们也可以直接指定主机的ip方式来通过Docker Machine来控制多个真正的主机Docker Machine的使用安装Docker Machinecurl -L https://github.com/docker/machine/releases/download/v0.12.0/docker-machine-`uname -s`-`uname -m` &gt; /tmp/docker-machinechmod +x /tmp/docker-machinesudo mv /tmp/docker-machine /usr/local/bin/docker-machine使用Docker Machine的前提条件sudo adduser nick sudo usermod -a -G sudo nick 在目标主机上创建一个用户并加入sudo 组sudo vi sudoer nick ALL=(ALL:ALL) NOPASSWD: ALL 为该用户设置 sudo 操作不需要输入密码ssh-copy-id -i ~/.ssh/id_rsa.pub nick@xxx.xxx.xxx.xxx 把本地用户的 ssh public key 添加到目标主机上第一种安装Docker方法：在目标主机上安装Docker命令 docker-machine create -d generic \ --generic-ip-address=xxx.xxx.xxx.xxx \ --generic-ssh-user=nick \ --generic-ssh-key ~/.ssh/id_rsa \ krdevdb备注：create 命令本是要创建虚拟主机并安装 Docker，因为本例中的目标主机已经存在，所以仅安装 Docker。解释-d 是 --driver 的简写形式，主要用来指定使用什么驱动程序来创建目标主机。Docker Machine 支持在云服务器上创建主机，就是靠使用不同的驱动来实现了。本例中使用 generic 就可以了--generic 开头的三个参数主要是指定操作的目标主机和使用的账户。最后一个参数 krdevdb 是虚拟机的名称第二种安装Docker方法：在本地主机上安装Docker虚拟机命令 docker-machine create \ --driver vmwarevsphere \ --vmwarevsphere-vcenter=xxx.xxx.xxx.xxx \ --vmwarevsphere-username=root \ --vmwarevsphere-password=12345678 \ --vmwarevsphere-cpu-count=1 \ --vmwarevsphere-memory-size=512 \ --vmwarevsphere-disk-size=10240 \ testvm备注：在实际使用中我们一般会在物理机上安装 vSphere 等虚拟机管理软件，并称之为虚拟机 host。然后通过 vSphere 工具安装虚拟机进行使用。接下来我们将介绍如何在本地的一台安装了 vSphere 的虚拟机 host 上安装带有 Docker 的虚拟机参数解释： --driver vmwarevsphere 需要给 Docker Machine 提供对应的驱动，这样才能够在上面安装新的虚拟机。 --vmwarevsphere-vcenter=xxx.xxx.xxx.xxx \ --vmwarevsphere-username=root \ --vmwarevsphere-password=12345678 \ 上面三行分别指定了虚拟机 host 的 IP 地址、用户名和密码。 --vmwarevsphere-cpu-count=1 \ --vmwarevsphere-memory-size=512 \ --vmwarevsphere-disk-size=10240 \ 上面三行则分别指定了新创建的虚拟机占用的 cpu、内存和磁盘资源。 testvm 最后一个参数则是新建虚拟机的名称。查看docker machine 管理的主机列表docker-machine ls 获取构造docker主机时使用的命令docker-machine config dev获取连接到某个主机需要的环境变量docker-machine env dev eval &quot;$(docker-machine env dev)&quot; 会指定接下来和哪台主机的docker daemon进行通信启动\停止\重启虚拟机docker-machine start/stop/restart dev SSH到宿主机上执行命令docker-machine ssh dev /start.sh 获取主机的urldocker-machine url 主机之间传递文件docker-machine scp]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像管理&Dockerfile]]></title>
    <url>%2F2018%2F06%2F20%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86%26Dockerfile%2F</url>
    <content type="text"><![CDATA[Docker镜像Docker镜像介绍Docker镜像时一个只读的Docker容器模板，含有启动Docker容器所需的文件系统结构及其内容，因此是启动一个Docker容器的基础。Docker镜像的文件内容以及一些运行Docker容器的配置文件组成了Docker容器的静态文件系统运行环境-rootfs。可以理解为Docker容器的静态视角，Docker容器是Docker镜像的运行状态rootfs介绍rootfs是Docker容器在启动时内部进程可见的文件系统，即Docker容器的根目录。rootfs通常包含一个操作系统运行所需的文件系统，例如可能包含典型的类Unix操作系统中的目录系统，/dev 、/proc 等,以及运行Docker容器所需的配置文件、工具等rootfs原理在传统的Linux操作系统内核启动时，首先挂在一个只读的rootfs，当系统检测其完整性之后，再将其切换为读写模式在Docker架构中，当Docker daemon 为容器挂在rootfs时，沿用了Linux内核启动时的方法，将rootfs设计为只读模式。在挂在完毕之后，再利用连个挂在技术在只读rootfs上再挂在一个读写层。这样可读写层处于Docker容器文件系统的最顶层，其下可能联合挂在多个只读层，只有在Docker容器运行过程中文件系统发生变化时，才会吧变化的文件内容写到可读写层，并隐藏只读层中的老版本文件Docker镜像的特点分层每个镜像都是由一系列的镜像层组成写时复制在多个容器之间共享镜像，每个容器在启动的时候并不需要单独复制一份镜像文件内容寻址根据文件内容来索引镜像和镜像层联合挂载可以在一个挂载点同时挂在多个文件系统，将挂载点的原目录与被挂在内容进行整合，使得最终课件的文件系统将会包含整合之后的各层的文件和目录Docker镜像的构建使用commit创建镜像docker commit命令只是提交容器镜像发生变更了的部分，即修改后的容器镜像与当前仓库中对应镜像之间的差异部分，这使得该操作实际需要提交的文件往往并不多使用commit实现步骤1.根据输入的pause参数的设置确定是否暂停该Docker容器的运行2.将容器的可读写层导出打包，该读写层代表了当前运行容器的文件系统与当初启动该容器镜像之间的差异3.在层存储中逐层可读写层差异包4.更新镜像历史信息rootfs，并据此在镜像存储中创建一个新的镜像，记录其原数据5.如果指定了repository信息，则给上述镜像添加tag信息使用build构建镜像--client端命令格式docker build [OPTIONS] PATH | URL | -path | URL : 其所指向的文件称为上下文context，context包含build Docker镜像过程中所需要的Dockerfile以及其他的资源文件。-t: 指定镜像的名字-f: 指定Dockerfile完整路径 和context的完整路径(可以是相对路径也可以是绝对路径) 不同参数，Docker client处理的方法1. 第一个参数为 &quot;-&quot;sudo docker build - Dockerfilesudo docker build - context.tar.gz该情况，根据命令行输入参数对Dockerfile和context进行设置2.第一个参数为URL，且是git repository URLsudo docker build github.com/creack/docker-firefox该情况，则调用git clone --depth 1 --recursive 命令克隆该Github repository，该操作会在本地的一个临时目录中进行命令你成功之后该目录会作为context传给Docker daemon，该目录中的Dockerfile会被用来进行后续构建Docker镜像3.第一个参数为URL，不是git repository URL该情况，会从该URL下载context，将其封装为一个io流，后面的处理与情况1相同4.context为本地文件或者目录sudo docker build -t vieux/apache:2.0 使用当前文件夹作为contextcd /home/myapp/some/dir/really/deep &amp;&amp;sudo docker build -f /home/memyapp/dockerfiles/debug /home/me/myapp 使用指定的两目录分别作为Dockerfile 和context备注：如果目录中有.dockerignore文件，则将context中文件名满足其定义的规则的文件都从上传到列表中排出，不打包给Docker daemon。但唯一的例外是.dockerignore文件中若误写入了.dockerignore本身或者Dockerfile，将不产生作用。如果用户定义了tag，则对其指定的repository和tag进行验证完成了想关信息的设置之后，Docker client想Docker server发送buildHTTP请求，包含了所需的context信息使用build构建镜像--Docker server端Docker server接受到HTTP请求后，其工作内容1.创建临时目录，将context指定的文件系统解压到该目录下2.读取并解析Dockerfile3.根据解析出的Dockerfile遍历其中的所有指令，并分发到不同的模块去执行。Dockerfile特定的关键词都会映射到不同parser进行处理4.parser为上述每一个指令创建 一个对应的临时容器，在临时容器中执行当前指令，然后通过commit使用此容器生成一个镜像层5.Dockerfile中所有的指令对应的层的集合，就是此次build后的结果DockerfileDockerfile介绍Dockerfile是在通过docker build命令构建自己的Docker镜像时需要使用到的定义文件。它允许用户使用基本的DSL语法来定义Docker镜像，每一条指令描述了构建镜像的步骤。用户可以使用这些统一的语法命令来根据需求进行配置，通过这份统一的配置文件，在不同的平台上进行分发，需要使用时就可以根据配置文件自动化构建，解决了开发人员构建镜像的复杂过程Dockerfile与镜像配合使用，使Docker在构建时可以充分利用镜像功能进行缓存，大大提升了Docker的使用效率Dockerfile生成镜像的过程本地主机的一个包含Dockerfile的目录中的所有内容作为上下文上下文通过docker build命令传入到Docker daemon后，便开始按照Dockerfile中的内容构造镜像按照Dockerfile中的指令进行读条指令的构建镜像，除了from指令，其他每一条指令都会在上一条指令所生成的镜像的基础上执行，执行完成后生辰一个新的镜像层镜像层不断的覆盖原来的镜像形成新的镜像。Dockerfile所生成的最终镜像就是在基础镜像上叠加一层层的镜像层组建的Docker指令介绍基本格式 INSTRUCTION arguments命令格式在Dockerfile中，指令INSTRUCTION不区分大小写，但是为了参数区分，推荐大写。Docker会顺序执行Dockerfile中的指令，第一条指令必须为FROM指令，它用于指定构建镜像的基础镜像ENV 指令格式ENV &lt;KEY&gt; =&lt;value&gt; 说明ENV指令可以为镜像创建出来的容器生命环境变量FROM指令格式：FROM &lt;iamge&gt;[:&lt;tag&gt;]说明FROM指令的功能是为后面的指令提供基础镜像，因为一个有效的Dockerfile必须以FROM指令你做为第一条非注释指令在Dockerfile中，FROM指令可以出现多次，这样会构建多个镜像。在每个镜像创建完成后，Docker命令行界面会输入该镜像的ID。若FROM指令中的参数tag为空，默认tag是latestCOPY指令格式COPY &lt;src&gt; &lt;dest&gt;说明COPY指令复制&lt;src&gt;所指向的文件或者目录，将它添加到新场景中，复制的文件或目录在镜像中的路径是&lt;dest&gt;&lt;src&gt;所指定的源可以有多个，但必须在上下文中，即必须是上下文根目录的相对路径若结尾符为/则表示为目录，否则为文件ADD指令格式ADD &lt;src&gt; &lt;dest&gt;说明ADD与COPY指令在功能上很相似，都支持复制本地文件到镜像的功能，但ADD指令还支持其他的功能&lt;src&gt;可以是一个指向一个网络文件的URL ，若&lt;dest&gt;为目录，则URL必须是完全路径，这样可以获得该网路文件的文件名filename，该文件会被复制添加到&lt;dest&gt;/&lt;file_name&gt;，若URL中的文件为压缩文件，默认不会被解压提取&lt;src&gt;还可以指向一个本地压缩归档文件，该文件再复制到容器中时会被解压提取RUN指令格式 RUN &lt;command&gt; (shell 格式)RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式，推荐使用)说明RUN指令会在前一条命令创建出镜像的基础上创建一个容器，并在容器中运行命令，在命令运行后提交容器为新的镜像，新镜像被Dockerfile中的下一条指令使用通常在需要创建镜像后安装依赖的情况下使用CMD指令格式CMD &lt;command&gt; (shell格式)CMD [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式，推荐格式)CMD [&quot;param1&quot;, &quot;param2&quot;] (为ENTRUPOINT指令提供参数)说明CMD命令提供容器运行时的默认值，这些默认值可以是一条指令，也可以是一些参数。一个Dockerfile中可以有多条CMD指令，但是只有最后一条CMD指令有效。CMD [&quot;param1&quot;, &quot;param2&quot;] 格式只有在CMD指令和ENTRYPOINT指令配合使用，CMD指令中的参数会添加到ENTRYPOINT指令中使用shell和exec格式时，命令在容器中运行的方式和RUN指令相同，不同在于，RUN指令在构建镜像时执行命令，并生成新的镜像，CMD指令在构建镜像的时候不执行任何命令，而是在容器启动的时候默认将CMD指令作为第一条执行命令。如果用户在命令行界面运行docker run命令时指定了命令参数，则会覆盖CMD指令中的命令ENTRYPOINT指令格式ENTRYPOINT &lt;command&gt; (shell格式)ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式，推荐格式)说明ENTRYPOINT指令和CMD指令类似，都可以让容器每次启动时候执行相同的命令一个Dockerfile中可以有多条ENTRYPOINT指令，但只有最后一条ENTRYPOINT指令有效当使用shell格式时，ENTRYPOINT指令会忽略任何CMD指令和docker run命令参数，并运行在/bin/sh -c 中。当使用 exec格式，docker run传入的命令参数会覆盖CMD指令的内容并附加到ENTRYPOINT指令参数中CMD可以是参数，也可以是指令，ENTRYPOINT只能是命令，灵台docker run命令你提供的运行命令参数可以覆盖CMD，但是不能覆盖ENTRYPOINTONBUILD格式ONBUILD [INSTRUCTION]说明添加一个将来执行的触发器指令到镜像中。当该镜像作为FROM指令的参数时，这些触发器指令就会在FROM指令执行的时候加入到构建的过程中尽管任何一个指令都可以被注册成一个触发器钟灵，但是ONBUILD指令中不能包含ONBUILD指令，并且不会触发FROM和MAINTAINER指令ONBUILD经常用于需要在创建镜像的时候，需要对导入的镜像做一些文件的导入，一些批处理的时候LABLE格式LABEL multi.label1=&quot;value1&quot; \multi.label2=&quot;value2&quot; \other=&quot;value3&quot;说明label是累积的，包括FROM镜像的lable。如果Docker遇到一个label/key已经存在，那么新的值将覆盖这个label/key。要查看一个镜像的label，使用docker inspect命令。EXPOSE格式EXPOSE &lt;port1&gt; &lt;port2&gt;说明EXPOSE宿主机需要暴露哪些端口在启动容器时需要通过-P（注意是大写），Docker主机会自动分配一个端口转发到指定的端口；使用-p，则可以具体指定哪个本地端口映射过来。WORKDIR格式WORKDIR &lt;path&gt;说明指定了RUN CMD ENTRYPOINT等命令运行的目录Dockerfile示例使用Dockerfile注意标签易读例如：docker build -t =&quot;ruby:2,0-onbuild&quot;谨慎选择基础镜像尽量选择当前官方镜像库中的镜像充分利用缓存尽量让相同的指令放在前面，充分使用缓存的结果正确使用ADD和COPY命令尽量使用COPY命令CMD和ENTRPOINT合用尽量使用CMD和ENTRYPOINT指令配合，可以使用exec格式的ENNTRYPOINT指令设置固定的默认命令参数，然后使用CMD指令设置可变的参数不要在Dockerfile中做端口映射如果在Dockerfile中做端口映射，导致只能在主机上启动一个容器应该使用EXPOSE 80 只暴露端口，另做映射使用Dockerfile共享Docker镜像]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像管理&Dockerfile]]></title>
    <url>%2F2018%2F06%2F19%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E9%9B%86%E7%BE%A4%26%E5%BA%94%E7%94%A8%E6%A0%88%2F</url>
    <content type="text"><![CDATA[Docker集群&amp;应用栈的介绍Docker集群&amp;应用栈概念Docker的设计理念是希望用户能够保证一个容器只运行一个进程，即只提供一种服务。然而对于用户而言，单一的容器是无法满足需求的。通常用户需要利用多个容器，分别提供不同的服务，并在不同的容器间互相通信，最后形成一个docker集群，以实现特定功能。基于Docker集群构建的应用我们成为Docker应用栈搭建应用栈所需过程对应用栈的描述示例搭建一个包含6个节点的Docker-Web服务应用栈，其中包括一个代理节点、两个Web应用节点、一个主数据库节点及两个从数据库节点图例:获取各节点所需的镜像镜像可以是Docker Hub中已经存在的镜像镜像也可以是自己提交的Docker镜像网络调通应用栈容器节点之间的互联（同主机下实现，如果跨主机还需解决跨主机通信）实现节点互联命令docker run [OPTIONS] --link name:alias --name Name IMAGE [COMMAND] [ARG1, ......] alias: 给接受的容器起一个别名可以应用于快速链接name: 容器名称示例sudo docker run -it --name redis-slave1 --link redis-master:master redis /bin/bash通过--link互联的优点通过--link方式互联，不但可以避免容器的IP端口暴露到外网所导致的安全问题，还可以放置容器在启动后Ip地址变化导致的访问失效可以防止容器重启导致容器IP地址变化访问失效，它的原理类似于DNS服务器的域名和地址映射当容器的IP地址发生变化时，Docker将自动维护映射关系中的IP地址查看连接信息cat /etc/host确定应用栈的启动顺序及暴露外网访问端口确定启动顺序(先启动被连接的节点)：redis-master--&gt;redis-slave--&gt;APP1/2--&gt;HAProxy暴露Ip端口sudo docker run -it --name HProxy --link APP1:APP1 --link APP2:APP2 -p 6301:6301 haproxy /bin/bash网络链接涉及到的参数--iptables: 将容器的端口暴露给外部主机，就是通过iptables做DNAT(目的地址转换)实现的。以及Docker容器间的通信业是通过修改FORWARD链中响应的itables规则的策略。当设为false时Docker daemon将不会改变你的宿主机上的iptable规则，默认是true。--icc: 当容器都链接到了docker0网线上，容器已经属于一个子网，已经满足通信前提，这时容器间是否可以通信取决于FORWARD链中的ACCEPT规则，设置为false，规则会被置位DROP，Docker荣期间的相互通信就被禁止，这种情况，想让容器互联一定要使用--link选项，容器间默认为true。注意修改FORWARD规则一定要设置--iptables为true--ip--forward: 在Docker容器和外界通信过程中，还设计数据包的多网卡间转发，需要内核将ip--forward功能打开，即将ip_forward系统参数设置为1修改Docker daemon的启动参数修改/etc/detault/docker应用栈容器节点间的配置/程序文件编辑查看挂在volume位置(如果镜像事先已经配置了volume)sudo docker inspect --format &quot;&quot; CONTAINERreturn: map[/data:/var/docker/vfs/dir/xxx] /data 为容器中的共享目录， /var/docker/vfs/dir/xxx 为主机的volume目录启动时指定volume共享目录(如果镜像中没有配置共享目录)sudo docker run -it -v ~/projects:/usr/src/app ubuntu ~/projects: 为主机的volume目录/usr/src/app: 为容器中的volume目录应用共享volume目录进行配置文件的编辑1.在volume中编辑配置/项目文件，或者将配置/项目文件传到volume目录2.通过volume目录的文件来启动程序Docker在各阶段的作用在开发阶段镜像的使用使得构建开发环境变得简单统一在测试阶段可以直接使用开发所构建的镜像进行测试，直接免除了测试环境构建的烦恼，也消除了因为环境不一致所带来的漏洞问题在部署和运维阶段与以往代码级别的部署不同，利用Docker可以进行容器级别的部署，把应用及其依赖环境打包成跨平台、轻量级、可移植的容器来进行部署]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker的使用]]></title>
    <url>%2F2018%2F06%2F17%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Docker容器介绍什么是DockerDocker是以Docker容器为资源分割的调度基本单位。封装软件运行的环境, 为开发者和系统管理员设计, 用于构建、发布和运行分布式应用的平台。他是一个跨平台、可移植并且简单易用的容器解决方案Docker可以在容器内部实现快速的自动化地部署应用, 并通过操作系统内核技术为容器提供资源隔离与安全保障Docker的优点功能实现容器技术的生态系统自上而下的分别覆盖了IaaS层和PaaS层所涉及的各类问题, 包括资源调度、编排、部署、监控、配置管理、存储网络管理、安全、容器化应用支撑平台等持续部署与测试,容器消除了线上线下的环境差异，保证了应用生命周期的环境一致性和标准化跨云平台支持容器带来的最大好处之一就是其适配性， 越来越多的云平台都支持容器，用户也不需要担心受到云平台的捆绑，同时也让应用多平台混合部署称为可能。环境标准化和版本控制基于容器提供的环境一致性和标准化，可以使用Git等工具对容器镜像进行版本控制，相比基于代码的版本控制来说，可以对整个应用的运行环境实现版本控制，一旦出现故障可以快速回滚。相比之前的虚拟机镜像，容器压缩和备份的速度更快，镜像启动也像启动一个普通进程一样快速高资源利用率与隔离容器没有管理程序的额外开销，与底层共享操作系统，性能更加优良，系统负载更低，在同等条件下可以运行更多的应用实例，可以更充分的利用系统资源。同事，容器拥有不错的资源隔离与限制能力，可以精确地对应用分配cpu、内存、等资源，宝整理应用之间不会互相影响容器跨平台性与镜像Linux容器虽然早在Linux2.6版本内核已经存在，但是缺少容器的跨平台性，难以推广。容器在原有的Linux容器的基础上进行大胆的个性，为容器设定了一整套标准化的配置方法，将应用及其依赖的运行环境打包成镜像，真正实现了构建一次到运行的理念，大大提高了容器的跨平台性易于理解而且易用Docker的英文愿意是处理集装箱的码头工人，标志是鲸鱼运送个一大堆集装箱，集装箱就是容器，生动好记。应用镜像仓库Docker官方构建了一个镜像仓库，组织和管理形式类似于Github，其上已经累积了成千上万的镜像。Docker的安装Ubuntu系统1.安装依赖sudo apt-get updatesudo apt-get install linux-image-generic-lts-trustysudo reboot2.安装wgetsudo apt-get install wget3.安装dockersudo wget -q0- https://get.docker.com/ | sh4.启动dockersudo service docker startWindows系统使用安装包安装1.下载安装包下载地址: https://download.docker.com/win/beta/InstallDocker.msi2.检查安装docker --versiondocker-compose --versiondocker-machinne --version使用Toolbox安装1.下载Toolbox下载地址：https://github.com/docker/toolbox/releases/download/v1.11.2/DockerToolbox-1.11.2.exe 2.安装Toolbox3.安装Docker双击快捷图标可以安装Docker、Docker Compose、Docker MachineCentOS系统1.更新系统sudo yum update2.请求安装脚本curl -fsSl https://get.docker.com/ | sh3.启动dockersudo service docker startDockerHub地址https://hub.docker.com/Docker命令行参数docker命令行通信方式在使用Docker时，需要使用Docker命令行工具的docker与Docker daemon建立通信。Docker daemon是Docker的守护进程，负责接收并分发执行Docker命令Docker daemon负责接收并执行来自docker的命令，它的运行需要root权限，所以一般执行docker命令都需要获取root权限docker命令结构图docker命令行1.获取命令清单docker --help 2.获取命令的详细使用方法docker COMMAND --help3.获取docker详细信息sudo docker info 获取docker详细信息sudo docker version 获取docker版本信息4.容器生命周期管理介绍：容器生命周期管理设计容器启动、停止等功能容器的创建命令 （基于特定的镜像创建一个容器, 并依据选项来控制该容器,会随机分配一个容器ID, 用以标识该容器）docker run [OPTIONS] IMAGE [COMMAND] [ARG1, ......] 如果本地没有该镜像，会默认从hub上拉取-i: 表示使用交互模式，终端始终保持输入流开放-t: 表示分配一个伪终端，一般-i, - t两个参数结合使用 -it，即可在容器中利用打开的伪终端进行交互操作--name: 选项表示可以指定docker run 命令启动容器的名字，若无此选项，Docker将为容器随机分配一个名字-m: 用于限制容器中所有进程分配的内存总量，以B\K\M\G为单位，超出内容限制后，容器会结束进程并退出-v: 用于和主机挂载共享目录，可以同时挂在多个目录。使用格式为[host-dir]:[container-dir]。会覆盖Dockerfile中通过Volume挂载的同名目录-p: 用于将容器的端口暴露给宿主机的端口，其常用的格式为 -p HostPort:ConrainerPort。 暴露端口后, 可以通过外部网络访问宿主机的方式来访问容器。--link: 可以实现同主机容器之间的互联, 使用方式--link container_name:alias 。可以避免容器的IP端口暴露到外网所导致的安全问题，还可以放置容器在启动后Ip地址变化导致的访问失效可以防止容器重启导致容器IP地址变化访问失效。若先要实现跨主机间的容器互联可以修改容器的配置文件/etc/hosts 增加 name 和 ip的映射。-d: 可以直接让容器后台启动。注意若启动的COMMAND命令不是持续前台运行的程序，容器会直接退出。解决办法:1.让COMMAND命令前台运行；2.在命令后面加上 tail top 这类持续前台运行的命令。--privileged：允许容器使用主机的设备，比如mount，也可以看到主机的网络设备，还可以在容器中启动容器等。--network: 指定容器使用哪种网络通讯方式，一共四种 host、None、bridge、 container。Host和主机共享网络；None不分配网络；bridge(默认)所有容器都分配独立的虚拟ip，网关为主机虚拟出来的docker0；container所有容器使用同一个ip。容器的启动、停止、重启命令docker start/stop/restart &lt;containner_id&gt;, &lt;containner_id&gt;-a: 启动一个容器并打印输出结果和错误-i:启动一个容器并进入交互模式-t: 停止或者重启容器的超时时间（秒），超时后系统将杀死进程。重命名容器docker rename &lt;old_name&gt; &lt;new_name&gt;示例sudo docker run -i -t --name mytest ubuntu:latest /bin/bash 为 ubuntu:latest 镜像分配一个容器, 并为它分配一个伪终端, 并执行/bin/bash命令 5.Docker镜像仓库使用介绍Docker registry 是存储容器镜像的仓库，用户可以通过Docker client 与Docker registry进行通信，以此来完成镜像的搜索、下载和上传等相关操作从Docker registry中拉取镜像docker pull [OPTIONS] NAME[:TAG|@DIGEST]将镜像推到镜像库docker push [OPTIONS] NAME[:TAG] # 将镜像推送到公共Hubdocker push SEL/ubuntu # 将镜像推送到指定库示例sudo docker pull ubuntu # 从官方Hub中拉取ubuntu:latest镜像sudo docker pull ubuntu:ubuntu12.04 # 从官方Hub中拉取ubuntu:12.04镜像sudo docker pull SEL/ubuntu # 从官方特定库中拉取ubuntu镜像sudo docker pull 10.10.103.215:5000/sshd # 从其他镜像服务器中拉取镜像6.镜像管理介绍用户可以在本地保存镜像资源，为此Docker提供了相应的管理子命令列出主机上的镜像docker images [OPTIONS] [REPOSITORY[:TAG]]-a: 默认显示最顶层的镜像，-a可以显示所有提交过的镜像删除镜像或者容器docker rm [OPTIONS] CONTAINER [CONRAINER...] # 删除容器docker rmi [OPTIONS] IMAGE [IMAGE...] # 删除镜像-f: 默认情况如果存在启动的容器，则对应的镜像无法删除，-f可以强制删除镜像或容器7.容器运维操作介绍作为Docker的核心，容器的操作是重中之重，Docker为用户提供了丰富的容器运维操作命令与正在运行的程序进行交互attach 进入容器docker attach [OPTIONS] CONTAINERattach 退出容器Ctr + c / exit / Ctr + P + Q 备注： Ctr + p + Q 不会结束容器，会让容器后台运行exec 进入容器当容器的终端被占用的时候，退出和其他的操作都会被限制，此时可以通过exec方式另外用一个终端连接容器docker exec [OPTIONS] CONTAINER COMMAND [ARG, ....]-d :分离模式: 在后台运行-i :交互模式-t :分配一个伪终端exec 退出容器Ctr + c / exit 退出容器后，容器运行不受影响查看镜像\容器的详细信息sudo docker inspect [OPTIONS] CONTAINER|IMAGE--format: 定义输出模板的格式查看容器相关信息(CONTAINER ID 、NAME、IMAGE、STATUS)docker ps [OPTIONS]-a：查看所有容器，默认查看正在启动容器-l: 查看最新创建的容器查看容器中正在运行的进程docker top CONTAINER8.镜像创建介绍commit命令可以将一个容器固话为一个新的镜像。当需要制作特定的镜像时，会进行修改容器的配置，如在容器中安装特定的工具等，通过commit命令可以将这些修改保存起来，使其不会因为容器的停止而丢失使用方法（官方不推荐, 下面会介绍DockerFile的使用）docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] # 注意：只能选择正在运行的容器进行提交9.容器和镜像的持久化和导入介绍可以实现再 某台机器上导出一个Docker容器并且在另外一台机器上导入将容器导出为镜像快照sudo docker export -o ubuntu.tar CONTAINER_ID 将本地快照导出为镜像docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]将镜像导出为镜像快照docker save -o haproxy.tar haproxy将本地快照导出为镜像docker load -i haproxy.tar10.events、history、logs命令介绍events、history、logs这三个命令用于查看Docker的系统日志信息。events命令会打印出实时的系统事件history会打印出指定的镜像的历史版本信息logs命令会打印出容器中的进程运行日期打印实时系统事件docker events [OPTIONS]打印指定镜像的历史版本信息docker history [OPTIONS] IAMGE打印容器中进程运行日志docker logs [OPTIONS] CONTAINERDocker开启端口监听介绍Docker默认只监听本地进程，即从其他主机上无法连接。如果需要远程连接，则需要开启2375端口的监听。通过命令行启动dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375Ubuntu下修改配置文件vim /etc/default/dockerDOCKER_OPTS=&quot;-H unix:///var/run/docker.sock -H 0.0.0.0:2375”Centos下修改配置文件vim /usr/lib/systemd/system/docker.service修改：ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensowflw-dropout(防止过拟合)]]></title>
    <url>%2F2018%2F06%2F06%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensowflw-dropout(%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88)%2F</url>
    <content type="text"><![CDATA[理解dropoutdropout是CNN中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。在下读到两篇代表性的论文，代表两种不同的观点，特此分享给大家。组合派参考文献中第一篇中的观点，Hinton老大爷提出来的，关于Hinton在深度学习界的地位我就不再赘述了，光是这地位，估计这一派的观点就是“武当少林”了。注意，派名是我自己起的，各位勿笑。观点该论文从神经网络的难题出发，一步一步引出dropout为何有效的解释。大规模的神经网络有两个缺点：费时容易过拟合这两个缺点真是抱在深度学习大腿上的两个大包袱，一左一右，相得益彰，额不，臭气相投。过拟合是很多机器学习的通病，过拟合了，得到的模型基本就废了。而为了解决过拟合问题，一般会采用ensemble方法，即训练多个模型做组合，此时，费时就成为一个大问题，不仅训练起来费时，测试起来多个模型也很费时。总之，几乎形成了一个死锁。Dropout的出现很好的可以解决这个问题，每次做完dropout，相当于从原始的网络中找到一个更瘦的网络，如下图所示：因而，对于一个有N个节点的神经网络，有了dropout后，就可以看做是2n个模型的集合了，但此时要训练的参数数目却是不变的，这就解脱了费时的问题。动机论虽然直观上看dropout是ensemble在分类性能上的一个近似，然而实际中，dropout毕竟还是在一个神经网络上进行的，只训练出了一套模型参数。那么他到底是因何而有效呢？这就要从动机上进行分析了。论文中作者对dropout的动机做了一个十分精彩的类比：在自然界中，在中大型动物中，一般是有性繁殖，有性繁殖是指后代的基因从父母两方各继承一半。但是从直观上看，似乎无性繁殖更加合理，因为无性繁殖可以保留大段大段的优秀基因。而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性。但是自然选择中毕竟没有选择无性繁殖，而选择了有性繁殖，须知物竞天择，适者生存。我们先做一个假设，那就是基因的力量在于混合的能力而非单个基因的能力。不管是有性繁殖还是无性繁殖都得遵循这个假设。为了证明有性繁殖的强大，我们先看一个概率学小知识。比如要搞一次恐怖袭击，两种方式：- 集中50人，让这50个人密切精准分工，搞一次大爆破。- 将50人分成10组，每组5人，分头行事，去随便什么地方搞点动作，成功一次就算。哪一个成功的概率比较大？ 显然是后者。因为将一个大团队作战变成了游击战。那么，类比过来，有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。个人补充一点：那就是植物和微生物大多采用无性繁殖，因为他们的生存环境的变化很小，因而不需要太强的适应新环境的能力，所以保留大段大段优秀的基因适应当前环境就足够了。而高等动物却不一样，要准备随时适应新的环境，因而将基因之间的联合适应性变成一个一个小的，更能提高生存的概率。dropout带来的模型的变化而为了达到ensemble的特性，有了dropout后，神经网络的训练和预测就会发生一些变化。训练层面无可避免的，训练网络的每个单元要添加一道概率流程。对应的公式变化如下如下：没有dropout的神经网络有dropout的神经网络测试层面预测的时候，每一个单元的参数要预乘以p。论文中的其他技术点防止过拟合的方法：提前终止（当验证集上的效果变差的时候）L1和L2正则化加权soft weight sharingdropoutdropout率的选择经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数。使得输入变化不会太大（0.8）训练过程对参数w的训练进行球形限制(max-normalization)，对dropout的训练非常有用。球形半径c是一个需要调整的参数。可以使用验证集进行参数调优dropout自己虽然也很牛，但是dropout、max-normalization、large decaying learning rates and high momentum组合起来效果更好，比如max-norm regularization就可以防止大的learning rate导致的参数blow up。使用pretraining方法也可以帮助dropout训练参数，在使用dropout时，要将所有参数都乘以1/p。部分实验结论该论文的实验部分很丰富，有大量的评测数据。maxout 神经网络中得另一种方法，Cifar-10上超越dropout文本分类上，dropout效果提升有限，分析原因可能是Reuters-RCV1数据量足够大，过拟合并不是模型的主要问题dropout与其他standerd regularizers的对比L2 weight decaylassoKL-sparsitymax-norm regularizationdropout特征学习标准神经网络，节点之间的相关性使得他们可以合作去fix其他节点中得噪声，但这些合作并不能在unseen data上泛化，于是，过拟合，dropout破坏了这种相关性。在autoencoder上，有dropout的算法更能学习有意义的特征（不过只能从直观上，不能量化）。产生的向量具有稀疏性。保持隐含节点数目不变，dropout率变化；保持激活的隐节点数目不变，隐节点数目变化。数据量小的时候，dropout效果不好，数据量大了，dropout效果好模型均值预测使用weight-scaling来做预测的均值化使用mente-carlo方法来做预测。即对每个样本根据dropout率先sample出来k个net，然后做预测，k越大，效果越好。Multiplicative Gaussian Noise使用高斯分布的dropout而不是伯努利模型dropoutdropout的缺点就在于训练时间是没有dropout网络的2-3倍。进一步需要了解的知识点dropout RBMMarginalizing Dropout具体来说就是将随机化的dropout变为确定性的，比如对于Logistic回归，其dropout相当于加了一个正则化项。Bayesian neural network对稀疏数据特别有用，比如medical diagnosis, genetics, drug discovery and other computational biology applications噪声派参考文献中第二篇论文中得观点，也很强有力。观点观点十分明确，就是对于每一个dropout后的网络，进行训练时，相当于做了Data Augmentation，因为，总可以找到一个样本，使得在原始的网络上也能达到dropout单元后的效果。 比如，对于某一层，dropout一些单元后，形成的结果是(1.5,0,2.5,0,1,2,0)，其中0是被drop的单元，那么总能找到一个样本，使得结果也是如此。这样，每一次dropout其实都相当于增加了样本。稀疏性知识点A首先，先了解一个知识点：When the data points belonging to a particular class are distributed along a linear manifold, or sub-space, of the input space, it is enough to learn a single set of features which can span the entire manifold. But when the data is distributed along a highly non-linear and discontinuous manifold, the best way to represent such a distribution is to learn features which can explicitly represent small local regions of the input space, effectively “tiling” the space to define non-linear decision boundaries.大致含义就是：在线性空间中，学习一个整个空间的特征集合是足够的，但是当数据分布在非线性不连续的空间中得时候，则学习局部空间的特征集合会比较好。知识点B假设有一堆数据，这些数据由M个不同的非连续性簇表示，给定K个数据。那么一个有效的特征表示是将输入的每个簇映射为特征以后，簇之间的重叠度最低。使用A来表示每个簇的特征表示中激活的维度集合。重叠度是指两个不同的簇的Ai和Aj之间的Jaccard相似度最小，那么：当K足够大时，即便A也很大，也可以学习到最小的重叠度当K小M大时，学习到最小的重叠度的方法就是减小A的大小，也就是稀疏性。上述的解释可能是有点太专业化，比较拗口。主旨意思是这样，我们要把不同的类别区分出来，就要是学习到的特征区分度比较大，在数据量足够的情况下不会发生过拟合的行为，不用担心。但当数据量小的时候，可以通过稀疏性，来增加特征的区分度。因而有意思的假设来了，使用了dropout后，相当于得到更多的局部簇，同等的数据下，簇变多了，因而为了使区分性变大，就使得稀疏性变大。为了验证这个数据，论文还做了一个实验，如下图：该实验使用了一个模拟数据，即在一个圆上，有15000个点，将这个圆分为若干个弧，在一个弧上的属于同一个类，一共10个类，即不同的弧也可能属于同一个类。改变弧的大小，就可以使属于同一类的弧变多。实验结论就是当弧长变大时，簇数目变少，稀疏度变低。与假设相符合。个人观点：该假设不仅仅解释了dropout何以导致稀疏性，还解释了dropout因为使局部簇的更加显露出来，而根据知识点A可得，使局部簇显露出来是dropout能防止过拟合的原因，而稀疏性只是其外在表现。论文中的其他技术知识点将dropout映射回得样本训练一个完整的网络，可以达到dropout的效果。dropout由固定值变为一个区间，可以提高效果将dropout后的表示映射回输入空间时，并不能找到一个样本x*使得所有层都能满足dropout的结果，但可以为每一层都找到一个样本，这样，对于每一个dropout，都可以找到一组样本可以模拟结果。dropout对应的还有一个dropConnect，公式如下：dropoutdropConnect试验中，纯二值化的特征的效果也非常好，说明了稀疏表示在进行空间分区的假设是成立的，一个特征是否被激活表示该样本是否在一个子空间中。]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensowFlow分布式]]></title>
    <url>%2F2018%2F06%2F05%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensowFlow%E5%88%86%E5%B8%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[自定义命令行参数定义命令行参数的方法tf.app.flags 支持应从命令行接收参数, 可以用来指定集群的配置等, 有下面的参数类型DEFINE_string(flag_name, default_value, docstring)DEFINE_integer(flag_name, default_value, docstring)DEFINE_boolean(flag_name, default_value, docstring)DEFINE_float(flag_name, default_value, docstring)调用命令行参数的方法tf.app.flags.FlAGS.xxxxxx 为上面定义的flag_name通过命令行启动的方法tf.app.run() 可以截止启动main(argv) 函数可以在命令行中启动并传入参数, 程序可以直接帮我们运行main()函数 分布式TensorFlow分布式TensowFlow介绍Tensorflow的一个特色就是分布式计算。分布式Tensorflow是由高性能的gRPC框架作为底层技术来支持的。这是一个通信框架gRPC(google remote procedure call)，是一个高性能、跨平台的RPC框架。RPC协议，即远程过程调用协议，是指通过网络从远程计算机程序上请求服务。分布式原理Tensorflow分布式是由多个服务器进程和客户端进程组成。有几种部署方式，列如单机多卡和多机多卡（分布式）。单机多卡(单台服务器有多块GPU设备)在单机单GPU的训练中，数据是一个batch一个batch的训练。 在单机多GPU中，数据一次处理4个batch(假设是4个GPU训练）， 每个GPU处理一个batch的数据计算。变量，或者说参数，保存在CPU上。数据由CPU分发给4个GPU，在GPU上完成计算，得到每个批次要更新的梯度在CPU上收集完4个GPU上要更新的梯度，计算一下平均梯度，然后更新。循环进行上面步骤多机多卡而分布式是指有多台计算机，充分使用多台计算机的性能，处理数据的能力。可以根据不同计算机划分不同的工作节点。当数据量或者计算量达到超过一台计算机处理能力的上上限的话，必须使用分布式分布式架构当我们知道的基本的分布式原理之后，我们来看看分布式的架构的组成。分布式架构的组成可以说是一个集群的组成方式。那么一般我们在进行Tensorflow分布式时，需要建立一个集群。通常是我们分布式的作业集合。一个作业中又包含了很多的任务（工作结点），每个任务由一个工作进程来执行。分布式节点之间的关系一般来说，在分布式机器学习框架中，我们会把作业分成参数作业（parameter job）和工作结点作业（worker job）。参数服务器:运行参数作业的服务器我们称之为参数服务器（parameter server,PS），负责管理参数的存储和更新参数服务器，当模型越来越大时，模型的参数越来越多，多到一台机器的性能不够完成对模型参数的更新的时候，就需要把参数分开放到不同的机器去存储和更新。参数服务器可以是由多台机器组成的集群。worker服务器:工作结点作业负责主要从事计算的任务，如运行操作。Tensorflow的分布式实现了作业间的数据传输，也就是参数作业到工作结点作业的前向传播，以及工作节点到参数作业的反向传播。所有的Worker服务器会有一个主worker服务器, 会话的创建, 文件的读取和存储只会在主worker上进行分布式的模式数据并行原理数据并总的原理很简单。其中CPU主要负责梯度平均和参数更新，而GPU主要负责训练模型副本。实现模型副本定义在GPU上对于每一个GPU，都是从CPU获得数据，前向传播进行计算，得到损失，并计算出梯度CPU接到GPU的梯度，取平均值，然后进行梯度更新存在的问题每一个设备的计算速度不一样，有的快有的慢，那么CPU在更新变量的时候，就应该等待每一个设备的一个batch进行完成，这样相当于降低了分布式的效率,解决办法请参考下面的异步更新同步更新&amp;异步更新同步更新(随机梯度下降法（Sync-SGD))同步随即梯度下降法的含义是在进行训练时，每个节点的工作任务需要读入共享参数，执行并行的梯度计算，同步需要等待所有工作节点把局部的梯度算好，然后将所有共享参数进行合并、累加，再一次性更新到模型的参数；下一个批次中，所有工作节点拿到模型更新后的参数再进行训练。这种方案的优势是，每个训练批次都考虑了所有工作节点的训练情况，损失下降比较稳定；劣势是，性能瓶颈在于最慢的工作结点上。异步更新(异步随机梯度下降法（Async-SGD))异步随机梯度下降法的含义是每个工作结点上的任务独立计算局部梯度，并异步更新到模型的参数中，不需要执行协调和等待操作。这种方案的优势是，性能不存在瓶颈；劣势是，每个工作节点计算的梯度值发送回参数服务器会有参数更新的冲突，一定程度上会影响算法的收敛速度，在损失下降的过程中抖动较大。分布式接口创建分布式集群的方法创建集群的方法是为每一个任务启动一个服务，这些任务可以分布在不同的机器上，也可以同一台机器上启动多个任务，使用不同的GPU等来运行。创建分布式集群的步骤1、创建一个tf.train.ClusterSpec，用于对集群中的所有任务进行描述，该描述内容对所有任务应该是相同的2、创建tf.train.Server，用于创建一个任务3、启动TensorFlow分布式API接口使用1. tf.train.ClusterSpec( ) 创建ClusterSpec,表示参与分布式TensorFlow计算的一组进程2. tf.train.Server( ) 创建Tensorflow的集群描述信息，其中ps和worker为作业名称，通过指定ip地址加端口创建，创建serverserver = tf.train.Server(server_or_cluster_def, job_name=None, task_index=None, protocol=None, config=None, start=True)server_or_cluster_def: 集群描述job_name: 任务类型名称 task_index: 任务数使用serverserver.target返回tf.Session连接到此服务器的目标server.join()参数服务器端等待接受参数任务，直到服务器关闭3. tf.device( ) 指定worker运行设备tf.device(device_name_or_function) 指定代码运行在CPU或者GPU上 device_name:例: /job:worker/task:0/cpu:0function:tf.train.reploca_device_setter(worker_device=worker_device, cluster=cluster)worker_device: 例： /job:worker/task:0/cpu:0cluster: 集群描述对象示例：4. sess = tf.train.MonitoredTrainingSession( ) 创建分布式会话创建会话方法tf.train.MonitoredTrainingSession(master, is_chief=True, checkpoint_dir=None, hooks=None, save_checkpoint_secs=600, save_summaries_steps=USE_DEFAULT, save_summaries_secs=USE_DEFAULT, config-None)master: 指定运行会话协议IP和端口(用于分布式)例: grpc://192.168.0.1:2000is_chief: 是否为主worker 主worker负责初始化和恢复基础的TensorFlow会话checkpoint_dir: 检查点文件记录, 同时也是events目录config: 会话运行的配置项, tf.ConfigProto(log_device_placement=True)hooks: 可选SessionRunHook 对象列表会话的方法sess.should_stop( ) : 当程序发生异常的时候should_stop 返回True sess.run() 跟session一样可以运行op分布式会话-钩子对象钩子对象作用当在开启分布式会话的时候, 钩子对象方法被调用指定了钩子对象(包含了begin，before_run, after_run方法), 可以实现分别在初始化会话, 调用run方法之前, 调用run方法之后, 分别运行钩子对象对应的方法该功能相当于django的中间件功能可以用来实现计步, 打印中间输出等功能创建钩子对象的方法1. 创建类并继承tf.train.SessionRunHook 2. 实现方法 begin 会话之前调用, 只会调用一次3. 实现方法 before_run(run_context) 接收run_context 参数run_context: 一个SessionRunContenxt对象, 包含会话的运行信息return: 一个SessionRunArgs对象, 例如: tf.train.SessionRunArgs(Tensow),接收的参数必须为Tensow类型4. 实现方法 after_run(run_context, run_value)run_context: 一个SessionRunContext 对象run_values: 一个SessionRunValues对象, run_values.results 为before_run返回的Tensow对象的eval属性, 即tensow的值常用的钩子对象tf.train.StopAtStepHook(last_stop) 计步钩子对象last_stop: 指定训练步数, 当达到的时候抛出异常注意: 在使用计步钩子的时候需要定义全局步数：global_step = tf.contrib.framework.get_or_create_global_step()]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensowFlow神经网络]]></title>
    <url>%2F2018%2F06%2F04%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensowFlow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[神经网络介绍神经网络在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络的结构和功能的计算模型，用于对函数进行估计或近似。神经网络的分类(人工神经网络)神经网络前身: 单层感知机基础神经网络：线性神经网络，BP神经网络，Hopfield神经网络等 进阶神经网络：玻尔兹曼机，受限玻尔兹曼机，递归神经网络等 深度神经网络：深度置信网络，卷积神经网络(CNN)，循环神经网络，LSTM网络等神经网络的特点输入向量的特征数量和输入神经元的个数相同输出层的神经元数量和种类数量相同每个连接都有个权重值同一层神经元之间没有连接 由输入层，隐层，输出层组成第N层与第N-1层的所有神经元连接，也叫全连接简单的神经网络模型神经网络模型示例输入层、隐层、输出层 （隐层和输出层统称为全连接层）神经元模型示例输入向量的特征数量和输入神经元的个数相同中间层三个神经元的示例神经网络的组成结构： 组成神经网络的神经元激励函数：隐层和输出层节点的输入和输出之间具有函数关系，这个函数称为激励函数学习规则：学习规则制定了网络中的权重如何随着时间的推进而调整(反向传播算法)TensowFlow提供神经网络功能的主要模块tf.nn：神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluationtf.layers:主要提供的高层的神经网络，主要和卷积相关的，对tf.nn的进一步封装tf.contrib:tf.contrib.layers提供够将计算图中的 网络层、正则化、摘要操作、是构建计算图的高级操作神经网络前身--感知机什么是感知机有n个输入数据，通过权重与各数据之间进行加权求和，求和后的结果经过激活函数结果(0/1)，得出输出很容易解决与、或问题通过多个感知机，可以将结果分类的区域划分的越来越准确用模型表示感知机与逻辑回归的联系和区别联系: 都可以用来做分类的学习区别：激活函数不一样，感知机使用的额是sign函数,输出值是0/1，只能判断简单的分类问题, 逻辑回归使用的是sigmid函数, 结果为0-1, 可以预测为某一个分类的准确概率, 更为广泛使用浅层(单一隐层)神经网络概念只有一层隐藏层的神经网络目标值处理one-hot编码one-hot api介绍作用: 将一组目标集转化为one-hot编码tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)indices: 数据集标签, 目标值集合depth: 张量的深度, 即类别数SoftMax回归函数作用: 将一组结果数据转化为[0, 1] 两个之间的数, 为1的数代表预测结果为该分类公式 使用位置在特征值经过加权和偏置计算后，经过softmax函数, 得到最终预测的概率结果图示特征的加权计算tensorflow加权计算apitf.matmul(a, b,name=None)a: 特征值矩阵b: 加权矩阵损失计算 损失计算-交叉熵损失公式损失计算apis = tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None,name=None) 计算交叉熵损失labels:标签值（真实值）logits：样本加权之后的值return:返回损失值列表损失均值计算apitf.reduce_mean(s) 获取损失的平均值损失的优化--梯度下降 tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)learning_rate: 学习率，一般为minimize(loss): 最小化损失return: 梯度下降op模型准确性计算equal_list = tf.equal(tf.argmax(y, 1), tf.argmax(y_label, 1)) 返回每一行的最大值的索引equal_list = tf.equal(tf.argmax(y, 0), tf.argmax(y_label, 0)) 返回每一列的最大值的索引equal_list = tf.equal(tf.argmax(y, 2), tf.argmax(y_label, 2)) 返回每行每列的最大值accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))y: 预测结果的集合x: 目标集合神经网络实现流程1、准备数据2、全连接结果计算3、损失优化4、模型评估（计算准确性）深层神经网络(卷积神经网络介绍)概念深度学习网络与更常见的单一隐藏层神经网络的区别在于深度，深度学习网络中，每一个节点层在前一层输出的基础上学习识别一组特定的特征。随着神经网络深度增加，节点所能识别的特征也就越来越复杂为什么引入深层神经网络全连接神经网络具有一定的局限性当数据的特征特别多的时候, 每一个特征值都会有一个权重, 这时每一个神经元都需要很多个权重值这样就会很影响资源， 造成资源的浪费， 影响训练增加隐藏层数, 可以每层逐渐的减少所需权重的数量什么是卷积神经网络卷积神经网络，是一种前馈神经网络，人工神经元可以响应周围单元，可以进行大型图像处理。卷积神经网络包括卷积层和池化层。卷积神经网络的发展历史卷积神经网络的特点神经网络(neural networks)的基本组成包括输入层、隐藏层、输出层。而卷积神经网络的特点在于隐藏层分为卷积层和池化层(pooling layer，又叫下采样层)卷积神经网络的结构分析训练结构图卷积层(CONV): 卷积层介绍通过Filter(过滤器)在原始图像上平移来提取并转化特征, 使转化后的特征可以提供给池化层进行数据的池化卷积层过滤器参数介绍个数大小(1*1,3*3,5*5)步长零填充卷积层输出的参数介绍深度由过滤器个数决定输出长度和宽度：由filter尺寸和步长决定RELU激活函数使用Relu函数的优点第一，采用sigmoid等函数，反向传播求误差梯度时，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（求不出权重和偏置）函数图形激活函数使用apitf.nn.relu(features, name=None)features:卷积后加上偏置的结果return:结果池化层(POOL):池化层介绍通过特征后稀疏参数来减少学习的参数，降低网络的复杂度，（最大池化和平均池化）Pooling层主要的作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Pooling的方法很多，最常用的是Max Pooling。2*2 2步长数据的池化过程图例池化使用apitf.nn.max_pool(value, ksize=, strides=, padding=,name=None) 输入上执行最大池数value:4-D Tensor形状[batch, height, width, channels]ksize:池化窗口大小，[1, ksize, ksize, 1]strides:步长大小，[1,strides,strides,1]padding:“SAME”, “VALID”，使用的填充算法的类型，使用“SAME”全连接层(FC):全连接层在卷积神经网络中介绍前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。最后的全连接层在整个卷积神经网络中起到“分类器”的作用。卷积神经网络相关参数计算输入值的体积H1 * W1 * D1卷积层过滤器四个超参数Filter数量KFilter大小K步长S零填充大小P卷积层过滤器输出体积大小H2 = (H1-F+2P)/S + 1W2 = (W1-F+2P)/S +1D2 = K数据特征的变化过程数据在经过多个卷积层后的变化宽度和高度在缩小, 深度不断的增加(深度和卷积层的过滤器数量相关)单个卷积层(2个Filter)的过程示例卷积层的零填充定义卷积核在提取特征映射时的动作称之为padding（零填充)由于移动步长不一定能整出整张图的像素宽度。其中有两种方式，SAME和VALID两种零填充的方式1. SAME：越过边缘取样，取样的面积和输入图像的像素宽度一致。2. VALID：不越过边缘取样，取样的面积小于输入人的图像的像素宽度卷积网络卷积操作零填充api介绍tf.nn.conv2d(input, filter, strides=, padding=, name=None) 卷计划操作input：给定的输入张量，具有[batch,heigth,width, channel]，类型为float32,64filter：指定过滤器的大小，[filter_height, filter_width,in_channels, out_channels]strides：strides = [1, stride, stride, 1],步长padding：“SAME”, “VALID”，使用的填充算法的类型，使用“SAME”。其中”VALID”表示滑动超出部分舍弃，SAME”表示填充，使得变化后height,width一样大神经网络总结输入层的矩阵的特征数量和特征的数量(样本的列数)相等输出层的矩阵的列数与分类数量相等经过矩阵变换的矩阵: 行数=左矩阵的行， 列数= 右矩阵的列每个矩阵代表全连接层的一层隐层矩阵中的每一列代表一个神经元一个全连接层和一有多个隐层，即多个矩阵偏置的列数应该等于矩阵的行数]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow线程对列&IO操作]]></title>
    <url>%2F2018%2F06%2F03%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensorFlow%E7%BA%BF%E7%A8%8B%E5%AF%B9%E5%88%97%26IO%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[TensorFlow--IO操作TensorFlow 异步IO操作的原理如下如， 文件对列用来存储需要读取文件或者文本， 阅读器用来不断从对列中获取文件进行内容的读取， Deoder用来对读取出来的内容进行解码， 结果对列用来存储读取的结果IO操作是真正的多线程(异步)操作流程如下图IO操作是怎么实现异步的当数据量很大时，入队操作从硬盘中读取数据，放入内存中，主线程需要等待入队操作完成，才能进行训练。会话里可以运行多个线程，实现异步读取。TensorFlow--队列队列的创建Q = tf.FIFOQueue(capacity, dtypes, name=&quot;fifo_queue&quot;) 创建先进先出的对列，按照顺序出对列capacity: 整数， 对列的容量dtypes: dtype 数据类型tf.RandomShuffleQueue() 创建随机出队的对列队列的操作Q.dequeue() 出列操作Q.enqueue(vals) 单个元素入队Q.enqueue_many(vals) 多个元素入队vals: 列表或者元组, 包含需要入队的元素Q.size() 返回一个tensor类型的对象, value是对列的大小使用示例TensorFlow--对列管理器队列管理器的作用当直接通过对列的方法向队列中添加数据的时候, 需要等待数据添加完成才能执行下一步操作, 这样的操作不是异步的当我们想一边自动的向对列中添加数据, 一边进行其他的操作的时候, 我们可以使用对列管理器其会自动的绑定对列和一些对对列的操作, 并另外开启线程执行绑定的操作对列管理器的创建qr = tf.train.QueueRunner(queue, enqueue_ops=None) 创建对列管理器queue: 绑定到对列管理器的对列enqueue_ops: 代表对列操作列表，每个操作会创建一个子线程来执行创建线程让对列管理器执行thd = qr.create_threads(sess, coord=None, start=False) 创建线程来执行队列管理器绑定的方法start: True， 直接启动队列管理器线程。False， 需要手动调用start来让线程运行coord：线程协调器return: 返回线程对象线程资源问题的解决问题： 当在会话中开启线程的时候, 当使用with方式开启线程并执行后, 当主线程结束后, 由于with方式开启会话会对资源进行回收这时候子线程失去会话资源, 会导致程序崩溃解决方法使用线程协调器TensorFlow--线程协调器线程协调器的作用实现一个简单的机制来协调一组线程的终止阻塞等操作创建线程协调器coord = tf.train.Coordinator() 创建线程斜体其起线程协调器的方法coord.request_stop() 终止绑定在线程协调器的所有线程coord.should_stop() 检查是否满足停止条件,如果满足便退出线程coord.join(threads=None) 阻塞等待线程结束使用代码示例文件读取文件队列的创建tf.train.string_input_producer(string_tensor, num_opochs=None, shuffle=True) 创建文件队列，用来存储即将操作的文件string_tensor: 含有文件名的1阶张量num_epochs: 过几遍数据，默认无限过数据return: 输出字符串的队列文件队列线程开启操作作用：开启线程创建图中定义的对列，并将设置的待读取内容读取到对列中thd = tf.train.start_queue_runners(sess=None,coord=None, start=True)sess: 所在的会话中coord：线程协调器start： 是否直接开启线程管道读取批量处理作用可以每次批量的进行文件内容的读取原理是会创建一个批处理对列,当读取文件内容的时候, 批处理对列会预先从文件对列中选取文件，读取内容将对列填满然后根据设定的batch_size, 返回对列中的内容当内容取出后， 再从读取文件将对列填满, 等待下一次的读取使用方法tf.train.batch(tensors, batch_size, num_threads=1, capacity=32, name=None) 读取指定大小（个数）的张量tensors：可以是包含张量的列表batch_size:从队列中读取的批处理大小num_threads：进入队列的线程数capacity：整数，队列中元素的最大数量return： tensors, 读取到内容的数组tf.train.shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1) 乱序读取指定大小（个数）的张量min_after_dequeue:留下队列里的张量个数，能够保持随机打乱注意：批处理接收的tensow对象必须都是规定了shape，并且统一的对象文件阅读器创建文本阅读器Reader = tf.TextLineReader()阅读文本，默认按行读取, 默认分隔符为,二进制文件阅读器Reader = tf.FixedLengthRecordReader(record_bytes)阅读二进制文件,可以指定每次读取的字节数record_bytes:整型，指定每次读取的字节数TFRecord文件阅读器Reader = tf.TFRecordReader()读取TfRecords文件图片阅读器Reader = tf.WholeFileReader() 图片阅读器文件阅读器的方法Reader.read(file_queue) 从队列中读取指定数量的内容返回一个Tensors元组（key文件名字，value默认的内容(行或者字节或者图片), tensow类型）文件内容解码器文本文件解码器tf.decode_csv(records, record_defaults=None, field_delim = None, name = None)由于从文件中读取的是字符串，需要函数去解析这些字符串到张量record_defaults: 字段的默认值，比如[[1]，[]，['string']]，不指定类型（设为空[]）也可以field_delim: 默认分割符”,”records: tensor型字符串，每个字符串是csv中的记录行二进制文件解码器tf.decode_raw(bytes, out_type, little_endian=None, name = None) 将字符串类型的二进制数据， 转换为uint8 或者int32类型的数据与函数tf.FixedLengthRecordReader搭配使用，将字符串表示的二进制读取为uint8格式bytes: 读取到的bytes字节内容out_type: 读取后输出的类型图片文件解码器解码方法tf.image.decode_jpeg(content, out_type)配合tf.WholiFileReader 使用,将JPEG编码的图像解码为uint8张量return:uint8张量，3-D形状[height, width, channels]tf.image.decode_png(contents, out_type)将PNG编码的图像解码为uint8或uint16张量return:张量类型，3-D形状[height, width, channels]图片的基本操作apitf.image.resize_images(images, size) 更改图片尺寸(只能更改尺寸, 不能更改通道)images：4-D形状[batch, height, width, channels]或3-D形状的张量[height, width, channels]的图片数据size：1-D int32张量：new_height, new_width，图像的新尺寸return: 返回4-D格式或者3-D格式图片的tensow类型，并且tensow对象的shape对应通道数为? 不固定类型TFRecords文件解码器feature = tf.parse_single_example(value,features={ &quot;image&quot;: tf.FixedLenFeature([], tf.string), &quot;label&quot;: tf.FixedLenFeature([], tf.int64)}) 根据协议将TFRecords的内容转化为featureimage = feature['image'] 通过feature可以读取里面的特征值label = feature['label']文件存储器(TFRecords文件存储器)建立TFRecord存储器Saver = tf.python_io.TFRecordWriter(path) 建立存储器path: TFRecords文件存储的路径存储器的方法write(record): 向文件中写入一个字符串记录(下面介绍)close(): 关闭文件写入器TFRecords存储的字符串记录构造样本的Example协议块example = tf.train.Example(features=None) 创建协议块features: tf.train.Features类型的特征实例feature 特征实例的创建feature 的创建feature = tf.train.Features(feature=None) 构建每个样本的信息键值对feature: 字典数据, key为要保存的名字，value为tf.train.Feature实例feature的value的创建tf.train. Int64List(value=[Value])tf.train. BytesList(value=[Bytes])tf.train. FloatList(value=[value])TFRecords的字符串记录创建实例example = tf.train.Example(features=tf.train.Features(feature={&quot;image&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),&quot;label&quot;: tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))}))文本文件读取代码示例图片文件的读取示例二进制文件读取示例TFRecords文件存储示例TFRecords文件读取示例]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow基础]]></title>
    <url>%2F2018%2F06%2F02%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensorFlow%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[深度学习（Deep Learning）和TensorFlow的介绍深度学习深度学习是利用多层神经网络结构，从大数据中学习现实世界中各类事物能直接用于计算机计算的表示形式（如图像中的事物、音频中的声音等），被认为是智能机器可能的“大脑结构”Learning：让计算机自动调整函数参数以拟合想要的函数的过程Deep：多个函数进行嵌套，构成一个多层神经网络，利用非监督贪心逐层训练算法调整有效地自动调整函数参数简单地说深度学习就是：使用多层神经网络来进行机器学习TensorFlowTensorFlow是用来进行深度学习开发的深度学习框架是Google Brain的计划产物应用于AlphaGo， Gmail， Google Maps等1000多个产品与2015年11月开源， 2017年2月发布1.0版本架构师 Jeff DeanTensorFlow的历史版本Tensorflow的特点1、真正的可移植性引入各种计算设备的支持包括CPU/GPU/TPU，以及能够很好地运行在移动端，如安卓设备、ios、树莓派等等2、多语言支持Tensorflow 有一个合理的c++使用界面，也有一个易用的python使用界面来构建和执行你的graphs，你可以直接写python/c++程序。3、高度的灵活性与效率TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库能够灵活进行组装图，执行图。随着开发的进展，Tensorflow的效率不段在提高4、支持TensorFlow 由谷歌提供支持，谷歌投入了大量精力开发 TensorFlow，它希望TensorFlow 成为机器学习研究人员和开发人员的通用语言TensorFlow的结构TensorFlow运算的定义和执行方式需要通过调用TensorFlow的接口来定义操作OP和张量并自动汇总成一个流程图需要通过session的会话对象来执行流程图中的操作TensorFlow的数据流图数据流图可以理解为整个训练的逻辑图通过以图的逻辑来进行不断的训练模型TensorFlow的组成理解Tensor + flowTensor: 提供给训练流动的数据flow: 组成训练逻辑的流程图TensorFlow调用时候的异常处理1、可以通过源码安装的方式避免警告2、通过加入如下的代码解决import osos.environ['TF_CPP_MIN_LOG_LEVEL']='2'TensorFlow文档https://www.tensorflow.org/versions/r1.0TensorFlow--图图的介绍图可以表示为是由一组计算单位对象tf.Operation和其中间流动的数据单元tf.Tensor组成其在定义好tf.Opetation和tf.Tensor后便默认生成调用图对象的方法tf.get_default_graph() 调用当前默认图的方法op.graph 获取当前op所存在的图sess .graph 获取当前会话所存在的图tensor.graph 获取当前的数据单元所存在的图图的创建\调用new_g = tf.Graph() 图的创建with new_g.as_default(): 使用新创建的图with tf.session(graph=new_g) as sess: 图的调用， 指定图创建会话TensorFlow--opop的介绍op可以理解为神经元, 一个神经元有多个输入，一个或者多个输出op在接收到输入的tensor数据后, 通过一系列的操作, 返回输出内容需要在sess会话中run调用才可以执被执行可以类比为python的函数/方法常用的op有哪些TensorFlow--会话会话的介绍会话 session可以理解为图和处理器之间的桥梁，只有通过session才能够将图中定义的op\tensor等执行创建会话的方法tf.Session() 需要手动close()with tf.Session() as sess: 不需要进行手动close()with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: 创建会话并指定运行会话的设备with tf.InteractiveSession() as sess: 创建交互方式的会话会话拥有的资源tf.Variabletf.QueueBasetf.ReaderBase会话资源在会话结束后会进行回收会话的run方法run(fetches, feed_dict=None,graph=None) 可以运行op或者计算tensorfetches: 可以是op，tensor，列表, 元组，namedtuple，dict，OrderedDictfeed_dict: 允许调用者覆盖图中指定的张量的值， 提供给placeholder使用返回的异常值RuntimeError：如果它Session处于无效状态（例如已关闭）。TypeError：如果fetches或feed_dict键是不合适的类型。 ValueError：如果fetches或feed_dict键无效或引用 Tensor不存在TensorFlow--Tensor(张量)张量的创建tf.constant() 直接创建确定值的张量tf.placeholder() 创建Feed类型的张量tf.Variable() 创建变量类型的张量参数：shape: 张量的形状， 定义张量的阶dtype: 定义张量的数据类型name: 定义张量的名称Feed操作作用在程序执行的时候, 不确定性输入什么的是什么的时候, 提前占位(相当于python的缺省参数的作用)使用方法tf.placholder(dtype, shape, name) 创建Feed类型占位张量调用方法sess.run([op_name], feed_dict={placeholder_name: value})图例张量的阶张量的数据类型 张量的属性graph: 张量所属的默认图op: 创建出张量的操作名name: 张量的字符串描述shape: 张量的形状操作演示张量的动态形状与静态形状静态形状创建一个张量,初始的形状为静态性状动态形状一种描述原始张量在执行过程中的一种形状(动态变化), 代表经过变化后的静态形状形状的获取Tensor.get_shape() 获取张量形状Tensor.set_shape([]) 对非固定形状的张量进行形状设置, 即只能对Feed类型张量进行设置tf.reshape(Tensor, shape=[]) 可以对张量进行形状的转换, 但是元素数量必须相等张量转化的要点1、转换静态形状的时候，1-D到1-D，2-D到2-D，不能跨阶数改变形状2、 对于已经固定或者设置静态形状的张量／变量，不能再次设置静态形状3、tf.reshape()动态创建新张量时，元素个数不能不匹配生成张量tf.zeros(shape, dtype=tf.float32, name=None) 创建所有元素为0的张量tf.ones(shape, dtype=tf.float32, name=None) 创建所有元素为一的张量tf.constant(value, dtype=Noe, shape=None, name=None) 创建一个常量张量tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 创建正态分布的随机张量张量数据类型变换张量形状的变换和切片TensorFlow--变量什么是变量变量也是一种OP，是一种特殊的张量，能够进行存储持久化，它的值就是张量,默认被训练变量的创建及其初始化tf.Variable(initial_value=None,name=None, trainable=True) 创建一个带值initial_value的新变量tf.global_variables_initializer() 添加一个初始化所有变量的op， 需要在会话中开启sess.run() 运行op变量的属性的方法assign(value) 为变量重新分配一个值name 变量的名字eval(session=None) 计算并返回变量的值、需要在会话中执行TensorFlow--Tensorboardevents文件events文件可以记录图的内容，和变量等信息TensorBoard 通过读取 TensorFlow 的事件文件来运行并进行可视化的创建event文件file_writer = tf.summary.FileWriter('XXX', graph) 创建writer对象,并创建events文件，在会话中进行写入事件(图)文件到指定目录(最好用绝对路径)，以提供给tensorboard使用在events文件中记录变量变化收集变量的变化tf.summary.histogram(name=&quot;&quot;,tensor) 收集高维度的变量参数tf.summary.image(name=&quot;&quot;,tensor) 收集输入的图片张量tf.summary.scalar(name=&quot;&quot;,tensor) 收集对于损失函数和准确率合并变量并写入事件文件merged = tf.summary.merge_all() 合并变量summary = sess.run(merged) 运行合并,每次添加前都需要运行，会话中进行FileWriter.add_summary(summary,i) 添加变量, i表示第几次添加，会话中进行开启tensorboard功能tensorboard --logdir=“XXX”tensorboard中符号的意义TensorFlow--变量作用域变量作用域让模型代码更加清晰，作用分明可以将作用域分开, 变量会按照作用域进行域的划分，在tensorboard中观察变量也更清晰创建作用域的方法with tf.variable_scope(&lt;scope_name&gt;)： 用上下文管理器的方式创建作用域scope_name: 为变量作用域的名称TensorFlow--命令行参数作用我们可以通过使用tf.app.flags提供的方法, 来定义参数, 同样我们也可以通过它提供的方法来调用定义的参数命令行参数定义的方式命令行参数的调用方式tf.app.flags.,在flags有一个FLAGS标志，它在程序中可以调用到我们前面具体定义的flag_nametf.app.flags.FLAGS.xxx调用main函数方法通过tf.app.run()可直接启动main(argv)函数代码示例：TensorFlow--模型的保存和加载保存模型的方法创建模型保存操作Opsaver_op = tf.train.Saver(var_list=None,max_to_keep=5)var_list:指定将要保存和还原的变量。它可以作为一个dict或一个列表传递. 默认保存图中全部变量max_to_keep： 指示要保留的最近检查点文件的最大数量。创建新文件时，会删除较旧的文件。如果无或0，则保留所有检查点文件。默认为5（即保留最新的5个检查点文件。）对模型进行保存(在会话中操作)saver_op.save(sess, save_path)sess: 会话对象， 默认当前会话save_path: 保存路径, 保存文件格式为checkpoint文件模型的加载(恢复)方法saver_op.restore(sess, save_path) 模型恢复, 在会话中操作sess: 会话对象save_path: 存储路径Tensorflow--简单的运算API矩阵运算tf.matmul(x, w)平方tf.square(error)均值tf.reduce_mean(error)Tensorflow--梯度下降训练api使用方法train_op = tf.train.GradientDescentOptimizer(learning_rate).method(minloss) 创建训练Oplearning_rate: 学习率minloss: minisize 学习方式, minisize为最小损失sess.run(train_op) 调用训练OP]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本分析-文本相似度和分类]]></title>
    <url>%2F2018%2F05%2F25%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90-%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%92%8C%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[文本相似度（词频统计）的流程实现流程的简介度量文本间的相似性使用词频表示文本特征文本中单词出现的频率或次数NLTK实现词频统计文本相似度分析的流程导入模块import nltkfrom nltk import FreqDist # 导入词频统计模块FreqDist先定义进行分析的几个文本text1 = 'I like the movie so much 'text2 = 'That is a good movie 'text3 = 'This is a great one 'text4 = 'That is a really bad movie 'text5 = 'This is a terrible movie'定义取出词频最好的n个单词的方法（词频统计方法）def get_most_common_words(n, *args):&quot;&quot;&quot;将文本内容汇总，并获取统计每个单词的词频的列表&quot;&quot;&quot;text = [text for text in args]text = ''.join(text)words = nltk.word_tokenize(text) # 对文本进行分词处理freq_dist = FreqDist(words) # 1.生成每个单词的词频统计结果,生成词频统计对象most_common_words = freq_dist.most_common(n) # 2.取出最高频率的n个单词，返回列表return most_common_words # 输出类似：[('a', 4), ('movie', 4), ('is', 4), ('This', 2), ('That', 2)]定义查找单词所在位置的方法def lookup_pos(most_common_words):result = {} # 事先设置好存储常用单词的字典pos = 0 # 表示常用单词的位置for word in get_most_common_words:result[word[0]] = pospos += 1return result # 返回数据类似：{'movie': 0, 'is': 1, 'a': 2, 'That': 3, 'This': 4}获取词频统计的结果列表most_common_words = get_most_common_words(5, text1, text2, text3, text4, text5)记录常用单词的位置，用来进行相似度的判断std_pos_dict = lookup_pos(most_common_words)计算新文本常用词的词频分别为多少new_text = 'That one is a good movie. This is so good!' # 定义新文本freq_vec = [0] * n # 初始化向量，用来存储常用词的词频new_words = nltk.word_tokenize(new_text) # 分词处理在“常用单词列表”上计算词频，当出现常用词频时，统计一次for new_word in new_words:if new_word in list(std_pos_dict.keys()):freq_vec[std_pos_dict[new_word]] += 1print(freq_vec) ---&gt; [1, 2, 1, 1, 1]文本分类的分析方法TF-IDF （词频-逆文档频率）介绍TF-IDF的作用用来描述一个单词对一篇文章的重要程度TF, Term Frequency（词频）表示某个词在该文件中出现的次数IDF，Inverse Document Frequency（逆文档频率）用于衡量某个词普 遍的重要性。TF-IDF（词频-逆文档频率)TF-IDF = TF * IDF 用来统计个单词对于一篇文章的重要程度代码示例]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[情感分析-文本情感分析]]></title>
    <url>%2F2018%2F05%2F23%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[对文本进行情感分析的流程导入模块(NaiveBayesClassifier)import nltkfrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsfrom nltk.classify import NaiveBayesClassifier 导入nltk 中的朴素贝叶斯定义文本处理的方法def proc_text(text):raw_words = nltk.word_tokenize(text) # 对文本进行分词处理wordnet_lematizer = WordNetLemmatizer() # 对文本进行归一化处理words = [wordnet_lematizer.lemmatize(raw_word) for raw_word in raw_words] # 对文本进行归一化处理filtered_words = [word for word in words if word not in stopwords.words('english')] # 去处停用词# 将用来学习的单词，构建成一个字典，键是每个词，值是True/False，True代表该词在文本中return {word: True for word in filtered_words} 返回训练样本对模型进行学习和测试if __name__ == &quot;__main__&quot;:构造5个用来学习的句子text1 = 'I like the movie so much!'text2 = 'That is a good movie.'text3 = 'This is a great one.'text4 = 'That is a really bad movie.'text5 = 'This is a terrible movie.'构造训练样本：是由每一个句子的词构成的字典和句子所对应的分数（情感值），构建成的列表train_data = [[proc_text(text1), 1],[proc_text(text2), 1],[proc_text(text3), 1],[proc_text(text4), 0],[proc_text(text5), 0]]使用朴素贝叶斯模型训练（默认会将文本数据向量化处理），返回贝叶斯模型nb_model = NaiveBayesClassifier.train(train_data)测试模型text6 = 'That is a bad one.' # 测试文本text7 = 'That is a good one!' # 测试文本nb_model.classify(proc_text(text6)) ---&gt;0 对测试文本进行测试并打分nb_model.classify(proc_text(text7)) ---&gt;1 对测试文本进行测试并打分]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理的流程]]></title>
    <url>%2F2018%2F05%2F22%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[自然语言的文本处理流程备注：汉语的自然语言处理流程和英语的自然语言处理流程差了一步词形归一化英文文本的处理流程0.语料库的使用（没有实际的作用）--查看语料库的信息使用方法导入语料库，以导入布朗大学语料库为例（brown）import nltkfrom nltk.corpus import brown brown为语料库，需要提前使用下载器进行下载查看语料库包含的类别（没有实际作用）print(brown.categories())查看语料库中包含的句子和单词（没有实际作用）brown.sents()brown.words()1.文本分词处理分词的作用将句子拆分成 具有语言语义学上意义的词中、英文分词区别：英文中，单词之间是以空格作为自然分界符的中文中没有一个形式上的分界符，分词比英文复杂的多分词处理的方法导入模块import nltk准备需要分词的句子或者文章text = &quot;Python is a high-level programming language, and i like it!&quot;对文本进行分句处理（只有当文本太大，需要对每一句话进行处理的时候使用）seg_list = nltk.sent(text) 对文本进行分句，返回有多个句子组成的列表对文本进行分词 (需要事先安装 punkt 分词模型)seg_list = nltk.word_tokenize(text) 对文本进行分词，返回分词后单词组成的列表分词后的结果：seg_list['Python', 'is', 'a', 'high-level', 'programming', 'language', '!']1.1.词性标注(Part-Of-Speech)-获取每个单词的词性，可选步骤（根据需要使用）词性标注的方法导入模块import nltk分词后进行词性标注words = nltk.word_tokenize('Python is a widely used programming language.')nltk.pos_tag(words) 需要下载 averaged_perceptron_tagger分词后的结果[('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('widely', 'RB'), ('used', 'VBN'), ('programming', 'NN'), ('language', 'NN'), ('.', '.')]、常见的词性2.词形归一化（只有处理英文问本的时候才需要）词形归一的作用类似 look, looked, looking这些不同时态的单词，会影响语料学习的准确度通过词形归一化，可以将这些单词转化为同一的单词①词干提取(stemming)作用可以将单词的ing, ed去掉，只保留单词主干NLTK中常用的词干提取工具stemmer：PorterStemmer, SnowballStemmer, LancasterStemmer词干提取的方法PorterStemmer（只支持英语） 的使用方法导入模块from nltk.stem.porter import PorterStemmer创建工具对象porter_stemmer = PorterStemmer()对单词进行词干提取porter_stemmer.stem('looked') 返回进行词干提取后的单词porter_stemmer.stem('looking') 返回进行词干提取后的单词提取后的结果look lookSnowballStemmer（支持多种语言）的使用方法导入模块from nltk.stem,snowball import SnowballStemmer创建工具对象，并选择语言SnowballStemmer.languages 查看支持的语言snowball_stemmer = SnowballStemmer('english')对单词进行词干提取snowball_stemmer.stem('looked') 返回进行词干提取后的单词snowball_stemmer.stem('looking')) 返回进行词干提取后的单词提取后的结果look lookLancasterStemmer（在大文本提取的时候速度更快） 的使用方法导入模块from nltk.stem.lancaster import LancasterStemmer创建工具对象lancaster_stemmer = LancasterStemmer()对单词进行词干提取lancaster_stemmer.stem('looked') 返回进行词干提取后的单词lancaster_stemmer.stem('looking') 返回进行词干提取后的单词提取后的结果look look②词形归并(lemmatization)词形归并的作用可以将单词的各种词形归并成一种形式如am, is, are -&gt; be, went-&gt;go，boxes-&gt;box指明词性可以更准确地进行词形归并词形归并的使用方法：WordNetLemmatizer导入模块from nltk.stem import WordNetLemmatizer创建工具对象wordnet_lematizer = WordNetLemmatizer() 需要下载wordnet语料库进单词进行词形归并wordnet_lematizer.lemmatize('cats')) 默认按照名词处理wordnet_lematizer.lemmatize('boxes')) 默认按照名词处理wordnet_lematizer.lemmatize('are')) 默认按照名词处理wordnet_lematizer.lemmatize('went')) 默认按照名词处理print(wordnet_lematizer.lemmatize('are', pos='v')) 指明词性print(wordnet_lematizer.lemmatize('went', pos='v')) 指明词性用pos指明词性可以更准确地进行lemma, lemmatize 默认为名词3.去除停用词去除停用词的作用为节省存储空间和提高搜索效率，NLP中会自动过滤掉某些字或词停用词都是人工输入、非自动化生成的，形成停用词表停用词的种类语言中的功能词，如the, is…词汇词，通常是使用广泛的词，如want停用词表http://www.ranks.nl/stopwords获取指定语言的停用歌词库stopwords.words('language_name')注意：所有停用词都是小写的去除停用词的方法导入模块from nltk.corpus import stopwords 需要下载stopwords获取未进行去除停用词的单词列表(已经进行了分词处理)words = ['Python', 'is', 'a', 'widely', 'used', 'programming', 'language', '.']对所有单词进行去除停用词filtered_words = [word for word in words if word not in stopwords.words('english')]去除停用词之后的结果：filtered_words['Python', 'widely', 'used', 'programming', 'language', '.']英文文本处理流程示例jieba分词器的介绍jieba分词器的介绍jieba分词器的三种模式全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义，适合情感分析；精确模式：试图将句子最精确地切开，适合文本分析，适合词频统计；搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。安装方式pip install jiebapip3 install jieba导入jieba库的方法import jieba 使用jieba分词器进行分词使用jieba分词器对词性词性进行标注导入jieba词性标注工具import jieba.posseg as pseg进行分词，并进行词性标注words = pseg.cut(&quot;我爱北京天安门&quot;) 返回的是分词后，有单词和词性构成的元组，并组成的列表结果打印for word, flag in words:print('%s %s' % (word, flag)) --&gt;我 r爱 v北京 ns天安门 ns中文文本的处理流程1.分词处理的方法导入jieba分词器（需要先pip install jieba）import jieba准备需要分词的句子或者文章text = '欢迎来到英雄联盟'对文本进行分词seg_list = jieba.cut(text, cut_all=True)cut_all: 表示使用全模式来进行分词分词后的结果：seg_list['欢迎', '来到', '英雄', '联盟', '英雄联盟']2.使用jieba分词器分词并标注词性--可选步骤（默认是精确模式）导入jieba词性标注工具import jieba.posseg as pseg准备需要分词的句子或者文章text = '欢迎来到英雄联盟'进行分词，并进行词性标注words_list = pseg.cut(text) 返回的是分词后，的单词和词性的对应结果分词后的结果:words_list3.去除停用词中文停用词表中文停用词库哈工大停用词表四川大学机器智能实验室停用词库百度停用词列表准备分词后的列表seg_list = ['我', '是' , '一个', '好人']读取中文停用表stopword_list = [line.rstrip() for line in open('哈工大停用词表.txt', 'r', encoding='utf-8')]去停用词操作filtered_list = [seg for seg in seg_list if seg not in stopword_list]取出后的结果: filtered_list['好人']]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理工具介绍]]></title>
    <url>%2F2018%2F05%2F21%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[NLTK 的介绍NLTK(Natural Language Toolkit)简介NTLK是著名的Python自然语言处理工具包，但是主要针对的是英文处理。NLTK配套有文档，有语料库，有书籍。NLP领域中最常用的一个Python库开源项目自带分类、分词等功能强大的社区支持语料库的概念语料库，语言的实际使用中真实出现过的语言材料http://www.nltk.org/py-modindex.htmlNTLK及其相关库的安装方法安装NTLK的方法在官网进行安装在NTLK的主页详细介绍了如何在Mac、Linux和Windows下安装NLTK：http://nltk.org/install.html 使用Anaconda直接进行安装Anaconda直接包含了NTLK相关的包和库使用pip进行安装pip install ntlk安装NTLK的相关包使用命令弹出包安装工具import nltknltk.download() 接下来回弹出栏一个下载工具框弹出下面的框，建议安装所有的包安装的内容Corpors ：语料库，常用语料库(brown)，停用词库(stopwords)，词形归并工具(wordbet)等Model：模型，比如词性标注(Tagger)，分词(Punkt)，词干提取(Porter)模型等常用模型]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell基础-表达式流程控制]]></title>
    <url>%2F2017%2F08%2F17%2F09.shell%2Fshell%E8%AF%AD%E6%B3%95%2Fshell%E5%9F%BA%E7%A1%80-%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[shell表达式shell的验证表达式表达式的表达方式方法1[ 表达式 ]方法2test 表达式备注表达式两边必须要有空格，表达式之间需要有空格如果结果为真返回0，结果不为真则返回其他表达式由变量和运算符构成表达式的运算符shell的数字运算符表达式格式[ n1 –gt n2 ]表达式参数-gt 大于 -lt 小于 -eq 等于 -ne 不等于shell的字符串运算符表达式格式[ n1 == n2 ][ -z n1]表达式参数== 判断两个字符串是否相等 != 判断连个字符串是否不一致 -z 判断字符串长度是否为0 -n 判断字符串长度是否不为0shell的文件运算符表达式格式[ 文件运算符 文件名 ] 文件运算符-f 判断输入的内容是否是一个文件 -d 判断输入的内容是否是一个目录 -x 判断输入的文件是否有执行权限 -e 判断输入的内容表示的文件是否存在 -r 判断文件是否可读 -w 判断文件是否可写shell逻辑表达式&amp;&amp; 并表达式格式 命令白表达式1 &amp;&amp; 命令表达式2 效果如果命令1执行成功，则执行命令2 如果命令1执行失败，则不执行命令2 || 或 表达式格式命令1 || 命令2 效果如果命令1执行成功，则不执行命令2 如果命令1执行不成功，则执行命令2 python的流程控制shell流程控制的分类简单的流程控制选择流程if语句单if语句 双if语句 多if语句case语句循环流程for循环while循环until循环复杂的流程控制函数shell中流程控制规则缩进shell中的缩进没有作用，只是为了好看符号成对写 [ ] ( ) { }流程控制语句一定要先写完选择流程控制语句if语句单if语句写法if 条件语句；then执行语句fi双if语句if 条件语句1 ；then执行语句1else执行语句2fi多if语句if 条件语句1；then执行语句1elif 条件语句2；then执行语句2else执行语句3ficase语句case语句写法case 值 in 值1) 执行语句1 ;; 值2) 执行语句2 ;; 值3) 执行语句3 ;; esac循环控制语句for循环语句for 条件表达式；do执行语句donewhile循环while 条件表达式；do执行语句doneuntil循环语句使用方法until 条件表达式；do执行语句doneuntil语句的特点当条件不成立时，执行循环，一旦条件成立则结束循环函数函数的使用场景不带参数的函数，应用在重复功能封装的情况下带参数的函数，应用在条件判断执行的情况下定义函数func_name(){args=$n (不写此行代表函数不需要接受参数)函数体}调用函数func_name args1 args2将shell脚本设置为系统环境变量的方法1、直接在命令行中设置PATH# PATH=$PATH:/usr/local/apache/bin使用这种方法,只对当前会话有效，也就是说每当登出或注销系统以后，PATH设置就会失效。2、在profile中设置PATH# vi /etc/profile找到export行，在下面新增加一行，内容为：export PATH=$PATH:/usr/local/apache/bin。注：＝ 等号两边不能有任何空格。这种方法最好,除非手动强制修改PATH的值,否则将不会被改变。编辑/etc/profile后PATH的修改不会立马生效，如果需要立即生效的话，可以执行# source profile命令。3、在当前用户的profile中设置PATH# vi ~/.bash_profile修改PATH行,把/usr/local/apache/bin添加进去,如：PATH=\$PATH:$HOME/bin:/usr/local/apache/bin。# source ~/.bash_profile让这次的修改生效。注：这种方法只对当前用户起作用的,其他用户该修改无效。]]></content>
      <categories>
        <category>shell</category>
        <category>shell语法</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell基础-变量]]></title>
    <url>%2F2017%2F08%2F16%2F09.shell%2Fshell%E8%AF%AD%E6%B3%95%2Fshell%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[shell的介绍什么是shell shell是一种命令解释器，而linux的shell则是用来保护内核的，或者和内核进行交互的 Python脚本首行的#！/bin/python其实就是指定了Python脚本运行的时候使用的解释器shell的分类窗口式的shell Windows的桌面 各个发行版的linux的桌面 命令行shell windows cmd(命令行提示符) linux sh … … bash linux查看默认shell解释器的方法 echo $SHELL 常见的linux发行版本ubuntu 界面化操作比较方便 centos 6.5-6.7 redhat 收费 debain kali 渗透测试shell命令与shell脚本的介绍shell命令什么是shell命令一条命令,包括shell自身的命令以及linux系统命令 特点: 逐行输入，逐行输出 无法重复使用shell脚本什么是shell命令脚本一堆shell命令的组合特点: 一次输入，逐行执行 脚本可以重复使用，提高工作效率shell脚本的规范命名 命名要有意义，最好能够表明脚本功能 后缀一般以.sh结尾 首行是并且必须是命令解释器 默认首行使用 #!/bin/bash 特殊情况可以根据环境修改 版权信息 首行之下必须使用注释写明脚本的基本信息 脚本名，功能，编写时间，编写人，联系方式等 脚本的执行顺序 根据脚本编写顺序，从上到下依次执行 成对编写 () [] {} “” ’’shell执行方式使用命令解释器加载脚本文件 执行方式 bash 脚本文件 使用场景: 生产常用 使用脚本的绝对路径或相对路径执行 执行方式直接用./文件路径执行脚本文件 当在脚本所在目录的时候，使用相对路径要加 ./脚本文件名使用条件条件: 脚本有用可执行权限 脚本首行选定了解释器 使用场景: 私下 source或.的形式 执行方式 source 或 . 脚本文件 将脚本中的变量都导入到当前的环境作用加载文件，将写在文件中的配置信息马上生效 使用场景: 保证环境变量的一致性运行方式让shell脚本在后台运行&amp; 运行shell脚本的时候在后面加上&amp;，代表以后台运行的方式运行的输出1&gt; 指标准信息输出路径（也就是默认的输出方式）2&gt; 指错误信息输出路径2&gt;&amp;1 指将标准信息输出路径指定为错误信息输出路径（也就是都输出在一起）习惯上标准输入（standard input）的文件描述符是 0标准输出（standard output）是 1标准错误（standard error）是 2shell变量shell的变量介绍变量包括两部分: （变量名就是存放变量值的容器，容器不变，但是存放的内容可变）变量名 不变 变量值 变化 shell中变量的统一格式基本格式：变量名=变量值注意：等号左右两边没有空格shell中的变量的分类本地变量全局变量内置变量本地变量本地变量定义在当前的shell或者当前脚本存续期间可用的变量数据变量的定义方式（三种）变量名=变量值 变量值必须是连续的，不能有空格，不能有特殊字符 变量名=‘变量值’ 变量值可以是不连续的，能有空格，特殊字符原样，引号中的是什么就是什么 变量名=&quot;变量值&quot; 值可以不连续，如果值中使用已定义变量，则会先调用，再赋值命令变量的定义定义方式变量名=`命令` 注意命令两边的符号变量名=$(命令) 执行顺序: 先运行命令，将命令执行的结果赋值给变量名全局变量什么是全局变量系统所有环境都可以使用的变量 查看全局变量方法env命令 定义方法分步的方式变量名=变量值 （定义一个本地变量，可以是数据或者是命令） export 变量名 （声明为全局变量） 同时定义 export 变量名=变量值 （变量可以是数据或者是命令）内置变量什么是内置变量ash命令内部已经定义好的变量，可以直接使用，不需要定义使用方法 和shell脚本有关的内置变量$0 获取当前脚本文件名称 $# 获取当前脚本获取的参数的个数 $n 获取当前脚本获取到的第n个参数 $? 获取上一条命令的执行情况，0表示成功，1表示失败，127表示找不到命令 $$ 脚本运行时使用的进程号 $@ 获取所有的参数获取hash序列$RANDOM 获取随机的5为随机数$RANDOM | md5sum(有的bash是md5) 将变量进行md5序列化返回的是一个序列化的字符串和一个 - 符号\$RANDOM | md5sum | awk '{printf $1}' 将awk接收到的第一个参数打印出来（即序列化结果）和字符串相关的内置变量截取使用方法 ${var_name:start:n} 备注var_name是字符串变量名 start表示的截取的起始位置 start整数 从开头开始 start:n 为 0-n 从结尾往前推n个位开始 n 表示截取字符的个数和默认值相关的内置变量第一种 var_name=$1 ${var_name:-default} default表示默认值,如果没有输入参数，default将会被使用 第二种 var_name=$1 ${var_name:+default} 将会无视输入参数，直接输出设定好的默认值查看变量的方式$变量名 使用场景: 私下使用时 &quot;$变量名&quot; 使用场景: 调用变量时 &quot;${变量名}&quot; 标准的使用方法用场景: 脚本中，规范化作业时变量的运算shell中变量运算方法 1.使用let进行运算 let n=n+1 let n+=1 实现自加 2.使用$(())实现运算 n=\$(($n+1))变量的其他操作将一个本地变量设置为只读，不允许删除变量，关闭终端才能删除 readonly 变量名 删除变量 unset 变量名]]></content>
      <categories>
        <category>shell</category>
        <category>shell语法</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单元测试]]></title>
    <url>%2F2017%2F08%2F10%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2F%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[单元测试概念为什么需要单元测试Web程序开发过程一般包括以下几个阶段：[需求分析，设计阶段，实现阶段，测试阶段]。其中测试阶段通过人工或自动来运行测试某个系统的功能。目的是检验其是否满足需求，并得出特定的结果，以达到弄清楚预期结果和实际结果之间的差别的最终目的测试的分类单元测试（与开发人员相关）对单独的代码块(例如函数)分别进行测试,以保证它们的正确性集成测试对大量的程序单元的协同工作情况做测试系统测试同时对整个系统的正确性进行检查,而不是针对独立的片段什么是单元测试当我们的某些功能代码完成后，为了检验其是否满足程序的需求。可以通过编写测试代码，模拟程序运行的过程，检验功能代码是否符合预期。单元测试就是开发者编写一小段代码，检验目标代码的功能是否符合预期。通常情况下，单元测试主要面向一些功能单一的模块进行。在Web开发过程中，单元测试实际上就是一些“断言”（assert）代码，判断执行结果是否符合预期单元测试步骤先定义一个类，继承自unittest.TestCaseimport unittestclass TestClass(unitest.TestCase): ...在测试类中定义两个方法setUp测试案例启动前调用，可以设置初始化代码def setUp(self):client = app.test_client() 获取flask中对应的测试客户端对象，通过它模拟发送请求app.testing = True 设置app为测试模式，异常会定位到内测试代码导致异常的代码passtearDown所有测试案列完成之后调用，可以用来关闭上下文管理器等def tearDown(self):...在测试类中，编写测试代码from main import app import jsondef test_function(self): 注意测试的方法一定要以test开头，才能被识别self.client.post('/login', data={ }) 通过测试客户端发起post请求，传入数据，获取响应response_data = response.data 获取响应中响应体的数据，字符串类型数据resp_dict = json.loads(resp_data) 将字符串转换成字典self.assert 。。。。。。 再通过获取的值进行assert断言运行测试代码第一种方法直接右键运行第二种方法unittest.main() 通过调用main方法来启动测试代码登录测试数据库测试程序断言的使用（assert）程序断言的作用断言就是判断一个函数或对象的一个方法所产生的结果是否符合你期望的那个结果。python中assert断言是声明布尔值为真的判定，如果表达式为假会抛出异常。单元测试中，一般使用assert来断言结果。程序断言的使用示例assert isinstance(num1, int) 断言num1是一个int类型，不是的话抛出异常常用的断言方法系统自带断言assert 后面的布尔表达式为True，则passunitest.TestCase封装的断言，通过self.assert来调用assertEqual 如果两个值相等，则passassertNotEqual 如果两个值不相等，则passassertTrue 判断bool值为True，则passassertFalse 判断bool值为False，则passassertIsNone 不存在，则passassertIsNotNone 存在，则pass]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-WTF表单]]></title>
    <url>%2F2017%2F08%2F08%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-WTF%E8%A1%A8%E5%8D%95%2F</url>
    <content type="text"><![CDATA[Web表单与Flask-WTF拓展web表单它是HTML页面中负责数据采集的部件。表单有三个部分组成：表单标签、表单域、表单按钮。表单允许用户输入数据，负责HTML页面数据采集，通过表单将用户输入的数据提交给服务器。Flask-WTF拓展Flask-WTF是一个Html表单的拓展，可以使用python代码写表单，然后传递到模板中渲染Flask-WTF扩展封装了WTForms，有验证表单数据的功能；FORM表单提交的实现通过HTML页面实现表单提交和验证在HTML中实现表单页面定义路由和视图函数，接收表单提交的数据进行验证GET请求获取表单，POST请求提交表单通过Flask-WTF实现表单的提交和验证导入FlaskForm表单、标准字段、calidators验证器from flask-wtf import FlaskFormfrom wtforms import 标准字段from wtforms.validators import 验证器所有的标准字段和验证器示例：标准字段WTForms常用验证器设置SECRET_KEY因为FlaskForm会自动的进行csrf的验证，生成csrf密文会使用到SECRET_KEY过程中会使用的用flash() 记录表单验证错误信息，flash会用到session，所以会用到SECRET_KEYapp.secret_key = &quot;liukaijian&quot;自定义表单类定义路由和视图函数GET请求：创建表单实例，将其传递给HTMl模板进行渲染POST请求：创建表单实例，FlaskForm实例会对表单的提交结果进行验证，如果验证通过，返回成功内容定义HTMl页面接收视图函数传递过来的表单对象，并获取其属性将其渲染到标签中因为FlaskForm会自动的进行csrf的验证，所以我们要设置一个csrf_token标签csrf_token 验证的设置csrf_token设置1.2.要首先配置SECRET_KEY3.在创建了表单类的时候，是默认配置了csrf_token的，但是在未使用表单类的时候，需要手动开启csrf验证csrf_token验证在响应在中给cookie添加csrf_token (在使用表单的时候默认添加，不使用表单手动添加)生成csrf_tokenfrom flask_wtf.csrf import generate_csrftoken = generate_csrf()response.set_cookie('csrf_token', token)在前端的请求给headers添加csrf_token参数]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-SQLAlchemy数据库交互]]></title>
    <url>%2F2017%2F08%2F06%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-SQLAlchemy%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%A4%E4%BA%92%2F</url>
    <content type="text"><![CDATA[Flaks-SQLALchemy扩展在Flask中使用数据库要用到Flask-SQLALchemy扩展，Flask-SQLALchemy应用了SQLALchemy框架SQLAlchemy是一个关系型数据库框架，它提供了高层的ORM和底层的原生数据库的操作。SQLALchemy 实际上是对数据库的抽象，让开发者不用直接和 SQL 语句打交道，而是通过 Python 对象来操作数据库，在舍弃一些性能开销的同时，换来的是开发效率的较大提升Django与Flask操作数据库的对比 使用SQLAlchemy操作数据库安装flask-sqlalchemy扩展pip install flask-sqlalchemy 安装sqlalchemy数据库拓展配置SQLALchemy导入SQLAlchemyfrom flask import flaskfrom flask_sqlalchemy import SQLAlchemy配置数据库连接的URI并初始化SQLAlchemy配置数据库的连接方式app = Flask(__name__)app.config[&quot;SQLALCHEMY_DATABASE_URI&quot;] =&quot;mysql://root:mysql@127.0.0.1:3306/py3&quot;配置是否追踪数据库的修改操作，即数据库有修改就会提示app.config[&quot;SQLALCHEMY_TRACK_MODIFICATIONS&quot;] = False初始化SQLAlchemydb = SQLAlchemy(app)SQLAlchemy其他的配置创建模型类（对应数据库的表）创建模型类应用到的方法和参数一对多模型__tablename__: 创建表的表名（不写默认为类型小写）db.Column: 代表创建表的字段第一个参数为数据类型，其他的为该字段的约束条件db.ForeignKey() 代表设置外键字段db.relationship （前提是设置了外键）设置表的关系属性，在一对多关系中常设置在一的一端，可以通过一端查询多端数据backref： 反向引用，可以通过关联表的另一端的实例，查询到该表的数据lazy: 默认为subquery设置为subquery，则会在设置关系字段的一端加载完对象后，就立即加载与其关联的对象，这样会让总查询数量减少，但如果返回的条目数量很多，就会比较慢设置为 subquery 的话，author.books 返回所有数据列表设置为动态方式dynamic，这样关联对象会在被使用的时候再进行加载，并且在返回前进行过滤，如果返回的对象数很多，或者未来会变得很多，那最好采用这种方式设置为 dynamic 的话，role.users 返回模型实例对象__repr__方法： 代表打印实例的时候，显示的内容代码示例：多对多模型代码示例常用的SQLALchemy字段类型常用的SQLALchemy列选项常用的SQLALchemy关系选项数据库表的操作在数据库中创建对应模型类的表db.creat_all()清空数据库中的所有表db.drop_all()数据库增、删、改、查操作数据库操作原理在Flask-SQLAlchemy中，插入、修改、删除操作，均由数据库会话管理。会话用db.session表示。在准备把数据写入数据库前，要先将数据添加到会话中然后调用 commit() 方法提交会话。在Flask-SQLAlchemy中，查询操作是通过query对象操作数据。最基本的查询是返回表中所有数据，可以通过过滤器进行更精确的数据库查询。数据库会话操作对操作进行提交，默认是不会进行提交的db.session.commit() 对数据库操作进行回滚操作db.session.rollback()对数据进行删除db.session.delete()取消和数据库的连接db.session.remove()数据库的增删改查添加数据（创建数据实例，向表中添加）单条添加rol = Role(name='admin')db.session.add(ro1)db.session.commit()批量添加数据rol = Role(name='admin')rol = Role(name='admin')db.session.add_all([ rol1, rol2 ])db.session.commit()删除数据（获取到数据实例，再进行删除）db.session.delete(rol1)修改数据（获取到数据实例，在进行修改）rol.name = '改变后的值'查询数据查询所用方法limit（）返回范围内的查询结果查询所用过滤器filter_by精确查询User.query.filter_by(id=4).first()filter模糊查询User.query.filter(User.name.endswith('g').all()User.query.filter(User.name.startswith('a')).all()filter条件查询等查询User.query.filter(User.id==4).first()非查询from sqlalchemy import not_User.query.filter(not_(User.name=='wang')).all()与查询from sqlanchemy import and_User.query.filter(and_(User.name.startswith('li'), User.email.startswith('li'))).all()可以直接不使用and_进行与查询，默认写多个条件就是与的关系或查询from sqlanchemy import or_User.query.filter(or_(User.password=='123', User.email.endswith('li'))).all()范围查询User.query.filter(User.id.in_([1, 2, 3, 4])).all()排序查询User.query.order_by(User.email.desc()).all()分页查询paginate = User.query.paginate(2, 3, False)参数分别：查询第几页，每页几条数据，是否有错误输出paginate.page 当前页paginate.pages 所有页数paginate.items 当前分页当中过得所有模型对象关联查询在一端查询多段数据rol1. users.all()users为在模型类中设置的关系字段通过一端的记录对象来查询多端的所用关联对象（前提是设置了关系字段）在多端查询一端的数据user.role.first()role为关系字段的backref属性的内容]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Script命令行、migrate数据库迁移]]></title>
    <url>%2F2017%2F08%2F06%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-Script%E5%91%BD%E4%BB%A4%E8%A1%8C%E3%80%81Flask-migrate%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[Flask-Script扩展命令行Flask-Script的作用可以实现在终端中使用命令行的方式对项目进行操作Flask-Script的配置在程序中导入拓展from flask_script import Manager创建manager实例manager = Manager(app) app为已经创建好的flask应用实例运行manager实例manager.run( )查看所有Flask-Script扩展命令python 文件名 runserver --help代码示例 --启动服务器通过在终端中使用命令启动python hello.py runserver (-host ip地址) 设置服务器在哪个机器的端口上监听客户端请求--数据库的迁移迁移初始化python 文件 db init 生成迁移文件python 文件 db migrate -m&quot;版本名(注释)&quot;数据库更新python 文件 db upgrade查看历史迁移记录python 文件 db history进行数据库迁移版本回退python 文件 db downgrade(upgrade) 版本号]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Migrate数据库迁移]]></title>
    <url>%2F2017%2F08%2F04%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-Migrate%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[数据库迁移扩展Flask-Migrate什么是数据库的迁移在开发过程中，需要修改数据库模型，而且还要在修改之后更新数据库。最直接的方式就是删除旧表，但这样会丢失数据。更好的解决办法是使用数据库迁移框架，它可以追踪数据库模式的变化，然后把变动应用到数据库中。Flask-Migrate拓展在Flask中可以使用Flask-Migrate扩展，来实现数据迁移。并且集成到Flask-Script中，所有操作通过命令就能完成。为了导出数据库迁移命令，Flask-Migrate提供了一个MigrateCommand类，可以附加到flask-script的manager对象上。使用Flask-migrate的步骤前提已经通过Alchemy拓展创建了数据库表的类安装拓展pip install flask-migrate通过Flask-Script注册迁移命令from flask import Flaskfrom flask_script import import Managerfrom flask_sqlalchemy import SQLAlchemyfrom flask_migrate import Migrate, MigrateCommandapp = Flask(__name__)db = SQLAlchemy(app)manager = Manager(app)Migrate(app, db, 'migrate_dir') 关联迁移，将app和数据库的操作对象传入migrate_dir 为迁移文件存储的文件夹，默认为migratesmanager.add_comment('db', MigrateCommand) 注册迁移命令到Flask-Script第一个参数是，在命令行中通过它来使用迁移相关命令初始化迁移，创建迁移仓库(创建migrations文件夹，所有迁移文件都放在里面)初始化迁移命令python database.py db init 生成迁移文件（每次对数据库模型类的修改都需要重新迁移）生成迁移文件命令python database.py db migrate -m 'initial migration'-m : 表示对迁移版本的 生成迁移的结果：自动创建一个迁移脚本包含两个函数，根据模型定义和数据库当前状态的差异，生成upgrade()和downgrade()函数两个函数操作内容不一定完全正确，有可能会遗漏一些细节，需要进行检查，自行进行改动upgrade()：函数把迁移中的数据库的改动同步到数据库中downgrade()：函数将upgrade()的改动回滚删除更新数据库表python database.py db upgrade查看历史迁移记录python app.py db history回滚到指定版本python app.py db downgrade 指定版本号实际使用的操作顺序-总结1.python 文件 db init2.python 文件 db migrate -m&quot;版本名(注释)&quot;3.python 文件 db upgrade 然后观察表结构4.根据需求修改模型5.python 文件 db migrate -m&quot;新版本名(注释)&quot;6.python 文件 db upgrade 然后观察表结构7.若返回版本,则利用 python 文件 db history查看版本号8.python 文件 db downgrade(upgrade) 版本号]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Mail邮件拓展]]></title>
    <url>%2F2017%2F08%2F02%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-Mail%E9%82%AE%E4%BB%B6%E6%8B%93%E5%B1%95%2F</url>
    <content type="text"><![CDATA[Flask-Mail邮件拓展在开发过程中，很多应用程序都需要通过邮件提醒用户，Flask的扩展包Flask-Mail通过包装了Python内置的smtplib包，可以用在Flask程序中发送邮件。Flask-Mail连接到简单邮件协议（Simple Mail Transfer Protocol,SMTP）服务器，并把邮件交给服务器发送。使用Flask-Mail邮件拓展的步骤设置邮箱授权码在视图中进行邮件的发送]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-模板]]></title>
    <url>%2F2017%2F08%2F01%2F05.Flask%2FFlask%E6%A8%A1%E5%9D%97%2FFlask%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask模块</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Blueprint蓝图]]></title>
    <url>%2F2017%2F08%2F01%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FBlueprint%E8%93%9D%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Blueprint的介绍使用Blueprint的目的为了将代码模块化，随着flask程序越来越复杂,我们需要对程序进行模块化的处理,之前学习过python的模块化管理,于是针对一个简单的flask程序进行模块化处理Blueprint的概念简单来说，Blueprint 是一个存储操作方法的容器，这些操作在这个Blueprint 被注册到一个应用之后就可以被调用，Flask 可以通过Blueprint来组织URL以及处理请求。Flask使用Blueprint让应用实现模块化。Blueprint的属性一个应用可以具有多个Blueprint可以将一个Blueprint注册到任何一个未使用的URL下比如 “/”、“/sample”或者子域名在一个应用中，一个模块可以注册多次Blueprint可以单独具有自己的模板、静态文件或者其它的通用操作方法，它并不是必须要实现应用的视图和函数的在一个应用初始化时，就应该要注册需要使用的Blueprint一个Blueprint并不是一个完整的应用，它不能独立于应用运行，而必须要注册到某一个应用中，相当于django中的app，而flask的应用实例相当于django的project蓝图的运行机制蓝图机制的介绍蓝图是保存了一组将来可以在应用对象上执行的操作，注册路由就是一种操作当在应用对象上调用 route 装饰器注册路由时,这个操作将修改对象的url_map路由表然而，蓝图对象根本没有路由表，当我们在蓝图对象上调用route装饰器注册路由时,它只是在内部的一个延迟操作记录列表defered_functions中添加了一个项当执行应用对象的 register_blueprint() 方法时，应用对象将从蓝图对象的 defered_functions 列表中取出每一项，并以自身作为参数执行该匿名函数，即调用应用对象的 add_url_rule() 方法，这将真正的修改应用对象的路由表Blueprint的基本使用方法创建蓝图对象（不在main模块中）from flask import Blueprintuser_api = Blue('user_api', __name__, static_folder， static_url_path, template_folder, url_prefix))第一个参数是创建的蓝图实例的名字，通过它来注册视图函数等第二个参数为蓝图所在模块名称,指定查找静态文件和模板文件的路径static_folder: 静态文件路径，默认为Nonestatic_url_path='py3': 访问前缀，默认和静态文件路径相等template_folder: 模板文件路径，默认为Noneurl_prefix: 访问前缀，也可以通过app在注册蓝图的时候设置在蓝图对象上进行操作，注册路由视图函数@user_api.route('/user_info')def user_info():return 'user_info'在应用对象上注册这个蓝图对象(一个应用可以注册多个蓝图对象，实现模块化)from *** import user_apiapp.register_blueprint(user_api， url_prefix='/user')url_prefix:可以指定一个url_prefix关键字参数（这个参数默认是/）应用最终的路由表 url_map中，在蓝图上注册的路由URL自动被加上了这个前缀将flask项目模块化目标效果：将项目的功能块进行查分，每一类功能创建一个目录，通过蓝图的功能将每一组功能分别创建在每一个目录下，每个目录自带static和templates（类似django项目），实现项目的模块化；实现步骤创建单个小应用的文件夹（以cart为例）在__init__中初始化蓝图，并初始化视图函数from flask import Blueprintcart_api = Blueprint('cart_api', __name__， static_folder， static_url_path, template_folder, url_prefix)from views import cart_list 因为初始化蓝图的时候没有走注册视图函数的代码，一定要手动导入在view中使用蓝图注册路由from . import cart_api@cart_api.route('/cart_list'):def cart_list():return 'cart_list'在main中注册蓝图from cart import cart_apiapp.register_blueprint(cart_api, url_prefix='/cart')备注：如果在根目录下的templates中存在和蓝图templates同名文件，则系统会优先使用根目录templates中的文件所以，尽量不要将模板文件设置同名]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-路由视图]]></title>
    <url>%2F2017%2F07%2F30%2F05.Flask%2FFlask%E6%A8%A1%E5%9D%97%2FFlask-%E8%B7%AF%E7%94%B1%E8%A7%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Flask对请求的处理的原理flask处理请求过程所有Flask程序必须有一个应用程序实例，相当于django的project。当客户端想要获取资源时，一般会通过浏览器发起HTTP请求。此时，Web服务器使用WSGI（Web Server Gateway Interface）协议，把来自客户端的所有请求都交给Flask程序实例，程序实例使用Werkzeug来做路由分发（URL请求和视图函数之间的对应关系）。根据每个URL请求，找到具体的视图函数并进行调用。Flask实现路由分发的方式在Flask程序中，路由的实现一般是通过程序实例的装饰器实现。Flask视图函数返回的内容字符串内容：将视图函数的返回值作为响应的内容，返回给客户端(浏览器)HTML模版内容：获取到数据后，把数据传入HTML模板文件中，模板引擎负责渲染HTTP响应数据，然后返回响应数据给客户端(浏览器)Flask应用程序（示例）实现流程1.创建一个项目文件夹2.创建hello.py的文件3.导入flask类4.创建Flask实例，Flask类接收参数name，它会指向程序所在的模块5.通过装饰器的方式，将路由映射到视图函数index6.启动web服务器，Flask应用程序实例的run方法启动WEB服务器，默认5000端口代码flask应用实例的使用和基本配置flask实例初始化from flask import Flaskapp = Flask(import_name, static_url_path, static_folder, template_folder)import_name: 模块名,一般写__name__static_url_path: 静态文件访问前缀static_folder: 默认‘static’template_folder: 默认‘templates’配置参数第一种方法：通过类来配置先定义配置相关类class Config(object):DEBUG = True在通过类来配置flask实例app.config.from_object(Config)第二种方法：通过文件的方式来配置创建配置文件，并编辑相关配置DEBUG = True通过文件来配置flask实例app.config.from_pyfile(“yourconfig.cfg”)第三种方法: 单条配置app.config['DEBUG'] = True第四种方法: 在启动应用实例的时候直接添加配置app.run(debug = True)运行flask引用程序运行方法app.run（）指定参数运行app.run(host=&quot;0.0.0.0&quot;, port=5000, debug = True)可以通过传入参数实现指定端口，设置主机，添加配置的效果host为0.0.0.0的时候，本地和局域网内都可以访问读取配置参数app.config.get() 读取配置信息current_app.config.get() 在视图中读取配置信息lask应用实例的其他属性\方法app.url_map 获取当前app所有的路由和视图函数的映射，默认包含了静态文件的映射关系app.url_map.converts 返回所有路由转换器路由介绍和配置Flask装饰器路由的实现：Flask实现路由的核心：Werkzeug-实现路由、调试和Web服务器网关接口。Werkzeug是一个遵循WSGI协议的python函数库。其内部实现了很多Web框架底层的东西，比如request和response对象；与WSGI规范的兼容；支持Unicode；支持基本的会话管理和签名Cookie；集成URL请求路由等。Werkzeug库的routing模块负责实现URL解析。不同的URL对应不同的视图函数，routing模块会对请求信息的URL进行解析，匹配到URL对应的视图函数，以此生成一个响应信息。routing模块内部有： Rule类（用来构造不同的路由URL对象），存储路由和视图的映射，Map类（存储所有的URL规则），将多个Rule存入map中，BaseConverter的子类(存储的是参数匹配规则)，存储所有的url匹配规则MapAdapter类（负责具体URL匹配的工作），拿到url与Rule进行匹配路由匹配视图函数的规则路由在匹配过程中，至上而下依次匹配；当匹配到视图函数的时候便会将请求信息传递给对应视图函数，不会再向下匹配；所以当同一个路由匹配多个视图函数的时候，只会去执行第一个视图函数的业务代码；限定请求方式配置方法使用 methods 参数指定可接受的请求方式，可以是多种，默认是GETFlask会默认就含有DEAD、OPTION请求方式代码示例指定路由参数配置方法将参数部分用尖括号&lt;&gt;括起来，括号中的部分代表是url中的参数部分，默认是字符串类型指定的参数会传递给视图函数代码示例路由转换器路由转换器的作用可以限定路由参数的内容可以通过转化器方法对匹配到的路由参数做相应的处理使用路由转换器通过在指定参数的时候调用路由转换器，对参数做限制和处理等# re:路由转换器名称；括号中的内容是创建转换器实例传递的参数，参数可以传递多个，用&quot; , &quot;隔开@app.route(/user/&lt;re(&quot;[0-9]{4}&quot;):user_id&gt;) def index(user_id):.......Flask自带的路由转换器查看自带路由转换器方法app.url_map.converts自带的路由转换器default UnicodeConverter可以匹配Unicode类型的数据string UnicodeConverter可以匹配Unicode类型的字符串any AnyConverter可以匹配固定的几个设定的参数path PathConverter可以匹配带&quot;/&quot;路径符号的参数int IntegerConverter可以匹配整数类型的字符串float FloatConverter可以匹配浮点类型的字符串uuid UUIDConverter可以匹配一个32位的唯一通用识别码（uuid风格的）字符串输出一个唯一的uuid类型的数据的方法import uuid1.uuid.uuid1()2.uuid.uuid4()uuid.uuid1()自定义路由转换器（正则路由）导入转换器的父类from werkzeug.routing import BaseConverter定义正则路由转换器第一种方法该方法只能用父类的regex属性定义一种匹配规则，适用性不强第二种方法该方法可以实现通过传递进来的匹配规则来进行匹配将自定义路由转换器添加到默认的转换器列表中，并指定转换器的名称app.url_map.converters['re'] = RegexConverter路由转换器的其他方法（在自定义路由转换器中使用）to_python( ) 方法功能在参数匹配完成之后再传入到视图函数之前调用，并将函数处理后的参数内容传入视图函数代码示例：def to_python(self, value):v = int(value) 将参数转化为整数return v 将参数传递给视图函数to_url():功能此方法会在进行路由跳转时在匹配跳转的路由函数之前调用，可使用此方法对url_for传入的内容进行处理代码示例def to_url(self, value):v = int(value)return '%d' % v视图函数的配置上下文Flask上下文的概念相当于一个容器，保存了Flask程序运行过程中的一些信息。Flask中有两种上下文，请求上下文和应用上下文。应用上下文概念它的字面意思是应用上下文，但它不是一直存在的，它的作用主要是帮助 request 获取当前的应用，它是伴 request 而生，随 request 而灭的。只有在程序完全运行起来之后才能调用应用上下文包含current_app应用程序上下文,用于存储应用程序中的变量，可以通过current_app.变量获取，例如：应用的启动脚 本是哪个文件，启动时指定了哪些参数加载了哪些配置文件，导入了哪些配置连了哪个数据库有哪些public的工具类、常量应用跑再哪个机器上，IP多少，内存多大g 变量g作为flask程序全局的一个临时变量,充当者中间媒介的作用,我们可以通过它传递一些数据g保存的是当前请求的全局变量，不同的请求会有不同的全局变量，通过不同的thread_id区别current_app使用方法current_app.name 获取应用名current_app.test_value='value' 给应用实例添加变量g 变量使用方法g.name 获取g的name属性g.name='abc' 添加g全局变量name请求上下文概念Flask从客户端收到请求时，要让视图函数能访问一些对象，这样才能处理请求。请求对象是一个很好的例子，它封装了客户端发送的HTTP请求。要想让视图函数能够访问请求对象，一个 显而易见的方式是将其作为参数传入视图函数，不过这会导致程序中的每个视图函数都增加一个参数，除了访问请求对象,如果视图函数在处理请求时还要访问其他对象，情况会变得更糟。为了避免大量可有可无的参数把视图函数弄得一团糟，Flask使用上下文临时把某些对象变为全局可访问。只有在程序完全运行起来之后才能调用请求上下文包含request封装了HTTP请求的内容，针对的是http请求session用来记录请求会话中的信息，针对的是用户信息request使用方法导入方式from flask import request使用方法request.属性名 获取request上下文中的请求相关信息request对象的属性data 记录请求(请求发送的数据为raw_data)的数据， 数据类型为str form 记录请求(请求发送的数据为form_data)中的表单数据， 数据类型MultiDictargs 记录请求中的查询参数，? 后面的查询字符串， 数据类型MultiDictcookies 记录请求中的cookie信息， 数据类型Dictheaders 记录请求中的报文头， 数据类型EnvironHeadersmethod 记录请求使用的HTTP方法， 数据为GET/POST等url 记录请求的URL地址， 数据类型stringfiles 记录请求上传的文件，为form表单中提交的文件对象组成的字典， 数据类型Dictjson 获取前端传递过来的json字符串，并转换成dict类型session使用方法导入方式from flask import session使用方法使用方法参考session状态保持请求上下文\应用上下文之间的区别请求上下文：保存了客户端和服务器交互的数据应用上下文：flask 应用程序运行过程中，保存的一些配置信息，比如程序名、数据库连接、应用信息等视图函数返回内容重定向redirect示例重定向方法跳转至其他urlfrom flask import redirectreturn redirect('url') 在视图函数中通过redirect函数返回跳转至其他路由方法一：这种方法不适用，因为当路由变化后，将不能成功的匹配到视图函数方法二：from flask import url_for_external=True 参数表示已绝对路径的形式跳转，默认为相对路径的形式跳转这种方法可以直接跳转到另一个视图函数去处理，尽管路由映射改变也不影响返回JSON返回json数据的方法from flask import jsonifyreturn jsonify(&lt;dict&gt;) 在视图函数中通过jsonify函数返回 返回模板返回模板的方法from flask import Flask, render_templatereturn render_template(template_name, **content)备注：**content参数可以是任意个命名参数示例：return render_template(remplate_name, key1=value1, key2=value2)参数中的key1，key2便是模板中模板变量调用参数使用的名字返回一个respones对象返回的方法from flask import make_responseresponse = make_response(params) params：普通python类型数据（需要转换成response对象）return response返回状态码返回的方法return response, status_code 在返回的响应后面接一个数字，代表响应的状态码返回一个静态文件返回静态文件的方法状态保持为什么状态保持因为http是一种无状态协议，不会保持某一次请求所产生的信息，如果想实现状态保持，在开发中解决方式有：状态保持实现两种方法cookie：数据存储在客户端，节省服务器空间，但是不安全session：会话，数据存储在服务器端设置与获取cookie的方法设置cookie的方法response.set_cookie('key', 'value', max_age=second)max_age: cookie最大生存时间，默认是关闭浏览器失效获取cookie的方法request.cookies.get('key')删除cookieresponse.delete_cookie('key')设置与获取session的方法（默认是将session存储在cookie中的）设置session的方法配置SCRET_KEY，导入session方法app.config['SCRET_KEY'] = 'fadfas' 设置为任意不规则字符串from flask import session设置session的方法不带过期时间的设置方法session['key'] = value带过期时间的设置方法app.permanent_session_lifetime = 数字 单位为秒获取session的方法session.get('key')删除session的方法session.pop('key')通过使用redis，将session存储在redis中异常的捕获抛出异常（状态码异常）抛出异常的方法通过调用abort()方法，抛出一个给定状态代码的 HTTPException代码示例from flask import abortabort(500) # 抛出一个状态码为500的HTTPException异常捕获异常捕获异常的方法注册一个错误处理程序，当程序抛出指定错误状态码或者指定的异常的时候，调用该装饰器所装饰的方法装饰器接收的参数可以是：1.错误状态码；2.程序抛出的异常对象捕获到的异常对象对传递给处理程序，程序需要用一个形参来接收可以将异常对象return回去，会将异常信息字符串返回给客户端代码示例@app.errorhandler(500) 注册异常处理程序，传入错误状态码\ 指定错误异常类def internal_server_error(e): 定义一个异常处理程序return '服务器搬家了'请求钩子请求钩子作用在客户端和服务器交互的过程中，有些准备工作或扫尾工作需要处理通过装饰器装饰处理函数的方式，在每次请求的过程中插入对应的处理程序比如：在请求开始时，建立数据库连接；在请求结束时，指定数据的交互格式。为了让每个视图函数避免编写重复功能的代码，Flask提供了通用设施的功能，即请求钩子。四种请求钩子的实现before_first_request：在处理第一个请求前运行。 使用场景：可以在第一次请求到达的时候，开启数据库，一些初始化操作等before_request：在每次请求前运行。注意：可以在处理函数中，直接return一个response对象，那样便不会执行view函数，会直接去调用after_request使用场景：判断访问我们服务器的客户端的身份，如果身份验证通过再将请求传递给路由after_request：如果没有未处理的异常，在每次请求后运行。注意：会传递给处理函数一个response对象，处理函数需要接收此参数一定要return一个response对象，不然客户端接收不到服务器端的响应使用场景： 比如将批量视图函数返回的响应，加上统一的请求头，设置cookie等；teardown_request：在每次请求后运行，即使有未处理的异常抛出。备注：会传递给处理函数一个异常对象参数，处理函数需要接收此参数如果要针对异常返回给客户端响应，可以return一个异常信息的response使用场景：当捕获到未处理的异常后，可以根据异常的信息返回给客户端错误的提示生成铭文进行加密和解密导入模块from werkzeug.security import generate_password_hash, check_password_hash对密码进行铭文加密password_hash = generate_pass_hash(加密前的密码)对加密后的密码与未加密字符串匹配（匹配返回True，失败返回False）password = check_password_hash(加密后的字符串，待校验的密码)]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask模块</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-简介]]></title>
    <url>%2F2017%2F07%2F29%2F05.Flask%2FFlask%E6%A8%A1%E5%9D%97%2FFlask-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Web应用程序的本质Web框架的作用Web应用程序 (World Wide Web)诞生最初的目的，是为了利用互联网交流工作文档。web框架实现的业务WSGI服务器做的事情：将客户端的请求将请求行，解析请求；通过web应用框架的接口，将解析后的请求内容交给web应用框架进行处理；接收到web应用框架处理的结果，构造响应报文对象，发送给客户端；web框架做的事情定义路由，将收到的请求进行一定规则的路由分发，将给对应的视图函数进行处理实现具体的业务，视图函数执行请求对应的业务代码，将处理结果返回给WSGi服务器为什么使用web框架服务器端涉及到的知识、内容，非常广泛。这对程序员的要求会越来越高。如果采用成熟，稳健的框架，那么一些基础的工作，比如，网络操作、数据库访问、会话管理等都可以让框架来处理，那么程序开发人员可以把精力放在具体的业务逻辑上面。使用Web框架开发Web应用程序可以降低开发难度，提高开发效率。总结一句话：避免重复造轮子。Flask框架的介绍Flask框架Flask本身相当于一个内核，其他几乎所有的功能都要用到扩展（邮件扩展Flask-Mail，用户认证Flask-Login），都需要用第三方的扩展来实现。比如可以用Flask-extension加入ORM、窗体验证工具，文件上传、身份验证等。Flask没有默认使用的数据库，你可以选择MySQL，也可以用NoSQL。其 WSGI 工具箱采用 Werkzeug（路由模块） ，模板引擎则使用 Jinja2，也是通过默认安装拓展的形式实现的 。可以说Flask框架的核心就是Werkzeug路由模块和Jinja2模板引擎。Flask常用的扩展包Flask-SQLalchemy：操作数据库；Flask-migrate：管理迁移数据库；Flask-Mail:邮件；Flask-WTF：表单；Flask-script：插入脚本；Flask-Login：认证用户状态；Flask-RESTful：开发REST API的工具；Flask-Bootstrap：集成前端Twitter Bootstrap框架；Flask-Moment：本地化日期和时间；Flask与Django的对比总体区别Django功能大而全，Django的一站式解决的思路，能让开发者不用在开发之前就在选择应用的基础设施上花费大量时间。 Flask只包含基本的配置Django有模板，表单，路由，认证，基本的数据库管理等等内建功能。 与之相反，Flask只是一个内核，默认依赖于两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 工具集，其他很多功能都是以扩展的形式进行嵌入使用。Flask 比 Django 更灵活用Flask来构建应用之前，选择组件的时候会给开发者带来更多的灵活性；项目区别Django项目和应用创建好了之后，只包含空的模型和模板文件； Flask创建项目之后，目录里面没有任何文件，需要我们手动创建，是没有像Django一样组件分离，而对于需要把组件分离开的项目，Flask有blueprints。例如，你可以这样构建你的应用，将与用户有关的功能放在user.py里，把与销售相关的功能放在ecommerce.py里。Django把一个项目分成各自独立的应用，而Flask认为一个项目应该是一个包含一些视图和模型的单个应用。当然我们也可以在Flask里复制出像Django那样的项目结构。模板语言的对比Django的模板语言过滤器最多只能传递一个参数在Flask扩展的Jinja的模板语言里，可以把任何数量的参数传给过滤器，因为Jinja像调用一个Python函数的方式来看待它，用圆括号来封装参数。Flask环境的安装虚拟环境的配置虚拟环境的安装虚拟环境安装在django项目中有，不再进行记录创建虚拟环境mkvirtualenv Flask_py安装Flaskpip install flask==0.10.1安装其他的相关拓展pip freeze &gt;requirements.txt 将虚拟环境依赖包导入文件pip install -r requirements.txt 安装依赖包文件中的所有包requirements包Flask文档扩展列表：http://flask.pocoo.org/extensions/中文文档（http://docs.jinkan.org/docs/flask/）]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask模块</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置文件]]></title>
    <url>%2F2017%2F07%2F27%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2Fnginx%2Fnginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[user nobody; #运行用户worker_processes 1; #启动进程,通常设置成和cpu的数量相等#全局错误日志及PID文件#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;#工作模式及连接数上限events { accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off #use epoll; #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport worker_connections 1024; #最大连接数，默认为512}http { include mime.types; #设定mime类型,类型由mime.type文件定义 文件扩展名与文件类型映射表 default_type application/octet-stream; #设定日志格式 log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; access_log logs/access.log main; # 服务日志 sendfile on; #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块 # tcp_nopush on; # 按照大小发送数据包,不是按照请求 keepalive_timeout 65; #连接超时时间 tcp_nodelay on; # 按照请求发送数据包 gzip on; # 开启gzip压缩 gzip_disable &quot;MSIE [1-6].&quot;; # 不给IE6 Gzip client_header_buffer_size 128k; # 如果请求头大小大于指定的缓冲区，则使用large_client_header_buffers指令分配更大的缓冲区。 large_client_header_buffers 4 128k; # 规定了用于读取大型客户端请求头的缓冲区的最大数量和大小。 这些缓冲区仅在缺省缓冲区不足时按需分配。 sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。 upstream mysvr { # 服务集群 server 127.0.0.1:7878; server 192.168.10.121:3333 backup; #热备 } #设定虚拟主机配置 server { listen 80; # 侦听80端口 server_name mysvr; # 定义使用 mysvr 访问 root html; # 定义服务器的默认网站根目录位置 access_log logs/nginx.access.log main; # 设定本虚拟主机的访问日志 location / { # 默认请求 index index.php index.html index.htm; # 定义首页索引文件的名称 client_max_body_size 100m; # 配置请求体最大容量 proxy_set_header Host $http_host; # Host包含客户端真实的域名和端口号； proxy_set_header X-Real-IP $remote_addr; # X-Forwarded-Proto表示客户端真实的协议（http还是https) proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # X-Real-IP表示客户端真实的IP； proxy_set_header X-Forwarded-Proto $scheme; # X-Forwarded-For这个Header和X-Real-IP类似，但它在多层代理时会包含真实客户端及中间每个代理服务器的IP。 deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip } # 定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html { } #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ { expires 30d; # 过期30天，静态文件不怎么更新 } #禁止访问 .htxxx 文件 location ~ /.ht { deny all; } }}]]></content>
      <categories>
        <category>开发工具</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery&flower后台运行部署]]></title>
    <url>%2F2017%2F06%2F06%2F02.python%E7%88%AC%E8%99%AB%2Fcelery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%2FCelery%26flower%20%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[celery在后台运行在生产环境中，你可能希望在后台运行worker.下面的文档中有详细的介绍：daemonization tutorial.下面的脚本使用celery multi命令在后台启动一个或多个worker.$ celery multi start w1 -A proj -l infocelery multi v4.0.0 (0today8)&gt; Starting nodes...&gt; w1.halcyon.local: OK你也可以重新启动:$ celery multi restart w1 -A proj -l infocelery multi v4.0.0 (0today8)&gt; Stopping nodes...&gt; w1.halcyon.local: TERM -&gt; 64024&gt; Waiting for 1 node.....&gt; w1.halcyon.local: OK&gt; Restarting node w1.halcyon.local: OKcelery multi v4.0.0 (0today8)&gt; Stopping nodes...&gt; w1.halcyon.local: TERM -&gt; 64052或者停止它:$ celery multi stop w1 -A proj -l infostop命令是异步的，它不会等待所有的worker真正关闭。你可能需要使用stopwait命令，这个命令会保证当前所有的任务都执行完毕。$ celery multi stopwait w1 -A proj -l info注:celery multi命令不会保存workers的信息，所以当重新启动时你需要使用相同的命令行参数。当停止时，只有相同的pidfile和logfile参数是必须的。默认情况下，celery会在当前目录下创建pidfile和logfile.为了防止多个worker在启动时相互影响，你可以指定一个特定的目录。$ mkdir -p /var/run/celery$ mkdir -p /var/log/celery$ celery multi start w1 -A proj -l info --pidfile=/var/run/celery/%n.pid \--logfile=/var/log/celery/%n%I.log通过multi命令你可以启动多个workers,另外这个命令还支持强大的命令行语法来为不同的workers指定不同的参数。$ celery multi start 10 -A proj -l info -Q:1-3 images,video -Q:4,5 data \-Q default -L:4,5 debug更多关闭multi的举例请参考multi模块的API手册。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>celery分布式爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式任务队列Celery]]></title>
    <url>%2F2017%2F06%2F05%2F02.python%E7%88%AC%E8%99%AB%2Fcelery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97Celery%2F</url>
    <content type="text"><![CDATA[分布式任务队列CeleryCelery （芹菜）是基于Python开发的分布式任务队列。它支持使用任务队列的方式在分布的机器／进程／线程上执行任务调度。结构核心部件broker消息队列，由第三方消息中间件完成常见有RabbitMQ, Redis, MongoDB等worker任务执行器可以有多个worker进程worker又可以起多个queue来并行消费消息backend后端存储，用于持久化任务执行结果功能部件beat定时器，用于周期性调起任务flowerweb管理界面任务基本用法是在程序里引用celery，并将函数方法绑定到taskfrom celery import Celeryapp = Celery('tasks', backend='amqp', broker='amqp://guest@localhost//')app.conf.CELERY_RESULT_BACKEND = 'db+sqlite:///results.sqlite'@app.taskdef add(x, y):return x + y然后调用相应方法即可(delay与apply_async都是异步调用)from tasks import addimport timeresult = add.delay(4,4)while not result.ready():print &quot;not ready yet&quot;time.sleep(5)print result.get()由于是采用消息队列，因此任务提交之后，程序立刻返回一个任务ID。之后可以通过该ID查询该任务的执行状态和结果。关联任务执行1个任务，完成后再执行第2个，第一个任务的结果做第二个任务的入参add.apply_async((2, 2), link=add.s(16))结果：2+2+16=20还可以做错误处理@app.task(bind=True)def error_handler(self, uuid):result = self.app.AsyncResult(uuid)print('Task {0} raised exception: {1!r}\n{2!r}'.format(uuid, result.result, result.traceback))add.apply_async((2, 2), link_error=error_handler.s())定时任务让任务在指定的时间执行，与下文叙述的周期性任务是不同的。ETA, 指定任务执行时间,注意时区countdown, 倒计时,单位秒from datetime import datetime, timedeltatomorrow = datetime.utcnow() + timedelta(seconds=3)add.apply_async((2, 2), eta=tomorrow)result = add.apply_async((2, 2), countdown=3)tip任务的信息是保存在broker中的，因此关闭worker并不会丢失任务信息回收任务(revoke)并非是将队列中的任务删除，而是在worker的内存中保存回收的任务task-id，不同worker之间会自动同步上述revoked task-id。由于信息是保存在内存当中的，因此如果将所有worker都关闭了，revoked task-id信息就丢失了，回收过的任务就又可以执行了。要防治这点，需要在启动worker时指定一个文件用于保存信息celery -A app.celery worker --loglevel=info &amp;&gt; celery_worker.log --statedb=/var/tmp/celery_worker.state过期时间expires单位秒，超过过期时间还未开始执行的任务会被回收add.apply_async((10, 10), expires=60)重试max_retries:最大重试次数interval_start:重试等待时间interval_step:每次重试叠加时长，假设第一重试等待1s，第二次等待1＋n秒interval_max:最大等待时间add.apply_async((2, 2), retry=True, retry_policy={'max_retries': 3,'interval_start': 0,'interval_step': 0.2,'interval_max': 0.2,})序列化将任务结果按照一定格式序列化处理，支持pickle, JSON, YAML and msgpackadd.apply_async((10, 10), serializer='json')压缩将任务结果压缩add.apply_async((2, 2), compression='zlib')任务路由使用-Q参数为队列(queue)命名，然后调用任务时可以指定相应队列$ celery -A proj worker -l info -Q celery,priority.highadd.apply_async(queue='priority.high')工作流按照一定关系一次调用多个任务group: 并行调度chain: 串行调度chord: 类似group，但分header和body2个部分，header可以是一个group任务，执行完成后调用body的任务map: 映射调度，通过输入多个入参来多次调度同一个任务starmap: 类似map，入参类似＊argschunks:将任务按照一定数量进行分组groupfrom celery import group&gt;&gt;&gt; res = group(add.s(i, i) for i in xrange(10))()&gt;&gt;&gt; res.get(timeout=1)[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]chain&gt;&gt;&gt; from celery import chain# 2 + 2 + 4 + 8&gt;&gt;&gt; res = chain(add.s(2, 2), add.s(4), add.s(8))()&gt;&gt;&gt; res.get()16可以用｜来表示chain# ((4 + 16) * 2 + 4) * 8&gt;&gt;&gt; c2 = (add.s(4, 16) | mul.s(2) | (add.s(4) | mul.s(8)))&gt;&gt;&gt; res = c2()&gt;&gt;&gt; res.get()chord&gt;&gt;&gt; from celery import chord#1*2+2*2+...9*2&gt;&gt;&gt; res = chord((add.s(i, i) for i in xrange(10)), xsum.s())()&gt;&gt;&gt; res.get()90map&gt;&gt;&gt; from proj.tasks import add&gt;&gt;&gt; ~xsum.map([range(10), range(100)])[45, 4950]starmap&gt;&gt;&gt; ~add.starmap(zip(range(10), range(10)))[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]chunks&gt;&gt;&gt; from proj.tasks import add&gt;&gt;&gt; res = add.chunks(zip(range(100), range(100)), 10)()&gt;&gt;&gt; res.get()[[0, 2, 4, 6, 8, 10, 12, 14, 16, 18],[20, 22, 24, 26, 28, 30, 32, 34, 36, 38],[40, 42, 44, 46, 48, 50, 52, 54, 56, 58],[60, 62, 64, 66, 68, 70, 72, 74, 76, 78],[80, 82, 84, 86, 88, 90, 92, 94, 96, 98],[100, 102, 104, 106, 108, 110, 112, 114, 116, 118],[120, 122, 124, 126, 128, 130, 132, 134, 136, 138],[140, 142, 144, 146, 148, 150, 152, 154, 156, 158],[160, 162, 164, 166, 168, 170, 172, 174, 176, 178],[180, 182, 184, 186, 188, 190, 192, 194, 196, 198]]周期性任务周期性任务就是按照一定的时间检查反复执行的任务。前面描述的定时任务值的是一次性的任务。程序中引入并配置好周期性任务后，beat进程就会定期调起相关任务beat进程是需要单独启动的$ celery -A proj beat或者在worker启动时一起拉起$ celery -A proj worker -B注意一套celery只能启一个beat进程时区配置由于python中时间默认是utc时间，因此最简便的方法是celery也用utc时区CELERY_TIMEZONE = 'UTC'这么配置可以保证任务调度的时间是准确的，但由于服务器一般都配置时区，因此flower、以及日志中的时间可能会有偏差另外一种方法，就是配置正确的时区CELERY_TIMEZONE = 'Asia/Shanghai'然后任务调起时，将时间带入时区配置local_tz = pytz.timezone(app.config['CELERY_TIMEZONE'])format_eta = local_tz.localize(datetime.strptime(eta.strip(), '%Y/%m/%d %H:%M:%S'))add.apply_async((2, 2),eta=format_eta)周期性任务配置from datetime import timedeltaCELERYBEAT_SCHEDULE = {'add-every-30-seconds': {'task': 'tasks.add','schedule': timedelta(seconds=30),'args': (16, 16)},}周期性任务配置crontabfrom celery.schedules import crontabCELERYBEAT_SCHEDULE = {# Executes every Monday morning at 7:30 A.M'add-every-monday-morning': {'task': 'tasks.add','schedule': crontab(hour=7, minute=30, day_of_week=1),'args': (16, 16),},}本文参考官方文档]]></content>
      <categories>
        <category>python爬虫</category>
        <category>celery分布式爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery分布式-监控界面]]></title>
    <url>%2F2017%2F06%2F05%2F02.python%E7%88%AC%E8%99%AB%2Fcelery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%2Fcelery%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%9A%E7%9B%91%E6%8E%A7%E7%95%8C%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[flower监控服务器flower安装与运行的方法通过浏览器打开地址可能遇到的错误pymongo.errors.NotMasterError: not master关闭mongodb的数据集其他当服务器数量较多的时候，管理起来会很不方便，可以使用python的supervisor来管理后台进程，遗憾的是它并不支持python3，不过也可以装在python2的环境虽然用了supervisor可以很方便的管理python程序，但是还是得一个个登陆不同的服务器的去管理，咋办捏？我在github上找到一个工具supervisor-easy，可以批量管理supervisor，如图:地址：https://github.com/trytofix/supervisor-easy]]></content>
      <categories>
        <category>python爬虫</category>
        <category>celery分布式爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery构建一个分布式爬虫-基础篇]]></title>
    <url>%2F2017%2F06%2F03%2F02.python%E7%88%AC%E8%99%AB%2Fcelery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%2FCelery%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[如何构建一个分布式爬虫：基础篇首先，我们新建目录distributedspider，然后再在其中新建文件workers.py,里面内容如下from celery import Celeryapp = Celery('crawl_task', include=['tasks'], broker='redis://223.129.0.190:6379/1', backend='redis://223.129.0.190:6379/2')# 官方推荐使用json作为消息序列化方式app.conf.update(CELERY_TIMEZONE='Asia/Shanghai',CELERY_ENABLE_UTC=True,CELERY_ACCEPT_CONTENT=['json'],CELERY_TASK_SERIALIZER='json',CELERY_RESULT_SERIALIZER='json',)上述代码主要是做Celery实例的初始化工作，include是在初始化celery app的时候需要引入的内容，主要就是注册为网络调用的函数所在的文件。然后我们再编写任务函数，新建文件tasks.py,内容如下import requestsfrom bs4 import BeautifulSoupfrom workers import app@app.taskdef crawl(url):print('正在抓取链接{}'.format(url))resp_text = requests.get(url).textsoup = BeautifulSoup(resp_text, 'html.parser')return soup.find('h1').text它的作用很简单，就是抓取指定的url，并且把标签为h1的元素提取出来最后，我们新建文件task_dispatcher.py，内容如下from workers import appurl_list = ['http://docs.celeryproject.org/en/latest/getting-started/introduction.html','http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html','http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html','http://docs.celeryproject.org/en/latest/getting-started/next-steps.html','http://docs.celeryproject.org/en/latest/getting-started/resources.html','http://docs.celeryproject.org/en/latest/userguide/application.html','http://docs.celeryproject.org/en/latest/userguide/tasks.html','http://docs.celeryproject.org/en/latest/userguide/canvas.html','http://docs.celeryproject.org/en/latest/userguide/workers.html','http://docs.celeryproject.org/en/latest/userguide/daemonizing.html','http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html']def manage_crawl_task(urls):for url in urls:app.send_task('tasks.crawl', args=(url,))if __name__ == '__main__':manage_crawl_task(url_list)这段代码的作用主要就是给worker发送任务，任务是tasks.crawl，参数是url(元祖的形式)现在，让我们在节点A(hostname为resolvewang的主机)上启动workercelery -A workers worker -c 2 -l info这里 -c指定了线程数为2， -l表示日志等级是info。我们把代码拷贝到节点B(节点名为wpm的主机)，同样以相同命令启动worker，便可以看到以下输出可以看到左边节点(A)先是all alone，表示只有一个节点；后来再节点B启动后，它便和B同步了sync with celery@wpm这个时候，我们运行给这两个worker节点发送抓取任务python task_dispatcher.py可以看到如下输出可以看到两个节点都在执行抓取任务，并且它们的任务不会重复。我们再在redis里看看结果可以看到一共有11条结果，说明 tasks.crawl中返回的数据都在db2(backend)中了，并且以json的形式存储了起来，除了返回的结果，还有执行是否成功等信息。到此，我们就实现了一个很基础的分布式网络爬虫，但是它还不具有很好的扩展性，而且貌似太简单了…下一篇我将以微博数据采集为例来演示如何构建一个稳健的分布式网络爬虫。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>celery分布式爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Celery构建一个分布式爬虫-理论篇]]></title>
    <url>%2F2017%2F06%2F02%2F02.python%E7%88%AC%E8%99%AB%2Fcelery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%2FCelery%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%EF%BC%9A%E7%90%86%E8%AE%BA%E7%AF%87%2F</url>
    <content type="text"><![CDATA[如何构建一个分布式爬虫：理论篇前言本系列文章计划分三个章节进行讲述，分别是理论篇、基础篇和实战篇。理论篇主要为构建分布式爬虫而储备的理论知识，基础篇会基于理论篇的知识写一个简易的分布式爬虫，实战篇则会以微博为例，教大家做一个比较完整且足够健壮的分布式微博爬虫。通过这三篇文章，希望大家能掌握如何构建一个分布式爬虫的方法；能举一反三，将celery用于除爬虫外的其它场景。目前基本上的博客都是教大家使用scrapyd或者scrapy-redis构建分布式爬虫，本系列文章会从另外一个角度讲述如何用requests+celery构建一个健壮的、可伸缩并且可扩展的分布式爬虫。本系列文章属于爬虫进阶文章，期望受众是具有一定Python基础知识和编程能力、有爬虫经验并且希望提升自己的同学。小白要是感兴趣，也可以看看，看不懂的话，可以等有了一定基础和经验后回过头来再看。另外一点说明，本系列文章不是旨在构建一个分布式爬虫框架或者分布式任务调度框架，而是利用现有的分布式任务调度工具来实现分布式爬虫，所以请轻喷。分布式爬虫概览何谓分布式爬虫？通俗的讲，分布式爬虫就是多台机器多个 spider 对多个 url 的同时处理问题，分布式的方式可以极大提高程序的抓取效率。构建分布式爬虫通畅需要考虑的问题（1）如何能保证多台机器同时抓取同一个URL？（2）如果某个节点挂掉，会不会影响其它节点，任务如何继续？（3）既然是分布式，如何保证架构的可伸缩性和可扩展性？不同优先级的抓取任务如何进行资源分配和调度？基于上述问题，我选择使用celery作为分布式任务调度工具，是分布式爬虫中任务和资源调度的核心模块。它会把所有任务都通过消息队列发送给各个分布式节点进行执行，所以可以很好的保证url不会被重复抓取；它在检测到worker挂掉的情况下，会尝试向其他的worker重新发送这个任务信息，这样第二个问题也可以得到解决；celery自带任务路由，我们可以根据实际情况在不同的节点上运行不同的抓取任务（在实战篇我会讲到）。本文主要就是带大家了解一下celery的方方面面(有celery相关经验的同学和大牛可以直接跳过了)Celery知识储备celery基础讲解按celery官网的介绍来说Celery 是一个简单、灵活且可靠的，处理大量消息的分布式系统，并且提供维护这样一个系统的必需工具。它是一个专注于实时处理的任务队列，同时也支持任务调度。下面几个关于celery的核心知识点broker：翻译过来叫做中间人。它是一个消息传输的中间件，可以理解为一个邮箱。每当应用程序调用celery的异步任务的时候，会向broker传递消息，而后celery的worker将会取到消息，执行相应程序。这其实就是消费者和生产者之间的桥梁。backend: 通常程序发送的消息，发完就完了，可能都不知道对方时候接受了。为此，celery实现了一个backend，用于存储这些消息以及celery执行的一些消息和结果。worker: Celery类的实例，作用就是执行各种任务。注意在celery3.1.25后windows是不支持celery worker的！producer: 发送任务，将其传递给brokerbeat: celery实现的定时任务。可以将其理解为一个producer，因为它也是通过网络调用定时将任务发送给worker执行。注意在windows上celery是不支持定时任务的！下面是关于celery的架构示意图，结合上面文字的话应该会更好理解由于celery只是任务队列，而不是真正意义上的消息队列，它自身不具有存储数据的功能，所以broker和backend需要通过第三方工具来存储信息，celery官方推荐的是 RabbitMQ和Redis，另外mongodb等也可以作为broker或者backend，可能不会很稳定，我们这里选择Redis作为broker兼backend。关于redis的安装和配置可以查看这里实际例子先安装celerypip install celery我们以官网给出的例子来做说明，并对其进行扩展。首先在项目根目录下，这里我新建一个项目叫做celerystudy，然后切换到该项目目录下，新建文件tasks.py，然后在其中输入下面代码from celery import Celeryapp = Celery('tasks', broker='redis://:''@223.129.0.190:6379/2', backend='redis://:''@223.129.0.190:6379/3')@app.taskdef add(x, y):return x + y这里我详细讲一下代码：我们先通过app=Celery()来实例化一个celery对象，在这个过程中，我们指定了它的broker，是redis的db 2,也指定了它的backend,是redis的db3, broker和backend的连接形式大概是这样redis://:password@hostname:port/db_number然后定义了一个add函数，重点是@app.task，它的作用在我看来就是**将add()注册为一个类似服务的东西，本来只能通过本地调用的函数被它装饰后，就可以通过网络来调用。这个tasks.py中的app就是一个worker。它可以有很多任务，比如这里的任务函数add。我们再通过在命令行切换到项目根目录**，执行celery -A tasks worker -l info启动成功后就是下图所示的样子这里我说一下各个参数的意思，-A指定的是app(即Celery实例)所在的文件模块，我们的app是放在tasks.py中，所以这里是 tasks；worker表示当前以worker的方式运行，难道还有别的方式？对的，比如运行定时任务就不用指定worker这个关键字; -l info表示该worker节点的日志等级是info，更多关于启动worker的参数(比如-c、-Q等常用的)请使用celery worker –help进行查看将worker启动起来后，我们就可以通过网络来调用add函数了。我们在后面的分布式爬虫构建中也是采用这种方式分发和消费url的。在命令行先切换到项目根目录，然后打开python交互端from tasks import addrs = add.delay(2, 2) # 这里的add.delay就是通过网络调用将任务发送给add所在的worker执行这个时候我们可以在worker的界面看到接收的任务和计算的结果。[2017-05-19 14:22:43,038: INFO/MainProcess] Received task: tasks.add[c0dfcd0b-d05f-4285-b944-0a8aba3e7e61] # worker接收的任务[2017-05-19 14:22:43,065: INFO/MainProcess] Task tasks.add[c0dfcd0b-d05f-4285-b944-0a8aba3e7e61] succeeded in 0.025274309000451467s: 4 # 执行结果这里是异步调用，如果我们需要返回的结果，那么要等rs的ready状态true才行。这里add看不出效果，不过试想一下，如果我们是调用的比较占时间的io任务，那么异步任务就比较有价值了rs #rs.ready() # true 表示已经返回结果了rs.status # ‘SUCCESS’ 任务执行状态，失败还是成功rs.successful() # True 表示执行成功rs.result # 4 返回的结果rs.get() # 4 返回的结果from tasks import addif __name__ == '__main__':add.delay(5, 10)这时候可以在celery的worker界面看到执行的结果[2017-05-19 14:25:48,039: INFO/MainProcess] Received task: tasks.add[f5ed0d5e-a337-45a2-a6b3-38a58efd9760][2017-05-19 14:25:48,074: INFO/MainProcess] Task tasks.add[f5ed0d5e-a337-45a2-a6b3-38a58efd9760] succeeded in 0.03369094600020617s: 15此外，我们还可以通过send_task()来调用，将excute_tasks.py改成这样from tasks import appif __name__ == '__main__':app.send_task('tasks.add', args=(10, 15),)这种方式也是可以的。send_task()还可能接收到为注册（即通过@app.task装饰）的任务，这个时候worker会忽略这个消息[2017-05-19 14:34:15,352: ERROR/MainProcess] Received unregistered task of type ‘tasks.adds’.The message has been ignored and discarded.定时任务上面部分讲了怎么启动worker和调用worker的相关函数，这里再讲一下celery的定时任务。爬虫由于其特殊性，可能需要定时做增量抓取，也可能需要定时做模拟登陆，以防止cookie过期，而celery恰恰就实现了定时任务的功能。在上述基础上，我们将tasks.py文件改成如下内容from celery import Celeryapp = Celery('add_tasks', broker='redis:''//223.129.0.190:6379/2', backend='redis:''//223.129.0.190:6379/3')app.conf.update(# 配置所在时区CELERY_TIMEZONE='Asia/Shanghai',CELERY_ENABLE_UTC=True,# 官网推荐消息序列化方式为jsonCELERY_ACCEPT_CONTENT=['json'],CELERY_TASK_SERIALIZER='json',CELERY_RESULT_SERIALIZER='json',# 配置定时任务CELERYBEAT_SCHEDULE={'my_task': {'task': 'tasks.add', # tasks.py模块下的add方法'schedule': 60, # 每隔60运行一次'args': (23, 12),}})@app.taskdef add(x, y):return x + y然后先通过ctrl+c停掉前一个worker，因为我们代码改了，需要重启worker才会生效。我们再次以celery -A tasks worker -l info这个命令开启worker。这个时候我们只是开启了worker，如果要让worker执行任务，那么还需要通过beat给它定时发送，我们再开一个命令行，切换到项目根目录，通过celery beat -A tasks -l infocelery beat v3.1.25 (Cipater) is starting.__ - ... __ - _Configuration -&gt;. broker -&gt; redis://223.129.0.190:6379/2. loader -&gt; celery.loaders.app.AppLoader. scheduler -&gt; celery.beat.PersistentScheduler. db -&gt; celerybeat-schedule. logfile -&gt; [stderr]@%INFO. maxinterval -&gt; now (0s)[2017-05-19 15:56:57,125: INFO/MainProcess] beat: Starting...这样就表示定时任务已经开始运行了。眼尖的同学可能看到我这里celery的版本是3.1.25，这是因为celery支持的windows最高版本是3.1.25。由于我的分布式微博爬虫的worker也同时部署在了windows上，所以我选择了使用 3.1.25。如果全是linux系统，建议使用celery4。此外，还有一点需要注意，在celery4后，定时任务（通过schedule调度的会这样，通过crontab调度的会马上执行）会在当前时间再过定时间隔执行第一次任务，比如我这里设置的是60秒的间隔，那么第一次执行add会在我们通过celery beat -A tasks -l info启动定时任务后60秒才执行；celery3.1.25则会马上执行该任务。关于定时任务更详细的请看官方文档celery定时任务]]></content>
      <categories>
        <category>python爬虫</category>
        <category>celery分布式爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>celery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib绘图工具]]></title>
    <url>%2F2017%2F05%2F26%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FMatplotlib%E7%BB%98%E5%9B%BE%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[Matplotlib工具介绍Matplotlib的简介Matplotlib 是一个 Python 的 2D绘图库，通过 Matplotlib，开发者可以仅需要几行代码，便可以生成绘图，直方图，功率谱，条形图，错误图，散点图等。Matplotlib官方网站http://matplotlib.orgMatplotlib的作用用于创建出版质量图表的绘图工具库目的是为Python构建一个Matlab式的绘图接口常用的导包方式import matplotlib.pyplot as pltpyplot的常用参数pyplot模块包含了常用的matplotlib API函数，承担了大部分的绘图任务。http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plotpyplot绘图的基本使用创建画板figure创建画板方法fig = plt.figure(figsize=(x, y), dpi=z) 创建一个画板，返回一个Figure图像对象figsize: 表示生成画板的尺寸大小，生成画板的默认大小为432*288dpi：表示画板的尺寸单位，生成画板的大小是由figsize*dpi获得Figure 对象在Matplotlib中，整个图像为一个figure对象Matplotlib 的图像均位于figure对象中如果不创建figure对象，matplotlib会自动创建一个figure对象。图形的绘制绘制方法plt.plot(arr， color='r'，linestyle='dashed'，marker = 'o', markerfacecolor='y', markersize=10， alpha=0.7， label='lable_name')arr：表示绘制所用数据，如果是ndarry，x,y坐标都由range(n)来表示，如果是DataFrame则行索引表示横坐标，列索引表示纵坐标marker: 表示标记形状markersize: 表示标记的大小linestyle: 表示线型color：线的颜色linestyle: 线的样式markerfacecolor：标记的颜色alpha: 透明度label: 线或者柱子的名字设置颜色的方法plt.plot(x, y, ‘or--’)等价于ax.plot(x, y, linestyle=‘--’, color=‘r', marker='o')常用颜色、标记、线型常用的颜色常用的标记常用的线型图像的保存plt.savefig('pic_name') 将绘制好的图片进行保存,要在图像显示前进行保存pic_name: 表示生成图片的名字给图中的坐标点标记添加文本plt.text(x, y, s, fontdict=None)x, y：表示坐标；s：字符串文本；fontdict：字典，可选；给绘制的图形指定名字plt.rcParams['font.sans-serif'] = ['SimHei'] 先将字体改为中文黑体,避免无法显示中文plt.rcParams['font-size'] = 15.0 修改字体的大小plt.title('title_name') 给图像指定标题名称修改图像的x、y轴坐标的参数修改x/y轴的刻度的名称plt.xticks(x， [n1, n2, n3, n4, n5]) x：当前每个刻度值构成的列表n1~n5: 表示每个刻度的别名调整x/y轴刻度的位置plt.xticks(x+width)x：当前每个刻度值构成的数组width: 表示修改x轴的刻度 给x/y轴起名称plt.xlabel('lable_name'， fontsize=20)plt.ylabel('lable_name'， fontsize=20)lable_name: 表示给x轴起的名称fontsize: 表示名称的字体大小显示图例plt.legend()显示网络plt.grid()图形的显示plt.show() 对已经生成的图形进行显示，并清空内存数据subplot 画板分隔区域（用来同时绘制多个图形）画板分割的作用subplot命令是将图片窗口划分成若干区域,按照一定顺序使得图形在每个小区域内呈现其图形。在figure对象中可以包含一个或者多个Axes对象。每个Axes(ax)对象都是一个拥有自己坐标系统的绘图区域将画板分割的方法fig.add_subplot(a, b, c)a, b 表示将fig分割成 a * b 的区域c 表示当前选中要操作的区域，注意：从1开始编号（不是从0开始）示例代码绘制线性图：plt.plot() 表示一组数据创建画板 fig = plt.figure(figsize=(8, 6), dpi=100)创建ndarray对象arr=np.random.randint(10, 100, 100)绘制线形图plt.plot(arr) 绘制线形图，可以同时绘制多个图形图像的显示plt.show() 将图像进行显示，并清空内存数据直方图绘制： plt.hist() 表示一组数据创建画板 fig = plt.figure(figsize=(8, 6), dpi=100)创建ndarray对象arr=np.random.randint(10, 100, 100)绘制直方图plt.hist(arr，bins=n1)bins：表示绘制多少个区间图像的显示plt.show() 将图像进行显示，并清空内存数据示例代码散点图绘制：scatter() 表示两组数据，分别对应x轴和y轴创建画板 fig = plt.figure(figsize=(8, 6), dpi=100)获取x，y坐标的两个数组x = np.arange(50)y = x + 5 * np.random.rand(50)x 表示横坐标，y 表示纵坐标绘制散点图plt.scatter(x, y)图像显示plt.show()示例代码柱形图（bar）: 用来表示多组不同的数据对比创建画板 fig = plt.figure(figsize=(8, 6), dpi=100)获取两组数据(数据个数相同)y1 = np.random.randint(10, 30, 5) 表示多个柱子的数据y1 = np.random.randint(10, 30, 5) 表示多个柱子的数据创建x轴刻度的位置x = np.range(10, 15) 每个刻度代表每组数据柱形图的绘制plt.bar(x, y1, [z , color='r', alpha='o.7'， label='lable_name'])plt.bar(x, y2, [z , color='r', alpha='o.7'， label='lable_name'])x: 表示相对刻度的位置,(x-1) 表示像左移动1z: 表示柱子的宽度y: 表示显示的每组数据color：表示柱子的颜色。alpha：表示柱子的透明度label：表示给柱子起的名称备注：当使用barh的时候绘制的是直立柱形图指定x轴y轴坐标的参数修改x/y轴的刻度的名称plt.xticks(x， [n1, n2, n3, n4, n5]) x：当前每个刻度值构成的列表n1~n5: 表示每个刻度的别名调整x/y轴刻度的位置plt.xticks(x+width)x：当前每个刻度值构成的列表x+width: 表示修改后x轴刻度的位置 给x/y轴起名称plt.xlabel('lable_name'， fontsize=20)plt.ylabel('lable_name'， fontsize=20)lable_name: 表示给x轴起的名称fontsize: 表示名称的字体大小显示图例（指明每个柱形图代码什么内容）plt.legend()图像显示plt.show()示例代码饼图pie：用来显示一组数据中各部分数据的百分比创建画板 fig = plt.figure(figsize=(8, 6), dpi=100)获取饼图所需的数据arr = np.random.randint(10, 100, 5) 生成5个随机数构成的数组图形的绘制plt.pie( arr, # 数据labels = [&quot;北京&quot;, &quot;上海&quot;, &quot;广州&quot;, &quot;深圳&quot;, &quot;杭州&quot;], # 各组数据的名称，根据下标对应colors = [&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;cyan&quot;], # 各组数据的颜色，根据下标对应 explode = [0, 0.1, 0, 0, 0], # 0表示不突出，大于0表示突出显示，数值越大突出越明显，一般0.1即可shadow = True, # 显示饼图的底部阴影autopct = &quot;%1.1f%%&quot; # 表示数据显示的有效小数部分 1.1% 表示小数点前后各一位小数)显示图例plt.legend()图像显示plt.show()代码示例混淆矩阵绘图imshow()：用于二维数据显示可用于区域性数据可视化创建画板 fig = plt.figure(figsize=(8, 6), dpi=100)获取区域性的数据arr = np.random.randint(0, 100, (10, 10)) 生成10*10的区域图形的绘制plt.imshow(m, interpolation='nearest', cmap=plt.cm.ocean)interpolation代表的是插值运算，'nearest'只是选取了其中的一种插值方式（邻近算法，了解即可）；cmap表示绘图时的样式，这里选择的是ocean主题。显示右边的颜色条plt.colorbar()将图像进行显示plt.show()示例代码]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas数据处理、合并、转化、重构]]></title>
    <url>%2F2017%2F05%2F25%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FPandas%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E3%80%81%E5%90%88%E5%B9%B6%E3%80%81%E8%BD%AC%E5%8C%96%E3%80%81%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Pandas数据清洗数据清洗的介绍数据清洗是数据分析关键的一步，直接影响之后的处理工作数据清洗是一个迭代的过程，实际项目中可能需要不止一次地执行这些清洗操作数据的缺失数据处理pd.fillna() 补充缺失数据pd.dropna() 删除缺失数据数据的联接数据联接的介绍根据单个或多个键将不同DataFrame的行连接起来类似数据库的连接操作0.先定义两个表1.通过merge将两个表通过外键关联到一起 (默认使用相同且唯一的列作为外键，合并成内联接的结果)使用方法 ()pd.merge(df_obj1, df_obj_2)示例代码2.on指定列作为&quot;外键&quot;（如果多个相同列名的情况下，通过on来指定）使用方法pd.merge(df_obj1, df_obj_2， on='key')示例代码3.left_on, right_on,分别指定两张表的外键（如果两张表不存在相关联同名列的时候）使用方法pd.merge(df_obj1, df_obj_2， left_on='key1', right_on='key2')left_on/right_on: 表示指定两张表的哪个列作为外键进行关联示例代码4.通过how指定连接方式，默认是内联接(inner)使用方法pd.merge(df_obj1, df_obj2, left_on='key1', right_on='key2', how=&quot;outer&quot;)how：表示指定的联接方式，默认为inner内联接。outer：外联接；left：左联接；right：右联接示例代码5.按索引连接使用场景（当一个表的索引是另一张表的列数据的时候）使用方法pd.merge(df_obj3, df_obj4, left_on=&quot;key&quot;, right_index=True, suffixes=[&quot;_left&quot;, &quot;_right&quot;])left_index/right_index: True 表示使用对应左/右表的索引作为外键，与另一张表的列进行关联示例代码处理合并后的同名数据列使用方法pd.merge(df_obj1, df_obj2, left_on='key1', right_on='key2', suffixes=[&quot;_left&quot;, &quot;_right&quot;])suffixes：指定同名数据列后缀，可接收一个列表，分别表示左/右表同名列后缀默认为_x, _y；示例代码Pandas数据合并数据合并的介绍将两个pandas的对象的数据进行合并与表的联接不同，合并数据只会将两个表存在的数据进行合并，不会生成额外关联的数据Series对象的数据合并1.当索引有重复的情况使用方法pd.concat([ser_obj1, ser_obj_2, ser_obj3], [axis=1, join='inner'])axis: 1表示将Series转化为DataFrame对象数据按列追加，0表示直接在行下面对数据进行追加join: 数据的连接方式，默认为outer，还支持inner，表示内连接，默认为outer；代码示例2.索引不重复的情况使用方法pd.concat([ser_obj1, ser_obj_2, ser_obj3], [axis=1])axis: 1表示将Series转化为DataFrame对象数据按列追加，0表示直接在行下面对数据进行追加代码示例DataFrame对象的数据合并0.创建两组数据1.将两组数据进行合并 （默认列索引共享，行索引不共享，默认在行的尾部添加数据）使用方法pd.concat([df_obj1, df_obj2], axis = 1, join='inner')axis：1 按照行方向进行追加，共享行索引；0 按照列方向进行追加，共享列索引；join：inner(当axis=1时，包含nan的行不显示，当axis=0时，包含nan的列不显示)，还支持outer，默认为outer代码示例pd.concat与pd.merge的区别pd.concat() 表示多个数据结构按行/按列合并，并返回一个完整的数据结构，concat合并不会制造新的数据（尽量保证结构的索引个数匹配）数据合并到一起pd.merge()表示多个数据结构按指定的列进行关联，并返回一个完整的数据结构，merge关联需要保证数据结构的形状完整，会填充新的数据（关联不依赖索引个数—）类似于数据库的表关联Pandas的数据转化duplicated处理重复数据使用方法 (依次判断指定列数据，如果判断当前数据之前有重复，则设为True，否则为False)df_obj.duplicated('col_name') 返回布尔类型的Series对象col_name: 表示指定那一列来判断重复drop_duplicates 过滤重复的行使用方法 (依次判断指定数据列的重复情况，并将重复的行删除（注意，根据行索引删除，会影响其他列数据))df_obj.drop_duplicates('col_name')col_name: 表示指定那一列来判断重复 map 将每列数据进行转换使用方法（将数据对象传递给函数，对所有的数据进行处理）DataFrame.map(lambda x : x ** 2))函数的参数为DataFrame数据对象，或者Series数据对象replace 数据替换将所有不想要的数据进行替换df_obj.replace(old_data, new_data)old_data: 为将要被替换的数据new_data: 为替换的目标数据替换固定个数数据df_obj.replace([old_data, new_data], num)old_data: 为将要被替换的数据new_data: 为替换的目标数据num：替换的最大数量同时替换多个数据df_obj.replace([old_data1, old_data2], [new_data1, old_data2])old_data: 为将要被替换的数据new_data: 为替换的目标数据rename 修改数据结构属性df_obj.rename(columns={&quot;old_col1&quot; : &quot;new_col1&quot;, &quot;old_col2&quot;: &quot;new_col2&quot;}, inplace=True)old_data: 为将要被替换的列名new_data: 为替换的目标列名inplace：True在源文件上进行修改，False会返回修改后的数据，默认为False]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas统计计算和描述分组与聚合]]></title>
    <url>%2F2017%2F05%2F24%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FPandas%E7%BB%9F%E8%AE%A1%E8%AE%A1%E7%AE%97%E5%92%8C%E6%8F%8F%E8%BF%B0%E5%88%86%E7%BB%84%E4%B8%8E%E8%81%9A%E5%90%88%2F</url>
    <content type="text"><![CDATA[Pandas统计计算和描述Pandas常用的统计计算和统计描述方法Pandas统计计算常用的统计计算sum, mean, max, min…使用方法（以sum来举例）df_obj.sum() 参数axis=0 按列统计，axis=1按行统计，默认按照列来统计skipna 排除缺失值， 默认为True,False时，Nan会参与运算，不会被忽略示例代码：Pandas统计描述使用方法describe() 产生每列的多个类型统计数据参数axis=0 按列统计，axis=1按行统计，默认按照列来统计skipna 排除缺失值， 默认为True,False时，Nan会参与运算，不会被忽略示例代码Pandas分组与聚合Pandas的分组Pandas分组的介绍对数据集进行分组，然后对每组进行统计分析SQL能够对数据进行过滤，分组聚合pandas能利用groupby进行更加复杂的分组运算分组运算过程：split-&gt;apply-&gt;combine拆分：进行分组的根据应用：每个分组运行的计算规则合并：把每个分组的计算结果合并起来GroupBy对象分组操作（分组类型：DataFrameGroupBy，SeriesGroupBy）对数据进行分组对数据进行分组（基本操作）使用方法groupby(lable) 对数据进行分组，返回一个GroupBy对象lable：分组的依据，例如df_obj['key1'],按照df_obj的key1列进行分组示例代码对应整个数据集的指定列进行分组分组方法df_obj.groupby(df_obj['key'])代码示例对指定的列进行分组分组方法df_obj['data1'].groupby(df_obj['key1'])代码示例对整个数据集，对指定多个分组依据进行分组分组方法df_obj.groupby([df_obj['key1'], df_obj['key2']])示例代码 对指定的列，多个分组依据进行分组分组方法df_obj[ ['data1', 'data2'] ].groupby([df_obj['key1'], df_obj['key2']])示例代码按照自定义key进行分组使用方法自定义分组依据，保证和数据集的行数量一致self_key = ['key1', 'key2', 'key3', 'key4'，'key1', 'key2']当两行数据的分组依据相同时，被分为同一组将数据进行自定义分组df_obj.groupby(self_key).sum()代码示例按照数据类型进行分组其他分组方法通过字典来进行分组通过函数进行分组通过索引进行分组对分组后的数据进行运算（后面的聚合操作，这里只做简单演示）使用方法(假定已分组的数据为grouped)grouped.sum() 对分组后的数据，每组进行运算支持的运算方法包括pandas支持的统计运算方法，sum\mean\count等示例代码GroupBy对象的基本操作GroupBy对象的迭代操作GroupBy可迭代对象GroupBy是一个可迭代的对象每次迭代返回一个元组 (group_name, group_data)，可用于分组数据的具体运算单层分组的迭代多层分组的迭代GroupBy对象转化为字典或列表将GroupBy对象转化为列表list(grouped1)将GroupBy对象转化为字典dict(list(grouped1))代码示例Pandas的聚合Pandas聚合的介绍常用于对分组后的数据进行计算，求平均数，计算总数量等。常用内置聚合函数内置聚合函数常用的内置聚合函数sum(), mean(), max(), min(), count(), size(), describe()使用内置聚合函数的方法(以sum为例子)grouped.sum()grouped为分组后的对象示例代码使用自定义聚合函数（单个）使用方法grouped.agg(func).add_prefix('sum_')grouped：对Pandas分组后的对象func：是自定义的聚合函数，其参数为分组后的对象grouped的数据部分.add_prefix() 表示给聚合运算后的列名添加一个前缀示例代码同时对多个列应用多个聚合函数使用方法grouped.agg(['sum', 'mean', func]).add_prefix('sum_')grouped: 为分组后的对象在agg中传入列表，其中包含所有想要使用的聚合函数名.add_prefix() 表示给聚合运算后的列名添加一个前缀给聚合后的结果起别名grouped.agg([('总和'， 'sum'), （'平均数', mean）, （'max-min', func）])('max-min', func): 前面的参数表示给聚合函数其一个别名，在显示的时候会显示别名示例代码对不同列分别作用不同的聚合函数，使用dict使用方法dict_obj = { 'data1': 'mean', 'data2': 'sum' }grouped.agg(dict_obj).add_prefix('sum_')grouped：是分组后的对象dict_obj: 用来表示pandas分组后每个字段用什么聚合函数来统计计算.add_prefix('sum_') 表示给聚合运算后的列名添加一个前缀给聚合后的结果起别名dict_obj = { 'data1': [('平均数'，'mean'), ('和', 'sum')]}grouped.agg(dict_obj)示例代码分组聚合后的多表连接作用将DataFrame进行分组和聚合运算后，生成的对象是另外一个DataFrame对象如果想要将两张表的数据集合到一张表上，可以使用夺标连接的方法将多张表进行合并先定义两张表（一个是原始数据表，一个是经过分组、聚合运算过后的表）merge多表联接使用方法（将会生成一个新的关联后的表，默认会给生成新表若有重复列名，会给列名添加x,y的后缀）key1_sum_merge = pd.merge(df_obj, key1_sum, left_on='key1', right_index=True)df_obj,key1_sum: 分别表示左表和右表left_on/right_on: 表示左/右表的列作为连接键right_index/left_index: True表示使用左/右表的行索引作为连接键示例代码transform多表联接（接收聚合函数进行运算，并将运算结果和原数据保持一致，可直接和原表进行联表操作）使用方法第一步：生成聚合后的对象new_obj = grouped.transform(&quot;sum&quot;).add_prefix(&quot;sum_&quot;)grouped：表示分组后的对象add_prefix(&quot;sum_&quot;)： 给聚合生成的列名添加前缀第二步：将两个表进行联接(将聚合运算后的列添加的原数据中)orign_obj[new_obj.columns] = new_obj示例代码grouped.apply(func)多组数据合并 （分组后，将各组数据分别应用自定义的func，并自行合并成一组数据）使用方法第一步：定义自定义函数（也可以是匿名函数）def func(df, n=n1, col=n2):return df.sort_values(by=col, ascending=False)[:n]第二步：对数据进行分组，并使用applay将数据应用到自定义的func上df_obj.groupby(df_obj['col_name'], [group_keys=False]).apply(func， [n1, n2])func: 自定义的函数，接收的第一个参数为分组后的对象n2, n2: 是传递给func函数其他的参数group_keys: 在使用apply时有时候会产生层级索引(外层索引是分组名，内层索引是df_obj的行索引)，当设置为False可以禁用示例代码]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas的函数应用与层级索引]]></title>
    <url>%2F2017%2F05%2F24%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FPandas%E7%9A%84%E5%87%BD%E6%95%B0%E5%BA%94%E7%94%A8%E5%B1%82%E7%BA%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[Pandas的函数应用Numpy的ufunc函数（pandas的数据结构支持Numpy的ufunc函数）示例备注在使用计算类函数的时候会直接对所有数据运算输出一个最终结果当使用统计类函数的时候，会对每一列进行统计并输出每列的结果apply将函数应用到列或者行上使用方法定义一个函数func = lambda x : np.max(x) - np.min(x)定义的方法，要能处理每一列/行的数据将每一列的数据应用到自定义的函数上ser_obj.apply(func， [axis=0])axis：0 表示将DataFrame的每一列作为func函数的输入，1表示将每一行作为func函数的输入，默认为0备注对于DataFrame，x 是将每一 列/行 做为参数传给自定义函数，默认操作的这一行/列（Series对象）；如果是 统计函数，每一行/列返回一个数字，最后组合为一个Series （np.sum\np.mean\np.max..)；如果是 计算函数/广播运算，每一 行/列 都会返回一个Series，最后组合为一个DataFrame对于Series是将每一数据都作为func的输入，每一个数组返回一个结果，最后再拼接成一个Series示例代码applymap将函数应用到每个数据上使用方法定义一个函数func = lambda x : x ** 2定义的方法，要能对没个数据进行处理，并且数据类型也匹配将每一列的数据应用到自定义的函数上ser_obj.applymap(func)备注对于DataFrame对象，x 将所有行和列的每一个元素做为参数传递给自定义函数，默认操作的是一个数据（str、int、float）；如果是统计函数，每个元素做统计计算，并返回一个值，最后组合为一个DataFrame；如果是计算函数和算术运算，每个元素计算后，返回一个值，最后组合为一个DataFrame对于Series也同样将每个元素作为参数传递为自定义函数，并每次都输出一个结果示例代码Pandas的排序按照索引排序使用方法sort_index([ascending=False， axis=1]) ascending: True 表示按升序进行排序， False按照降序进行排序，默认为升序axis： 1表示按照列索引进行排序，0表示按照行索引进行排序示例代码按照值进行排序使用方法sort_values(by='column name') 根据某个唯一的列名进行排序，如果有其他相同列名则报错。ascending: True 表示按升序进行排序， False按照降序进行排序，默认为升序by：表示按照哪一列进行排序示例代码Pandas处理缺失数据判断是否存在缺失值判断方法isnull() 判断pandas数据类型是否存在nan值isnotnull() 判断pandas数据类型不为Nan的值示例代码丢弃缺失的数据使用方法dropna(axis=0) 删除包含Nan值的行或列axis：0删除包含nan的行，1删除包含nan的列，默认删除nan所在的行示例代码填充缺失的数据使用方法fillna() 将所有的nan值填充为指定的值示例代码Pandas层级索引层级索引的创建怎么创建层级索引创建pandas对象的时候，指定Index为两个列表构成的索引，第一个子list是外层索引，第二个list是内层索引。示例代码MultiIndex多层索引对象查看层级索引打印这个Series的索引类型，显示是MultiIndex直接将索引打印出来，可以看到有lavels,和labels两个信息。lavels表示两个层级中分别有那些标签，labels是每个位置分别是什么标签。示例代码通过索引对子集的选取选取方法的介绍根据索引获取数据。因为现在有两层索引，当通过外层索引获取数据的时候，可以直接利用外层索引的标签来获取。当要通过内层索引获取数据的时候，在list中传入两个元素，前者是表示要选取的外层索引，后者表示要选取的内层索引通过外层选取内层ser_obj['outer_index'] 通过外层索引名选取其包含的所有内层索引通过外层和内层索引选取对应的值ser_obj['a']['3'] 通过外层和内层索引选取某个值选取连续外层索引对应的内层索引ser_obj[ : , '3']交换分层顺序使用方法swaplevel( (x1, x2) ) 交换内层与外层索引，交换两层默认为(0, 1)。(x1, x2): 交换索引的两个索引下标，如果是两层索引，默认内外层索引转换，如果是多层索引，可以通过索引下标来表示需要交换的分层示例代码交换并排序分层使用方法sort_index(level=0) level: 表示按照哪一层的索引进行排序，0表示按照最外层，默认为0示例代码数据重构将具有多层索引的Series对象重构为DataFrame对象数据重构方法（前提是series要有多层索引）unstack(x) 将有层级索引的series重构为DataFramex: 表示将多层索引的哪一层索引作为列索引，默认将最内层索引作为列索引代码示例将DataFrame对象重构为Series对象使用方法stack() 将DataFrame对象重构为具有多层索引的Series对象默认将列索引作为Series的内层索引代码示例]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参数伪造]]></title>
    <url>%2F2017%2F05%2F23%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC%2F%E5%8F%82%E6%95%B0%E4%BC%AA%E9%80%A0%2F</url>
    <content type="text"><![CDATA[携程反爬中的Eleven参数-反爬与反反爬的奇技淫巧今天我们要聊点什么呢，之前说要聊去哪儿的，不过暂且咱们再放一放，先聊一聊去哪儿的干爹携程吧，上次我记得看了携程工程师霸气回应说懂爬虫的来去哪儿，懂反爬的来携程。我觉得特别棒，这种开放的心态和自信，正是一个开放的互联网环境所需要的。所以今天这节课虽然咱们以携程为例，但是我们还是以学习的目的为主，因此我不会把完整的代码放出来，大家掌握思路，拿到渔网比直接copy代码有用的多。上篇文章用邮箱加密给大家演示了爬虫中简单的JS对抗，今天这节课咱们就用携程的Eleven参数来演示下复杂的JS对抗。对了，这个题图，主要是因为携程给他们这个反爬的JS起了一个名字叫oceanball-海洋球，不明觉厉啊。好了，言归正传。做过携程酒店爬虫的朋友，估计都研究过这个eleven参数，这个参数到底是哪里的呢，我们先看下页面请求：就是这样一个页面，打开一个酒店页面会发现实际的酒店房型列表是一个ajax请求，如下：http://hotels.ctrip.com/Domestic/tool/AjaxHote1RoomListForDetai1.aspx?psid=&amp;MasterHotelID=441351&amp;hotel=441351&amp;EDM=F&amp;roomId=&amp;IncludeRoom=&amp;city=2&amp;showspothotel=T&amp;supplier=&amp;IsDecoupleSpotHotelAndGroup=F&amp;contrast=0&amp;brand=0&amp;startDate=2017-08-28&amp;depDate=2017-08-29&amp;IsFlash=F&amp;RequestTravelMoney=F&amp;hsids=&amp;IsJustConfirm=&amp;contyped=0&amp;priceInfo=-1&amp;equip=&amp;filter=&amp;productcode=&amp;couponList=&amp;abForHuaZhu=&amp;defaultLoad=T&amp;TmFromList=F&amp;eleven=c4350e460862b69d9d76724e1325a0a54ef23c2e0648636c855a329418018a85&amp;callback=CASuBCgrghIfIUqemNE&amp;_=1503884369495前面咱说过，出于对反爬工程师工作的尊重，我们今天的文章不去完整介绍整个携程爬虫的做法，其实除了这eleven参数，携程还是在代码了下了不少毒的。一般来说，一个ajax请求，如果没有做cookie限制的话，最难的问题就是能把所有的参数拼接完整，这个请求中最难的就是这个eleven参数的获取，那咱们遇到这种问题，应该具体如何处理，同时对于反爬工作来说，又有什么可以借鉴的地方呢。一、反反爬中遇到复杂JS请求的处理流程凡是题目总有一个解题思路，反反爬也不例外，解题思路很重要。不然就像没头苍蝇一样到处乱撞，今天这篇文章最重要的就是说说这个解题思路。1.查看发起这个请求的JS来源首先，Ajax请求大多都是由JS发起的（今天我们不讨论flash或者其他情况，不常见），我们使用Chrome工具，将鼠标移动到这个请求的Initiator这一栏上，就可以看到完整的调用栈，非常清晰。2.确认核心JS文件可以看到调用栈中有非常多的JS，那具体我们要分析哪个呢？一般来说我们可以首先排除掉VM开头的和常见库如jQuery这类，当然也不绝对，有些JS也会把自己注册到VM中去，这个另说，携程这里的情况挺明显，一眼就看到了那个不寻常的oceanball，看着就不是一般人，就他了。3.分析JS文件点击这个oceanball咱们就可以看到完整了源码了，下面也是最难的一步，分析JS文件。上一篇文章已经简单说了一些，首先，可读性非常重要，咱们复制完整的JS，贴进Snippet中，咱们大概看下，文件太大，没办法贴到文章中来：一看就是在代码里面下毒，携程反爬工程师不简单啊。不过呢，咱们看到这种很多数字的其实不要害怕，特别是在看到eval函数和String.fromCharCode，因为这就是把代码加密了一下，这种加密就跟没加密一样，咱们把eval函数直接换成console.log函数，运行一下这个Snippet。在控制台就可以看到原始代码。解密后如下：看来这个是毒中毒啊，解密后的代码虽然比刚刚的可读性大为增加，但是依然可读性不强，不过根据经验，这样的文件已经没有一个特别简单的方案可以一举解密，只能逐行分析加部分替换了。当然直接Debug也是一个最省力的方案。通过Debug调试，我们可以看到很多变量的中间值，这中间最重要的莫过于这最后一行，我们可以看到Eleven参数这个时候已经算出来了，而向CASttwHNetheyMWqSJ这个函数中传入一串实现，而这个实现正式返回eleven的值（这段代码是要把人绕死的节奏吗），所以我们只需要构造一个假的CASttwHNetheyMWqSJ来接收这段实现即可，而CASttwHNetheyMWqSJ这个函数名也是通过url传入的，好了，基本上大体的分析搞定了，下面就是构造这个代码的运行环境了。4.构造运行环境重要的话要重复很多遍：写爬虫最好的语言就是JS，因为JS对抗是爬虫中最难的部分，而一个合适的JS环境则可以事半功倍。全球最好的JS爬虫框架-在线网络爬虫/大数据分析/机器学习开发平台-神箭手云。有了JS运行环境，我们其实只需要构造一个函数接受Eleven参数，请求oceanball，并直接运行他就可以了。我们看下我们构造的CASttwHNetheyMWqSJ函数：var callbackFunc = randomCallback(15); var getEleven; eval(&quot;var &quot;+callbackFunc+&quot; = function(a){getEleven = a}&quot;);这里对于初学者可能还是有点绕，简单说明下，首先我们随机了一个函数名，然后我们定义了一个空函数来接受oceanball中的返回eleven参数的函数实现，然后我把这个函数名定义为接受new Function的实现，这样我们后面就可以用getEleven来直接获取eleven参数了。这里还得说下普通JS环境的缺陷，由于在普通JS环境中（非浏览器中）缺少一些重要的内置变量，如window，document等等，导致很多JS是运行不了的，这里我们补上这些变量：var Image = function(){}; var window = {}; window.document = {}; var document = window.document; window.navigator = {&quot;appCodeName&quot;:&quot;Mozilla&quot;, &quot;appName&quot;:&quot;Netscape&quot;, &quot;language&quot;:&quot;zh-CN&quot;, &quot;platform&quot;:&quot;Win&quot;}; window.navigator.userAgent = site.getUserAgent(); var navigator = window.navigator; window.location = {}; window.location.href = &quot;http://hotels.ctrip.com/hotel/&quot;+hotel_id+&quot;.html&quot;; var location = window.location;JS如果比较熟悉的话，应该能看出来这里有一个变量不常规，就是第一行的Image变量。这又是携程下的毒啊，我们回到刚刚oceanball的代码看下：代码中有一句尝试new一个Image对象，如果失败了，则把某一个参数+1，而这个参数正是eleven参数中的某一位，也就是说eleven参数中有一位记录了JS运行环境是否支持new Image，我们这种伪造的浏览器环境当然不支持，所以我们得补上。5.大功告成好了，解了这么多毒，咱们终于可以获取到了eleven参数，当然这只是漫漫长征第一步，后面请求到的结果中，还有更深更辣的毒等着大家：二、反爬中可以借鉴的地方首先非常感谢携程反爬工程师给了一个教科书般的JS反爬案例，从这个例子我们也可以看出反爬中使用JS代码加密混淆的威力，基本上如果不是JS熟手，或者不熟悉解题思路的话，就是束手无策。JS混淆的方案有很多，大家可以到网上搜一搜，这里还是很推荐携程这种ajax配合回调+js二次混淆的方案，可以说极大提升了反反爬难度的同时也做到了基本不影响性能。但是这个方案依然对于直接渲染JS页面效果一般，还是建议配合之前文章提到的css:content的方案，这样处理之后那对于反反爬工程师的酸爽绝对够得上100桶统一老坛。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫特殊反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CssContent字体破解]]></title>
    <url>%2F2017%2F05%2F23%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC%2FCssContent%E5%AD%97%E4%BD%93%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[爬虫与汽车之家的Css:Content-反爬与反反爬的奇技淫巧发布于话说感觉这个系列的我起名字越来越不走心了。越写越像哈利波特的起名套路了-爬虫与混血王子。（嗯，一点都不违和）我这个爱扯犊子的性格真是很难收敛啊。话说Css自从越来越强大之后，被很多反爬工程师看上了。上篇文章介绍的字体就需要用到Css中的font-face和font-family。这节我们通过汽车之家来看看Css在反爬中另外一个妙用-content。本来网页上显示的字无非两种，一种就是文本，一种就是显示在图片上的。所以之前有一种常规的反爬，就是把字当做图片替换来显示，目前百度还经常喜欢这么处理，比如百度知道，百度指数（参考天坑），当然不少的电话号码和邮箱这类重要信息，依然也再沿用这个方案。然后随着css的content兼容性越来越好之后，就又有了一个性能更好的反爬，就是用css的content来代替原来文本中的文字，辅以合适的随机生成的方法，确实也不错。算是一个性能和反爬折中的方案吧，比如我们今天要提到的 汽车之家论坛 就是采用这样一个反爬方案。一.为什么Css:Content能反爬。事实上任何能让原来html中文本隐藏混淆加密又不影响正常用户显示的方式都可以来做反爬，比如我们上一篇文章提到的字体方式混淆，比如我们后面要提到的JS加密。而这篇文章主要是采用Css的content属性来隐藏。我们简单的看下content属性使用，大家可以尝试复制这段写入一个网页的Css中div:before {content: &quot;神箭手&quot;;}我们会发现每一个div的开头都加入的神箭手的文字，而且这些文字还是无法复制的。再加上并不像生成图片那样会严重拖慢服务器性能，所以算是一个挺折中的方案。二.如何好好利用Css:Content反爬我们一起来看看汽车之家的反爬学习一下：我们打开一个汽车之家论坛网页，然后直接Ctrl+A全选，就可以很明显的看到用Css的Content插入进的字符，我们掀开被子仔细研究下：可以看到，汽车之家是将一些常用的字，包括&lt;了&gt;&lt;，&gt;&lt;的&gt;&lt;九&gt;等等变换成Css，这样做优点是可以提前写好css，缺点是由于是固定的库，很容易先把映射的关系解析好，然后直接全文替换。因此比较推荐的，当然还是每次的动态生成，不过这依然产生一个工程性的优化问题，这里还是比较推荐用池的方案，或者用客户的IP做一个hash映射也不错。不过汽车之家做的比较好的在于很好的隐藏了这段固定映射的Css，同时Css中的class也每次会替换一些名称部分，同时又不是每次都把所有的映射库输出出来，而是节选文章中有的文字，因此既缩小了输出大小，又隐藏了整体库，看得出工程师也是打得一手好牌啊。三.如何应对Css:Content这种类型的反爬我们就以汽车之家为例吧：1.获取映射的Css其实汽车之家这个例子来说，主要的问题并不是如何解决这种反爬，因为这种模式的反爬如果知道映射，搞起来太简单，直接正则替换就行了。这里最难的是怎么找到这个映射，汽车之家可以说隐藏的是教科书级的完美，解析方案我就不展开讲了，因为这个是Javascript对抗里的内容（后面我可能会拿出几篇文章单独讲JS对抗），重点就是解析跟在正文后面的这一段JS。好的工具等于成功的一半，想要很好的解析JS，一定要有一个好的JS解释引擎。这样可以省掉大量的破解的工作而直接运行别人的JS，在代码中整合脚本引擎相当麻烦，最完美的莫过于找一个好的JS的爬虫框架。具体的Javascript对抗我在后面再讲，这里简单说下，直接运行这段代码，我们可以看到在Nl_（可变的）这个变量中存储了完整的Css映射，因此我们想办法在运行中保存该变量，带入下一步即可。2.根据映射进行内容替换我们既然拥有了Css映射，那还有啥难的，直接读取CssRules，循环用正则替换把&lt;span class='hs_kw4_mainYM'&gt;&lt;/span&gt;这类标签直接替换成content中的文字：九好了，大功告成。贴一下部分神箭手中运行的解析代码：var code = extract(page.raw, &quot;//div[@class='conttxt']/div/script&quot;); if(code){ eval(code); var cssMapping = {}; if(rules.length &gt; 0){ for(var ri in rules){ var rule = rules[ri]; var matches1 = /content:\s*&quot;([^&quot;]+)&quot;/.exec(rule) var matches2 = /\.([^:]+)::before/.exec(rule) if(matches1 &amp;&amp; matches2){ cssMapping[matches2[1]] = matches1[1]; } } page.raw = page.raw.replace(/&lt;span\s+class='(hs_kw\d+_[^']+)'&gt;&lt;\/span&gt;/g, function(match,p1,p2){ return cssMapping[p1]; }); } } page.raw = exclude(page.raw, &quot;//div[@class='conttxt']/div/script&quot;);————————————————最后说两句————————————————————大家会发现，很多非常奇特的反爬措施，大多依赖Javascript加密的功力。当然有些Javascript加密可以直接通过JS渲染，如[Email Protection]，但是也有很多单纯通过渲染JS无法解决，如今天这篇文章中提到，因此也让Css的这种反爬措施有别于普通的JS加密形式，给反反爬工程师设置了更高的障碍，是我个人比较推荐的一种反爬手段。反过来对于反反爬来说：在有了好的工具前提下，熟练掌握JS则是一个必备内功之一。文章导航]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫特殊反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CloudFlare邮箱加密]]></title>
    <url>%2F2017%2F05%2F23%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC%2FCloudFlare%E9%82%AE%E7%AE%B1%E5%8A%A0%E5%AF%86(cfemail)%2F</url>
    <content type="text"><![CDATA[爬虫与CloudFlare邮箱加密(cfemail)-反爬与反反爬的奇技淫巧发布于由于Javascript这块的内容很多，难度也不一样。所以这一篇文章希望给大家讲一些入门级别的。本来找来找去感觉都是高段位玩家，然而今天在做邮箱地址爬虫的过程中一段代码突然跳到了我的面前-邮箱地址加密JS。恰好难度适中，就今天拿出来跟大家一起聊一聊：下面我们就进入正题，先来看看这个文章标题上的内容跟我们这篇文章到底有啥关系：CloudFlare是一家美国的跨国科技企业，总部位于旧金山，在英国伦敦亦设有办事处。CloudFlare以向客户提供网站安全管理、性能优化及相关的技术支持为主要业务。通过基于反向代理的内容传递网络(ContentDeliveryNetwork,CDN)及分布式域名解析服务(DistributedDomainNameServer)，CloudFlare可以帮助受保护站点抵御包括拒绝服务攻击(DenialofService)在内的大多数网络攻击，确保该网站长期在线，同时提升网站的性能、访问速度以改善访客体验。看完了好像还是没关系，别急，我们今天要讲的是CloudFlare中的一个功能，叫做Email Obfuscation，也就是邮箱混淆。当我们使用了 CloudFlare 的服务，如果开启了 Email Obfuscation ，页面里真正的 Email 地址会被隐藏，具体隐藏的代码如下：&lt;a class=&quot;__cf_email__&quot; href=&quot;/cdn-cgi/l/email-protection&quot; data-cfemail=&quot;a89b9b9c919d9b909a989ae8d9d986cbc7c5&quot;&gt;[email&amp;#160;protected]&lt;/a&gt;&lt;script data-cfhash='f9e31' type=&quot;text/javascript&quot;&gt;/* &lt;![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]&gt; */&lt;/script&gt;做过爬虫的朋友应该都对这段代码不陌生，非常常见的一个邮箱混淆服务。一.为什么要使用邮箱混淆服务。事实上很多站长可能并不知道自己使用了这样的服务，因为CloudFlare主要还是以CDN为主，这个服务是附加的，而且CloudFlare也做的很贴心，几乎不用什么设置，就可以自动混淆。（实际上是识别了输出的页面里是否有邮箱，有邮箱就替换成[email protected]并在下面添加一段JS代码）。无论邮箱和电话号码，即使主动留在互联网上，也不希望被人批量获取，所以邮箱混淆一直是件很重要的事情。包括用at代替@，用#代替@，还有生成图片的。而CloudFlare提供了一种完全不需要修改代码的方案，确实给了大家很大的方便。二.CloudFlare邮箱混淆服务的优劣那么我们使用这种方案有什么优点和缺点呢？我们先说说优点：首先不用改代码，很方便；其次全局替换，不会有遗漏；最后JS混淆，比at和#更加彻底也对显示影响最小，毕竟at和#用的人多了，就和@一样了，有时候还会让真正想联系的客户摸不着头脑。我们再说说缺点：我在验证码那篇文章中就提到过的成本回报比概念，当很多人用一个方案的时候，由于回报无限增大导致无论这个方案有多么好，他被破解的概率都会大大增加。当然CloudFlare混淆远远不止这个问题这么简单。他的这个混淆有时候恰恰使得这个页面中的邮箱标志更加明显，有点类似本来你把金子放在地上，很危险。现在把金子埋在地下，然后为了自己能找到，又画了一个此处有金子的标记。事实上在真正识别邮箱的爬虫中，CloudFlare反而降低获取Email的难度。所以先提前说反爬的结论，严重不推荐大家使用这个方案！三.写爬虫时遇到CloudFlare邮箱混淆，如何解密？说完了反爬，再说反反爬。这个我就不得不提工具的重要性了。有时候你遇到一个好工具，那真的一身轻松。这里需要再次强调，写爬虫最好的语言是JS，因为JS对抗是爬虫中最难的部分，框架本身就是JS的环境将使得事半功倍：世界上最好的JS爬虫开发框架- 在线网络爬虫/大数据分析/机器学习开发平台-神箭手云。好了，今天的盒饭有着落了。当然我们这篇文章还是得给爬虫工程师来点干货的。我们今天就通过这个简单的例子来看看写爬虫时遇到复杂的JS到底怎么分析。1.格式化代码JS分析最重要的是格式，因为大部分JS代码都是混淆且压缩的。基本是不具备任何可读性，先格式化成可读的形式最重要。格式化的工具很多，这里我最推荐Chrome浏览其中的Snippet。因为格式化完了之后还可以调试，简直是神器。我们把前面例子中HTML代码部分删除掉，留下JS部分，贴进Snippet，点击左下角的{}按钮格式化：2.分析代码是不是看着舒服太多了，已经到了人眼能看的级别了。我们贴出来看看：!function(t, e, r, n, c, a, p) { try { t = document.currentScript || function() { for (t = document.getElementsByTagName('script'), e = t.length; e--; ) if (t[e].getAttribute('data-cfhash')) return t[e] }(); if (t &amp;&amp; (c = t.previousSibling)) { p = t.parentNode; if (a = c.getAttribute('data-cfemail')) { for (e = '', r = '0x' + a.substr(0, 2) | 0, n = 2; a.length - n; n += 2) e += '%' + ('0' + ('0x' + a.substr(n, 2) ^ r).toString(16)).slice(-2); p.replaceChild(document.createTextNode(decodeURIComponent(e)), c) } p.removeChild(t) } } catch (u) {} }()我们先大概看下整个代码，首先外层就是一个函数的定义及直接调用。大部分的JS库都会采用这种方法，既可以保证代码模块之间变量不相互污染，又可以通过传入参数实现内外部变量的传输。再读代码第一段：获取变量t，这个过程我们结合前面的HTML代码可以看出，这个是在获取Email被加密后的Dom元素，为了后面获取data-cfemail做准备。最后看第二段：显然就是从Dom元素中获取data-cfemail并解密出真实的Email并替换到页面显示中去。3.整合进爬虫一般来说，对于复杂的JS，我们还会有断点调试和其他分析的过程，这里的JS很简单，所以咱直接开始写代码。我们怎么在像神箭手这样的JS爬虫框架中处理呢，同样也非常简单：var cfemails = extractList(content, &quot;//*[@data-cfemail]/@data-cfemail&quot;); for(var c in cfemails){ var a = cfemails[c]; for (e = '', r = '0x' + a.substr(0, 2) | 0, n = 2; a.length - n; n += 2) e += '%' + ('0' + ('0x' + a.substr(n, 2) ^ r).toString(16)).slice(-2); var emailDecoded = decodeURIComponent(e); console.log(emailDecoded); }可以看到，我们先通过xpath直接获取所有的data-cfemail的值，然后直接把CloudFlare这段解密JS复制过来就行了。运行后就可以直接获取该页面所有被CloudFlare混淆过的邮箱，简直比直接用正则提取邮箱还要简单还要准确。————————————————最后再说两句——————————————————–通过这篇文章中这个非常常见却又相对入门的例子，我们一起看了下Javascript在反爬与反反爬中扮演的角色，同时也看到了一个JS爬虫框架的重要性，如果是其他爬虫框架，要么需要自己整合JS引擎，要么就得读懂整段解密代码再翻译成那种语言，这个难度在我们后面会提到的一些JS加密中，将不敢想象。同时我们也可以看到类似CloudFlare这类通用邮箱混淆的脆弱性。文章导航]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫特殊反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy ufunc和文件读写]]></title>
    <url>%2F2017%2F05%2F23%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FNumpy%20ufunc%E5%92%8C%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99%2F</url>
    <content type="text"><![CDATA[Numpy的ufuncNumpy的ufunc函数的介绍ufunc是universal function（通用函数）的缩写，是一种对ndarray中的数据执行元素级的运算。既可以对单个数值操作，也可以对数组的每个元素进行操作的函数（支持广播运算）。NumPy内置的许多ufunc函数都是在C语言实现的，因此它们的计算速度非常快，我们可以将其看做是简单的函数(接收一个或多个参数，返回一个或者多个返回值)。常用的元素计算函数一元ufuncnp.ceil (x): 求每个元素向上最接近的数参数x: number 或 ndarranp.floor(x): 求每个元素向下最接近的数参数x: number 或 arraynp.rint(x): 将数组的每个元素四舍五入参数x：number 或 arraynp.negative(x): 将数组的每个元素取反参数x: number 或 arraynp.abs(x)：取数组每个元素的绝对值参数x: number 或 arraynp.square(x)：求数组每个元素的平方参数x: number 或 arraynp.aqrt(x)：求数组元素的平方根参数x: number 或 array注意：当被开平方的元素为负数的时候，返回的数值类型为sp.nannp.sign(x)：计算各元素的正负号, 返回1(正数)、0（零）、-1(负数)参数x: number 或 arraynp.modf(x)：将数组的小数和整数部分以两个独立数组的形式，并返回一个包含所有数组的元祖参数x: number 或 arraynp.isnan(x): 判断元素是否为 NaN(Not a Number)，返回bool参数x: number 或 array示例二元ufuncnp.add(x, y): 元素相加，x + y参数x, y: number 或 array（一维、或者相同维度多维）np.subtract(x, y): 元素相减，x - y参数x, y: number 或 array（一维、或者相同维度多维）np.multiply(x, y): 元素相乘，x * y参数x, y: number 或array（一维、或者相同维度多维）np.divide(x, y): 元素相除，x / y，分母不能包含零参数x, y: number 或 array（一维、或者相同维度多维）np.floor_divide(x, y): 元素相除取整数商(丢弃余数)，x // y参数x, y: number 或 array（一维、或者相同维度多维）np.mod(x, y): 元素求余数，x % y，分母不能包含零 参数x, y: number 或 array（一维、或者相同维度多维）np.power(x, y): 元素求次方，x ** y参数 x, y: number 或array（一维、或者相同维度多维）三元ufuncnp.where(condition, x, y): 通过condition条件判断，条件满足返回x，否则返回y，最终返回一维数组参数condition: 条件语句 x , y : number 或 与condition的bool数组维度个数和大小相等的数组示例常用的元素统计计算函数备注多维数组默认统计全部的数据，添加axis参数可以按指定轴心统计值为0则按列统计，值为1则按行统计，默认统计所有的数据np.mean(x [, axis])：所有元素的平均值，返回的每个元素为浮点数参数x: arraynp.sum(x [, axis])：所有元素的和参数x: arraynp.max(x [, axis])：所有元素的最大值参数:x: arraynp.min(x [, axis])：所有元素的最小值参数x: arraynp.std(x [, axis])：所有元素的标准差，返回的每个元素为浮点数参数x: arraynp.var(x [, axis])：所有元素的方差，返回的每个元素为浮点数参数x: arraynp.argmax(x [, axis])：最大值的下标索引值（会把多维数组当成一维数组来计算）参数x: arraynp.argmin(x [, axis])：最小值的下标索引值（会把多维数组当成一维数组来计算）参数x: arraynp.cumsum(x [, axis])：返回一个一维数组，每个元素都是之前所有元素的累加和参数x: arraynp.cumprod(x [, axis])：返回一个一维数组，每个元素都是之前所有元素的 累乘积参数x: array条件判断函数np.any(conditions , [ axis]) 有一个元素满足指定条件，返回True,否则返回false参数condition: 条件语句axis：多维数组默认统计全部的数据，添加axis参数可以按指定轴心统计值为0则按列统计，值为1则按行统计，默认统计所有的数据np.all(conditions , [ axis]) 所有的元素满足指定条件，返回True，否则返回false参数condition: 条件语句axis：多维数组默认统计全部的数据，添加axis参数可以按指定轴心统计值为0则按列统计，值为1则按行统计，默认统计所有的数据示例 增加、删除元素的函数（ndarray支持常用的增加和删除操作，以及数组的合并）np.append(arr1, arr2)：在arr1后面追加arr2的数据，并返回一维数组参数arr1：number 或array,都当做一维数组处理,两数组维度可以不一致arr2： number或array,都当做一维数组处理,两数组维度可以不一致np.insert(arr1, index, arr2, [ axis])：在arr1的 index 位置增加 arr2 数据参数arr1： number或arrayindex：执行插入数据的位置下标arr2：number或列表或array, 插入array的时候只能是一维数组，需要注意和arr1的维度大小一致示例np.delete(arr1, index, [ axis])：按axis的方向，删除arr1的指定下标位置元素参数arr1： number或arrayindex：number或列表，代表删除元素的下标集合axis：axis = 1，删除每一行的index下标列数据；axis =0，删除index对应多行数据np.concatenate((arr1,arr2,...), [axis=0])：合并多个数组,数组的维度不改变参数arrx：number 或array， 多个数组的维度个数保持一致axis：axis = 1，按照行合并；axis =0，按照列合并，默认为列合并示例集合操作函数(ndarray的集合运算)np.unique(x) : 对x里的数据去重，并返回有序的一维数组.参数：x：array多维数组np.intersect1d(x, y) :计算x和y中的公共元素、去重，并返回有序一维数组, x &amp; y参数：x, y: array多维数组np.union1d(x, y) :计算x和y的并集、去重，并返回有序一维数组. x | y参数x, y: array多维数组np.setdiff1d(x, y): 集合的差,即元素在x中且不在y中,并去重,返回有序一维数组. x - y, y - x参数x, y: array多维数组np.setxor1d(x, y): 对称差集,两个数组中互相不包含的元素,并去重,返回有序一维数组。x ^ y参数x, y: number或array多维数组np.in1d(x, y) : 得到一个表示“x中的的每个元素是否包含于y”的布尔型数组[ bool ].参数x: number或array多维数据y: array多维数组排序函数(ndarray的数组排序函数)ndarray.sort([axis]) 在原数组上进行递增排序，改变原数组的元素顺序，不改变维度参数axis：axis = 1，按照行排序；axis =0，按照列排序，默认对全部的数据进行排序np.sort(ndarray, [axis]) 返回递增排序排序后的新数组，改变元素顺序，不改变维度参数axis：axis = 1，按照行排序；axis =0，按照列排序，默认对全部的数据进行排序ndarray：多维数组对象adarray.argsort() 对数组进行从小到达排序，并返回排序后的每个元素之前对应的索引Numpy的文件读写不指定文件数据格式的保存和读取np.save('file_name', arr) 保存数据到磁盘文件中，默认格式为.npy参数file_path: 保存文件的文件名，不需要添加 文件后缀，默认格式为.npyarr：将要被保存的数组arr = np.load('file_name.npy') 读取文件内容，默认格式为.npy，返回一个包含文件中数据的Numpy数组对象参数file_name: 读取的文件名，需要添加文件的格式指定文件数据格式的保存和读取（通常可以指定文件格式为csv等）savetxt('file_name', arr, delimiter=',', fmt='%d') 将数据保存到磁盘文件里。参数：filename: 保存文件的文件名，需要指定文件的后缀，例如.csvarr: 将要被保存的数组delimiter: 指定分隔符的类型，通常使用','fmt：指定保存文件的数据类型，%d为数字arr = np.genfromtxt('file_name', delimiter=',', dtype=np.unicode, usecols=[x, y, z]) 将数据加载到普通的Numpy数组中参数：filename: 读取文件的文件名，需要指定文件的后缀，例如.csvdelimiter: 指定分隔符的类型，通常使用','dtype： 指定数据的读取产生数组的数据类型usecols：指定读取文件中数据的哪几列np.loadtxt() 将数据加载到普通的Numpy数组中，一般不常用]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas的索引查询操作与对其运算]]></title>
    <url>%2F2017%2F05%2F23%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FPadas%E7%9A%84%E7%B4%A2%E5%BC%95%E6%9F%A5%E8%AF%A2%E6%93%8D%E4%BD%9C%E5%AF%B9%E9%BD%90%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[索引对象Index介绍索引的介绍Series和DataFrame中的索引都是Index对象索引对象的代码示例索引对象的性质索引对象不可变，保证了数据的安全性（下面尝试对索引进行修改，会报错）常见的Index种类Index 索引Int64Index 整数索引MultiIndex 层级索引DatetimeIndex 时间戳索引Pandas数据结构的常见索引操作Series的索引查询操作创建Series对象并指定Series行索引名通过行索引获取行数据（通过索引名或索引下标）通过索引名来取值ser_obj['name']name: 为索引的名字通过索引的下标来取值ser_obj[n]n： 为索引的下标示例备注：如果索引名也是数字，那么按索引名进行取值通过连续索引获取行数据（切片）通过不连续索引获取行数据通过布尔索引获取行数据 DataFrame索引查询操作创建DataFrame对象并指定行\列索引通过列索引获取列数据（通过索引名） 返回Series类型通过不连续索引获取多列数据 返回DataFrame类型（不支持连续多列）通过连续索引取所有列的多行数据（不支持不连续多行）通过索引取多行多列数据通过布尔索引获取行数据不满足条件的返回Nan值高级索引：标签、位置、混合loc标签索引介绍DataFrame 不能直接列切片，可以通过loc来做切片loc是基于标签名（索引名），也就是我们自定义的索引名来取值Series对象取多行数据DataFrame取值方法取单行数据备注:取单行数据的时候，会将不同列的数据类型进行转化为同一类型取单列数据取单行单列数据取连续行的所有列数据取连续列的所有行数据取连续的行和列取不连续的行取不连续的列取不连续的行和列根据布尔条件取值根据某一列的数据判断所有行的该列，返回符合条件的行iloc位置索引介绍作用和loc一样，不过是基于索引编号（下标）来取索引值Series对象取多行数据DataFrame取值方法取多行数据（取值下标不包含末位置）取连续多行多列数据（取值下标不包含末位置）取不连续多行多列数据ix标签与位置混合索引介绍ix是以上二者的综合，既可以使用索引编号（索引下标），又可以使用自定义索引（自定义索引名），要视情况不同来使用；如果索引既有数字又有英文，那么这种方式是不建议使用的，容易导致定位的混乱。由于经常会引起混淆，不推荐使用，上面两种方法，可以涵盖所有的需求Pandas的对齐运算pandas对齐运算的介绍是数据清洗的重要过程，可以按索引对齐进行运算，如果没对齐的位置则补NaN，最后也可以填充NaNPandas的对其运算的操作常见的数据运算方法add, sub, div, mulSeries数据间的对齐运算方法:（对未对齐的部分会进行Nan填充，直接相加是不会进行填充的）ser_obj3 = ser_obj1.add(ser_obj2, [fill_value=0.])参数fill_value: 表示提供一个数据进行未对齐的运算（不是填充到运算结果里面）示例DataFrame数据间的对齐运算操作方法（对未对齐的部分会进行Nan填充，直接相加是不会进行填充的）df_obj4 = df_obj1.add(df_obj2, [fill_value=0.])参数fill_value: 表示提供一个数据进行未对齐的运算（不是填充到运算结果里面）备注：当存在运算的两个数据都不存在某索引的情况，即使给出fill_value参数结果还会是NaN示例DataFrame 和 Series 之间对齐运算方法df_obj1.add(ser_obj1, axis = 0)axis：可以通过axis参数指定Series的运算方向， axis = 0 将Series看成一列多行， axis = 1 将Series看成一行多列示例]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas的数据结构和基础操作]]></title>
    <url>%2F2017%2F05%2F23%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FPandas%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[什么是PandasPandas的介绍Pandas的名称来自于面板数据（panel data）和Python数据分析（data analysis）。Pandas是一个强大的分析结构化数据的工具集，基于NumPy构建，提供了 高级数据结构 和 数据操作工具，它是使Python成为强大而高效的数据分析环境的重要因素之一。Pandas的作用一个强大的分析和操作大型结构化数据集所需的工具集基础是NumPy，提供了高性能矩阵的运算提供了大量能够快速便捷地处理数据的函数和方法应用于数据挖掘，数据分析提供数据清洗功能Pandas的导包形式import pandas as pdPandas文档http://pandas.pydata.orgPandas的数据结构Pandas最主要也是最重要的两个数据结构Series和DataFramePandas-Series数据结构Series数据结构的介绍Series是一种类似于一维数组的 对象，由一组数据（各种NumPy数据类型）以及一组与之对应的索引（数据标签）组成。类似一维数组对象由数据和索引组成索引(index)在左，数据(values)在右索引是自动创建的Series数据结构的基本操作Series数据 结构的创建通过列表或者range()函数来创建Series，索引类型为从零开始以一为步长递增ser_obj = pd.Series(range(10))通过ndarray来创建Series，索引默认为从零开始以一为步长递增ser_obj = pd.Series(arr) arr 为一维ndarry数组通过dict来构建Series，字典的键对应索引ser_obj2 = pd.Series(dict)创建的时候指定索引,注意索引数量和值的数量相等ser_obj = pd.Series(range(10, 15), index=list('abcde'))通过索引来获取数据（后面有详细通过索引对数据进行查询的方法）ser_obj[idx]name属性定义Series对象名：ser_obj.name定义Series对象索引名：ser_obj.index.name查看前几行的数据ser_obj.head(x)x 代表 查看前多少行的数据，默认为查看所有行的数据删除行数据第一种方法(不会修改原数据)ser_obj.drop([index1, index2, ...])第二种方法（会在原数据的基础上进行删除）del(ser_obj[index])Series获取数据和索引索引的获取ser_obj.index 返回Series的索引对象索引对象的转化数据的获取ser_obj.values 返回一个一维ndarray Pandas-DataFrame数据结构DataFrame数据结构的介绍DataFrame是一个表格型的数据结构，每一列的数据都是有序的，不同列之间的数据类型可以不相同。DataFrame既有行索引也有列索引，它可以被看做是由多个Series组成的字典（共用行索引），数据是以二维结构存放的。类似多维数组/表格数据 (如，excel, R中的data.frame)每列数据可以是不同的类型索引包括列索引和行索引DataFrame数据结构的基本操作DataFrame结构的创建通过ndarray创建DataFrame，行列索引都是从0开始以一不步长递增 df_obj = pd.DataFrame(array)array 为numpy的数组对象通过dict创建DataFrame，字典的键为列索引 ，值为每一列的所有数据df_obj = pd.DataFrame(dict_data)dict_data为python的字典类型数据，每个键对应的数据类型可以不一致示例创建DataFrame同时指定行索引df_obj = pd.DataFrame(arr, index=list('abc'), colums=list('abcd'))index: 指定的行索引colums：指定的列索引通过列索引获取数据-返回的数据为series类型第一种方法df_obj[col_idx]第二种方法df_obj.col_idx示例增加列数据df_obj[new_col_idx] = data 类似于Python的dict添加key-value示例删除列数据del(df_obj[col_idx])示例删除行或列数据（不会修改原数据）df_obj.drop([index1, index2, ...], axis = 0)axis: 0表示删除每一列中的指定行数据，1表示删除每行的某些列查看前几行的数据df_obj.head(x)x 代表 查看前多少行的数据，默认为查看所有行的数据DataFrame设置行索引DataFrame.set_index(keys, drop=True, append=False, inplace=False) keys：列表，表示设置为索引的列，列表中包含 多个值，表示设置联合索引append：添加新索引drop：True表示将该列作为索引后，数据列将会丢弃inplace：True表示在原数据上进行修改DataFrame还原行索引（将行索引还原为普通数字索引，并将索引还原为数据列）DataFrame.reset_index(level=None, drop=False, inplace=False) level：表示具体要还原的哪个等级的索引 drop：False表示索引列会被还原为普通列，否则会丢失inplace：True表示在原数据上进行修改DataFrame获取数据和索引行索引的获取df_obj.index 返回行索引对象列索引的获取df_obj.columns 返回列索引对象索引对象的转化数据的获取ser_obj.values 返回一个多维ndarray重命名列名df_obj.rename(columns={'old_key': 'new_key'}, [inplace=True])将old_key一列的列名，改为new_keyinplace: True在原数据的基础上进行修改，False会返回新的修改后的数据，默认为False使用pandas读写文件读取csv文件数据的方法df_obj = pd.read_csv('filename', [usecols=['col1', 'col2'], skiprows=n1, skipfooter=n2, index_col=n3, engine='python']) 返回的是DataFrame对象filename：表示读取的文件名usecols：表示读取文件的哪些列skiprows: 表示跳过文件的前几行skipfooter： 表示跳过文件的后几行index_col：表示使用那一列作为索引列engine： 使用的解释器引擎， 当读取中文文件的时候需要指定写入csv数据的方法df_obj.to_csv(&quot;filename&quot;)filename：表示读取的文件名,要加上.csv后缀；读取excel的方法data_frame = pd.read_excel(report_path, sheet_name=sheet_name, skiprows=skiprows, index_col=index_col, usecols=usecols)report_path: 读取的文件路径sheet_name: 读取的sheet名称，不指定默认读取第一个sheetskiprows: 跳过前面的几行， 读取的结果会将包含内容的第一行作为索引行index_col: 指定某列当做索引列use_cols: 使用A, B,C ... 拼接的字符串写入excel的方法excel_writer = pd.ExcelWriter(save_path, engine='openpyxl')dataframe.to_excel(excel_writer=excel_writer, sheet_name=sheet, index=False)sheet = sexcel_writer.book.worksheets[sheet_index]sheet.column_dimensions[column_dimension_list[index]].width = sizeexcel_writer.save()excel_writer.close()]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy ndarray基本操作]]></title>
    <url>%2F2017%2F05%2F23%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FNumpy%20ndarray%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[ndarray 多维数组(N Dimension Array)ndarray多维数组的介绍NumPy数组是一个多维的数组对象（矩阵），称为ndarray，具有高效的算术运算能力和复杂的广播能力，并具有执行速度快和节省空间的特点。ndarray拥有的属性ndim属性： 维度个数，整数类型shape属性：维度大小，返回的是一个元组，表示每个维度包含下级元素的个数dtype属性：数据类型，ndarray的数据类型其中一种导入模块的方式import numpy as npndarray的数据类型ndarray的数据类型要保持统一ndarray数据的创建和转化数组的创建 随机数组创建 np.randomnp.random.rand(v1, v2, v3 , ...) 生成指定唯独的随机多浮点数组，数据的区间为0.0~1.0参数：有几个代表创建的是几维数组最后一个参数表示每个一维数组有几个元素倒数第二个参数表示二维数组有几个一维数组 倒数第三个参数表示三维数组有几个二维数组，以此类推生成一个v1*v2*v3的多维数组示例：np.random.randn(n) 生成一个包含指定元素个数的正太分布数组参数n:表示数组中包含正态分布元素个数示例np.random.uniform(start, end, size=（x, y, z））生成指定维度大小的随机多维浮点数组，可指定数字区间参数start: 表示数据的起始范围end：表示数据的终止范围区间size：表示数组的大小, 维度大小为x*y*z示例np.random.randint(start, end, size=（x, y, z) ) 生成指定维度大小的随机多维整形数组，可指定数字区间参数start: 表示数据的起始范围end：表示数据的终止范围区间size：表示数组的大小, 维度大小为x*y*z示例指定序列的方式创建np.array(list/list of list，[dtype]) 生成指定的数组参数list 为 序列型对象(list)、嵌套序列对象(list of list)dtype表示数据类型 （int、float、str）示例np.arange(param1, [param2, step]) 可以像 np.array(range()) 一样创建一个连续元素的一维的ndarray参数param1: 表示param1-param2区间(左闭右开)的所有整数，param2省略表示0-param1step: 表示选取区间元素时的步长，默认为1示例数组的初始化np.zeros((param1, param2), [dtype]) 生成指定大小的全0数组参数：生成param1*param2的多维数组dtype：指定数据的类型，默认为float浮点型示例np.ones((param1, param2), [dtype]) 生成大小全部为1的数组参数：生成param1*param2的多维数组dtype：指定数据的类型，默认为float浮点型示例数组的转化ndarray.reshape(param1， param2) 用来重新修改数组的维度，修改每个维度大小，不改变原数组参数param2：表示重新定义的维度的数组的一维数组有多少个元素param1：表示重新定义维度的数组的二维数组有多少个一维数组，以此类推示例备注必须保证修改后的数组的元素和原数组元素个数相同np.random.shuffle(ndarray) 用来打乱数组序列，重新排列（类似于洗牌），会改变原数组参数：ndarray：传入一个数组对象示例ndarray.astype(dtype) 用来转换数组的数据类型 参数dtype：转化的数据类型示例ndarray.transpose((x, y, z)) ndarray的维度转置，不改变原数组参数x，y, z: 对应将之前维度替换为当前的维度映射备注二维数组直接使用转换函数，将行和列进行转换：transpose()高维数组转换要指定维度编号参数 (0, 1, 2, …)，注意参数是元组维度和元素总个数不能改变，只能改变每个维度的大小示例ndarray的矩阵运算ndarray的矩阵运算的介绍矩阵、标量、向量/矢量是数学概念，数组是编程中的概念。在计算机编程中，矩阵可以用数组形式表示，标量可以用数值表示，向量/矢量可以用数据结构表示。向量化/矢量化：就是将自然数据，转化为计算机可以处理的数据结构。数组和数组之间的运算运算方法：数组和数组之间的运算：arr2 + arr2 , 将两个数组维度大小一致、数组维度个数一致、元素个数一致的数组，按下标对应进行运算，并返回新的数组副本尾部维度相同情况：如果一个一维数组的列数和另一个多维数组的列数相同，可以参与运算，相当于将多维数组拆分为多个一维数组运算，最后合并一个多维数组ndarray的矩阵运算 + - * / ** // %， &lt; &gt; == 当数据类型不一致时，那么int--&gt;float, int/float--&gt;str示例数组和数值之间的运算（广播运算）运算方法将数组的每个元素与数值相运算，返回维度一致的新数组ndarray的矩阵运算 + - * / ** // %， &lt; &gt; == 示例ndarray的索引与切片一维度数据的索引与切片语法：索引：arr[i]切片：arr[r1:r2]示例多维数的的索引与切片语法：索引：取单个索引arr[1,1] &lt;==&gt; arr[1][1]切片：连续切片arr[1][1:4] 取一行多列arr[r1:r2, c1:c2] 取多行多列arr[r1:r2][c1:c2] 多行多列错误的写法不连续索引切片arr[ [0, 2] ] 返回 [ arr[0] arr[2] ]arr[ [0, 2], [0, -1] ] 返回 [ arr[0][0] arr[2][-1] ]示例条件索引语法：arr[condition]，condition也可以是多个条件组合。多个条件组合要使用 &amp; | ~ 连接，而不是Python的 and or not。原理将bool索引应用到被过滤的数组上，将返回bool值为True的数据组成的一维数组示例查询数组中元素的索引np.argwhere(condition)查询满足条件元素的索引数组和python列表之间的转化将数组转化为列表list = arr.tolist( ) 返回的是一个列表]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[破解诡异字体]]></title>
    <url>%2F2017%2F05%2F22%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC%2F%E7%A0%B4%E8%A7%A3%E8%AF%A1%E5%BC%82%E5%AD%97%E4%BD%93%2F</url>
    <content type="text"><![CDATA[爬虫与诡异的字体-反爬与反反爬的奇技淫巧从今天这篇文章开始，主要给大家过一些小众的反爬策略以及其中的利弊，所以一般来说都会有一个特定的示例，比如今天的反爬策略就来自反爬界大神之一的去哪儿（当然也是爬虫界的）手机版。我们先来用chrome的手机模式开一下去哪儿的机票:看着叫一个干净整洁，完全没毛病。但是！当我们掀开被子看源码的时候 震惊了：不知道大家能不能看清图，我码字注解一下，显示的价格是560，而源码中的价格则是540！我们这篇文章就来看看去哪儿手机版是怎么做到的（当然偷偷剧透下，下篇文章咱们主角还是去哪儿）其实同学们只要智商在线，看到标题就知道，这是用字体实现，我们看右侧的css面板也可以看到，这个dom元素的字体是单独设置的。也就是去哪儿通过修改字体，让4显示成了6。1.为什么字体可以反爬感觉从这篇文章开始这个套路快进行不下去了。为什么字体可以反爬？ 首先这个迷惑性太大，很多马虎的工程师以为可以了，直接就爬下来，当然就是被老板骂的体无完肤了，对反反爬工程师的精神可以造成1万点伤害。同时呢，去哪儿这个字体是动态生成的字体，也就是说你也不知道下一次刷出来的4到底是几显示出来的。这个道行就比较高了。2.怎么做好字体反爬？这种反爬策略用在数字上确实天衣无缝，非常优雅。唯一值得提的就是一点，如何动态生成字体和页面来做好对应关系。其实这是一个工程性的问题，大部分编程语言都含有生成字体的库，如果对网站整体性能比较自信的话，完全可以每次请求都动态生成，当然这样确实会比较慢，比较推荐是通过定时任务，去更新一个字体池。每次有请求过来，从字体池中随机拿一个字体，换一个随机的名字（可以通过url rewrite来实现），并和现在的数字做一次映射，调整页面显示后整体输出，就可以在尽量不影响性能的情况下搞死反反爬。当然使用字体也是有局限的，其中最大的问题莫过于@font-face的兼容性问题，所以去哪儿只在移动端采用这个反爬策略，可能也有兼容性的考虑吧，相比之下，去哪儿的pc端的反爬写法则丑陋很多。3.遇到字体反爬怎么办？好了，万一我就是要爬去哪儿机票了怎么办呢？其实字体反爬处理也并不复杂，就像前面说到的，大部分语言都有字体处理类库，而这种情况大概率来讲只有10个数字的字体，我们将字体解析后只要能找到对应关系，就简单了。这里如果是Java工程师的话，推荐用Apache.Fontbox，贴一段解析的代码，注意这段代码不保证现在可用，仅做参考：int[] digits = null;BASE64Decoder decoder = new BASE64Decoder();TTFParser parser = new TTFParser();try {byte[] bytes = decoder.decodeBuffer(fontBase64);InputStream inputStream = new ByteArrayInputStream(bytes);TrueTypeFont ttf = parser.parse(inputStream);if(ttf != null &amp;&amp; ttf.getCmap() != null &amp;&amp;ttf.getCmap().getCmaps() != null &amp;&amp;ttf.getCmap().getCmaps().length &gt; 0){digits = new int[10];CmapSubtable[] tables = ttf.getCmap().getCmaps();CmapSubtable table = tables[0];// No matter whatfor(int i =0;i&lt;10;i++){digits[i] = table.getGlyphId(i+48)-1;}}} catch (IOException e) {digits = null;}return digits;]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫特殊反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy和SciPy的介绍]]></title>
    <url>%2F2017%2F05%2F22%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FNumpy%E3%80%81Pandas%2FNumpyScipy%E7%9A%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Numpy（Numerical Python）Numpy介绍：提供了一个在Python中做科学计算的基础库，重在数值计算，主要用于多维数组（矩阵）处理的库。用来存储和处理大型矩阵，比Python自身的嵌套列表结构要高效的多。本身是由C语言开发，是个很基础的扩展，Python其余的科学计算扩展大部分都是以此为基础。功能高性能科学计算和数据分析的基础包ndarray对象，表示多维数组（矩阵），具有矢量运算能力矩阵运算，无需循环，可完成类似Matlab中的矢量运算线性代数、随机数生成导入方法import numpy as npScipyScipy介绍 基于Numpy提供了一个在Python中做科学计算的工具集，专为科学和工程设计的Python工具包。主要应用于统计优化、线性代数模块、傅里叶变换、信号和图像处理、常微分方程求解、积分方程、稀疏矩阵等，在数学系或者工程系相对用的多一些，和数据处理的关系不大，我们知道即可，这里不做讲解。功能在NumPy库的基础上增加了众多的数学、科学及工程常用的库函数线性代数、常微分方程求解、信号处理、图像处理一般的数据处理numpy已经够用导入方法import scipy as sp参考学习资料：Python、NumPy和SciPy介绍：http://cs231n.github.io/python-numpy-tutorialNumPy和SciPy快速入门：https://docs.scipy.org/doc/numpy-dev/user/quickstart.html一、数据分析工作环境Anaconda:Anaconda（水蟒）是一个科学计算软件发行版，集成了大量常用扩展包的环境，包含了 Python 解释器，conda 包管理工具，以及 NumPy、Pandas、Matplotlib 等 180 多个科学计算包及其依赖项，并且支持所有操作系统平台。下载地址：https://www.continuum.io/downloadsconda命令和pip命令对比：安装包：pip install xxx, conda install xxx卸载包：pip uninstall xxx, conda uninstall xxx升级包：pip install upgrade xxx, conda update xxxconda常用命令conda相关命令 1）conda list 查看安装了哪些包。2）conda env list 或 conda info -e 查看当前存在哪些虚拟环境3）conda update conda 检查更新当前conda创建Python虚拟环境。conda create -n your_env_name python=X.X（2.7、3.6等） anaconda 命令创建python版本为X.X名字为your_env_name的虚拟环境。your_env_name文件可以在Anaconda安装目录envs文件下找到。使用激活(或切换不同python版本)的虚拟环境。Linux: source activate your_env_name(虚拟环境名称)Windows: activate your_env_name(虚拟环境名称)对虚拟环境中安装额外的包。conda install -n your_env_name [package]即可安装package到your_env_name中关闭虚拟环境(即从当前环境退出返回使用PATH环境中的默认python版本)。Linux: source deactivateWindows: deactivate删除虚拟环境。conda remove -n your_env_name(虚拟环境名称) --all， 即可删除。删除环境中的某个包。conda remove --name $your_env_name $package_name二、数据分析工具常用的数据分析工具Python本身的数据分析功能不强，需要第三方的扩展库来增强的它的能力。我们课程用到的库包括NumPy、Pandas、Matplotlib等，下面对这三个库做一个简单介绍，后面会通过案例深入讲解相关库的使用。NumpyPython并没有提供数组的功能。虽然列表可以完成基本的数组功能，但它不是真正的数组，而且在数据量较大的时候，使用列表的速度会慢的让人难以接受。为此，Numpy提供了真正的数组功能，以及对数据进行快速高效处理的函数。Numpy还是很多更高级的扩展库的依赖库，后面讲解的Matplotlib库、Pandas库都依赖于它。PandasPandas是Python下最强大的数据分析工具。它包含高级的数据结构，使得在Python中处理表格型数据非常快速和简单。Pandas构建与Numpy之上，它使得以Numpy为中心的应用很容易被使用，最初是被作为金融数据分析工具而开发出来的。Pandas功能非常强大，支持类似与SQL的数据增、删、改、查，并且带有丰富的数据处理函数，支持灵活的处理缺失数据。Matplotlib处理数据分析的结果，都免不了数据可视化的问题。对于Python来说， Matplotlib来说是最著名的绘图库，它主要用于二维绘图。它可以让我们非常快捷的用Python可视化数据。]]></content>
      <categories>
        <category>机器学习</category>
        <category>Numpy、Pandas</category>
      </categories>
      <tags>
        <tag>数据处理</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ADSL代理服务器搭建&代理池]]></title>
    <url>%2F2017%2F05%2F10%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC%2FADSL%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%26%E4%BB%A3%E7%90%86%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[ADSL的介绍和使用什么是ADSLADSL全称叫做Asymmetric Digital Subscriber Line，非对称数字用户环路，因为它的上行和下行带宽不对称。它采用频分复用技术把普通的电话线分成了电话、上行和下行三个相对独立的信道，从而避免了相互之间的干扰。有种主机叫做动态拨号VPS主机，这种主机在连接上网的时候是需要拨号的，只有拨号成功后才可以上网；每拨一次号，主机就会获取一个新的IP，也就是它的IP并不是固定的，而且IP量特别大，几乎不会拨到相同的IP，如果我们用它来搭建代理，既能保证高度可用，又可以自由控制拨号切换。连接ADSL服务器(推荐云代理)ssh root@153.36.65.214 -p 20063ADSL服务器的使用拨号初始化进入之后，可以发现有一个可用的脚本文件，叫做ppp.sh，这是拨号初始化的脚本，运行它会让我们输入拨号的用户名和密码，然后它就会开始各种拨号配置，一次配置成功；后面的拨号就不需要重复输入用户名和密码了；都提示成功之后就可以进行拨号了。拨号命令adsl-start停止拨号adsl-stop断线重播先执行adsl-stop再执行adsl-start将ADSL服务器设置为代理服务器使用的工具在Linux下搭建HTTP代理服务器，推荐TinyProxy和Squid，配置都非常简单，在这里我们以TinyProxy为例来讲解一下怎样搭建代理服务器。配置步骤安装TinyProxyyum install -y epel-releaseyum update -yyum install -y tinyproxy配置TinyProxyvi /etc/tinyproxy/tinyproxy.conf修改如下行Port 8888 代理的端口Allow 127.0.0.1 允许连接的主机，默认只允许本机连接，可以注释掉，允许所有主机连接重启tinyproxyservice tinyproxy start测试(在其他主机上操作)curl -x 112.84.118.216:8888 httpbin.org/get查看请求结果是不是设置的代理ip动态获取IP怎么动态获取ipDDNS动态域名解析我们需要使用一个域名来解析，也就是虽然IP是变的，但域名解析的地址可以随着IP的变化而变化。它的原理其实是拨号主机向固定的服务器发出请求，服务器获取客户端的IP，然后再将域名解析到这个IP上就可以了。国内比较有名的服务就是花生壳了，也提供了免费版的动态域名解析，另外DNSPOD也提供了解析接口来动态修改域名解析设置，DNSPOD，但是这样的方式都有一个通病，那就是慢！自己配置ADSL代理池所以根据花生壳的原理，可以完全自己实现一下动态获取IP的方法。要实现这个需要两台主机，一台主机就是这台动态拨号VPS主机，另一台是具有固定公网IP的主机。动态VPS主机拨号成功之后就请求远程的固定主机，远程主机获取动态VPS主机的IP，就可以得到这个代理，将代理保存下来，这样拨号主机每拨号一次，远程主机就会及时得到拨号主机的IP，如果有多台拨号VPS，也统一发送到远程主机，这样我们只需要从远程主机取下代理就好了，保准是实时可用，稳定高效的。ADSL代理池远程主机和ADSL拨号服务器的功能划分远程主机监听主机请求，获取动态VPS主机IP将VPS主机IP记录下来存入数据库，支持多个客户端检测当前接收到的IP可用情况，如果不可用则删除提供API接口，通过API接口可获取当前可用代理IPADSL拨号服务器定时执行拨号脚本换IP换IP后立即请求远程主机拨号后检测是否拨号成功，如果失败立即重新拨号远程主机实现存储模块功能设计远程主机作为一台服务器，动态拨号VPS会定时请求远程主机，远程主机接收到请求后将IP记录下来存入数据库。因为IP是一直在变化的，IP更新了之后，原来的IP就不能用了，所以对于一个主机来说我们可能需要多次更新一条数据。另外我们不能仅限于维护一台拨号VPS主机，当然是需要支持多台维护的。在这里我们直接选用Key-Value形式的非关系型数据库存储更加方便，所以在此选用Redis数据库。使用redis的Hash类型存储方式，Key就是拨号主机的名称，可以自己指定，Value就是代理的值。接口模块功能设计使用Tornado来实现实现random随机获取一个可用的动态代理功能实现count获取代理池中的代理个数ADSL拨号服务器实现拨号模块功能设计定时拨号，每隔一段时间拨号一次，更新ip更新前先清除远程主机数据库中的当前ip更新后请求远程主机，将新的ip更新到redis散列里示例代码]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫常见反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代理池的搭建]]></title>
    <url>%2F2017%2F05%2F08%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC%2F%E4%BB%A3%E7%90%86%E6%B1%A0%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[代理池的介绍应用模块aiohttp、requests、redis-py、pyquery、Flask代理池模块存储模块、获取代理模块、检测模块、接口模块、调度模块各模块的功能存储模块负责存储抓取下来的代理，并对代理去重获取模块需要定时抓取代理，代理可以是免费抓取的代理，也可以是调用付费代理商购买的代理检测模块需要定时检测数据库中的代理，对效果不好的代理及时进行删除接口模块需要用API来提供对外的服务接口。外界通过访问接口的方式来获取代理调度模块调度前面的所有模块，使之协调工作各模块的设计存储模块使用Redis有序集合，集合的每一个元素都是不重复的每个元素都有一个分数字段，分数是可以重复的，数值小的排在前面，数值大的排在后面每个代理100分为最高分，表示可用，0分为最低分，代表最不可用，自动进行移除获取随机代理的逻辑，先随机获取最高分代理，如果不存在最高分代理，按照排名获取获取模块从各代理商处爬取或购买调用接口获得代理调用存储模块，向数据库中添加代理，初始的分数是10检测模块不断的检测代理的可用性(检测的目标网站可以自定义)，进行代理的加分和减分或者移除检测到代理可用，分数立即置为100分，检测到代理请求失败，则减少一分接口模块定义接口模块的随机获取代理，统计代理数量等方法调度模块通过配置每个模块的开关来设置调度模块的启动功能通过多进程启动每一个模块代码示例]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫常见反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[滑动验证码识别]]></title>
    <url>%2F2017%2F05%2F04%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC%2F%E6%BB%91%E5%8A%A8%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[极验三代验证码破解破解思路1.使用selenium的模拟点击和滑动等来实现破解2.首先寻找快速识别按钮，如果找到进行点击验证3.获取验证码的位置(4角坐标)4.对验证码进行局域截图5.点击滑块获取第二张的验证码缺口图片6.遍历每一个像素点的位置，寻找缺口位置7.构造运动轨迹，要注意避免规则的运动轨迹被识别出是人工构造轨迹8.获取运动轨迹，并进行滑动示例代码微博九宫格验证码识别破解思路1.使用selenium的模拟点击和滑动来实现破解2.首先获取页面的验证码图片3.获取图片根据所有可能的滑动轨迹模板库的图片进行对比，获取正确的滑动顺序4.根据得到的拖动顺序，对其进行拖动示例代码]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫常见反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS压缩、混淆、加密及破解]]></title>
    <url>%2F2017%2F05%2F03%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC%2FJs%E5%8E%8B%E7%BC%A9%E3%80%81%E6%B7%B7%E6%B7%86%E3%80%81%E5%8A%A0%E5%AF%86%E5%8F%8A%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[js压缩、混淆、加密介绍三者的定义与区别压缩：删除 Javascript 代码中所有注释、跳格符号、换行符号及无用的空格，从而压缩 JS 文件大小，优化页面加载速度。混淆：经过编码将变量和函数原命名改为毫无意义的命名（如function(a,b,c,e,g)等），以防止他人窥视和窃取 Javascript 源代码，也有一定压缩效果。加密：一般用eval方法加密，效果与混淆相似，也做到了压缩的效果。三者的作用压缩的主要目的是消除注释等无用字符，达到精简js代码，减小js文件大小的目的，这也是页面优化的一种方式；混淆和加密的目的比较接近，都是为了防止他人直接查看源码，对代码（如重要的api等）起保护作用，但这也只是增加了阅读代码的代价，也就是所谓的防君子不防小人。加密就是通过已经有的或者自己编写的加密方法，对js代码进行加密转换，但是当混淆和加密联合使用时，如先混淆在加密（或者先加密再混淆）时，破解时间就会增加。关于js的加密浏览器是怎么解析混淆和加密后的js代码的其实变量名只要是Unicode字符就行了，对于js引擎来说都是一样的，只是人类觉得他们不同而已。是否可以破解压缩\混淆\加密后的代码压缩和混淆后的js代码是不可以被还原的，但是我们可以将js加密的代码进行解密，后面的几篇是介绍如何进行js解密的。常见的加密和解密方式介绍加密方式的分类一：最简单的加密解密 二：转义字符&quot;\&quot;的妙用 三：使用Microsoft出品的脚本编码器Script Encoder来进行编码 （自创简单解码） 四：任意添加NUL空字符（十六进制00H） （自创） 五：无用内容混乱以及换行空格TAB大法 六：自写解密函数法 一、escape()和unescape()加密方式的介绍大家对于JAVASCRIPT函数escape()和unescape()想必是比较了解啦（很多网页加密在用它们），分别是编码和解码字符串比如例子代码用escape()函数加密后变为如下格式：alert%28%22%u9ED1%u5BA2%u9632%u7EBF%22%29%3B如果愿意我们可以写点JAVASCRIPT代码重新把它加密如下%61%6C%65%72%74%28%22%u9ED1%u5BA2%u9632%u7EBF%22%29%3B这样加密后的代码是不能直接运行的，可以使用eval(codeString)，这个函数的作用就是检查JavaScript代码并执行，必选项 codeString 参数是包含有效 JavaScript 代码的字符串值，加上上面的解码unescape()解密方式直接将eval换成alert配合unescape便实现了解密加密示例加密方法 &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; varcode=unescape(&quot;%61%6C%65%72%74%28%22%u9ED1%u5BA2%u9632%u7EBF%22%29%3B&quot;); eval(varcode) &lt;/SCRIPT&gt;二、转义字符&quot;\&quot;加密加密方式介绍JavaScript提供了一些特殊字符如：\n （换行）、 \r （回车）、\’ （单引号 ）等应该是有所了解其实&quot;\&quot;后面还可以跟八进制或十六进制的数字，如字符&quot;a&quot;则可以表示为：&quot;\141&quot;或&quot;\x61&quot;（注意是小写字符&quot;x&quot;），至于双字节字符如汉字&quot;黑&quot;则仅能用十六进制表示为&quot;\u9ED1&quot;（注意是小写字符&quot;u&quot;），其中字符&quot;u&quot;表示是双字节字符；解密示例直接将eval换成alert便实现了解密 # 八进制转义字符串如下: &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; eval(&quot;\141\154\145\162\164\50\42\u9ED1\u5BA2\u9632\u7EBF\42\51\73&quot;) &lt;/SCRIPT&gt; # 十六进制转义字符串如下: &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; eval(&quot;\x61\x6C\x65\x72\x74\x28\x22\u9ED1\u5BA2\u9632\u7EBF\x22\x29\x3B&quot;) &lt;/SCRIPT&gt;三：使用Microsoft出品的脚本编码器Script Encoder来进行编码加密方式的介绍直接使用Microsoft出品的JavaScript调用控件Scripting.Encoder完成的编码！如果你觉得这样编码得到的代码LANGUAGE属性是JScript.Encode，很容易让人识破，那么还有一个几乎不为人知的window对象的方法execScript()，其原形为：window.execScript( sExpression, sLanguage ) sExpression: 必选项。字符串(String)。要被执行的代码。 sLanguage : 必选项。字符串(String)。指定执行的代码的语言。默认值为 Microsoft JScript解密方式编码后的代码运行前IE会先对其进行解码，如果我们先把加密后的代码放入一个自定义函数如上面的decode()中，然后对自定义函数decode调用toString()方法，得到的将是解码后的代码！加密示例加密方法 # 使用Scripting.Encoder &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; var Senc=new ActiveXObject(&quot;Scripting.Encoder&quot;); var code='&lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt;\r\nalert(&quot;黑客防线&quot;);\r\n&lt;\/SCRIPT&gt;'; var Encode=Senc.EncodeScriptFile(&quot;.htm&quot;,code,0,&quot;&quot;); alert(Encode); &lt;/SCRIPT&gt; # 使用execScript() &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; execScript(&quot;#@~^FgAAAA==@#@&amp;ls DD`J黑客防线r#p@#@&amp;FgMAAA==^#~@&quot;,&quot;JScript.Encode&quot;) &lt;/SCRIPT&gt; # 加密结果 &lt;SCRIPT LANGUAGE=&quot;JScript.Encode&quot;&gt;#@~^FgAAAA==@#@&amp;ls DD`J黑客防线r#p@#@&amp;FgMAAA==^#~@&lt;/SCRIPT&gt; 解密示例解密方法 &lt;SCRIPT LANGUAGE=&quot;JScript.Encode&quot;&gt; function decode(){}; alert(decode.toString()); &lt;/SCRIPT&gt;四：任意添加NUL空字符（十六进制00H）加密方式介绍一次偶然的实验，使我发现在HTML网页中任意位置添加任意个数的&quot;空字符&quot;，IE照样会正常显示其中的内容，并正常执行其中的JavaScript 代码。而添加的&quot;空字符&quot;我们在用一般的编辑器查看时，会显示形如空格或黑块，使得原码很难看懂，如用记事本查看则&quot;空字符&quot;会变成&quot;空格&quot;，利用这个原理加密结果如下：（其中显示的&quot;空格&quot;代表&quot;空字符&quot;）如果不知道方法的人很难想到要去掉里面的&quot;空字符&quot;（00H）的解密方式去掉代码中的00H加密示例加密方法 &lt;S C RI P T L ANG U A GE =&quot; J a v a S c r i p t&quot;&gt; a l er t (&quot; 黑 客 防 线&quot;) ; &lt; / SC R I P T&gt; 五：无用内容混乱以及换行空格TAB大法加密方式介绍在JAVASCRIPT代码中我们可以加入大量的无用字符串或数字，以及无用代码和注释内容等等这使真正的有用代码埋没在其中，并把有用的代码中能加入换行、空格、TAB的地方加入大量换行、空格、TAB，并可以把正常的字符串用&quot;&quot;来进行换行，这样就会使得代码难以看懂！如我加密后的形式如下：解密方式慢慢的分析吧加密示例加密方法 &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; &quot;xajgxsadffgds&quot;;1234567890 625623216;var $=0;alert//@$%%&amp;*()(&amp;(^%^ //cctv function// (//hhsaasajx xc /* asjgdsgu*/ &quot;黑 客 防线&quot;//ashjgfgf /* @#%$^&amp;%$96667r45fggbhytjty */ //window ) ;&quot;#@$#%@#432hu&quot;;212351436 &lt;/SCRIPT&gt;六：自写解密函数法加密方式介绍这个方法和一、二差不多，只不过是自己写个函数对代码进行加密很多VBS病毒使用这种方法对自身进行加密，来防止特征码扫描！下面是我写的一个简单的加密解密函数，加密示例 加密方法 # 加密方法 &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; function compile(code) { var c=String.fromCharCode(code.charCodeAt(0)+code.length); for(var i=1;i&lt;code.length;i++){ c+=String.fromCharCode(code.charCodeAt(i)+code.charCodeAt(i-1)); } alert(escape(c)); } compile(’alert(&quot;黑客防线&quot;);’) &lt;/SCRIPT&gt; # 加密结果 o%CD%D1%D7%E6%9CJ%u9EF3%uFA73%uF1D4%u14F1%u7EE1Kd 解密示例解密方法 &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt; function uncompile(code) { code=unescape(code); var c=String.fromCharCode(code.charCodeAt(0)-code.length); for(var i=1;i&lt;code.length;i++){ c+=String.fromCharCode(code.charCodeAt(i)-c.charCodeAt(i-1)); } return c; } eval(uncompile(&quot;o%CD%D1%D7%E6%9CJ%u9EF3%uFA73%uF1D4%u14F1%u7EE1Kd&quot;)); &lt;/SCRIPT&gt;]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫常见反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片验证码-Tesseract破解]]></title>
    <url>%2F2017%2F05%2F02%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC%2F%E5%9B%BE%E7%89%87%E9%AA%8C%E8%AF%81%E7%A0%81-Tesseract%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Evernote Export body, td { font-family: 微软雅黑; font-size: 10pt; } 图像的翻译-Tesseract文字识别文字识别-机器视觉的一个分支，将图像翻译成文字一般被称为 光学文字识别(Optical Character Recognition, OCR)。可以实现OCR的底层库并不多,目前很多库都是使用共同的几个底层 OCR 库,或者是在上面 进行定制ORC库的概念在读取和处理图像、图像相关的机器学习以及创建图像等任务中，Python 一直都是非常出色的语言。虽然有很多库可以进行图像处理，但在这里我们只重点介绍： Tesseract作用Tesseract是一个将图像翻译成文字的OCR库(光学文字识别，Optical Character Recognition)，可以翻译图片中的文字内容，目前由 Google 赞助(Google 也是一家以 OCR 和机器学习技术闻名于世的公司)。Tesseract 是目前公认最优秀、最精确的开源 OCR 系统，除了极高的精确度，Tesseract 也具有很高的灵活性。它可以通过训练识别出任何字体，也可以识别出任何 Unicode 字符。tesseract处理的文字规范tesseract处理的文字大都是格式规范的，格式规范的文字具有以下特点:使用一个标准字体(不包含手写体、草书,或者十分“花哨的”字体)即使被复印或拍照，字体还是很清晰，没有多余的痕迹或污点排列整齐，没有歪歪斜斜的字没有超出图片范围，也没有残缺不全，或紧紧贴在图片的边缘怎么将图片格式化文字的一些格式问题在图片预处理时可以进行解决。例如,可以把图片转换成灰度图，调整亮度和对比度，还可以根据需要进行裁剪和旋转（详情需要了解图像与信号处理）等使用步骤安装tesseract和tensor-ocrwindows下安装先下载tesseract-ocr下载地址: https://digi.bib.uni-mannheim.de/tesseract下载 pytenseractpip install pytesseract pillowubuntu下安装sudo apt-get install -y tesseract-ocr libtesseract-dev libleptonica-devpip install pytesseract pillowcentos下安装yum install -y tesseractpip install pytesseract pillow 在终端中使用tesseracttesseract test.jpg text 将翻译结果保存在text.txt中tesseract chi_sim text.png paixu 将翻译结果保存在paixu.txt中（可以翻译中文简体）tesseract --list-langs 查看当前支持的语言在Python中使用tesesractimport pytesseractfrom PIL import Imageimage = Image.open(jpg) 获取图片text = pytesseract.image_to_string(image, lang='chi_sim') 对图片的内容进行翻译import subprocesssubprocess.call(['tessetact', '-1', 'chi_sim', filePath, 'paixu']) 使用终端命令进行翻译pytesseract.file_to_text('image.png') 直接对图片进行识别使用tesseract识别代码(包含图片处理)import pytesseractfrom PIL import Imageimage = Image.open('code2.jpg')# 图片灰度化处理image = image.convert('L') # 图片二值化处理threshold = 127 设置二值化阈值 filter_func = lambda x: 0 if x &lt; threshold else 1image = image.point(filter_func, '1')image.show()result = pytesseract.image_to_text(image) 对处理后的图片进行识别print(result)对tesseract进行训练训练的目的tesseract一般只能识别符合文字规范的字体，其他的稍微复杂的字很难识别，tesseract可以通过训练对一种格式字体的反复训练，来实现字体的高精度识别训练tesseract的方法要训练 Tesseract 识别一种文字，无论是晦涩难懂的字体还是验证码，你都需要向 Tesseract 提供每个字符不同形式的样本。首先要收集大量的验证码样本，样本的数量和复杂程度，会决定训练的效果。第二步是准确地告诉 Tesseract 一张图片中的每个字符是什么，以及每个字符的具体位置。这里需要创建一些矩形定位文件(box file)，一个验证码图片生成一个矩形定位文件，也可以通过jTessBoxEditor软件来修改矩形的定位。一个图片的矩形定位文件如下所示:第一列符号是图片中的每个字符，后面的 4 个数字分别是包围这个字符的最小矩形的坐标 (图片左下角是原点 (0，0)，4 个数字分别对应每个字符的左下角 x 坐标、左下角 y 坐标、右上角 x 坐标和右上角 y 坐标)，最后一个数字“0”表示图片样本的编号。矩形定位文件必须保存在一个 .box 后缀的文本文件中，(例如 4MmC3.box)。tesseract训练教程http://www.cnblogs.com/mjorcen/p/3800739.html?utm_source=tuicool&amp;utm_medium=referral]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫常见反反爬</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>反反爬</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy对接BloomFilter]]></title>
    <url>%2F2017%2F04%2F21%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2FScrapy%E5%AF%B9%E6%8E%A5BloomFilter%2F</url>
    <content type="text"><![CDATA[Scrapy对接BloomFilter通过修改源码的方式对接对接的原则实现Bloom Filter时，首先要保证不能破坏Scrapy-Redis分布式爬取的运行架构。我们需要修改Scrapy-Redis的源码，将它的去重类替换掉。同时，Bloom Filter的实现需要借助于一个位数组，既然当前架构还是依赖于Redis，那么位数组的维护直接使用Redis就好了。首先实现一个基本的散列算法，将一个值经过散列运算后映射到一个m位数组的某一位上，代码如下：实现BloomFilter过滤器类接下来继续修改Scrapy-Redis的源码修改源码的位置将scrapy-redis的dupefilter逻辑替换为Bloom Filter的逻辑。主要是修改RFPDupeFilter类的request_seen()方法利用request_fingerprint()方法获取Request的指纹，调用Bloom Filter的exists()方法判定该指纹是否存在。如果存在，则说明该Request是重复的，返回True，否则调用Bloom Filter的insert()方法将该指纹添加并返回False。这样就成功利用Bloom Filter替换了Scrapy-Redis的集合去重。修改request_seen方法，修改判断元素是否重复的方法修改__init__ 方法，将判断重复类修改为BloomFilter相关参数，通过from_setting方法获取修改setting文件BLOOMFILTER_HASH_NUMBER = 6 配置布隆过滤器的哈希函数的数量BLOOMFILTER_BIT = 30 设置存储使用的位数直接使用scrapy-redis-bloomfilter框架下载scrapy-redis-bloomfilterpip3 install scrapy-redis-bloomfilter修改setting.py文件DUPEFILTER_CLASS = &quot;scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter&quot; # 去重类，要使用Bloom Filter请替换DUPEFILTER_CLASSBLOOMFILTER_HASH_NUMBER = 6 # 散列函数的个数，默认为6，可以自行修改BLOOMFILTER_BIT = 30 # Bloom Filter的bit位移参数(代表 1 &lt;&lt; 30 )，默认30，占用128MB空间，去重量级1亿]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy对接splash]]></title>
    <url>%2F2017%2F04%2F18%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2Fscrapy%E5%AF%B9%E6%8E%A5splash%2F</url>
    <content type="text"><![CDATA[Scrapy 对接 Splash环境准备首先在这之前请确保已经正确安装好了Splash并正常运行，同时安装好了ScrapySplash库Scrapy-Splash文档https://github.com/scrapy-plugins/scrapy-splashScrapy-splash的配置新建项目和spiderscrapy startproject scrapysplashtest 新建项目scrapy genspider taobao www.taobao.com 新建spider修改setting.py文件, 添加splash配置SPLASH_URL = 'http://localhost:8050' 添加splash服务的地址DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter' 配置去重类HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage' 还需要配置一个Cache存储HTTPCACHE_STORAGE添加splash中间件DOWNLOADER_MIDDLEWARES = {'scrapy_splash.SplashCookiesMiddleware': 723,'scrapy_splash.SplashMiddleware': 725,'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,}SPIDER_MIDDLEWARES = {'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,}SplashRequest请求的使用使用splash请求的说明配置完成之后我们就可以利用Splash来抓取页面了，例如我们可以直接生成一个SplashRequest对象并传递相应的参数，Scrapy会将此请求转发给SplashSplash对页面进行渲染加载，然后再将渲染结果传递回来，此时Response的内容就是渲染完成的页面结果了，最后交给Spider解析即可。使用请求的方法第一种方法通过SplashRequest发送请求第二种方法scrapy.Request对象发送请求给splash服务器，只需将配置属性给meta参数即可通过lua源码控制splash服务的示例我们把Lua脚本定义成长字符串，通过SplashRequest的args来传递参数，同时接口修改为execute，另外args参数里还有一个lua_source字段用于指定Lua脚本内容，这样我们就成功构造了一个SplashRequest，对接Splash的工作就完成了。 from scrapy import Spider from urllib.parse import quote from scrapysplashtest.items import ProductItem from scrapy_splash import SplashRequest script = &quot;&quot;&quot; function main(splash, args) splash.images_enabled = false assert(splash:go(args.url)) assert(splash:wait(args.wait)) js = string.format(&quot;document.querySelector('#mainsrp-pager div.form &gt; input').value=%d;document.querySelector('#mainsrp-pager div.form &gt; span.btn.J_Submit').click()&quot;, args.page) splash:evaljs(js) assert(splash:wait(args.wait)) return splash:html() end &quot;&quot;&quot; class TaobaoSpider(Spider): name = 'taobao' allowed_domains = ['www.taobao.com'] base_url = 'https://s.taobao.com/search?q=' def start_requests(self): for keyword in self.settings.get('KEYWORDS'): for page in range(1, self.settings.get('MAX_PAGE') + 1): url = self.base_url + quote(keyword) yield SplashRequest( url, callback=self.parse, endpoint='execute', args={'lua_source': script, 'page': page, 'wait': 7})使用scrapy-splash比使用selenium的优点由于Splash和Scrapy都支持异步处理，我们可以看到同时会有多个抓取成功的结果，而Selenium的对接过程中每个页面渲染下载过程是在Downloader Middleware里面完成的，所以整个过程是堵塞式的，Scrapy会等待这个过程完成后再继续处理和调度其他请求，影响了爬取效率。使用Splash，是在中间件中将请求和渲染等工作交给了splash服务器, 各请求之间是异步的，因此使用Splash爬取效率上比Selenium高出很多。因此，在Scrapy中要处理JavaScript渲染的页面建议使用Splash，这样不会破坏Scrapy中的异步处理过程，会大大提高爬取效率，而且Splash的安装和配置比较简单，通过API调用的方式也实现了模块分离，大规模爬取时部署起来也更加方便。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gearapy的介绍和使用]]></title>
    <url>%2F2017%2F04%2F12%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2FGerapy%E7%9A%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Evernote Export body, td { font-family: 微软雅黑; font-size: 10pt; } Gerapy的介绍分布式的爬虫部署和管理工具，基于scrapy、scrapyd、scrapyd-API、Django、Vue.jsGerapy的使用Gerapy的安装pip3 install gerapyGerapy应用步骤1.初始化Gerapygerapy init执行完毕之后，本地便会生成一个名字为 gerapy 的文件夹，接着进入该文件夹，可以看到有一个 projects 文件夹，我们后面会用到。2.初始化数据库cd gerapygerapy migrate会在 gerapy 目录下生成一个 SQLite 数据库，同时建立数据库表。3.启动Gerapygerapy runserver这样我们就可以看到 Gerapy 已经在 8000 端口上运行了。接下来我们在浏览器中打开 http://localhost:8000/，就可以看到 Gerapy 的主界面了使用Gerapy管理项目主机管理我们可以点击左侧 Clients 选项卡，即主机管理页面，添加我们的 Scrapyd 远程服务，点击右上角的创建按钮即可添加我们需要管理的 Scrapyd 服务。需要添加 IP、端口，以及名称，点击创建即可完成添加，点击返回即可看到当前添加的 Scrapyd 服务列表添加主机前后这样我们可以在状态一栏看到各个 Scrapyd 服务是否可用，同时可以一目了然当前所有 Scrapyd 服务列表，另外我们还可以自由地进行编辑和删除。项目管理Gerapy 的核心功能当然是项目管理，在这里我们可以自由地配置、编辑、部署我们的 Scrapy 项目，点击左侧的 Projects ，即项目管理选项，我们可以看到如下空白的页面：将项目拖动到刚才 gerapy 运行目录的 projects 文件夹下，例如我这里写好了一个 Scrapy 项目，这时刷新页面，我们便可以看到 Gerapy 检测到了这个项目，同时它是不可配置、没有打包的这时我们可以点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于 Git 的 commit 信息，然后点击打包按钮，即可发现 Gerapy 会提示打包成功，同时在左侧显示打包的结果和打包名称打包成功之后，我们便可以进行部署了，我们可以选择需要部署的主机，点击后方的部署按钮进行部署，同时也可以批量选择主机进行部署，示例如下监控任务部署完毕之后就可以回到主机管理页面进行任务调度了，任选一台主机，点击调度按钮即可进入任务管理页面，此页面可以查看当前 Scrapyd 服务的所有项目、所有爬虫及运行状态我们可以通过点击新任务、停止等按钮来实现任务的启动和停止等操作，同时也可以通过展开任务条目查看日志详情编辑项目同时 Gerapy 还支持项目编辑功能，有了它我们不再需要 IDE 即可完成项目的编写，我们点击项目的编辑按钮即可进入到编辑页面，如图所示CrawlSpider代码自动生成在 Scrapy 中，其实提供了一个可配置化的爬虫 CrawlSpider，它可以利用一些规则来完成爬取规则和解析规则的配置，这样可配置化程度就非常高，这样我们只需要维护爬取规则、提取逻辑就可以了。如果要新增一个爬虫，我们只需要写好对应的规则即可，这类爬虫就叫做可配置化爬虫。Gerapy 可以做到：我们写好爬虫规则，它帮我们自动生成 Scrapy 项目代码。我们可以点击项目页面的右上角的创建按钮，增加一个可配置化爬虫，接着我们便可以在此处添加提取实体、爬取规则、抽取规则了，例如这里的解析器，我们可以配置解析成为哪个实体，每个字段使用怎样的解析方式，如 XPath 或 CSS 解析器、直接获取属性、直接添加值等多重方式，另外还可以指定处理器进行数据清洗，或直接指定正则表达式进行解析等等，通过这些流程我们可以做到任何字段的解析。再比如爬取规则，我们可以指定从哪个链接开始爬取，允许爬取的域名是什么，该链接提取哪些跟进的链接，用什么解析方法来处理等等配置。通过这些配置，我们可以完成爬取规则的设置。最后点击生成按钮即可完成代码的生成。生成的代码示例结果如图所示，可见其结构和 Scrapy 代码是完全一致的。Gerapy新增功能和Docker部署上面的介绍当然都是人家开发好的，下面介绍下我给gerapy增加的很高效的功能1.项目本地上传在project界面增加了选择项目select(需要提前打成zip包),和项目上传(upload)功能，可以直接将本地的项目上传的gerapy， 就不需要手动将代码去放到服务器上了2.批量启动和批量停止在schedule界面， 如果想要一次启动多个爬虫进程可以点击run some来一起启动多个，即快速又准确又方便，另外也可以点击爬虫名称后面的stop 批量停止爬虫进程3.docker快速部署这部分去我的dockerhub找我的dockerfile吧]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
        <tag>gerapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapyd部署总结]]></title>
    <url>%2F2017%2F04%2F06%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2Fscrapyd%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[二、环境安装安装scprayd，网址：https://github.com/scrapy/scrapyd安装scrapyd-client，网址：https://github.com/scrapy/scrapyd-client建议从github上下载最新源码，然后用python setup.py install安装，因为pip安装源有可能不是最新版的。三、验证安装成功在命令框中输入scrapyd，输出如下说明安装成功打开http://localhost:6800/ 可以看到点击jobs可以查看爬虫运行情况。接下来就是让人很头疼的scrapyd-deploy问题了，查看官方文档上说用scrapyd-deploy -l可以看到当前部署的爬虫项目，但是当我输入这段命令的时候显示这个命令不存在或者有错误、不合法之类的。解决方案：在你的python目录下的Scripts文件夹中，我的路径是“D:\program files\python2.7.0\Scripts”，增加一个scrapyd-deploy.bat文件。内容为：@echo off&quot;D:\program files\python2.7.0\python.exe&quot; &quot;D:\program files\python2.7.0\Scripts\scrapyd-deploy&quot; %*然后重新打开命令框，再运行scrapyd-deploy -l 就可以了。四、发布工程到scrapydscrapyd-deploy &lt;target&gt; -p &lt;project&gt;target为你的服务器命令，project是你的工程名字。首先对你要发布的爬虫工程的scrapy.cfg 文件进行修改，我这个文件的内容如下：[deploy:scrapyd1]url = http://localhost:6800/project = baidu因此我输入的命令是：scrapyd-deploy scrapyd1 -p baidu输出如下五、启动爬虫使用如下命令启动一个爬虫curl http://localhost:6800/schedule.json -d project=PROJECT_NAME -d spider=SPIDER_NAMEPROJECT_NAME填入你爬虫工程的名字，SPIDER_NAME填入你爬虫的名字我输入的代码如下：curl http://localhost:6800/schedule.json -d project=baidu -d spider=baidu因为这个测试爬虫写的非常简单，一下子就运行完了。查看网站的jobs可以看到有一个爬虫已经运行完，处于Finished一列中六、停止一个爬虫curl http://localhost:6800/cancel.json -d project=PROJECT_NAME -d job=JOB_ID更多API可以查看官网：http://scrapyd.readthedocs.io/en/latest/api.html七、根据文档编写的调度脚本前提: 已安装scrapyd-client# coding=utf-8import requestsimport timeimport jsonimport osimport sysreload(sys)sys.setdefaultencoding('utf-8')BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(__file__)))sys.path.insert(0, os.path.join(BASE_DIR, 'conf'))# 需要配置SCRAPYD_NODE = &quot;&quot;PROJECT_NAME = &quot;&quot;class SpiderSwitch(object): def __init__(self, project_name=&quot;&quot;, node=&quot;&quot;): self.project_name = project_name self.ip_port = node def delete_project(self): &quot;&quot;&quot; 删除项目 &quot;&quot;&quot; delete_data = {&quot;project&quot;: self.project_name} delete_url = &quot;http://{}/delproject.json&quot;.format(self.ip_port) response = requests.post(delete_url, data=delete_data) print &quot;delete_project: [{}]&quot;.format(response.content.decode()) def start_spider(self, spider_name): &quot;&quot;&quot; 启动爬虫的方法 :return: 返回启动的状态 &quot;&quot;&quot; start_data = {'project': self.project_name, 'spider': spider_name} start_url = &quot;http://{}/schedule.json&quot;.format(self.ip_port) response = requests.post(url=start_url, data=start_data) print &quot;start_spider: [{}]&quot;.format(response.content.decode()) def stop_spider(self, job_id): &quot;&quot;&quot; 关闭爬虫的方法 :return:返回关闭结果 &quot;&quot;&quot; stop_data = {'project': self.project_name, 'job': job_id} stop_url = &quot;http://{}/cancel.json&quot;.format(self.ip_port) response = requests.post(stop_url, data=stop_data) print &quot;stop_spider: [{}]&quot;.format(response.content.decode()) def show_spiders(self): &quot;&quot;&quot; 获取scrapyd服务器上名为myproject的工程下的爬虫清单 :return: 返回查询到的状态 &quot;&quot;&quot; show_url = 'http://{}/listspiders.json?project={}'.format(self.ip_port, self.project_name) response = requests.get(show_url) print &quot;show_spiders : [{}]&quot;.format(json.loads(response.content.decode())) def show_project(self): &quot;&quot;&quot; 查看所有项目 :return:返回查询结果 &quot;&quot;&quot; show_url = &quot;http://{}/listprojects.json&quot;.format(self.ip_port) response = requests.get(show_url) print &quot;show_projects: [{}]&quot;.format(response.content.decode()) def show_jobs(self): &quot;&quot;&quot; 查看正在运行的job :return: 返回查看结果 &quot;&quot;&quot; job_url = &quot;http://{}/listjobs.json?project={}&quot;.format(self.ip_port, self.project_name) response = requests.get(job_url) print &quot;jobs: [{}]&quot;.format(response.content.decode()) return response.content.decode() def start_some(self, name, n): &quot;&quot;&quot; 一次启动多个任务 :param:n: 启动的任务数量 &quot;&quot;&quot; for i in xrange(n): time.sleep(0.2) self.start_spider(name) def stop_project(self): &quot;&quot;&quot; 停止改项目下的所有爬虫 &quot;&quot;&quot; job_list = json.loads(self.show_jobs()).get(u&quot;running&quot;) if job_list: for job in job_list: spider_job_id = job.get(u&quot;id&quot;) self.stop_spider(spider_job_id) time.sleep(0.2) print u&quot;the jobs for the project are all closed&quot;if __name__ == '__main__': scrpayd_node = SCRAPYD_NODE spider_switch = SpiderSwitch(project_name=PROJECT_NAME, node=scrpayd_node) task = sys.argv[1] if task == &quot;start&quot;: spider_name = sys.argv[2] spider_switch.start_spider(spider_name) if task == &quot;stop&quot;: job_id = sys.argv[2] spider_switch.stop_spider(job_id) if task == &quot;show_jobs&quot;: spider_switch.show_jobs() if task == &quot;show_spiders&quot;: spider_switch.show_spiders() if task == &quot;delete_project&quot;: spider_switch.delete_project() if task == &quot;show_projects&quot;: spider_switch.show_project() if task == &quot;start_some&quot;: spider_name = sys.argv[2] job_num = int(sys.argv[3]) spider_switch.start_some(spider_name, job_num) if task == &quot;stop_project&quot;: spider_switch.stop_project()]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据格式化输出]]></title>
    <url>%2F2017%2F03%2F26%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2F%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[自动格式化输出文件scrapy支持的格式化输出文件格式jsonjsonlinescsvxml通过配置setting中属性，来定义存储方式FEED_URI 存储路径，必须配置，如果不配置，需要在启动时候指定路径示例：file://tmp/export.csv备注：路径的名称可以格式化，例如：FEED_FORMAT 用于序列化输出的文件格式，就是上面的几种FEED_EXPORT_ENCODING 文件编码格式，一般默认utf-8，json默认是安全编码格式FEED_EXPORT_FIELDS 定义输出文件中包含的字段，为列表格式FEED_EXPORT_INDENT 定义输出内容的缩进，默认为0,0和负数表示内容会放在一行上None会选择最紧凑的方式，将数据进行摆放其他，将会按照指定的缩进，对对象的成员进行格式化显示FEED_STORE_EMPTY 如果没输出是否会导出文件，默认为FalseFEED_STORAGES_BASE 字典类型，包含所有支持存储方式模板引擎， 一般不需要手动配置默认为FEED_STORAGES 一个字典类型，可以手动添加存储方式的模板引擎默认为 { }如果想要禁用某一个模板文件类型的引擎，可以将上面对应字段值设置为NoneFEED_EXPORTERS_BASE 字典类型，包含了所有支持的格式文件处理引擎， 一般不需要手动配置默认为FEED_EXPORTERS 一个字典类型，可以手动添加文件处理引擎默认为{ }如果想要禁用某一个模板文件类型的引擎，可以将上面对应字段值设置为Nonescrapy手动输出指定csv格式文件目标：获取如下格式的csv文件配置的步骤1. 在scrapy的spiders同层目录，新建my_project_csv_item_exporter.py文件内容如下（文件名可改，目录定死） from scrapy.conf import settings from scrapy.contrib.exporter import CsvItemExporter class MyProjectCsvItemExporter(CsvItemExporter): def __init__(self, *args, **kwargs): delimiter = settings.get('CSV_DELIMITER', ',') kwargs['delimiter'] = delimiter fields_to_export = settings.get('FIELDS_TO_EXPORT', []) if fields_to_export : kwargs['fields_to_export'] = fields_to_export super(MyProjectCsvItemExporter, self).__init__(*args, **kwargs)2. 在同层目录，settings.py文件新增如下内容（指定item,field顺序） FEED_EXPORTERS = { 'csv': 'my_project.my_project_csv_item_exporter.MyProjectCsvItemExporter', } #这里假设你的project名字为my_project FIELDS_TO_EXPORT = [ 'id', 'name', 'email', 'address' 3. 在同层目录，settings.py文件指定分隔符CSV_DELIMITER = &quot;\t&quot;4.启动项目全部设定完后，执行scrapy crawl spider -o spider.csv的时候，字段就按顺序来了。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-redis框架]]></title>
    <url>%2F2017%2F03%2F18%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2Fscrapy_redis%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Evernote Export body, td { font-family: 微软雅黑; font-size: 10pt; } scrapy_redis的介绍scrapy_redis的官方介绍基于redis的scrapy组件使用scrapy_redis要安装scrapy_redis组件（pip install scrapy_redis）scrapy_redis使用场景增量式抓取、分布式抓取、持久化抓取scrapy_redis实现的功能及与原生scrapy的区别request去重scrapy：可以实现去重，但是scrapy去重不能实现持久化，即当下次如果再次运行程序，会再次存储之前存储过得内容scrapy_redis：会将请求过的指纹集合保存起来，用来判断重复我请求，即便重新运行程序，已经请求过的请求，也不会进入待请求对列中爬虫持久化scrapy：每次运行爬虫都是一次全新的开始scrapy_redis：会将待请求的序列化后的request对象保存起来，这样每次运行程序都会从待请求列队中读取request对象进行请求，即使终止程序，再次运行，依然会从待请求对列中获取请求对象分布式爬虫scrapy：不能实现分布式scrapy_redis：可以通过redis存储爬取的状态（已爬取，和待爬取），让多台机器上的爬虫程序，共用一个redis服务器，这样数据是共享的，实现了分布式的爬虫分布式和集群的区别（备注）：分布式：一个业务拆分成多个子业务，部署在不同的服务器上集群，同一个业务，部署在多个服务器上scrapy_redis的爬虫工作流程Scrapy_redis和scrapy框架使用的区别克隆scrapy_redis的示例代码克隆 github上的scrapy-redis源码文件git clone https://github.com/rolando/scrapy-redis.git打开example-project的scrapy_redis示例源代码mv scrapy-redis/example-project 项目目录example-project项目目录结构如下redis_scrapy代码上和普通scrapy爬虫的区别一、spider爬虫文件第一种Scrapy爬虫（如示例dmoz.py）区别：和普通的爬虫文件没有区别文件主要内容： 第二种RedisSpider爬虫（如示例的myspider_redis.py）区别爬虫类继承自RedisSpider要指定爬虫类的redis_key类属性，指定reids中存储start_url的位置新增了爬虫类的方法def make_request_from_data(self, data): # data 为接收到额redis_key传递过来的url，我们可以自行拼接成想要的url进行请求search_key = dataurl = self.URL_MAIN % (search_key, 1)result_item = SearchInfoResultItem()result_item[&quot;task_value&quot;] = search_keyreturn Request(url, meta={&quot;result_item&quot;: result_item}, dont_filter=True)作用防止分布式爬虫在不同的程序中，都会去请求该start_url，这样会造成请求的重复，响应也会重复的交给解析函数去处理，从redis中取出的start_url第一次被爬虫取出之后便会删除掉，不会造成重复的请求在持久化和去重的功能上实现了RedisSpider爬虫才实现了分布式代码示例：第三种RedisCrawSpider爬虫（如示例的myspider_redis.py）区别爬虫类继承自RedisCrawlSpider比RedisSpider爬虫就多了一个rules过滤功能作用在持久化和去重的功能上实现了RedisSpider爬虫才实现了分布式另外可以实现自动的对响应中的连接进行匹配，进行请求代码示例二、setting文件区别：设置了三个新的属性DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;指定爬虫给request对象去重的方法SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;指定爬虫处理请求列队的方法SCHEDULER_PERSIST = True列队中的内容是否持久保存，为False的时候，在关闭程序的时候，清空redis添加了一个处理item的pipeline 'scrapy_redis.pipelines.RedisPipeline': 400用来将items自动保存到reids中，我们可以取消，不用reids来保存设置了redis的地址REDIS_URL = 'redis://192.168.207.130: 6379'不写使用哪个数据库的话，默认使用0数据库文件主要内容：三、运行爬虫后，reids数据库的中多出的键（运行程序后）redis数据库多个三个键爬虫名：dupefilter 用来存储爬取过的request指纹集合，数据类型为set类型爬虫名：requests 用来存储已经序列化的待请求对象，数据类型为zset类型爬虫名：items 用来存储爬取出来的数据，数据类型为list类型每个键存储的内容：Scrapy_redis功能实现的方法(Scrapy_redis的源码)scrapy_redis之redispipeline（实现item数据自动存储redis数据库）文件位置/python3.5/site-packages/scrapy_redis/pipelines/RedisPipeline源码主要功能实现将item存储在redis中源码片段scrapy_redis之RFPDupeFilter（主要实现去重）文件位置/python3.5/site-packages/scrapy_redis、dupefilter/RFPDpeFilter源码主要功能判断请求是否存在内存中，如果不存在对请求进行一系列操作后，将其转化为指纹将指纹存储在内存中主要实现去重功能源码片段加密指纹生成过程 import hashlib fp = hashlib.sha1() fp.update(url) fp.update(request.method) fp.update(request.body or b&quot;&quot;) return fp.hexdiget() #sha1结果的16进制的字符串scrapy_redis之Scheduler（主要实现持久化）文件位置/python3.5/site-packages/scrapy_redis/scheduler/Scheduler代码主要功能判断取消过滤是否为true判断url地址是否是第一次看到如果过滤，且是第一次看到的请求，那么将会进入待请求的列队主要实现持久化功能源码片段scrapy_redis的总结scrapy_redis与scrapy的区别本质区别scrapy_redis_spider相比于之前的scrapy_spider多了持久化和持久化去重的功能主要是因为scrapy_redis将指纹和请求进行了在redis中的存储,正是redis实现了持久化代码区别仅仅是在setting中增加了五行代码拓展：scrapy_redis实现了请求的增量式爬虫，还可以实现内容增量式爬虫可以将dont_filter设置不进行过滤会将每条数据根据发帖人，发帖时间，帖子更新时间等使用md5算法生成指纹在进行存储的时候 进行判断是否已经存在，以及是否更新如果不存在那么便插入如果存在但是更新时间已经更新，那么便更新工作场景常用的数据存储模式优点可以利用redis的读取快速的特点，进行数据的快速读取可以实现读取和存储分离，防止锁表的产生存储数据用mysql或者mongodb先将爬取到的数据去重后存储到mysql或mongodb中也可以先存储到数据库中，在对数据库进行数据的去重redis应用其速度快的特点，用来展示数据定期从mysql或者mongodb中将去重后的数据读取到redis中读取数据从redis中读取，其速度远远优于mysql和mongodb]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-middleware]]></title>
    <url>%2F2017%2F03%2F15%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2FScrapy-middleware%2F</url>
    <content type="text"><![CDATA[中间件使用方法使用方法在middlewares文件中编写一个Downloader Middlewares的类，定义中间件在setting中开启Downloader Middlewares，数值越小权限越大request会先经过权限大的中间件，response会先经过权限小的中间件 SPIDER_MIDDLEWARES = { # 开启middleware，写清楚middleware所在的路径 'login.middlewares.LoginSpiderMiddleware': 543, }常用自定义中间件Downloader Middlewares（下载中间件）的方法process_request(self, request, spider)作用当每个请求通过下载中间件时，该方法被调用,必须返回如下值的其中一个返回内容返回NoneScrapy将继续处理该请求，执行其他的中间件的相应方法，直到合适的下载器处理函数（下载处理程序）被调用，该请求被执行（其响应被下载）返回Request对象如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。返回Response对象Scrapy将不会调用任何其他的 process_request或 process_exception方法，或相应地下载函数； 将返回该response,已安装的中间件的 process_response() 方法则会在每个response返回时被调用。抛出异常（包括抛出一个IgnoreRequest异常）停止调用 process_request，下载中间件的 process_exception方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。process_response（self, request, response, spider） 作用当响应通过每个下载中间件的时候，调用方法，必须返回如下之一返回内容返回Response对象（可以与传入的响应相同，也可以是全新的对象）该响应将被在链中的其他中间件的process_response方法处理。返回Request对象中间件链停止，返回的请求将被重新调度下载。处理类似于process_request()返回请求所做的那样。产生异常（包括抛出一个IgnoreRequest异常）停止调用 process_response，下载中间件链的 process_exception方法会被调用，如果没有任何一个方法处理该异常，则调用请求的errback（Request.errback）。如果没有代码处理抛出的异常，则该异常被忽略且不记录（不同于其他异常那样）。process_exception(self, request，exception, spider) 作用：当下载处理器（下载处理程序）、下载中间件的方法抛出异常（包括IgnoreRequest异常）时，Scrapy调用process_exception,返回如下几个参数之一返回内容：返回None：Scrapy将会继续处理该异常，调用接着已安装的其他中间件的process_exception方法，直到所有中间件都被调用完毕，则调用默认的异常处理。返回Request对象，则返回的请求将被重新调用下载。这将停止中间件的process_exception方法执行，就如返回一个响应的那样。常设置的中间件给请求添加请求头的内容 from .settings import USER_AGENTS class RandomUserAgent(object): # 配置下载中间件 def process_request(self, request, spider): ug_list = USER_AGENTS user_agent = random.choice(ug_list) # 在requests请求前添加自定义的USER_AGENT request.headers['User-Agent'] = user_agent给请求添加代理ip class RandomProxy(object): def process_request(self, request, spider): proxy = random.choice(PROXIES) if proxy['user_passwd'] is None: # 没有代理账户验证的代理使用方式 request.meta['proxy'] = &quot;http://&quot; + proxy['ip_port'] else: # 对账户密码进行base64编码转换 base64_userpasswd = base64.b64encode(proxy['user_passwd']) # 对应到代理服务器的信令格式里 request.headers['Proxy-Authorization'] = 'Basic ' + base64_userpasswd request.meta['proxy'] = &quot;https://&quot; + proxy['ip_port']将跳转（302）等状态码不是200的响应，重新发起请求（只适用于scrapy，不适用与scrapy-redis） class Forbidden302Middleware(object): def process_response(self, request, response, spider): if response.status != 200: print('捕获到一个相应状态码不是200的请求:', request.url, ': ', response.status_code) return request return response给请求添加cookie的中间件 import random class CookiesMiddleware(object): def process_request(self,request,spider): cookie = random.choice(cookie_pool) # cookie_poll是通过其他模块定期爬取获得的COOKIE集合 request.cookies = cooki在下载中间件中通过selenium实现模拟登录所有内置下载中间件关闭内置中间件的方法示例：'scrapy.downloadermiddlewares.retry.RetryMiddleware': None DOWNLOADER_MIDDLEWARES = { # Engine side 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100, 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300, 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350, 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500, 'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550, 'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560, 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580, # 对压缩(gzip, deflate)数据的支持 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590, 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600, 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700, 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750, 'scrapy.downloadermiddlewares.stats.DownloaderStats': 850, 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900, 常用的伪造User-Agentuser_agent_list = [ &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&quot;, &quot;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&quot;, &quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&quot;, &quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&quot;, &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&quot;, &quot;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;, &quot;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;, &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;, &quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;, &quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;, &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;, &quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&quot;, &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;, &quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/531.21.8 (KHTML, like Gecko) Version/4.0.4 Safari/531.21.10&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/533.17.8 (KHTML, like Gecko) Version/5.0.1 Safari/533.17.8&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/533.19.4 (KHTML, like Gecko) Version/5.0.2 Safari/533.18.5&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-GB; rv:1.9.1.17) Gecko/20110123 (like Firefox/3.x) SeaMonkey/2.0.12&quot;, &quot;Mozilla/5.0 (Windows NT 5.2; rv:10.0.1) Gecko/20100101 Firefox/10.0.1 SeaMonkey/2.7.1&quot;, &quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_5_8; en-US) AppleWebKit/532.8 (KHTML, like Gecko) Chrome/4.0.302.2 Safari/532.8&quot;, &quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US) AppleWebKit/534.3 (KHTML, like Gecko) Chrome/6.0.464.0 Safari/534.3&quot;, &quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_5; en-US) AppleWebKit/534.13 (KHTML, like Gecko) Chrome/9.0.597.15 Safari/534.13&quot;, &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.186 Safari/535.1&quot;, &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.54 Safari/535.2&quot;, &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7&quot;, &quot;Mozilla/5.0 (Macintosh; U; Mac OS X Mach-O; en-US; rv:2.0a) Gecko/20040614 Firefox/3.0.0 &quot;, &quot;Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10.5; en-US; rv:1.9.0.3) Gecko/2008092414 Firefox/3.0.3&quot;, &quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5&quot;, &quot;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.14) Gecko/20110218 AlexaToolbar/alxf-2.0 Firefox/3.6.14&quot;, &quot;Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10.5; en-US; rv:1.9.2.15) Gecko/20110303 Firefox/3.6.15&quot;, &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&quot; ]]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-setting & log]]></title>
    <url>%2F2017%2F03%2F12%2F02.python%E7%88%AC%E8%99%AB%2Fscrapy%E6%A1%86%E6%9E%B6%2FScrapy-settinglog%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[配置scrapy的setting文件为什么需要配置文件配置文件存放一些公共的变量（比如数据库的地址，账号密码等）方便自己和别人的使用和修改为了方便识别，一般配置文件中变量的命名都为大写，例如：SQL_HOST = '192.168.0.1'在pider和pipeline中访问setting内容在spider中访问setting的内容 class Myspider(scrapy.Spider): name = 'myspider' start_urls = ['http://*****';] def parse(self, response): # 可以直接通过访问实例属性的方法，访问setting文件中的内容 # self.settings 返回的是一个字典，里面按照键值对存储所有的配置参数 key = self.settings.attributes.get('KEY'，None) 在pipeline文件中访问setting的内容 class Mypipeline(object): def open_spider(self, spider): # 可以通过传递过来的spider实例来访问setting中的内容 BOT_NAME = spider.setting.get('KEY', None) 配置文件的常用的配置 BOT_NAME = '***' 项目名称 SPIDER_MOUDLES = ['ys.spiders'] # 爬虫创建的位置 NEWSPIDER_MOUDLE = 'yg.spiders' # 新建爬虫的位置 USER_AGENT ='***' # 设置主句名称，用来告诉服务器请求的身份，注意，scrapy不能将USER_AGENT配置在 HEADERS中 ITEM_PIPLINES # 管道 DOWNLOAD_MIDDLEWARES # 下载中间件 OBEY_ROBOTFILES = False # 是否遵守robots协议 SPIDER_MIDDLEWARES # 爬虫中间件 DEAFAULT_REQUEAT_HEADERS = { ***} # 配置scrapy 默认的请求头，不能包含USER-AGRNT和cookies、refer # 使用增量式，或分布式爬虫需要配置 DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot; SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot; SCHEDULER_PERSIST = True ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 400 } REDIS_URL = 'redis://127.0.0.1: 6379' RETRY_HTTP_CODES = [] # 设置会retry的状态码响应 HTTPERROR_ALLOWED_CODES = [302,] # 设置来指定spider能处理的response返回值 LOG_LEVEL = &quot;WARNING&quot; # 设置log提示的等级，log的从高到低的级别分别为：error、warning、info、debug RETRY_ENABLED = False 开启请求失败重新请求，默认是开启的 RETRY_TIMES = 3 设置重新请求的次数 DOWNLOAD_TIMEOUT = 6 设置请求最大等待时间 COOKIES_ENABLE = True # 是否启用cookie中间件，如果关闭是不会向服务器发送cookie的 COOKIES_DEBUG = True # 可以看带cookie在函数中传递的过程 DOWNLOAD_DELAY = 3 # 请求延时 CONCURRENT_REQUESTS = 16 # 并发数量，默认值为16 CONCURRENT_REQUESTS_PRE_DOMAIN = 16 # 每个域名最大并发 CONCURRENT_REQUESTS_PR_IP = 16 # 每个ip最大的并发 AUTOTHROTTLE_ENABLED = True # 动态调整下载延时 CONCURRENT_REQUESTS_PRE_DOMAIN = 1 # 设置允许对同一域名发起请求的并发数量限制 JOBDIR = '路径' # 配置记录爬虫状态目录的文件，使爬虫中途停止再启动的时候会接着上一次的状态继续爬取log的配置与scrapy的debug信息scrapy-log相关的配置log的作用为了让我们自己希望输出到终端的内容能容易看一些配置log的显示等级在setting中设置log显示的级别 LOG_LEVEL = &quot;WARNING&quot; # 在setting文件中添加一行log的从高到低的级别分别为：error、warning、info、debug自定义log日志的方法在想要显示log的位置添加log的输出第一种打印方式：（不带log产生的位置）第二种打印方式：（带log产生的具体文件）在普通的py文件中配置logger日志的方法作用我们配置完了logger的输出文件后，我们可以在任何其他的程序后，可以导入logger模块，帮助我们来可以看到错误产生的位置和时间等信息我们还可以将log存储的文件命名为每日的日期，这样方便我们分别存储我们每日的bug示例： import logging # 配置logger的输出格式，和输出内容 logging.basicConfig( level=logging.INFO, format='%(levelname)s : %(filename)s ' '[%(lineno)d] : %(message)s' ' - %(asctime)s', datefmt='[%d-%b-%Y %H:%M:%S]' filename='./logdebug.log') if __name__ == '__main__': logger = logging.getLogger(__name__) log_message = 'error' logger.warning(log_message）输出的格式详细示例常见的scrapy的debug信息]]></content>
      <categories>
        <category>python爬虫</category>
        <category>scrapy框架</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
</search>
