<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>模型评估、优化、设计过程</title>
      <link href="/2018/11/24/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"/>
      <url>/2018/11/24/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="1-机器学习模型的评估和选择"><a href="#1-机器学习模型的评估和选择" class="headerlink" title="1.机器学习模型的评估和选择"></a>1.机器学习模型的评估和选择</h2><h3 id="有啥方法能优化我们的模型"><a href="#有啥方法能优化我们的模型" class="headerlink" title="有啥方法能优化我们的模型"></a>有啥方法能优化我们的模型</h3><p>整理了一下优化模型时，经常做的一些操作，优化模型无外乎以下几种方法:</p><ol><li>获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</li><li>尝试减少特征的数量</li><li>尝试获得更多的特征</li><li>尝试增加多项式特征</li><li>尝试减少正则化程度</li><li>尝试增加正则化程度</li></ol><p>但是，也如我们所见，并不是任何模型都适用于这些优化方法，我们还需要对症下药，接下来可以看一下怎么确定，我们的模型需要什么<strong>优化服务</strong></p><a id="more"></a><h3 id="模型过拟合的判断"><a href="#模型过拟合的判断" class="headerlink" title="模型过拟合的判断"></a>模型过拟合的判断</h3><p><img src="/img/1543045466595.png" alt="1543045466595"></p><p>当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。此时很可能模型已经产生了过拟合。</p><p>为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。</p><p><img src="/img/1543045796477.png" alt="1543045796477"></p><p>测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：</p><ol><li><p>对于线性回归模型，我们利用测试集数据计算代价函数 <strong>J</strong></p></li><li><p>对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外还可以计算误分类比率</p><p><img src="/img/1543046614286.png" alt="1543046614286"></p></li></ol><h3 id="模型的选择和交叉验证集"><a href="#模型的选择和交叉验证集" class="headerlink" title="模型的选择和交叉验证集"></a>模型的选择和交叉验证集</h3><p>当我们的使用多项式模型的时候，经常会不确定多项式的项数该如何确定，下面是一种比较简单的处理思路:</p><p>假设我们要在10个不同次数的二项式模型之间进行选择：</p><p>​        <img src="/img/1543046781735.png" alt="1543046781735"></p><p>显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能适应我们的测试集或者推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。</p><p>选择模型的方法：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集</p><p><img src="/img/1543046920483.png" alt="1543046920483"></p><ol><li><p>使用训练集训练出10个模型</p></li><li><p>用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</p></li><li><p>选取代价函数值最小的模型</p></li><li><p>用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）Train/validation/test error</p><p><strong>Training error:</strong><br>$J{train}(\theta) = \frac{1}{2m}\sum\limits{i=1}{m}(h_{\theta}(x{(i)})-y{(i)})2$<br><strong>Cross Validation error:</strong><br>$J{cv}(\theta) = \frac{1}{2m{cv}}\sum\limits{i=1}^{m}(h{\theta}(x{(i)}<em>{cv})-y{(i)}</em>{cv})^2 $<br><strong>Test error:</strong><br>$J{test}(\theta)=\frac{1}{2m{test}}\sum\limits{i=1}^{m{test}}(h{\theta}(x^{(i)}{cv})-y{(i)}_{cv})2$</p></li></ol><h3 id="模型的偏差和方差"><a href="#模型的偏差和方差" class="headerlink" title="模型的偏差和方差"></a>模型的偏差和方差</h3><h4 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h4><p>当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？</p><p>其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。</p><p><img src="/img/1543048169732.png" alt="1543048169732"></p><p>我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：</p><p><img src="/img/1543048300914.png" alt="1543048300914"></p><p>通过训练误差和交叉验证集误差判断偏差和方差问题，总结如下:</p><blockquote><p>训练集误差和交叉验证集误差近似时：偏差/欠拟合<br>交叉验证集误差远大于训练集误差时：方差/过拟合</p></blockquote><h4 id="正则化力度与偏差-方差"><a href="#正则化力度与偏差-方差" class="headerlink" title="正则化力度与偏差/方差"></a>正则化力度与偏差/方差</h4><p>在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。</p><p><img src="/img/1543048899675.png" alt="1543048899675"></p><p>我们选择一系列的想要测试的 λ 值，通常是 0-10之间的呈现2倍关系的值（如 0， 0.01， 0.01， 0.04， 0.08， 0.015， 0.032， 0.064， 1.28,  2.56,  5.12,  10共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。</p><p><img src="/img/1543049032194.png" alt="1543049032194"></p><p>选择正则化系数的方法为：</p><ol><li>使用训练集训练出12个不同程度正则化的模型</li><li>用12个模型分别对交叉验证集计算的出交叉验证误差</li><li>选择得出交叉验证误差最小的模型</li><li>运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：</li></ol><p><img src="/img/1543049356556.png" alt="1543049356556"></p><blockquote><p>当 λ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大<br>随着 λ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</p></blockquote><h4 id="通过学习曲线评估偏差和方差"><a href="#通过学习曲线评估偏差和方差" class="headerlink" title="通过学习曲线评估偏差和方差"></a>通过学习曲线评估偏差和方差</h4><p>学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。</p><p>例如： 如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。</p><p><img src="/img/1543049717914.png" alt="1543049717914"></p><p><img src="/img/1543049729709.png" alt="1543049729709"></p><p>如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观</p><p><img src="/img/1543050534124.png" alt="1543050534124"></p><p>也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。</p><p>如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。</p><p><img src="/img/1543050657218.png" alt="1543050657218"></p><p>也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。</p><p>学习曲线的总结:</p><blockquote><p>当Jtrain和Jcv都偏高的时候，处于高偏差(欠拟合)的情况，此时增加训练数据不会有更好的结果</p><p>当Jtrain偏低而Jcv偏高的时候，处于高方差(过拟合)的情况，此时增加训练数据往往会获得更好的训练结果</p></blockquote><h4 id="神经网络的方差和偏差"><a href="#神经网络的方差和偏差" class="headerlink" title="神经网络的方差和偏差"></a>神经网络的方差和偏差</h4><p><img src="/img/1543052413063.png" alt="1543052413063"></p><p>使用<strong>较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合</strong>，但计算代价较小使用<strong>较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合</strong>，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。</p><p>通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。</p><p>对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。</p><h4 id="解决高偏差和高方差的总结"><a href="#解决高偏差和高方差的总结" class="headerlink" title="解决高偏差和高方差的总结"></a>解决高偏差和高方差的总结</h4><p><strong>解决高方差问题:</strong></p><ol><li>获得更多的训练实例——解决高方差</li><li>尝试减少特征的数量——解决高方差</li><li>尝试增加正则化程度λ——解决高方差</li></ol><p><strong>解决高偏差的问题</strong></p><ol><li>尝试获得更多的特征——解决高偏差</li><li>尝试增加多项式特征——解决高偏差</li><li>尝试减少正则化程度λ——解决高偏差</li></ol><h3 id="类偏斜的误差度量"><a href="#类偏斜的误差度量" class="headerlink" title="类偏斜的误差度量"></a>类偏斜的误差度量</h3><h4 id="类偏斜和查准率查全率的介绍"><a href="#类偏斜和查准率查全率的介绍" class="headerlink" title="类偏斜和查准率查全率的介绍"></a>类偏斜和查准率查全率的介绍</h4><p>类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例， 这时候很难用一般的误差度量方法来度量其真实的误差。</p><p>例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 因为，下面将引入查准率和查全率的概念。</p><p>我们将算法预测的结果分成四种情况：</p><ol><li>正确肯定（True Positive,TP）：预测为真，实际为真</li><li>正确否定（True Negative,TN）：预测为假，实际为假</li><li>错误肯定（False Positive,FP）：预测为真，实际为假</li><li>错误否定（False Negative,FN）：预测为假，实际为真</li></ol><blockquote><p><strong>查准率（Precision）和查全率（Recall</strong>）的公式为 ：</p><p>查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p><p>查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p></blockquote><p>这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。</p><p>因此，可以看出用于度量偏斜类问题，查准率和查全率更能体现模型表现的好坏。</p><p><img src="/img/1543054968076.png" alt="1543054968076"></p><h4 id="查准率和查全率之间的选择"><a href="#查准率和查全率之间的选择" class="headerlink" title="查准率和查全率之间的选择"></a>查准率和查全率之间的选择</h4><p>在很多的应用中，我们希望能够保证查准率和召回率的相对平衡。</p><p>继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。</p><p><img src="/img/1543055405994.png" alt="1543055405994"></p><p>查准率(Precision)=TP/(TP+FP) 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p><p>查全率(Recall)=TP/(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p><p>如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。</p><p>如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。</p><p>我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同：</p><p><img src="/img/1543055622819.png" alt="1543055622819"></p><p>我们希望有一个帮助我们选择这个阀值的方法。</p><p>一种方法是计算F1 值（F1 Score），其计算公式为：</p><p>$F_1Score：2{PR\over{P+R}}$</p><p>我们选择使得F1值最高的阀值, F1的想法就是尽量让查准率和查全率都不会太小。</p><h2 id="2-机器学习系统的设计"><a href="#2-机器学习系统的设计" class="headerlink" title="2.机器学习系统的设计"></a>2.机器学习系统的设计</h2><h3 id="机器学习系统设计思路"><a href="#机器学习系统设计思路" class="headerlink" title="机器学习系统设计思路"></a>机器学习系统设计思路</h3><p>以一个垃圾邮件分类器算法为例</p><p>为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。</p><p>为了构建这个分类器算法，我们可以做很多事，例如：</p><ol><li>收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本</li><li>基于邮件的路由信息开发一系列复杂的特征</li><li>基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理</li><li>为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法</li></ol><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p><strong>设计机器学习系统的首要任务</strong></p><p>当设计一个模型的时候， 最好的方法是快速的将最简单版本的算法实现，一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。</p><p>这么做的原因是：因为通常你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。</p><p>除了画出学习曲线之外，一件非常有用的事是误差分析。比如在构造垃圾邮件分类器时，可以看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。</p><p><strong>构建一个学习算法的推荐方法为：</strong></p><ol><li>从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</li><li>绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</li><li>进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势</li></ol><p><strong>举个误差分析的栗子:</strong></p><p>以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：</p><ol><li><p>是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。</p></li><li><p>发现是否缺少某些特征，记下这些特征出现的次数。<br>例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。</p></li></ol><p><strong>误差分析需要交叉验证来验证</strong></p><p>误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。</p><p>在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。</p><p><strong>不要用测试集来做交叉验证</strong></p><p>因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。</p><p><strong>总结:</strong></p><p>当你在研究一个新的机器学习问题时，推荐你实现一个较为简单快速、即便不是那么完美的算法。目前大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。</p><p>另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计过程 </tag>
            
            <tag> 模型评估 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>keras构建简单神经网络</title>
      <link href="/2018/11/11/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/keras%E6%9E%84%E5%BB%BA%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2018/11/11/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/keras%E6%9E%84%E5%BB%BA%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="Keras-构建神经网络"><a href="#Keras-构建神经网络" class="headerlink" title="Keras 构建神经网络"></a>Keras 构建神经网络</h3><p>该示例的一般流程是首先加载数据，然后定义网络，最后训练网络。</p><p>要使用 Keras，你需要知道以下几个核心概念。</p><h4 id="创建神经网络序列模型"><a href="#创建神经网络序列模型" class="headerlink" title="创建神经网络序列模型"></a>创建神经网络序列模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line">   <span class="comment"># Create the Sequential model</span></span><br><span class="line">   model = Sequential()</span><br></pre></td></tr></table></figure><p><a href="https://link.jianshu.com?t=https%3A%2F%2Fkeras.io%2Fmodels%2Fsequential%2F" target="_blank" rel="noopener">keras.models.Sequential</a> 类是神经网络模型的封装容器。它会提供常见的函数，例如 <code>fit()</code>、<code>evaluate()</code> 和 <code>compile()</code>。我们将介绍这些函数（在碰到这些函数的时候）。我们开始研究模型的层级吧。</p><a id="more"></a><h4 id="层级"><a href="#层级" class="headerlink" title="层级"></a>层级</h4><p>Keras 层级就像神经网络层级。有完全连接的层级、最大池化层级和激活层级。你可以使用模型的 add() 函数添加层级。例如，简单的模型可以如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Activation, Flatten</span><br><span class="line">    <span class="comment">#创建序列模型</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    <span class="comment">#第一层级 - 添加有 32 个节点的输入层</span></span><br><span class="line">    model.add(Dense, input_dim=<span class="number">32</span>)</span><br><span class="line">    <span class="comment">#第二层级 - 添加有 128 个节点的完全连接层级</span></span><br><span class="line">    model.add(Dense(<span class="number">128</span>))</span><br><span class="line">    <span class="comment">#第三层级 - 添加 softmax 激活层级</span></span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">    <span class="comment">#第四层级 - 添加完全连接的层级</span></span><br><span class="line">    model.add(Dense(<span class="number">10</span>))</span><br><span class="line">    <span class="comment">#第五层级 - 添加 Sigmoid 激活层级</span></span><br><span class="line">    model.add(Activation(<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure><p>Keras 将根据第一层级自动推断后续所有层级的形状。这意味着，你只需为第一层级设置输入维度。</p><p>上面的第一层级 model.add(Flatten(input_dim=32)) 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。</p><h4 id="模型编译和训练"><a href="#模型编译和训练" class="headerlink" title="模型编译和训练"></a>模型编译和训练</h4><p>构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 categorical_crossentropy。我们还可以指定优化程序，暂时将使用 adam。最后，我们可以指定评估模型用到的指标。我们将使用准确率。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics = [&apos;accuracy&apos;])</span><br></pre></td></tr></table></figure><p>我们可以使用以下命令来查看模型架构：</p><p><code>model.summary()</code><br> 然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。</p><p><code>model.fit(X, y, nb_epoch=1000, verbose=0)</code><br> 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。</p><p>最后，我们可以使用以下命令来评估模型：<br> <code>model.evaluate()</code><br> 很简单，对吧？我们实践操作下。</p><h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><p>我们从最简单的示例开始。在此测验中，你将构建一个简单的多层前向反馈神经网络以解决 XOR 问题。</p><ol><li>将第一层级设为 <code>Flatten()</code> 层级，并将 <code>input_dim</code> 设为 2。</li><li>将第二层级设为 <code>Dense()</code> 层级，并将输出宽度设为 8。</li><li>在第二层级之后使用 softmax 激活函数。</li><li>将输出层级宽度设为 2，因为输出只有 2 个类别。</li><li>在输出层级之后使用 softmax 激活函数。</li><li>对模型运行 10 个 epoch。</li></ol><p>准确度应该为 50%。可以接受，当然肯定不是太理想！在 4 个点中，只有 2 个点分类正确？我们试着修改某些参数，以改变这一状况。例如，你可以增加 epoch 次数。如果准确率达到 75%，你将通过这道测验。能尝试达到 100% 吗？</p><p>首先，查看关于模型和层级的 Keras 文档。 Keras <a href="https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2Ffchollet%2Fkeras%2Fblob%2Fmaster%2Fexamples%2Fmnist_mlp.py" target="_blank" rel="noopener">多层感知器</a>网络示例和你要构建的类似。请将该示例当做指南，但是注意有很多不同之处。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.python.control_flow_ops = tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Our data</span></span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>]]).astype(<span class="string">'float32'</span>)</span><br><span class="line">y = np.array([[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>]]).astype(<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial Setup for Keras</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Activation, Flatten</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encoding the output</span></span><br><span class="line">y = np_utils.to_categorical(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Building the model</span></span><br><span class="line">xor = Sequential()</span><br><span class="line">xor.add(Dense(<span class="number">32</span>, input_dim=<span class="number">2</span>))</span><br><span class="line">xor.add(Activation(<span class="string">"sigmoid"</span>))</span><br><span class="line">xor.add(Dense(<span class="number">2</span>))</span><br><span class="line">xor.add(Activation(<span class="string">"sigmoid"</span>))</span><br><span class="line"></span><br><span class="line">xor.compile(loss=<span class="string">"categorical_crossentropy"</span>, optimizer=<span class="string">"adam"</span>, metrics = [<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment this line to print the model architecture</span></span><br><span class="line"><span class="comment"># xor.summary()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model</span></span><br><span class="line">history = xor.fit(X, y, nb_epoch=<span class="number">100</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scoring the model</span></span><br><span class="line">score = xor.evaluate(X, y)</span><br><span class="line">print(<span class="string">"\nAccuracy: "</span>, score[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Checking the predictions</span></span><br><span class="line">print(<span class="string">"\nPredictions:"</span>)</span><br><span class="line">print(xor.predict_proba(X))</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Using TensorFlow backend.</span><br><span class="line">4/4 [==============================] - 0s</span><br><span class="line">Accuracy:  0.75</span><br><span class="line">Predictions:</span><br><span class="line">4/4 [==============================] - 0s</span><br><span class="line">[[0.6914389  0.6965836 ]</span><br><span class="line"> [0.7073754  0.7086655 ]</span><br><span class="line"> [0.6919555  0.68419015]</span><br><span class="line"> [0.70766294 0.6967294 ]]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> keras </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络</title>
      <link href="/2018/11/07/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2018/11/07/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="1-非线性假设"><a href="#1-非线性假设" class="headerlink" title="1.非线性假设"></a>1.非线性假设</h3><p>当我们使用线型回归或者逻辑回归的时候，有这样一个缺点，当特征太多的时候，计算的负荷会非常大。</p><p>下面是一个例子:</p><p><img src="/img/1541684546191.png" alt="1541684546191"></p><p>当我们使用 x1, x2的多次项式进行预测时，我们可以应用的很好。 使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合（x1x2 + x1x3 + x1x4 + …. + x99x100），，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。</p><a id="more"></a><p>假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一<br>种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来<br>作为特征。<br>假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB值），作为假设，我们可以选取图片上的两个不同位置上的两<br>个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车， 如下图所示：</p><p><img src="/img/1541684864817.png" alt="1541684864817"></p><p>假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约3百万个特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。</p><h3 id="2-神经元和大脑介绍2"><a href="#2-神经元和大脑介绍2" class="headerlink" title="2.神经元和大脑介绍2"></a>2.神经元和大脑介绍2</h3><blockquote><p>神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。</p></blockquote><p>神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过让我和你分享，一些这方面的证据。</p><p><img src="/img/1541685328783.png" alt="1541685328783"></p><p>大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此，你才能明白我的话。神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。</p><p>下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。<br>举几个例子：</p><p><img src="/img/1541685350471.png" alt="1541685350471"></p><p>这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA (美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。</p><p>第二个例子：</p><p><img src="/img/1541685371888.png" alt="1541685371888"></p><p>关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有<br>眼球的孩子。</p><h3 id="3-模型表示"><a href="#3-模型表示" class="headerlink" title="3.模型表示"></a>3.模型表示</h3><h4 id="神经网络模型表示"><a href="#神经网络模型表示" class="headerlink" title="神经网络模型表示"></a>神经网络模型表示</h4><p>为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？</p><p>每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多输入/树突，并且有一个输出/轴突。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。</p><p>下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。</p><p><img src="/img/1541685477041.png" alt="1541685477041"></p><p>轴突是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。</p><p><img src="/img/1541685576582.png" alt="1541685576582"></p><p>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（weight）。</p><p>逻辑回归学习模型的神经元：</p><p><img src="/img/1541686908794.png" alt="1541686908794"></p><p>单个神经元的效果和逻辑回归的效果没有区别，但是神经网络会组成有神经元组成的网络。</p><p>一个简单的神经网络</p><p><img src="/img/1541687140133.png" alt="1541687140133"></p><p>其中 x1, x2, x3是输入单元（input units），我们将原始数据输入给它们。 a1, a2, a3是中间单元，代表三个神经元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算 hθ(x)。<br>神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）：</p><p><img src="/img/1541688086117.png" alt="1541688086117"></p><p>下面引入一些标记法来帮助描述模型： $a_i^{(j)}$代表第 j 层的第 i 个激活单元(神经元)。 $\theta^{(j)}$代表从第 j 层映射到第 j+1 层时的权重的矩阵，例如 $\theta^{(1)}$ 代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 j + 1 层的激活单元数量为行数，以第 j 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中 $\theta^{(1)}$ 的尺寸为 3*4。</p><blockquote><p>注: 每层权重矩阵的列数为 ( 神经元数 + 1 ), 行数为 ( 上一层神经元数 + 1 )</p></blockquote><p><strong>对于上图所示的模型，激活单元和输出分别表达为：</strong></p><p><img src="/img/1541688405565.png" alt="1541688405565"></p><p>上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型</p><p>我们可以知道：每一个 a  的输出都是由上一层所有的 x 和每一个 x 所对应的 θ 决定的（我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )）把 x , θ, a分别用矩阵表示：</p><p><img src="/img/1541689201260.png" alt="1541689201260"></p><h4 id="神经网络模型向量化"><a href="#神经网络模型向量化" class="headerlink" title="神经网络模型向量化"></a>神经网络模型向量化</h4><p>( FORWARD PROPAGATION ) 相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值：</p><p><img src="/img/1541689568821.png" alt="1541689568821"></p><p><img src="/img/1541689589734.png" alt="1541689589734"></p><p><strong>为了更好了了解Neuron Networks的工作原理，我们先把左半部分遮住：</strong></p><p><img src="/img/1541689986501.png" alt="1541689986501"></p><p>右半部分其实就是以a0, a1, a2, a3 , 按照Logistic Regression的方式输出 hθ(x)：</p><p><img src="/img/1541690041021.png" alt="1541690041021"></p><p>其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量 [x1 - x3] 变成了中间层的 [ $a_1^{(2)}$ -  $a_3^{(2)}$]  ,我们可以把a0, a1, a2, a3看成更为高级的特征值，也就是x0, x1, x2, x3 的进化体，并且它们是由 x 与 θ 决定的，因为是梯度下降的，所以 a 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 x 次方厉害，也能更好的预测新数据。 这就是神经网络相比于逻辑回归和线性回归的优势。</p><h3 id="4-特征的直观理解"><a href="#4-特征的直观理解" class="headerlink" title="4. 特征的直观理解"></a>4. 特征的直观理解</h3><p>从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。</p><p>神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。<br><strong>举例说明：</strong></p><p><strong>逻辑与(AND)：</strong>下图中左半部分是神经网络的设计与output层表达式，右边上部分是sigmod函数，下半部分是真值表。</p><p>其中 θ0 = -30, θ1 = 20, θ2 = 20 我们的输出函数hθ(x) 即为：hθ(x) = g(-30 + 20x1 + 20x2), g(x) 的图像是：</p><p><img src="/img/1541690399999.png" alt="1541690399999"><img src="/img/1541690413531.png" alt="1541690413531"></p><p><img src="/img/1541690578952.png" alt="1541690578952"></p><p>所以我们有：hθ(x) ≈ x1 AND x2</p><p><strong>逻辑或(OR)</strong>: 下图是神经网络的设计与output层表达式和真值表。</p><p><img src="/img/1541690859375.png" alt="1541690859375"></p><p><strong>逻辑非(NOT)：</strong></p><p><img src="/img/1541690992427.png" alt="1541690992427"></p><p><strong>异或(XNOR):</strong></p><p><img src="/img/1541691150027.png" alt="1541691150027"></p><h3 id="5-多分类神经网络"><a href="#5-多分类神经网络" class="headerlink" title="5.多分类神经网络"></a>5.多分类神经网络</h3><p>当我们有不止两种分类时（也就是 y = 1, 2, 3, … ），比如以下这种情况，该怎么办？</p><p>如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。<br>假如输入向量 x 有三个维度，两个中间层，输出层4个神经元分别用来表示4类，也就是每一个数据在输出层都会出现 [a b c d] ^T, 且a, b, c,  d 中仅有一个为1，表示当前类。下面是该神经网络的可能结构示例：</p><p><img src="/img/1541691440767.png" alt="1541691440767"></p><h3 id="6-神经网络常用的激活函数"><a href="#6-神经网络常用的激活函数" class="headerlink" title="6. 神经网络常用的激活函数"></a>6. 神经网络常用的激活函数</h3><h4 id="什么是激活函数"><a href="#什么是激活函数" class="headerlink" title="什么是激活函数"></a>什么是激活函数</h4><p>在神经网络中，我们经常可以看到对于<strong>某一个隐藏层的节点</strong>，该节点的激活值计算一般分为<strong>两步</strong>：<br>  （1）输入该节点的值为 x1,x2时，在进入这个隐藏节点后，<strong>会先进行一个线性变换</strong>，计算出值 $z^{[1]}=w_1x_1+w_2x_2+b^{[1]}=W^{[1]}x+b^{[1]}$，上标 1表示第 1 层隐藏层。<br>  （2）<strong>再进行一个非线性变换</strong>，也就是经过<strong>非线性激活函数</strong>，计算出该节点的<strong>输出值(激活值)</strong> $a^{(1)}=g(z^{(1)})$ ，其中 g(z)为非线性函数。</p><p><img src="/img/1541914303454.png" alt="1541914303454"></p><h3 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h3><p>在深度学习中，常用的激活函数主要有：<strong>sigmoid函数，tanh函数，ReLU函数</strong>。</p><p><strong>1.sigmoid函数</strong></p><p>在逻辑回归中我们介绍过sigmoid函数，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下：</p><p><img src="/img/1541916961661.png" alt="1541916961661"></p><p>sigmoid激活函数<strong>缺点</strong>：</p><p>（1）当 z 值<strong>非常大</strong>或者<strong>非常小</strong>时，通过上图我们可以看到，sigmoid函数的<strong>导数</strong> g′(z) 将接近 0 。这会导致权重 W 的<strong>梯度</strong>将接近 0 ，使得<strong>梯度更新十分缓慢</strong>，即<strong>梯度消失</strong>。</p><p>（2）<strong>函数的输出不是以0为均值</strong>，将不便于下层的计算。<strong>sigmoid函数可用在网络最后一层，作为输出层进行二分类</strong>，尽量不要使用在隐藏层。</p><p>   (3) 指数计算消耗资源</p><p><strong>2.tanh函数</strong></p><p> tanh函数相较于sigmoid函数要常见一些，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1)之间，其公式与图形为：</p><p><img src="/img/1541917618607.png" alt="1541917618607"></p><p>tanh函数的<strong>优点</strong></p><p>(1) tanh解决了sigmoid的输出非“零为中心”的问题。</p><p>tanh函数的<strong>缺点</strong></p><p>（1）同sigmoid函数的第一个缺点一样，当 z <strong>很大或很小</strong>时，g′(z) 接近于 0 ，会导致梯度很小，权重更新非常缓慢，即<strong>梯度消失问题</strong>。</p><p>   (2)  依旧是指数计算</p><p><strong>3.ReLU函数</strong></p><p>ReLU函数又称为<strong>修正线性单元（Rectified Linear Unit）</strong>，是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的<strong>梯度消失问题</strong>。ReLU函数的公式以及图形如下：</p><p><img src="/img/1541917904036.png" alt="1541917904036"></p><p>ReLU函数的<strong>优点</strong>：<br>（1）在输入为正数的时候（对于大多数输入 zz 空间来说），不存在梯度消失问题。<br>（2） 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）<br>ReLU函数的<strong>缺点</strong>：</p><p>（1）当输入为负时，梯度为0，会产生梯度消失问题。</p><p><strong>4.Leaky ReLU 函数</strong></p><p> 这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下：</p><p><img src="/img/1541918305822.png" alt="1541918305822"></p><p>Leaky ReLU函数解决了ReLU函数在输入为负的情况下产生的<strong>梯度消失问题</strong>。</p><p>优点:</p><p>1.神经元不会出现死亡的情况。</p><p>2.对于所有的输入，不管是大于等于0还是小于0，神经元不会饱和。</p><p>3.由于Leaky ReLU线性、非饱和的形式，在SGD中能够快速收敛。</p><p>4.计算速度要快很多。Leaky ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。</p><h3 id="7-神经网络代价函数"><a href="#7-神经网络代价函数" class="headerlink" title="7.神经网络代价函数"></a>7.神经网络代价函数</h3><p>假设神经网络的训练样本有 m 个，每个包含一组输入x 和一组输出信号 y ，L 表示神经网络层数，Si 表示每层的<br>neuron个数( Sl表示输出层神经元个数)，SL 代表最后一层中处理单元的个数。</p><p>将神经网络的分类定义为两种情况：二类分类和多类分类，</p><p>二类分类: SL = 1, y = 0 or 1 表示哪一类；</p><p>K类分类:  SL = k, yi = 1 表示分到第i类； (k &gt; 2)</p><p><img src="/img/1542509643821.png" alt="1542509643821"></p><p><img src="/img/1542510435521.png" alt="1542510435521"></p><p>这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。<br>正则化的那一项只是排除了每一层 θ0 后，每一层的 θ 矩阵的和。最里层的循环 j 循环所有的行（由 sl +1 层的激活单元数决定），循环 i 则循环所有的列，由该层（ sl 层）的激活单元数所决定。即：hθ(x) 与真实值之间的距离为每个样本-每个类输出的加和，对参数进行<strong>regularization</strong>的<strong>bias</strong>项处理所有参数的平方和。</p><h3 id="8-反向传播算法"><a href="#8-反向传播算法" class="headerlink" title="8.反向传播算法"></a>8.反向传播算法</h3><h4 id="反向传播算法介绍"><a href="#反向传播算法介绍" class="headerlink" title="反向传播算法介绍"></a>反向传播算法介绍</h4><p>在计算神经网络预测结果的时候采用了正向传播的算法，即从第一层开始向正向一层的神经元一层一层进行计算，知道最后一层的hθ(x).</p><p>为了计算代价函数的偏导数${\delta\over\delta\theta_{ij}^{(i)}} J(\theta)$ , 现在需要用到反向传播算法，以极具是首先计算最后一层的误差，然后再一层一层的反向求出各层的误差，直到倒数第二层。下面举例说明反向传播算法：</p><p>假设我们的训练集只有一个实例(x^(1), y^(1)), 我们的神经网络是一个四层的神经网络，其中: K = 4, SL = 4, L = 4</p><p><strong>其前向传播算法为:</strong> </p><p><img src="/img/1542511793330.png" alt="1542511793330"></p><p><strong>反向传播算法推导过程</strong></p><p><img src="/img/1542512354542.png" alt="1542512354542"></p><p>即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。</p><p><img src="/img/1542519045241.png" alt="1542519045241"></p><h3 id="9-模型构建常用技巧"><a href="#9-模型构建常用技巧" class="headerlink" title="9. 模型构建常用技巧"></a>9. 模型构建常用技巧</h3><h4 id="1-梯度检验"><a href="#1-梯度检验" class="headerlink" title="1.梯度检验"></a>1.梯度检验</h4><p><img src="/img/1542520879656.png" alt="1542520879656"></p><h4 id="2-随机初始化"><a href="#2-随机初始化" class="headerlink" title="2. 随机初始化"></a>2. 随机初始化</h4><p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的， 会导致梯度消失的情况。我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：<br><code>Theta1 = rand(10, 11) * (2*eps) – ep</code></p><h3 id="10-梯度小时和梯度爆照"><a href="#10-梯度小时和梯度爆照" class="headerlink" title="10. 梯度小时和梯度爆照"></a>10. 梯度小时和梯度爆照</h3><p><strong>(1)简介梯度消失与梯度爆炸</strong></p><p>层数比较多的神经网络模型在训练的时候会出现梯度消失(gradient vanishing problem)和梯度爆炸(gradient exploding problem)问题。梯度消失问题和梯度爆炸问题一般会随着网络层数的增加变得越来越明显。</p><p>例如，对于图1所示的含有3个隐藏层的神经网络，梯度消失问题发生时，靠近输出层的hidden layer 3的权值更新相对正常，但是靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。梯度爆炸的情况是：当初始的权值过大，靠近输入层的hidden layer 1的权值变化比靠近输出层的hidden layer 3的权值变化更快，就会引起梯度爆炸的问题。</p><p><img src="/img/1542521622023.png" alt="1542521622023"></p><p><strong>(2)梯度不稳定问题</strong></p><p>在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题。</p><p>梯度不稳定的原因：前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸。</p><p><strong>(3)产生梯度消失的根本原因</strong></p><p>我们以图2的反向传播为例，假设每一层只有一个神经元且对于每一层都可以用公式1表示，其中σ为sigmoid函数，C表示的是代价函数，前一层的输出和后一层的输入关系如公式1所示。我们可以推导出公式2。</p><p><img src="/img/1542521637136.png" alt="1542521637136"></p><p>图2：简单的深度神经网络</p><p><img src="/img/1542521714489.png" alt="1542521714489"></p><p>而sigmoid函数的导数<img src="https://mmbiz.qpic.cn/mmbiz_png/rB4jswrswuzG0Gp7iccdylnaKJkcUQsVbETAGbf8lNv0hkicOlHVCn1f2GQibibkLOOVyVnq6b0xEjbDvocTjxdZeA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img">如图3所示。</p><p><img src="/img/1542521735544.png" alt="1542521735544"></p><p>图3：sigmoid函数导数图像</p><p>可见，σ’(x)的最大值为 1/4，而我们一般会使用标准方法来初始化网络权重，即使用一个均值为0标准差为1的高斯分布。因此，初始化的网络权值通常都小于1，从而有 |σ’(z)w &lt;= 1/4|。对于2式的链式求导，层数越多，求导结果越小，最终导致梯度消失的情况出现。</p><p><img src="/img/1542521829814.png" alt="1542521829814"></p><p><strong>(4)产生梯度爆炸的根本原因</strong></p><p>当<img src="/img/1542521870103.png" alt="1542521870103">，也就是w比较大的情况。则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题。</p><p><strong>(5)如何解决梯度消失和梯度爆炸</strong></p><p>梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。<strong>对于更普遍的梯度消失问题，可以考虑以下三种方案解决：</strong></p><ol><li><strong>用ReLU、Leaky ReLU、PReLU、RReLU、Maxout等替代sigmoid函数。</strong></li><li><strong>用Batch Normalization。</strong></li><li><strong>LSTM的结构设计也可以改善RNN中的梯度消失问题。</strong></li></ol><h3 id="11-构建神经网络的综合步骤"><a href="#11-构建神经网络的综合步骤" class="headerlink" title="11.构建神经网络的综合步骤"></a>11.构建神经网络的综合步骤</h3><p>网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。<br>第一层的单元数即我们训练集的特征数量。<br>最后一层的单元数是我们训练集的结果的类的数量。<br>如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。<br>我们真正要决定的是隐藏层的层数和每个中间层的单元数。<br>训练神经网络：</p><ol><li>参数的随机初始化</li><li><p>利用正向传播方法计算所有的hθ(x)</p></li><li><p>编写计算代价函数 J 的代码</p></li><li>利用反向传播方法计算所有偏导数</li><li>利用数值检验方法检验这些偏导数</li><li>使用优化算法来最小化代价函数</li></ol><h3 id="一句话总结神经网络"><a href="#一句话总结神经网络" class="headerlink" title="一句话总结神经网络"></a>一句话总结神经网络</h3><p>我认为神经网络就是通过各层神经元将变量的重新计算， 导致特征的维度被大大放大， 总之经过神经网络瞎搞之后，特征已不是简单的特征，而原理从根本还是逻辑回归，当特征数量较多，而且模型不能满足线型要求， 可以考虑使用神经网络</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sklearn 逻辑回归</title>
      <link href="/2018/11/03/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/11/03/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h3 id="逻辑回归作用"><a href="#逻辑回归作用" class="headerlink" title="逻辑回归作用"></a>逻辑回归作用</h3><p>可以做概率预测，也可用于分类，仅能用于线性问题。通过计算真实值与预测值的概率，然后变换成损失函数，求损失函数最小值来计算模型参数，从而得出模型。</p><h3 id="sklearn调用接口"><a href="#sklearn调用接口" class="headerlink" title="sklearn调用接口"></a>sklearn调用接口</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">linear_model</span>.<span class="title">LogisticRegression</span><span class="params">(penalty=’l2’, dual=False, tol=<span class="number">0.0001</span>, C=<span class="number">1.0</span>, fit_intercept=True, intercept_scaling=<span class="number">1</span>, class_weight=None, *random_state=None*, solver=’liblinear‘, max_iter=<span class="number">100</span>, multi_class=’ovr’, verbose=<span class="number">0</span>, warm_start=False, n_jobs=<span class="number">1</span>)</span></span></span><br></pre></td></tr></table></figure><a id="more"></a><p>参数**</p><p>　　=&gt; <strong>penalty</strong> : str, ‘l1’ or ‘l2’　　</p><p>　　　　LogisticRegression和LogisticRegressionCV默认就带了正则化项。penalty参数可选择的值为”l1”和”l2”，分别对应L1的正则化和L2的正则化，默认是L2的正则化。</p><p>　　　　在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。</p><p>　　　　另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。</p><p>　　　　penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。</p><p>　　　　但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。</p><p>　　=&gt; <strong>dual</strong> : bool</p><p>　　　　对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False</p><p>　　=&gt; <strong>tol</strong> : float, optional</p><p>　　迭代终止判据的误差范围。</p><p>　　=&gt; <strong>C</strong> : float, default: 1.0</p><p>　　　　C为正则化系数λ的倒数，通常默认为1。设置越小则对应越强的正则化。</p><p>　　=&gt; <strong>fit_intercept</strong> : bool, default: True</p><p>　　　　是否存在截距，默认存在</p><p>　　=&gt; <strong>intercept_scaling</strong> : float, default 1.</p><p>　　　　仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。</p><p>　　=&gt; <strong>class_weight</strong> : dict or ‘balanced’, default: None</p><p>　　　　class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重，</p><p>　　　　或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。</p><p>　　　　如果class_weight<strong>选择**</strong>balanced**，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低；样本量越少，则权重越高。</p><p>　　　　当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))</p><p>　　　　n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3]  0,1分别出现2次和三次</p><p>　　　　<strong>那么**</strong>class_weight<strong>**有什么作用呢？</strong></p><p>​        　　在分类模型中，我们经常会遇到两类问题：</p><p>​       　　 第一种是<strong>误分类的代价很高</strong>。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。</p><p>​      　　 第二种是<strong>样本是高度失衡的</strong>，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。</p><p>　　　　这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。</p><p>　　=&gt; <strong>random_state</strong> : int, RandomState instance or None, optional, default: None</p><p>　　　　随机数种子，默认为无，仅在正则化优化算法为sag,liblinear时有用。</p><p>　　=&gt; <strong>solver</strong> : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}</p><p>　　　　solver参数决定了我们对逻辑回归损失函数的优化方法，有4种算法可以选择，分别是：</p><p>　　　　　　a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。</p><p>　　　　　　b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</p><p>　　　　　　c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</p><p>　　　　　　d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候，SAG是一种线性收敛算法，这个速度远比SGD快。</p><p>　　　　从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear则既可以用L1正则化也可以用L2正则化。</p><p>　　　　同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了</p><p>　　=&gt; <strong>max_iter</strong> : int, optional</p><p>　　　　仅在正则化优化算法为newton-cg, sag and lbfgs 才有用，算法收敛的最大迭代次数。</p><p>　　=&gt; <strong>multi_class</strong> : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’　　　　</p><p> 　  　　OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。</p><p>　　　　其他类的分类模型获得以此类推。</p><p>​         　 而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，</p><p>　　　   得到模型参数。我们一共需要T(T-1)/2次分类。</p><p>　　　   可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。</p><p>　　　   但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。</p><p>　　=&gt; <strong>verbose</strong> : int, default: 0</p><p>　　=&gt; <strong>warm_start</strong> : bool, default: False</p><p>　　=&gt; <strong>n_jobs</strong> : int, default: 1</p><p>　　　　如果multi_class =’ovr’“，并行数等于CPU内核数量。当“solver”设置为“liblinear”时，无论是否指定“multi_class”，该参数将被忽略。如果给定值-1，则使用所有内核。</p><p>　　</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
            <tag> sklearn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>过拟合、正则化(Regularization)</title>
      <link href="/2018/11/02/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
      <url>/2018/11/02/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h3 id="1-过拟合问题"><a href="#1-过拟合问题" class="headerlink" title="1.过拟合问题"></a>1.过拟合问题</h3><h4 id="过拟合问题描述"><a href="#过拟合问题描述" class="headerlink" title="过拟合问题描述"></a>过拟合问题描述</h4><p>常用的机器学习优化算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(over-fitting)的问题，可能会导致它们效果很差。使用正则化技术可以减少过拟合问题。<br>如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。<br>下图是一个回归问题的例子：</p><p><img src="/img/1541172957837.png" alt="1541172957837"></p><a id="more"></a><p>第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。<br>分类问题中也存在这样的问题：</p><p><img src="/img/1541173045662.png" alt="1541173045662"></p><p>就以多项式理解， 的次数越高，拟合的越好，但相应的预测的能力就可能变差。</p><blockquote><p>如果我们发现了过拟合问题，应该如何处理？</p></blockquote><ol><li>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA）</li><li>正则化。 保留所有的特征，但是减少参数的大小（magnitude）。</li></ol><h3 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2.代价函数"></a>2.代价函数</h3><p>上面的回归问题中如果我们的模型是：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3 + θ_4x_4^4$ 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数 θ 的值，这就是正则化的基本方法。</p><p>我们决定要减少θ3和 θ4的大小，我们要做的便是修改代价函数，在其中 θ3和 θ4 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 θ3和θ4。 修改后的代价函数如下：</p><p>$min_θ {1\over 2m}[h_θ(x^{(i)} - y^{(i)})^2 + 1000θ_3^2 + 10000θ_4^2]$</p><p>通过这样的代价函数选择出的 θ3和 θ4 对预测结果的影响就比之前要小许多， 因为为了θ3和θ4的系数很大，为了优化代价函数， 其值一定会特别的小。</p><p>假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p><blockquote><p>$J_θ = {1\over 2m}[\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)}) + λ\sum_{j=1}^nθ_j^2]$<br>其中又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对 θ0 进行惩罚。</p></blockquote><p>经过正则化处理的模型与原模型的可能对比如下图所示：</p><p><img src="/img/1541174343216.png" alt="1541174343216"></p><p>如果选择的正则化参数λ过大，则会把所有的参数都最小化了，导致模型变成 $h_θ(x) = θ_0$，也就是上图中红色直线所示的情况，造成欠拟合。 </p><p>那为什么增加的一项 $λ\sum_{j=1}^nθ_j^2$  可以使的值减小呢？ </p><p>因为如果我们令 λ的值很大的话，为了使Cost Function 尽可能的小，所有的 θ 的值（不包括θ0） 都会在一定程度上减小。 但若λ的值太大了，那么不包括θ0）都会趋近于0，这样我们所得到的只能是一条平行于轴的直线。 所以对于正则化，我们要取一个合理的 λ 的值，这样才能更好的应用正则化。 回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。</p><h3 id="3-正则化线性回归"><a href="#3-正则化线性回归" class="headerlink" title="3.正则化线性回归"></a>3.正则化线性回归</h3><p>对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。<br>正则化线性回归的代价函数为：</p><p> $J_{(θ)} = {1\over 2m} \sum_{i=1}^m[(h_θ(x^{(i)})-y^{(i)})^2 + λ\sum_{j=1}^nθ_j^2]$</p><p>如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对θ进行正则化，所以梯度下降算法将分两种情形：</p><p><img src="/img/1541174923066.png" alt="1541174923066"></p><h3 id="4-正规方程逻辑回归"><a href="#4-正规方程逻辑回归" class="headerlink" title="4.正规方程逻辑回归"></a>4.正规方程逻辑回归</h3><p>针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数J(θ)，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数J(θ)。</p><p><img src="/img/1541175045035.png" alt="1541175045035"></p><p><strong>用Python实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costReg</span><span class="params">(theta, X, y, learningRate)</span>:</span></span><br><span class="line">theta = np.matrix(theta)</span><br><span class="line">X = np.matrix(X)</span><br><span class="line">y = np.matrix(y)</span><br><span class="line">first = np.multiply(‐y, np.log(sigmoid(X*theta.T)))</span><br><span class="line">second = np.multiply((<span class="number">1</span> ‐ y), np.log(<span class="number">1</span> ‐ sigmoid(X*theta.T)))</span><br><span class="line">reg = (learningRate / (<span class="number">2</span> * len(X))* np.sum(np.power(theta[:,<span class="number">1</span>:theta.shape[<span class="number">1</span>]],<span class="number">2</span>))</span><br><span class="line"><span class="keyword">return</span> np.sum(first ‐ second) / (len(X)) + reg</span><br></pre></td></tr></table></figure><p><strong>要最小化该代价函数，通过求导，得出梯度下降算法为：</strong></p><p><img src="/img/1541175163753.png" alt="1541175163753"></p><p>注：看上去同线性回归一样，但是知道 $h_θ(x) = g(θ^TX)$ ，所以与线性回归不同。 Octave 中，我们依旧可以用fminuc 函数来求解代价函数最小化的参数，值得注意的是θ0参数的更新规则与其他情况不同。 </p><blockquote><p>注意：</p><ol><li>虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者hθ(x)的不同所以还是有很大差别。</li><li>θ不参与其中的任何一个正则化。</li></ol></blockquote><h3 id="5-其他防止过拟合的方法"><a href="#5-其他防止过拟合的方法" class="headerlink" title="5.其他防止过拟合的方法"></a>5.其他防止过拟合的方法</h3><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>Dropout是在深度学习中降低过拟合风险的常见方法，它是由大牛Hinton老爷子提出的。老爷子认为在神经网络产生过拟合主要是因为神经元之间的协同作用产生的。因此老爷子在神经网络进行训练的时候，让部分神经元失活，这样就阻断了部分神经元之间的协同作用，从而强制要求一个神经元和随机挑选出的神经元共同进行工作，减轻了部分神经元之间的联合适应性。<br>Dropout的具体流程如下：</p><p>1.对 l 层第 j 个神经元产生一个随机数 $r^{(l)}_j∼Bernouli(p)$</p><p>2.将 l 层第 j 个神经元的输入乘上产生的随机数作为这个神经元新的输入：$x^{(l)∗}_j=x^{(l)}_j∗r^{(l)}_j$</p><p>3.此时第 l 层第 j 个神经元的输出为：<br>$y{(l+1)}<em>j=f(∑^k</em>{j=1}(w^{(l+1)}_j  ∗  x^{(l)∗}_j  +  b^{(l+1)}))$<br>其中，k为第l层神经元的个数，f为该神经元的激活函数，b为偏置，w为权重向量。<br>注意：当我们采用了Dropout之后再训练结束之后，应当将网络的权重乘上概率p得到测试网络的权重，或者可以在训练时，将样本乘上 1/P。</p><h4 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h4><p>在对模型进行训练时，我们可以将我们的数据集分为三个部分，训练集、验证集、测试集。我们在训练的过程中，可以每隔一定量的step，使用验证集对训练的模型进行预测，一般来说，模型在训练集和验证集的损失变化如下图所示： </p><p><img src="/img/1541249965141.png" alt="1541249965141"></p><p>可以看出，模型在验证集上的误差在一开始是随着训练集的误差的下降而下降的。当超过一定训练步数后，模型在训练集上的误差虽然还在下降，但是在验证集上的误差却不在下降了。此时我们的模型就过拟合了。因此我们可以观察我们训练模型在验证集上的误差，一旦当验证集的误差不再下降时，我们就可以提前终止我们训练的模型。</p><h4 id="增加样本量"><a href="#增加样本量" class="headerlink" title="增加样本量"></a>增加样本量</h4><p>在实际的项目中，你会发现，上面讲述的那些技巧虽然都可以减轻过拟合的风险，但是却都比不上增加样本量来的更实在。</p><p>为什么增加样本可以减轻过拟合的风险呢？这个要从过拟合是啥来说。过拟合可以理解为我们的模型对样本量学习的太好了，把一些样本的特殊的特征当做是所有样本都具有的特征。举个简单的例子，当我们模型去训练如何判断一个东西是不是叶子时，我们样本中叶子如果都是锯齿状的话，如果模型产生过拟合了，会认为叶子都是锯齿状的，而不是锯齿状的就不是叶子了。如果此时我们把不是锯齿状的叶子数据增加进来，此时我们的模型就不会再过拟合了。因此其实上述的那些技巧虽然有用，但是在实际项目中，你会发现，其实大多数情况都比不上增加样本数据来的实在。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sklearn 线型回归</title>
      <link href="/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h2 id="简单线性回归"><a href="#简单线性回归" class="headerlink" title="简单线性回归"></a>简单线性回归</h2><p>线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。</p><a id="more"></a><h3 id="使用sklearn-linear-model-LinearRegression进行线性回归"><a href="#使用sklearn-linear-model-LinearRegression进行线性回归" class="headerlink" title="使用sklearn.linear_model.LinearRegression进行线性回归"></a>使用sklearn.linear_model.LinearRegression进行线性回归</h3><p>sklearn对Data Mining的各类算法已经有了较好的封装，基本可以使用<code>fit</code>、<code>predict</code>、<code>score</code>来训练、评价模型，并使用模型进行预测，一个简单的例子如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">clf = linear_model.LinearRegression()</span><br><span class="line">X = [[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">y = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">clf.fit(X,y)</span><br><span class="line">print(clf.coef_)</span><br><span class="line">[ <span class="number">0.5</span> <span class="number">0.5</span>]</span><br><span class="line">print(clf.intercept_)</span><br><span class="line"><span class="number">1.11022302463e-16</span></span><br></pre></td></tr></table></figure><p><code>LinearRegression</code>已经实现了多元线性回归模型，当然，也可以用来计算一元线性模型，通过使用list[list]传递数据就行。下面是<code>LinearRegression</code>的具体说明。</p><h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><h5 id="实例化"><a href="#实例化" class="headerlink" title="实例化"></a>实例化</h5><p>sklearn一直秉承着简洁为美得思想设计着估计器，实例化的方式很简单，使用<code>clf = LinearRegression()</code>就可以完成，但是仍然推荐看一下几个可能会用到的参数：</p><ul><li><code>fit_intercept</code>：是否存在截距，默认存在</li><li><code>normalize</code>：标准化开关，默认关闭</li></ul><p>还有一些参数感觉不是太有用，就不再说明了，可以去官网文档中查看。</p><h5 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h5><p>其实在上面的例子中已经使用了<code>fit</code>进行回归计算了，使用的方法也是相当的简单。</p><ul><li><code>fit(X,y,sample_weight=None)</code>：<code>X</code>,<code>y</code>以矩阵的方式传入，而<code>sample_weight</code>则是每条测试数据的权重，同样以<code>array</code>格式传入。</li><li><code>predict(X)</code>：预测方法，将返回预测值<code>y_pred</code></li><li><code>score(X,y,sample_weight=None)</code>：评分函数，将返回一个小于1的得分，可能会小于0</li></ul><h5 id="方程"><a href="#方程" class="headerlink" title="方程"></a>方程</h5><p><code>LinearRegression</code>将方程分为两个部分存放，<code>coef_</code>存放回归系数，<code>intercept_</code>则存放截距，因此要查看方程，就是查看这两个变量的取值。</p><h4 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h4><p>其实，多项式就是多元回归的一个变种，只不过是原来需要传入的是X向量，而多项式则只要一个x值就行。通过将x扩展为指定阶数的向量，就可以使用<code>LinearRegression</code>进行回归了。sklearn已经提供了扩展的方法——<code>sklearn.preprocessing.PolynomialFeatures</code>。利用这个类可以轻松的将x扩展为X向量，下面是它的使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">&gt;&gt;&gt; X_train = [[1],[2],[3],[4]]</span><br><span class="line">&gt;&gt;&gt; quadratic_featurizer = PolynomialFeatures(degree=2)</span><br><span class="line">&gt;&gt;&gt; X_train_quadratic = quadratic_featurizer.fit_transform(X_train)</span><br><span class="line">&gt;&gt;&gt; print(X_train_quadratic)</span><br><span class="line">[[ 1  1  1]</span><br><span class="line"> [ 1  2  4]</span><br><span class="line"> [ 1  3  9]</span><br><span class="line"> [ 1  4 16]]</span><br></pre></td></tr></table></figure><p>经过以上处理，就可以使用<code>LinearRegression</code>进行回归计算了。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sklearn </tag>
            
            <tag> 线型回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>逻辑回归 &amp; 分类问题</title>
      <link href="/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h3 id="1-分类问题"><a href="#1-分类问题" class="headerlink" title="1. 分类问题"></a>1. 分类问题</h3><p>在分类问题中，如果想要预测的变量y是离散的值，那么可以只用一种叫做逻辑回归(logistic regression)的算法,这是在目前最流行的一种学习算法之一。</p><p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。  </p><a id="more"></a><p><img src="/img/1540997286168.png" alt="1540997286168"></p><p>我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈0,1 ，其中 0 表示负向类，1 表示正向类。</p><p><img src="/img/1540997531192.png" alt="1540997531192"></p><h4 id="线型回归不适合分类问题"><a href="#线型回归不适合分类问题" class="headerlink" title="线型回归不适合分类问题"></a>线型回归不适合分类问题</h4><p>如果我们要用线性回归算法来解决一个分类问题，对于分类， y 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 y 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。</p><h4 id="逻辑回归算法"><a href="#逻辑回归算法" class="headerlink" title="逻辑回归算法"></a>逻辑回归算法</h4><p>逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y 取值离散的情况，如：1 0 0 1。</p><h3 id="2-逻辑回归表达式"><a href="#2-逻辑回归表达式" class="headerlink" title="2. 逻辑回归表达式"></a>2. 逻辑回归表达式</h3><p>在分类问题中，要用什么样的函数来表示我们的假设。在解决分类问题的时候，我们希望分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。</p><h4 id="为什么线型回归不适合分类问题"><a href="#为什么线型回归不适合分类问题" class="headerlink" title="为什么线型回归不适合分类问题?"></a>为什么线型回归不适合分类问题?</h4><p>回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线：</p><p><img src="/img/1541078937881.png" alt="1541078937881"></p><p>根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：</p><p>当 $h_θ(x) &gt;= 0.5$ 时, 预测y= 1;   当$h_θ(x) &lt; 0.5$ 时，预测y = 0</p><p>对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。但是， 假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。</p><p><img src="/img/1541079100064.png" alt="1541079100064"></p><p>这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了， 所以使用线型回归模型，很难确定断定一个问题分类的边界。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决分类这样的问题。</p><h4 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h4><p>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是：</p><p>$h_θ(x) = g(θ^TX)$      函数g的表达式为:  $g(z) = {1\over1+e^{-z}}$</p><blockquote><p> 其中: X 代表特征向量, g代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function）</p></blockquote><p>g(z) 的函数图像为:</p><p><img src="/img/1541080097808.png" alt="1541080097808"></p><blockquote><p>对逻辑回归模型理解</p><p>$h_θ(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算出输出的变量=1 的可能性, 即$h_θ(x) =P(y=1|x; θ)$ </p><p>例如, 如果对于给定的x， 通过已经确定的参数计算出$h_θ(x) = 0.7$ ， 则表示有70%的几率y为正向类，响应地y为负向类的几率为1-0.7=0.3</p><p>g(z) （Sigmoid function）函数的作用就是将$θ^TX$的结果映射到0-1之间，</p></blockquote><h3 id="3-决策边界"><a href="#3-决策边界" class="headerlink" title="3. 决策边界"></a>3. 决策边界</h3><h4 id="对决策边界的理解"><a href="#对决策边界的理解" class="headerlink" title="对决策边界的理解"></a>对决策边界的理解</h4><blockquote><p>决策边界(decision boundary)，能够更好地帮助我们理解逻辑回归的假设函数中计算什么</p></blockquote><p><img src="/img/1541080767309.png" alt="1541080767309"></p><p>在逻辑回归中， 我们预测到: </p><p>当$h_θ(x) &gt;= 0.5$ 时， 预测 y= 1;     当 $h_θ(x) &gt;= 0.5$ 时，预测 y  = 0；</p><p>根据上面绘制的S形函数图像，我们知道当</p><p>z = 0 时, g(z) = 0.5</p><p>z &gt; 时, g(z) &gt; 0.5</p><p>z &lt; 0 时, g(z) &lt; 0.5</p><p>又 $z = θ^Tx$, 即: $θ^TX &gt;= 0$ 时, 预测 y = 1 , $θ^TX &lt; 0$ 时， 预测 y = 0</p><p>现在假设我们有一个模型: </p><p><img src="/img/1541081223807.png" alt="1541081223807"></p><p>并且参数θ是向量[-3 1 1]. 则当 -3 + x1 + x2 &gt;= 0, 即x1 + x2 &gt;= 3时， 模型将预测 y = 1. 我们可以绘制直线x1 + x2 = 3, 这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。</p><p><img src="/img/1541081375902.png" alt="1541081375902"></p><h4 id="复杂形状的决策边界"><a href="#复杂形状的决策边界" class="headerlink" title="复杂形状的决策边界"></a>复杂形状的决策边界</h4><p>假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？</p><p><img src="/img/1541081567959.png" alt="1541081567959"></p><p>因为需要用曲线才能分隔 y = 0 的区域和 y = 1 的区域，我们需要二次方特征：</p><p>所以按照假设函数应该趋近， $h_θ(x) = g(θ_0 + θ_1x_1 + θ_2x_2 + θ_1x_1^2 + θ_2x_2^2 )$  θ最终结果趋近 [-1 0 0 1 1], 我们最终得到的判定边界恰好是圆点在原点且半径为1的圆形 , 综上我们可以用复杂的模型来适应飞象复杂形状的判定边界</p><h3 id="4-逻辑回归代价函数和梯度下降"><a href="#4-逻辑回归代价函数和梯度下降" class="headerlink" title="4. 逻辑回归代价函数和梯度下降"></a>4. 逻辑回归代价函数和梯度下降</h3><h4 id="逻辑回归代价函数及简化"><a href="#逻辑回归代价函数及简化" class="headerlink" title="逻辑回归代价函数及简化"></a>逻辑回归代价函数及简化</h4><p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_θ(x) = g(θ^TX)$  带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）如下图，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p><p><img src="/img/1541082558471.png" alt="1541082558471"></p><p>线型回归代价函数为: $J_{(θ)} = {1\over m} \sum_{i=1}^m{1\over2}(h_θ(x^{(i)})-y^{(i)})^2​$</p><p>重新定义逻辑回归代价函数，使代价函数在逻辑回归算法中为凸函数, 过程如下: </p><p><img src="/img/1541083131135.png" alt="1541083131135"></p><blockquote><p>根据推倒可以证明, 我们推倒的的代价值函数会给我们一个凸优化问题。代价函数 J(θ) 会是一个凸函数，并且没有局部最优值。</p></blockquote><p><strong>python代码实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">   theta = np.matrix(theta)</span><br><span class="line">   X = np.matrix(X)</span><br><span class="line">   y = np.matrix(y)</span><br><span class="line">   first = np.multiply(‐y, np.log(sigmoid(X* theta.T)))</span><br><span class="line">   second = np.multiply((<span class="number">1</span> ‐ y), np.log(<span class="number">1</span> ‐ sigmoid(X* theta.T)))</span><br><span class="line">   <span class="keyword">return</span> np.sum(first ‐ second) / (len(X))</span><br></pre></td></tr></table></figure><h4 id="梯度下降算法推倒及简化"><a href="#梯度下降算法推倒及简化" class="headerlink" title="梯度下降算法推倒及简化"></a>梯度下降算法推倒及简化</h4><p>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：</p><p>Repeat {  $θ_j : =θ_j  −  α{∂\over∂θ_j }J(θ)$         (simultaneously update all ) }</p><p>求导后得到: </p><p>Repeat { $θ_j : =θ<em>j  −  α{1\over m}\sum</em>{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$    (simultaneously update all ) }</p><p><img src="/img/1541084166476.png" alt="1541084166476"></p><blockquote><p>注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的 $h_θ(x) = g(θ^TX)$ 与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。</p></blockquote><h3 id="5-高级优化算法"><a href="#5-高级优化算法" class="headerlink" title="5. 高级优化算法"></a>5. 高级优化算法</h3><h4 id="一些高级算法的介绍"><a href="#一些高级算法的介绍" class="headerlink" title="一些高级算法的介绍"></a>一些高级算法的介绍</h4><p>现在我们换个角度来看，什么是梯度下降，我们有个代价函数 J(θ)，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数θ时，它们会计算出两样东西：J(θ) 以及 J 等于 0、1直到 n 时的偏导数项。</p><p><img src="/img/1541166796206.png" alt="1541166796206"></p><p>假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。 另一种考虑梯度下降的思路是：我们需要写出代码来计算 J(θ) 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。 对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数J(θ)。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些 J(θ)的收敛性，那么我们就需要自己编写代码来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$。所以，在写完能够计算这两者的代码之后，我们就可以使用梯度下降。 </p><p>然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$两个项的话，那么这些算法就是为我们优化代价函数的不同方法，<strong>共轭梯度法、 BFGS (变尺度法)、 L-BFGS (限制变尺度法)</strong> 就是其中一些更高级的优化算法，它们需要有一种方法来计算 J(θ)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。</p><blockquote><p><strong>这三种算法的优点：</strong></p><p>一个是使用这其中任何一个算法，你通常不需要手动选择学习率 α ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 α ，并自动选择一个好的学习速率 α，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。</p></blockquote><p>Octave 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，因此，如果你正在你的机器学习程序中使用一种不同的语言，比如如果你正在使用C、C++、Java等等，你可能会想尝试一些不同的库，以确保你找到一个能很好实现这些算法的库。</p><h4 id="如何使用这些算法"><a href="#如何使用这些算法" class="headerlink" title="如何使用这些算法"></a>如何使用这些算法</h4><p>比方说，你有一个含两个参数的问题，这两个参数是θ1和θ2，因此，通过这个代价函数，你可以得到 θ1 和 θ2 的值，如果你将 J(θ) 最小化的话，那么它的最小值将是 θ1 = 5 ，θ2 = 5 。代价函数 J(θ) 的导数推出来就是这两个表达式：</p><p>$α{∂\over∂θ_1}J(θ) = 2(θ_1 - 5)$</p><p>$α{∂\over∂θ_2}J(θ) = 2(θ_2 - 5)$</p><p>如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的Octave 函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function [jVal, gradient]=costFunction(theta)</span><br><span class="line">jVal=(theta(1)‐5)^2+(theta(2)‐5)^2;</span><br><span class="line">gradient=zeros(2,1);</span><br><span class="line">gradient(1)=2*(theta(1)‐5);</span><br><span class="line">gradient(2)=2*(theta(2)‐5);</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个costFunction 函数后，你就可以调用高级的优化函数，这个函数叫fminunc，它表示Octave 里无约束最小化函数。调用它的方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">options=optimset(<span class="string">'GradObj'</span>,<span class="string">'on'</span>,<span class="string">'MaxIter'</span>,<span class="number">100</span>);</span><br><span class="line">initialTheta=zeros(<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">[optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure><p>你要设置几个options，这个 options 变量作为一个数据结构可以存储你想要的options，所以 GradObj 和On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说100，我们给出一个 的猜测初始值，它是一个2×1的向量，那么这个命令就调用fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的值。</p><p><strong>实际运行过程示例</strong></p><p><img src="/img/1541168153304.png" alt="1541168153304"></p><h3 id="6-多分类问题"><a href="#6-多分类问题" class="headerlink" title="6. 多分类问题"></a>6. 多分类问题</h3><h4 id="多分类的介绍"><a href="#多分类的介绍" class="headerlink" title="多分类的介绍"></a>多分类的介绍</h4><p><strong>一些多分类的例子:</strong></p><p>例一: 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用 y=1, y=2, y=3, y=4 来表示</p><p>例二: 如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用 y=1 这个类别来代表；或者患了感冒，用 y=2 来代表；或者得了流感用 y=3 来代表.</p><p>然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样：</p><p><img src="/img/1541168782303.png" alt="1541168782303"></p><p>对于一个多类分类问题，我们的数据集或许看起来像这样：</p><p><img src="/img/1541168809943.png" alt="1541168809943"></p><h4 id="一对于多分类思路"><a href="#一对于多分类思路" class="headerlink" title="一对于多分类思路"></a>一对于多分类思路</h4><p>我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为<strong>“一对余”</strong>方法。</p><p><img src="/img/1541169223823.png" alt="1541169223823"></p><p>现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 y = 1 ，方框表示 y = 2，叉叉表示 y = 3。下面要做的就是使用一个训练集，将其分成3个二元分类问题。我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。</p><p>这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。</p><p><img src="/img/1541169503523.png" alt="1541169503523"></p><p>为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作$h_θ^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作$h_θ^{(2)}(x)$ ,依此类推。 最后我们得到一系列的模型简记为：$h_θ^{(i)}(x) = P(y=i|x; θ)$ 其中：i = (1, 2, 3, 4, 5, …, k) k为总分类数量<br>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。<br>总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_θ^{(i)}(x)$ ， 其中 i 对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 x ，然后我们选择一个让 $h_θ^{(i)}(x)$ 最大的 i 作为最终的分类结果。<br>现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论值是多少，我们都有最高的概率值，我们预测就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> 分类问题 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Octave基础操作</title>
      <link href="/2018/10/26/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/octave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/10/26/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/octave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="基础功能命令"><a href="#基础功能命令" class="headerlink" title="基础功能命令"></a>基础功能命令</h2><h4 id="修改命令行的提示"><a href="#修改命令行的提示" class="headerlink" title="修改命令行的提示"></a>修改命令行的提示</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PS1('&gt;&gt; ')   % &gt;&gt; 就是修改后的提示符号</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="变量赋值语句"><a href="#变量赋值语句" class="headerlink" title="变量赋值语句"></a>变量赋值语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = 5 + 6 ; % 后面加上一个可以不在终端打印变量的值</span><br></pre></td></tr></table></figure><h4 id="显示工作空间的所有变量"><a href="#显示工作空间的所有变量" class="headerlink" title="显示工作空间的所有变量"></a>显示工作空间的所有变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">who  % 显示工作空间的所有变量</span><br><span class="line">whos   % 显示工作空间的所有变量和详细信息</span><br></pre></td></tr></table></figure><h4 id="删除变量"><a href="#删除变量" class="headerlink" title="删除变量"></a>删除变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clear A % 删除变量A</span><br><span class="line">clear  % 删除所有变量</span><br></pre></td></tr></table></figure><h4 id="打印变量"><a href="#打印变量" class="headerlink" title="打印变量"></a>打印变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A    % 直接再终端输入变量名称就可以将变量的值打印出来</span><br><span class="line">disp(A)    % 通过disp函数将变量打印出来</span><br></pre></td></tr></table></figure><h4 id="修改全局的输出内容的长短"><a href="#修改全局的输出内容的长短" class="headerlink" title="修改全局的输出内容的长短"></a>修改全局的输出内容的长短</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">format long   % 将输出数值的长度定义为long类型</span><br><span class="line">format short   % 将输出数值的长度定义为short类型</span><br></pre></td></tr></table></figure><h4 id="查看命令的帮助信息"><a href="#查看命令的帮助信息" class="headerlink" title="查看命令的帮助信息"></a>查看命令的帮助信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helo rand</span><br><span class="line">help eye</span><br><span class="line">help help</span><br></pre></td></tr></table></figure><h4 id="添加搜索路径"><a href="#添加搜索路径" class="headerlink" title="添加搜索路径"></a>添加搜索路径</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">addpath path % 添加路径到函数和数据等的某人搜索路径</span><br></pre></td></tr></table></figure><h2 id="基础运算"><a href="#基础运算" class="headerlink" title="基础运算"></a>基础运算</h2><h4 id="数值运算"><a href="#数值运算" class="headerlink" title="数值运算"></a>数值运算</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3-2； 5*8；  1/2； % 基础运算</span><br></pre></td></tr></table></figure><h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 &amp;&amp; 0  % 逻辑与</span><br><span class="line">1 || 0  % 逻辑或</span><br><span class="line">~ 1  % 逻辑费</span><br><span class="line">XOR(1, 0)   % 异或运算</span><br></pre></td></tr></table></figure><h4 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 == 2   % 相等判断</span><br><span class="line">1 ~= 2   % 不等于判断</span><br></pre></td></tr></table></figure><h2 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h2><h4 id="创建矩阵"><a href="#创建矩阵" class="headerlink" title="创建矩阵"></a>创建矩阵</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">v = [ 1 2 3 ]  % 创建一个行向量</span><br><span class="line">v = 1:6   % 创建一个从1到6的行向量</span><br><span class="line">v = 1:0.1:2    % 创建一个从1开始，以0.1为步长，直到2的行向量</span><br><span class="line">v = ones(2, 3)   % 创建一个2行3列的元素都是1的矩阵</span><br><span class="line">v = 2 * ones(2, 3)   % 创建一个2行3列的元素都是2的矩阵</span><br><span class="line">v = zeros(2, 3)   % 创建一个2行3列的元素都是0的矩阵</span><br><span class="line">v = rand(2, 3)   % 创建一个2行3列的元素都是0-1之间的随机数的矩阵</span><br><span class="line">v = randn(2, 3)   % 创建一个2行3列的元素都符合平局值为0,1为标准差的正态分布随机数的矩阵</span><br><span class="line">I = eye(6)    % 创建一个大小为6的单位矩阵</span><br><span class="line">v = type(3)   % 返回一个3*3的随机矩阵</span><br></pre></td></tr></table></figure><h4 id="矩阵运算-1"><a href="#矩阵运算-1" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">B = [ 11, 12; 13 14; 15 16 ]</span><br><span class="line">2 * A  % A矩阵中的每个元素都乘以2</span><br><span class="line">A .* B % 将A矩阵和B矩阵中的每一个对应位置的元素相乘,同样适用于+ - / ^等</span><br><span class="line">A .*   % A矩阵的每个元素取二次方</span><br><span class="line">log(v)     % 矩阵的每个院对对数运算</span><br><span class="line">exp(v)     % 矩阵的每个元素进行以为底，以这些元素为幂的运算</span><br><span class="line">abs(v)     % 对v矩阵的每个元素取绝对值</span><br><span class="line">A + 1    % 将A矩阵的每个元素加上1</span><br><span class="line">A&apos;     % 取A矩阵的转置矩阵</span><br></pre></td></tr></table></figure><h4 id="获取矩阵尺寸"><a href="#获取矩阵尺寸" class="headerlink" title="获取矩阵尺寸"></a>获取矩阵尺寸</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">size(A)     % 返回A矩阵的尺寸,返回的内容同样是行向量</span><br><span class="line">size(A，1)    % 返回矩阵的行数</span><br><span class="line">size(A，2)   % 返回矩阵的列数</span><br><span class="line">lengh(A)     % 返回A矩阵的维度大小，即矩阵的长宽中比较大的维度，矩阵A的length即为3</span><br></pre></td></tr></table></figure><h4 id="矩阵的索引"><a href="#矩阵的索引" class="headerlink" title="矩阵的索引"></a>矩阵的索引</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">A(3, 2)  % 取A矩阵的第三行第二列的元素</span><br><span class="line">A(2, :)    % 返回第二行的所有元素</span><br><span class="line">A(:, 2)     % 返回第二列的所有元素</span><br><span class="line">A([1 3], :)  % 取第1行和第3行的所有元素</span><br><span class="line">A(:, 2) = [10; 11; 12]   % 将矩阵的第二列重新赋值</span><br><span class="line">A = [A, [100, 101, 102]]   % 给矩阵附加了一列矩阵，给矩阵增加一列。带，和不带，相同</span><br><span class="line">[A B]    % 两个矩阵直接连在一起，矩阵A在左边，矩阵B在右边</span><br><span class="line">[A;B]    % 两个矩阵直接连在一起，矩阵A在上边, 矩阵B在下边</span><br><span class="line">A(:)     % 将矩阵的所有元素导向一个单独的列向量排列起来</span><br></pre></td></tr></table></figure><h4 id="矩阵的计算"><a href="#矩阵的计算" class="headerlink" title="矩阵的计算"></a>矩阵的计算</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">val = max(a)    % 返回a矩阵中的最大值, 当a为多维矩阵的时候返回的是每一列的最大值</span><br><span class="line">val = max(A, a)    % 两个矩阵所有元素逐个比较返回较大的值</span><br><span class="line">max(A,[],1)    % 得到矩阵每一列元素的最大值</span><br><span class="line">max(A,[],2)   % 得到矩阵每一行元素的最大值</span><br><span class="line">[val, ind] = max(a)  % 返回a矩阵中的最大值和对应的索引</span><br><span class="line">sum（a)   % 把a矩阵的所有元素相加起来, 如果是多维矩阵默认为每一列的总和</span><br><span class="line">sum（a,1) % 求多维矩阵每一列的总和</span><br><span class="line">sum(a, 3)   % 求多维矩阵每一行的总和</span><br><span class="line">a&lt;3  % 对矩阵每一个元素进行&lt;3判断，判断为真返回1, 否则返回0</span><br><span class="line">find(a&lt;3)  % 返回哪些元素小于3(是索引值)</span><br><span class="line">prod（a)   % 将a矩阵的所有元素相乘</span><br><span class="line">floor(a)   % 将a矩阵的所有元素进行向下取舍</span><br><span class="line">ceil(a)   % 将a矩阵的所有元素进行向上取整</span><br><span class="line">pinv(v)   % 求v矩阵的逆矩阵</span><br></pre></td></tr></table></figure><h2 id="绘制图像"><a href="#绘制图像" class="headerlink" title="绘制图像"></a>绘制图像</h2><h4 id="绘制直方图"><a href="#绘制直方图" class="headerlink" title="绘制直方图"></a>绘制直方图</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = -6 + sqrt(10) * (randn(1, 10000))</span><br><span class="line">hist(w)    % 绘制w矩阵的直方图</span><br><span class="line">hist(w, 50)    % 绘制w矩阵的直方图，并指定50个长方形</span><br></pre></td></tr></table></figure><h4 id="绘制曲线图"><a href="#绘制曲线图" class="headerlink" title="绘制曲线图"></a>绘制曲线图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">t = [0:0.1:1];</span><br><span class="line">y1 = sin(2*pi*4t);</span><br><span class="line">plot(t,  y1);</span><br><span class="line">hold on % 新绘制的图不会覆盖之前的图片，会在之前的图片上生成新的曲线</span><br><span class="line">y2 = cos(2*pi*4*t)</span><br><span class="line">plot(t, y2, &apos;r&apos;)  % 绘制新的直线图，r表示线的颜色为红色</span><br><span class="line">xlable(&apos;time&apos;)    % 添加x轴名称</span><br><span class="line">ylable(&apos;value&apos;)    % 给y轴添加名称</span><br><span class="line">legend(&apos;sin, &apos;cos&apos;)   % 给线命名</span><br><span class="line">title(&apos;myplot&apos;)    % 给图片一个标题名称</span><br><span class="line">print  -dpng  &apos;myplot.png&apos;    % 输出图片</span><br><span class="line">plot clos   % 关掉图片</span><br><span class="line">axis([0.5 1 ‐1 1])   % 改变坐标返回，前两个参数控制横坐标，后两参数控制纵坐标</span><br><span class="line">Clf  % 清除一个图像</span><br></pre></td></tr></table></figure><h4 id="在一张画纸上绘制两张直线图"><a href="#在一张画纸上绘制两张直线图" class="headerlink" title="在一张画纸上绘制两张直线图"></a>在一张画纸上绘制两张直线图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t = [0:0.1:1];</span><br><span class="line">y1 = sin(2*pi*4t);</span><br><span class="line">y2 = cos(2*pi*4*t)；</span><br><span class="line">figure(1); plot(t, y1);    % 绘制第一张图片</span><br><span class="line">figure(2); plot(t, y2);    % 绘制第二张图片</span><br><span class="line">subplot(1,2,1) %将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。</span><br><span class="line">plot(t,y1)   % 将图片绘制到第一个格子</span><br><span class="line">suplot(1,2,2)   % 使用第二个格子</span><br><span class="line">plot(t,y2)  % 将图片绘制到第二个格子</span><br></pre></td></tr></table></figure><h4 id="彩色格图绘制"><a href="#彩色格图绘制" class="headerlink" title="彩色格图绘制"></a>彩色格图绘制</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imagesc(A )   % 绘制彩色格子图</span><br><span class="line">imagesc(A)，colorbar，colormap gray   % 生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。</span><br></pre></td></tr></table></figure><h2 id="移动数据"><a href="#移动数据" class="headerlink" title="移动数据"></a>移动数据</h2><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load('featureD.dat')    % 加载featureD.dat中的所有数据，并将其复制给变量featureD</span><br></pre></td></tr></table></figure><h4 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">save hello.mat v   % 将变量A导出为一个叫hello.mat文件 二进制形式</span><br><span class="line">save hello.mat v -ascii  % 将变量A导出为一个叫hello.mat文件 ascii形式</span><br></pre></td></tr></table></figure><h2 id="控制语句"><a href="#控制语句" class="headerlink" title="控制语句"></a>控制语句</h2><h4 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = zeros(10,1)</span><br><span class="line">for i=1:10,</span><br><span class="line">v(i) = 2^1;</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h4 id="while-循环"><a href="#while-循环" class="headerlink" title="while 循环"></a>while 循环</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">v = zeros(10,1)</span><br><span class="line">i = 1</span><br><span class="line">while i &lt;= 5,</span><br><span class="line">v(i) = 100;</span><br><span class="line">i = i+1;</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h4 id="if-else-elif-语句"><a href="#if-else-elif-语句" class="headerlink" title="if - else - elif 语句"></a>if - else - elif 语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">v = zeros(10,1)</span><br><span class="line">if v(1) == 1,</span><br><span class="line">disp('1');</span><br><span class="line">elseif v(1) == 2,</span><br><span class="line">disp('2');</span><br><span class="line">else</span><br><span class="line">disp('3');</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><h4 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h4><p>1.先创建一个文件<br>​    squarethisnumber.m   # .m前定义的就是函数名<br>2.编写函数文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function y = squareThisNumber(x)</span><br><span class="line">y = x^2;</span><br></pre></td></tr></table></figure><blockquote><p>第一行写着 function y = squareThisNumber(x) ，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 里。另外，它告诉了Octave这个函数有一个参数，就是参数 ，还有定义的函数体，y  = x^2</p></blockquote><h4 id="使用自定义函数"><a href="#使用自定义函数" class="headerlink" title="使用自定义函数"></a>使用自定义函数</h4><p>1.切换到函数文件所在目录<br>2.直接通过函数名squareThisNumber() 调用函数</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> octave </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>线型回归 &amp; 梯度下降</title>
      <link href="/2018/10/20/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/10/20/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h2 id="1-模型表示"><a href="#1-模型表示" class="headerlink" title="1.模型表示"></a>1.模型表示</h2><h3 id="问题的概述"><a href="#问题的概述" class="headerlink" title="问题的概述"></a><strong>问题的概述</strong></h3><blockquote><p>在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。</p></blockquote><a id="more"></a><p><img src="/img/1539965168129.png" alt="1539965168129"></p><h3 id="模型引入"><a href="#模型引入" class="headerlink" title="模型引入"></a><strong>模型引入</strong></h3><p>假使我们回归问题的训练集（Training Set）如下表所示：</p><p><img src="/img/1539965299259.png" alt="1539965299259"></p><blockquote><p>我们将要用来描述这个回归问题的标记如下:<br>m 代表训练集中实例的数量<br>x 代表特征/输入变量<br>y 代表目标变量/输出变量<br>(x, y) 代表训练集中的实例<br>(x^i, y^i)代 表第 个观察实例<br>h 代表学习算法的解决方案或函数也称为假设（hypothesis）</p></blockquote><p><img src="/img/1539965543172.png" alt="1539965543172"></p><blockquote><p>这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。 h代表hypothesis(假设)， 表h示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h根据输入的 值来得出 y 值， y 值对应房子的价格因此， h 是一个从 x 到 y 的函数映射。</p><p>我将选择最初的使用规则代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 h？一种可能的表达方式为：<strong>hθ(x)=θ0+θ1∗x</strong> ，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p></blockquote><h2 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2.代价函数"></a>2.代价函数</h2><h3 id="什么是代价函数"><a href="#什么是代价函数" class="headerlink" title="什么是代价函数"></a>什么是代价函数</h3><p><img src="/img/1539965938888.png" alt="1539965938888"></p><p>在线性回归中我们有一个像这样的训练集， m代表了训练样本的数量 。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：<strong>hθ(x)=θ0+θ1∗x</strong>  。<br>我们通过训练获得的预测函数在预测时必然会和实际值有一些偏差，下面就给出了这部分预测偏差的定义</p><blockquote><p><strong>代价函数的定义</strong>: 我们选择的参数决定了我们得到的模型相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是<strong>建模误差（modeling error）</strong>,下图蓝色线段变为预测和实际的误差。</p><p><strong>平方误差代价函数</strong>: 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。</p></blockquote><p><img src="/img/1540021643524.png" alt="1540021643524"></p><h3 id="怎么优化线型回归模型"><a href="#怎么优化线型回归模型" class="headerlink" title="怎么优化线型回归模型"></a>怎么优化线型回归模型</h3><blockquote><p>优化回归模型的目标: 便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数最小。</p></blockquote><blockquote><p>代价函数公式:</p><p><img src="/img/1540029941521.png" alt="1540029941521"></p></blockquote><h3 id="代价函数坐标图"><a href="#代价函数坐标图" class="headerlink" title="代价函数坐标图"></a>代价函数坐标图</h3><p><img src="/img/1540022174651.png" alt="1540022174651"></p><blockquote><p>可以看出在三维空间中存在一个使得J(θ_0,θ_1)代价函数最小的点， 当代价函数取得最小值的时候，我们会获得最符合训练集的预测模型。</p></blockquote><h2 id="3-梯度下降算法"><a href="#3-梯度下降算法" class="headerlink" title="3.梯度下降算法"></a>3.梯度下降算法</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><blockquote><p>梯度下降是一个用来求函数最小值的算法。其思想是利用迭代的思想来逼近最低点，沿着梯度的负方向更新模型权重。我们将使用梯度下降算法来求出代价函数 J(θ_0,θ_1) 的最小值。</p></blockquote><blockquote><p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0, θ1, …., θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（globalminimum），选择不同的初始参数组合，可能会找到不同的<strong>局部最小值</strong>。</p></blockquote><p><img src="/img/1540024168426.png" alt="1540024168426"></p><h3 id="梯度下降算法公式"><a href="#梯度下降算法公式" class="headerlink" title="梯度下降算法公式:"></a>梯度下降算法公式:</h3><p><strong>公式介绍</strong></p><p>repeat until convergence{<br>$$<br>θ_j=θ_j−α∂/∂θ_j J(θ0,θ1)　(for　j=0　and　j=1)<br>$$<br>}</p><p><img src="/img/1540026766508.png" alt="1540026766508"></p><blockquote><p>参数α: 学习率, 它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。当到达损失函数的最低点的时候，其导数为0，已经到达了局部最优点，即θ不会再发生改变</p></blockquote><blockquote><p>注意:  在梯度下降算法中，一般都会采用同步更新的方式来优化模型, 即上面公式的θj同步更新。同步更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。</p></blockquote><p><strong>学习率的选择对算法的影响</strong></p><ul><li><p>学习率过小的影响: 则达到收敛所需的迭代次数会非常高</p></li><li><p>学习率过大的影响: 每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛</p></li></ul><p><strong>怎么确定模型是否收敛</strong></p><p><img src="/img/1540568088758.png" alt="1540568088758"></p><ul><li><p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛, 如上图在300步之后基本接近收敛。</p></li><li><p>也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较</p></li></ul><h3 id="梯度下降算法分类"><a href="#梯度下降算法分类" class="headerlink" title="梯度下降算法分类"></a>梯度下降算法分类</h3><h4 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a><strong>批量梯度下降</strong></h4><blockquote><p>在上面的线性回归场景中的梯度下降算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。</p></blockquote><blockquote><p>批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。</p></blockquote><p><strong>优点：</strong><br>  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。<br>  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。</p><p><strong>缺点：</strong><br>  （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。</p><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a><strong>随机梯度下降</strong></h4><p>公式: <img src="/img/1540042706219.png" alt="1540042706219"></p><blockquote><p>随机梯度下降指在梯度下降的每一步中只用到了训练集的一个样本或者一部分样本，最后得到的可能是全局最优解，也可能是局部最优解。</p></blockquote><p><strong>优点：</strong><br>  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。<br><strong>缺点：</strong><br>  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。<br>  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。<br>  （3）不易于并行实现。</p><h2 id="4-梯度下降线型回归模型"><a href="#4-梯度下降线型回归模型" class="headerlink" title="4.梯度下降线型回归模型"></a>4.梯度下降线型回归模型</h2><h3 id="单变量线型回归梯度下降"><a href="#单变量线型回归梯度下降" class="headerlink" title="单变量线型回归梯度下降"></a>单变量线型回归梯度下降</h3><h4 id="梯度下降、线型回归算法比较"><a href="#梯度下降、线型回归算法比较" class="headerlink" title="梯度下降、线型回归算法比较"></a><strong>梯度下降、线型回归算法比较</strong></h4><p><img src="/img/1540028773553.png" alt="1540028773553"></p><h4 id="单变量梯度下降公式"><a href="#单变量梯度下降公式" class="headerlink" title="单变量梯度下降公式"></a>单变量梯度下降公式</h4><p><strong>代价函数计算</strong></p><p><img src="/img/1540028837422.png" alt="1540028837422"></p><p><strong>参数θ的计算</strong></p><p><img src="/img/1540028905489.png" alt="1540028905489"></p><h3 id="多变量线型回归梯度下降"><a href="#多变量线型回归梯度下降" class="headerlink" title="多变量线型回归梯度下降"></a>多变量线型回归梯度下降</h3><h4 id="多变量特征"><a href="#多变量特征" class="headerlink" title="多变量特征"></a>多变量特征</h4><p>现在对回归模型增加更多的特征，例如房价预测问题，增加房间数楼层等，构成一个含有多个变量的模型，模型中的特征如下图。</p><p><img src="/img/1540563412916.png" alt="1540563412916"></p><p><strong>增添更多特征后，引入一系列新的注释</strong>：</p><blockquote><ul><li><p>n 代表特征的数量</p></li><li><p>x^(i)^ 代表第i个训练实例，是特征矩阵的第i行，是一个向量(vector).</p></li><li><p>x~j~^(i)^ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征  如:  $x_2^{(2)} = 3, x_3^{(2)} = 2$</p></li><li><p>支持多个变量的h表示为: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$</p><ul><li>这个公式中有个n+1参数和n个变量，为了使得公式能够简化一些，引入$x_0=1$，则公式转化为:$h_θ(x) = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$</li></ul></li><li><p>此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m* (n+1),</p><p>因此变量的假设公式可以表示为: $h_θ(x) = θ^TX$</p></li></ul></blockquote><h4 id="多变量梯度下降公式"><a href="#多变量梯度下降公式" class="headerlink" title="多变量梯度下降公式"></a>多变量梯度下降公式</h4><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：</p><p>$J_{(θ_0, .., θ<em>n)} = {1\over2m}\sum</em>{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 , 其中: h_θ(x) = θ^TX = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$<img src="/img/1540635502085.png" alt="1540635502085"></p><p>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度<br><strong>梯度下降下降公式：</strong></p><p><img src="/img/1540566461561.png" alt="1540566461561"></p><p><strong>求导数后得到:</strong></p><p><img src="/img/1540566614915.png" alt="1540566614915"></p><h3 id="5-特征和多项式回归"><a href="#5-特征和多项式回归" class="headerlink" title="5.特征和多项式回归"></a>5.特征和多项式回归</h3><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a><strong>特征选择</strong></h3><blockquote><p>有的时候的们的特征并不是很好的可以直接用来训练模型，比如房子价格预测模型的两个特征</p><p>特征: x1 房子的临街宽度， x2 房子的纵向深度</p><p>此时很明显房子的价格用 特征一和特征二的乘积，即房子的占地面积来预测更合适  $x=x_1 * x_2 = area (面积)$</p></blockquote><p><img src="/img/1540616820528.png" alt="1540616820528"></p><h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a><strong>多项式回归</strong></h3><blockquote><p>很多时候，我们并没有那么幸运，正好所有的数据之间都是线性关系，能用一条直线，一个平面就描述地出来，大多数时候，我们可能需要画成曲线、曲面或者曲啥更高维度的东西</p><p>比如下图数据集，线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，用二次方程（蓝色的线）和三次方程（绿色的线）来拟合这些点，但是二次方程有个缺点，到了后面，反而会下降，这和我们的认知不符合，而三次方程不会遇到这个问题，和二次方程相加又会和上升的部分相互抵消一部分，所以最后选择了三次方程加上二次方程。</p><p>二次方模型公式：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2$      三次方模型公式: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3$ </p></blockquote><p><img src="/img/1540616493267.png" alt="1540616493267"></p><blockquote><p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以把多项式回归看成是另类的多变量线性回归，我们可以令：$x_2 = x_2^2$ , $x_3 = x_3^3$ , 从而将模型转化为线型回归模型</p><p>注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要, 因为比如x的范围是1-100的话，$x^2$的范围就是1-10000，$x^3$的范围就是1-1000000。</p></blockquote><h2 id="6-特征缩放"><a href="#6-特征缩放" class="headerlink" title="6.特征缩放"></a>6.特征缩放</h2><h3 id="为什么要特征缩放"><a href="#为什么要特征缩放" class="headerlink" title="为什么要特征缩放"></a>为什么要特征缩放</h3><blockquote><p>在使用梯度下降的时候会遇到因为特征大小不统一，导致收敛速度过慢的问题，在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p></blockquote><p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p><p><img src="/img/1540567672941.png" alt="1540567672941"></p><blockquote><p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p></blockquote><h3 id="特征缩放的两种方法"><a href="#特征缩放的两种方法" class="headerlink" title="特征缩放的两种方法"></a>特征缩放的两种方法</h3><p><strong>线型归一化</strong></p><ul><li>原理： <ul><li>通过对原始数据进行变换把数据映射到(默认为[0,1])之间</li></ul></li><li>公式<ul><li><img src="/img/1540644955305.png" alt="1540644955305"></li></ul></li><li>归一化的弊端<ul><li>使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景</li></ul></li></ul><p><strong>特征标准化</strong></p><ul><li><p>原理：</p><ul><li>通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内</li></ul></li><li><p>公式：</p><ul><li><img src="/img/1540645069496.png" alt="1540645069496"></li></ul></li><li>标准化的有点<ul><li>如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。</li></ul></li></ul><h2 id="7-正规方程线型回归"><a href="#7-正规方程线型回归" class="headerlink" title="7.正规方程线型回归"></a>7.正规方程线型回归</h2><h3 id="正规方程算法介绍"><a href="#正规方程算法介绍" class="headerlink" title="正规方程算法介绍"></a>正规方程算法介绍</h3><blockquote><p>对于线型回归问题经常都是在使用梯度下降算法，但是对于某些线型回归问题，正规方程是更好的解决办法。如下面的代价函数:</p></blockquote><p><img src="/img/1540619927547.png" alt="1540619927547"></p><p><img src="/img/1540620250525.png" alt="1540620250525"></p><h3 id="梯度下降和正规方程比较"><a href="#梯度下降和正规方程比较" class="headerlink" title="梯度下降和正规方程比较"></a>梯度下降和正规方程比较</h3><table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择学习率</td><td>不需要</td></tr><tr><td>需要多次迭代</td><td>一次运算得出</td></tr><tr><td>当特征数量n特别大时能比较适用</td><td>需要计算$(X^TX)^{-1}$如果特征数量n较大则运算代价大因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受</td></tr><tr><td>适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等其他模型</td></tr></tbody></table><blockquote><p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习介绍</title>
      <link href="/2018/10/19/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/10/19/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习的发展"><a href="#机器学习的发展" class="headerlink" title="机器学习的发展"></a>机器学习的发展</h3><blockquote><p>机器学习被应用到越来越多的领域这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。</p><p>再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。</p><p>手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种<br>学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。<br>事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处<br>理和大部分的计算机视觉，都应用了机器学习。学习算法还广泛用于自定制程序。每次你去亚马逊或Net</p></blockquote><a id="more"></a><h3 id="一个比较好的机器学习定义"><a href="#一个比较好的机器学习定义" class="headerlink" title="一个比较好的机器学习定义"></a>一个比较好的机器学习定义</h3><blockquote><p>一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升</p><p>类比于下棋，经验E 就是程序上万次的自我练习的经验而任务T 就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。</p></blockquote><h3 id="机器学习基本算法"><a href="#机器学习基本算法" class="headerlink" title="机器学习基本算法"></a>机器学习基本算法</h3><h4 id="最常用的两个算法"><a href="#最常用的两个算法" class="headerlink" title="最常用的两个算法"></a>最常用的两个算法</h4><p><strong>监督学习算法</strong></p><blockquote><p>我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，即我们可以根据正确答案来不断的优化我们的模型。</p><p>监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。</p></blockquote><p><strong>无监督学习算法</strong></p><blockquote><p>根据未知的训练样本(未被标记分类), 从数据中找出不同的结构，将数据识别为不同的聚集簇。</p><p>无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？</p><p>还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。</p></blockquote><h4 id="监督学习算法常见问题"><a href="#监督学习算法常见问题" class="headerlink" title="监督学习算法常见问题"></a>监督学习算法常见问题</h4><p><strong>回归问题</strong></p><blockquote><p>通过像预测房子的价格一样，通过回归模型通过输入值来推出一个连续的输出值</p></blockquote><p> <strong>分类问题</strong></p><blockquote><p>通过数学模型来判断输入数据所属的类别，可以是二类别问题（是/不是），也可以是多类别问题（在多个类别中判断输入数据具体属于哪一个类别）</p></blockquote><h4 id="无监督学习常年问题"><a href="#无监督学习常年问题" class="headerlink" title="无监督学习常年问题"></a>无监督学习常年问题</h4><p><strong>聚类问题</strong></p><blockquote><p>依据研究对象（样品或指标）的特征，将其分为不同的分类。</p></blockquote><p>### </p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建Registry 私有库</title>
      <link href="/2018/10/17/11.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/Docker/%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93/"/>
      <url>/2018/10/17/11.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/Docker/%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h3 id="1-关于Registry仓库"><a href="#1-关于Registry仓库" class="headerlink" title="1.关于Registry仓库"></a><strong>1.关于Registry仓库</strong></h3><p>官方的<a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker hub</a>是一个用于管理公共镜像的好地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景需要我们拥有一个私有的镜像仓库用于管理我们自己的镜像。这个可以通过开源软件Registry来达成目的。</p><p> Registry在github上有两份代码：<a href="https://github.com/docker/docker-registry" target="_blank" rel="noopener">老代码库</a>和<a href="https://github.com/docker/distribution" target="_blank" rel="noopener">新代码库</a>。老代码是采用python编写的，存在pull和push的性能问题，出到0.9.1版本之后就标志为deprecated，不再继续开发。从2.0版本开始就到在新代码库进行开发，新代码库是采用go语言编写，修改了镜像id的生成算法、registry上镜像的保存结构，大大优化了pull和push镜像的效率。</p><p> 官方在Docker hub上提供了registry的镜像（<a href="https://hub.docker.com/_/registry/" target="_blank" rel="noopener">详情</a>），我们可以直接使用该registry镜像来构建一个容器，搭建我们自己的私有仓库服务。Tag为latest的registry镜像是0.9.1版本的，我们直接采用2.1.1版本。</p><a id="more"></a><h3 id="2-Registry的部署"><a href="#2-Registry的部署" class="headerlink" title="2.Registry的部署"></a>2.Registry的部署</h3><p><strong>运行下面命令获取registry镜像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker pull registry:2.1.1</span><br></pre></td></tr></table></figure><p><strong>然后启动一个容器</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker run -d -v /opt/data/registry/:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1</span><br></pre></td></tr></table></figure><p><strong>验证服务是否启动成功</strong></p><p>说明我们已经启动了registry服务，打开浏览器输入<a href="http://127.0.0.1:5000/v2" target="_blank" rel="noopener">http://127.0.0.1:5000/v2</a></p><h3 id="3-验证"><a href="#3-验证" class="headerlink" title="3.验证"></a>3.验证</h3><p><strong>向仓库中push镜像</strong></p><p>现在我们通过将镜像push到registry来验证一下。我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker tag hello-world 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><p>接下来，我们运行docker push将hello-world镜像push到我们的私有仓库中，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker push 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">The push refers to a repository [127.0.0.1:5000/hello-world] (len: 1)</span><br><span class="line"></span><br><span class="line">975b84d108f1: Image successfully pushed</span><br><span class="line"></span><br><span class="line">3f12c794407e: Image successfully pushed</span><br><span class="line"></span><br><span class="line">latest: digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b size: 2744</span><br></pre></td></tr></table></figure><p>现在我们可以查看我们本地/opt/registry目录下已经有了刚推送上来的hello-world。我们也在浏览器中输入<a href="http://127.0.0.1:5000/v2/_catalog，如下图所示，" target="_blank" rel="noopener">http://127.0.0.1:5000/v2/_catalog，如下图所示，</a></p><p><strong>从镜像库中拉取镜像</strong></p><p>现在我们可以先将我们本地的127.0.0.1:5000/hello-world和hello-world先删除掉，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker rmi hello-world</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo docker rmi 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><p>然后使用docker pull从我们的私有仓库中获取hello-world镜像，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker pull 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Using default tag: latest</span><br><span class="line"></span><br><span class="line">latest: Pulling from hello-world</span><br><span class="line"></span><br><span class="line">b901d36b6f2f: Pull complete</span><br><span class="line"></span><br><span class="line">0a6ba66e537a: Pull complete</span><br><span class="line"></span><br><span class="line">Digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b</span><br><span class="line"></span><br><span class="line">Status: Downloaded newer image for 127.0.0.1:5000/hello-world:latest</span><br><span class="line"></span><br><span class="line">lienhua34@lienhua34-Compaq-Presario-CQ35-Notebook-PC ~ $ sudo docker images</span><br><span class="line"></span><br><span class="line">REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE</span><br><span class="line"></span><br><span class="line">registry 2.1.1 b91f745cd233 5 days ago 220.1 MB</span><br><span class="line"></span><br><span class="line">ubuntu 14.04 a5a467fddcb8 6 days ago 187.9 MB</span><br><span class="line"></span><br><span class="line">127.0.0.1:5000/hello-world latest 0a6ba66e537a 2 weeks ago 960 B</span><br></pre></td></tr></table></figure><h3 id="4-查询镜像库"><a href="#4-查询镜像库" class="headerlink" title="4.查询镜像库"></a>4.查询镜像库</h3><p><strong>查询镜像库中的镜像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://10.0.110.218:5000/v2/_catalog</span><br></pre></td></tr></table></figure><h3 id="5-错误排查"><a href="#5-错误排查" class="headerlink" title="5.错误排查"></a>5.错误排查</h3><p><strong>错误描述</strong></p><p>在push 到docker registry时，可能会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The push refers to a repository [192.168.1.100:5000/registry]</span><br><span class="line"></span><br><span class="line">Get https://192.168.1.100:5000/v1/_ping: http: server gave HTTP response to HTTPS client</span><br></pre></td></tr></table></figure><p>这个问题可能是由于客户端采用https，docker registry未采用https服务所致。一种处理方式是把客户对地址“192.168.1.100:5000”请求改为http。</p><p>目前很多文章都是通过修改docker的配置文件“etc/systemconfig/docker”，重启docker来解决这个问题。但发现docker1.12.3版本并无此文件，根据网上创建此文件，并填入相应内容，重启docker无效果，仍然报此错误。</p><p> <strong>解决办法</strong></p><p>在”/etc/docker/“目录下，创建”daemon.json“文件。在文件中写入：</p><p>{ “insecure-registries”:[“192.168.1.100:5000”] }</p><p>保存退出后，重启docker。</p>]]></content>
      
      
      <categories>
          
          <category> Docker容器技术 </category>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>githubpage + hexo + yilia 搭建个人博客</title>
      <link href="/2018/10/17/20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/%E5%8D%9A%E5%AE%A2/github+hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/10/17/20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/%E5%8D%9A%E5%AE%A2/github+hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h4 id="0-本博客的由来"><a href="#0-本博客的由来" class="headerlink" title="0.本博客的由来"></a>0.本博客的由来</h4><p>本来感觉写博客很费时间，但是最近感觉这两年攒手里的笔记太多了，不方便整理和分享</p><p>所以打算以后就干脆直接将笔记整理到github，这样也比自己维护一个博客省心</p><p>下面就将搭建本博客的步骤也同步上来，当做一个hello-world吧。</p><a id="more"></a><h4 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1.环境准备"></a>1.环境准备</h4><p>电脑环境是Windows，安装好git后，所有搭建操作均在git bash内完成</p><p><strong>1）安装hexo(首先要安装git, node.js, npm)</strong></p><p>注意：首次安装git 要配置user信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>git config --global user.name "yourname"   #（yourname是git的用户名）</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>git config --global user.email email）</span><br></pre></td></tr></table></figure><p><strong>2）使用npm安装hexo</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>npm install -g hexo</span><br></pre></td></tr></table></figure><p><strong>3）创建hexo文件夹</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>mkdir hexo_blog</span><br><span class="line"><span class="meta">$</span>cd hexo_lobg</span><br></pre></td></tr></table></figure><p><strong>4）初始化框架</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>hexo init #hexo   #会自动创建网站所需要的文件</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>npm install    #安装依赖包</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>hexo generate </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>hexo server   #现在可以用127.0.0.1:4000访问hexo默认的hello world界面 ,hexo s = hexo server</span><br></pre></td></tr></table></figure><h4 id="2-部署到github"><a href="#2-部署到github" class="headerlink" title="2.部署到github"></a>2.部署到github</h4><p><strong>1）首次使用github需要配置密钥</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C "email"</span><br></pre></td></tr></table></figure><p>生成ssh密钥，按三次回车键，密码为空,这边会生成id_rsa和_rsa.pub文件</p><p>打开id_rsa.pub，复制全文添加到GitHub 的Add SSH key中。</p><p><strong>2）创建Respository， 并开启githubPage</strong></p><p>首先注册登录github,然后创建页面仓库，Repository name 命名应该是 youname.github.io </p><p>在setting界面， 配置</p><p><img src="/img/1539839479905-1539839725571.png" alt="1539839479905-1539839725571"></p><p><strong>3）安装hexo-deployer-git</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>npm install hexo-deployer-git --save     用来推送项目到github</span><br></pre></td></tr></table></figure><p><strong>4）生成博客，并push到github</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>hexo generate</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>hexo deploy</span><br></pre></td></tr></table></figure><p><strong>5）验证结果</strong></p><p>通过<a href="https://youname.github.io" target="_blank" rel="noopener">https://youname.github.io</a> 进行访问</p><h4 id="3-更换博客模板"><a href="#3-更换博客模板" class="headerlink" title="3.更换博客模板"></a>3.更换博客模板</h4><p>目前访问的博客模板比较简略，下面介绍使用：yilia模板</p><p><strong>1）拉取模板文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p><strong>2）更改配置文件修改模板为yilia</strong></p><p>打开项目目录下的_config.yml文件，更改主题theme;   <code>theme: yilia</code><br>然后配置yilia文件下的_config.yml（目录：<code>hexo/themes/yilia/_config.yml</code>） 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"># Header</span><br><span class="line">menu:</span><br><span class="line">  主页: /</span><br><span class="line">  归档: /archives</span><br><span class="line">  #分类: /categories</span><br><span class="line">  #标签: /tags</span><br><span class="line"></span><br><span class="line"># SubNav</span><br><span class="line">subnav:</span><br><span class="line">  github: &quot;https://github.com/KyleAdultHub&quot;</span><br><span class="line">  #weibo: &quot;#&quot;</span><br><span class="line">  #rss: &quot;#&quot;</span><br><span class="line">  #zhihu: &quot;#&quot;</span><br><span class="line">  qq: &quot;/information&quot;</span><br><span class="line">  #weixin: &quot;#&quot;</span><br><span class="line">  #jianshu: &quot;#&quot;</span><br><span class="line">  #douban: &quot;#&quot;</span><br><span class="line">  #segmentfault: &quot;#&quot;</span><br><span class="line">  #bilibili: &quot;#&quot;</span><br><span class="line">  #acfun: &quot;#&quot;</span><br><span class="line">  mail: &quot;/information&quot;</span><br><span class="line">  #facebook: &quot;#&quot;</span><br><span class="line">  #google: &quot;#&quot;</span><br><span class="line">  #twitter: &quot;#&quot;</span><br><span class="line">  #linkedin: &quot;#&quot;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">rss: /atom.xml</span><br><span class="line"></span><br><span class="line"># 是否需要修改 root 路径</span><br><span class="line"># 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，</span><br><span class="line"># 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。</span><br><span class="line">root: /</span><br><span class="line"></span><br><span class="line"># Content</span><br><span class="line"># 文章太长，截断按钮文字</span><br><span class="line">excerpt_link: more</span><br><span class="line"># 文章卡片右下角常驻链接，不需要请设置为false</span><br><span class="line">show_all_link: &apos;展开全文&apos;</span><br><span class="line"># 数学公式</span><br><span class="line">mathjax: false</span><br><span class="line"># 是否在新窗口打开链接</span><br><span class="line">open_in_new: false</span><br><span class="line"></span><br><span class="line"># 打赏</span><br><span class="line"># 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏</span><br><span class="line">reward_type: 0</span><br><span class="line"># 打赏wording</span><br><span class="line">reward_wording: &apos;谢谢你&apos;</span><br><span class="line"># 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg</span><br><span class="line">alipay: </span><br><span class="line"># 微信二维码图片地址</span><br><span class="line">weixin: </span><br><span class="line"></span><br><span class="line"># 目录</span><br><span class="line"># 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录</span><br><span class="line">toc: 1</span><br><span class="line"># 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为false</span><br><span class="line">toc_hide_index: true</span><br><span class="line"># 目录为空时的提示</span><br><span class="line">toc_empty_wording: &apos;目录，不存在的…&apos;</span><br><span class="line"></span><br><span class="line"># 是否有快速回到顶部的按钮</span><br><span class="line">top: true</span><br><span class="line"></span><br><span class="line"># Miscellaneous</span><br><span class="line">baidu_analytics: &apos;&apos;</span><br><span class="line">google_analytics: &apos;&apos;</span><br><span class="line">favicon: /favicon.png</span><br><span class="line"></span><br><span class="line">#你的头像url</span><br><span class="line">avatar: /img/header.jpg</span><br><span class="line"></span><br><span class="line">#是否开启分享</span><br><span class="line">share_jia: true</span><br><span class="line"></span><br><span class="line">#评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment</span><br><span class="line">#不需要使用某项，直接设置值为false，或注释掉</span><br><span class="line">#具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/</span><br><span class="line"></span><br><span class="line">#1、多说</span><br><span class="line">duoshuo: false</span><br><span class="line"></span><br><span class="line">#2、网易云跟帖</span><br><span class="line">wangyiyun: false</span><br><span class="line"></span><br><span class="line">#3、畅言</span><br><span class="line">changyan_appid: *** #这个畅言id和conf写自己的</span><br><span class="line">changyan_conf: ***</span><br><span class="line"></span><br><span class="line">#4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的</span><br><span class="line">disqus: false</span><br><span class="line"></span><br><span class="line">#5、Gitment</span><br><span class="line">gitment_owner: false      #你的 GitHub ID</span><br><span class="line">gitment_repo: &apos;&apos;          #存储评论的 repo</span><br><span class="line">gitment_oauth:</span><br><span class="line">  client_id: &apos;&apos;           #client ID</span><br><span class="line">  client_secret: &apos;&apos;       #client secret</span><br><span class="line"></span><br><span class="line"># 样式定制 - 一般不需要修改，除非有很强的定制欲望…</span><br><span class="line">style:</span><br><span class="line">  # 头像上面的背景颜色</span><br><span class="line">  header: &apos;#4d4d4d&apos;</span><br><span class="line">  # 右滑板块背景</span><br><span class="line">  slider: &apos;linear-gradient(200deg,#a0cfe4,#e8c37e)&apos;</span><br><span class="line"></span><br><span class="line"># slider的设置</span><br><span class="line">slider:</span><br><span class="line">  # 是否默认展开tags板块</span><br><span class="line">  showTags: false</span><br><span class="line"></span><br><span class="line"># 智能菜单</span><br><span class="line"># 如不需要，将该对应项置为false</span><br><span class="line"># 比如</span><br><span class="line">#smart_menu:</span><br><span class="line">#  friends: false</span><br><span class="line">smart_menu:</span><br><span class="line">  innerArchive: &apos;所有文章&apos;</span><br><span class="line">  friends: &apos;友链&apos;</span><br><span class="line">  aboutme: &apos;关于我&apos;</span><br><span class="line"></span><br><span class="line">friends:</span><br><span class="line">  #友情链接1: http://localhost:4000/</span><br><span class="line">  </span><br><span class="line">aboutme: </span><br><span class="line">  程序猿一枚&lt;br&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>正则表达式</title>
      <link href="/2018/01/12/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2018/01/12/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="正则表达式介绍"><a href="#正则表达式介绍" class="headerlink" title="正则表达式介绍"></a>正则表达式介绍</h3><p><strong>什么是正则表达式</strong></p><p>Regular Expression, 又称规则表达式。<br>正则表达式就是用事先定义好的一些特定字符（组合），组成一个“规则字符串”，这个“规则字符串”用来描述一种字符串的匹配模式（pattern）；</p><p><strong>正则的作用</strong></p><p>可以用来检查一个字串是否包含某种子串、将匹配的子串替换或者取出</p><p><strong>正则的特点</strong></p><p>灵活性、逻辑性和功能性非常强大</p><a id="more"></a><h3 id="正则表达式的使用方法"><a href="#正则表达式的使用方法" class="headerlink" title="正则表达式的使用方法"></a>正则表达式的使用方法</h3><h4 id="re模块常用函数和方法"><a href="#re模块常用函数和方法" class="headerlink" title="re模块常用函数和方法"></a>re模块常用函数和方法</h4><ul><li>import  re</li><li>result_obj = re.search(正则表达式， 数据，flag=0）     —-查找数据中第一个符合匹配规则的字符串<ul><li>search()函数从数据中只能查找到第一个符合正则数据放到result_obj中，  如果没有匹配到想要匹配的结果会返回None</li></ul></li><li>result_obj.group()      —-查看正则匹配的结果内容<ul><li>result_obj.group(1， 2/ 组名) 返回需要组的匹配的结果，返回一个包含多个组匹配结果的元组；    result_obj.group() == result_obj.group(0) == 正个正则表达式所有匹配的字符</li></ul></li></ul><h4 id="re模块其他常用方法"><a href="#re模块其他常用方法" class="headerlink" title="re模块其他常用方法"></a>re模块其他常用方法</h4><p><img src="/img/1541300873462.png" alt="1541300873462"></p><h4 id="compile-编译"><a href="#compile-编译" class="headerlink" title="compile 编译"></a>compile 编译</h4><ul><li><p>作用</p><ul><li>对正则表达式匹配规则进行预编译，在大量使用到正则的时候，可以提高匹配的速度</li></ul></li><li><p>使用方法</p><ul><li>p = re.compile(‘匹配规则’,  re.DATALL)</li><li>p.search(‘字符串’)       按照编译的规则对字符串进行匹配正则表达式中的特殊字符<ul><li>匹配单个字符</li></ul></li></ul></li></ul><h3 id="正则表达式常用匹配方式"><a href="#正则表达式常用匹配方式" class="headerlink" title="正则表达式常用匹配方式"></a>正则表达式常用匹配方式</h3><h4 id="匹配单个字符"><a href="#匹配单个字符" class="headerlink" title="匹配单个字符"></a>匹配单个字符</h4><p><img src="/img/1541301141193.png" alt="1541301141193"></p><ul><li>空白字符\s == [ \f\n\r\t\v]   非空白字符 \S == [^\f\t\v\n\r]   </li><li>在正则表达式中若只是想要匹配一个像特殊字符的普通字符需要在特殊字符前面加转义字符“\” 例如“.”</li><li>特殊字符在[ ]中例如：[.    |   * + ？等 ]没有特殊功能只代表普通字符</li><li>在[ ]中若是想使用“-”普通字符要加上转义字符\</li></ul><h4 id="匹配多个字符"><a href="#匹配多个字符" class="headerlink" title="匹配多个字符"></a>匹配多个字符</h4><p><img src="/img/1541301215631.png" alt="1541301215631"></p><h4 id="常用定位符"><a href="#常用定位符" class="headerlink" title="常用定位符"></a>常用定位符</h4><p><img src="/img/1541301242792.png" alt="1541301242792"></p><h4 id="正则表达式的分组"><a href="#正则表达式的分组" class="headerlink" title="正则表达式的分组"></a>正则表达式的分组</h4><p><img src="/img/1541301315452.png" alt="1541301315452"></p><ul><li>注意: <ul><li>在使用（|）的时候尽量使特殊的或者通用的变量放在前边</li><li>在引用分组的时候注意：\num表示八进制数num所表示的普通ASCII码字符，所以在引用的时候会默认表示ascii码字符，所以要注意转义或者使用原生字符串</li></ul></li></ul><h4 id="匹配所有的汉字的方法"><a href="#匹配所有的汉字的方法" class="headerlink" title="匹配所有的汉字的方法"></a>匹配所有的汉字的方法</h4><ul><li>re.compole(r’[\u4e00-\u9fa5]’)<br>  备注：匹配所有unicode编码的中文</li></ul><h3 id="正则表达式的贪婪与懒惰"><a href="#正则表达式的贪婪与懒惰" class="headerlink" title="正则表达式的贪婪与懒惰"></a>正则表达式的贪婪与懒惰</h3><h4 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h4><ul><li>贪婪-尽可能多的匹配</li><li>懒惰-尽可能少的匹配<h4 id="默认为贪婪模式的匹配模式"><a href="#默认为贪婪模式的匹配模式" class="headerlink" title="默认为贪婪模式的匹配模式"></a>默认为贪婪模式的匹配模式</h4></li><li>在python中 +/*/{m,n}默认情况下总是贪婪的<h4 id="如何让贪婪模式变为懒惰模式"><a href="#如何让贪婪模式变为懒惰模式" class="headerlink" title="如何让贪婪模式变为懒惰模式"></a>如何让贪婪模式变为懒惰模式</h4></li><li>在量词后加上一个?</li><li>例子：<br><img src="/img/1541301444152.png" alt="1541301444152"></li></ul><h3 id="原生字符串的应用"><a href="#原生字符串的应用" class="headerlink" title="原生字符串的应用"></a>原生字符串的应用</h3><h4 id="特殊字符的转义"><a href="#特殊字符的转义" class="headerlink" title="特殊字符的转义"></a>特殊字符的转义</h4><p>在表达式中如果包含“\”表示转义\后面的字符为八进制数字代表的ascii对应的特殊字符，在python中会对ascii包含的数字或者字符进行转义，这种情况会导致会将匹配规则的字符进行转义，结果不能匹配到想要匹配的字符串内容</p><h4 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h4><ul><li>取消转义<br>  在每一个’\’字符前加上’\’，对”\’进行转义，这样会取消\的转义功能，将\只代表一个\字符，不会对后边的字符进行转义<br>  ascii不包括的字符，如果前边有转义字符\，不需要加以转义，python会自动转义</li><li><p>原生字符串<br>  如果在表达式或字符串前边加上r“”对字符串中的\字符自动转义<br>  在使用的时候，匹配规则可以和想要匹配的内容写法相同，r会自动帮我们转义<br>  示例： re.search(r’abc\nabc’, ‘abc\nabc’)</p><h3 id="正则表达式的常见问题"><a href="#正则表达式的常见问题" class="headerlink" title="正则表达式的常见问题"></a>正则表达式的常见问题</h3><blockquote><p>如何让 . 特殊符号可以匹配所有内容（包括\n）</p></blockquote></li><li><p>解决办法：<br>  使用re.DOTALL 参数</p></li><li>示例：<br>  re.findall(r’abc.’,  ‘abc\n\nsfgs’, re.DOTALL)<br>  备注：也可以使用re.S 代替re.DOTALL 效果上是一样的<h3 id="Ascii码对应关系"><a href="#Ascii码对应关系" class="headerlink" title="Ascii码对应关系"></a>Ascii码对应关系</h3></li><li><p>在字符串，或者正则表达式中，\n\t等控制字符 或者 \数字（表示八进制的num所表示的普通ascii码）等显示字符，在应用的时候会默认为在调用ascii码的控制字符或者是显示字符，所以如果只是想表达单纯的字符 需要用\n或者r””这种形式进行转意，可以使用chr（八进制数）来查询对应的ascii字符</p></li><li><p><strong>ascii码表</strong></p><p><img src="/img/1541301800714.png" alt="1541301800714"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> python爬虫 </category>
          
          <category> 爬虫基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
