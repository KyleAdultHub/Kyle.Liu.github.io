<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2018%2F11%2F30%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[1.支持向量机优化目标与逻辑回归和神经网络相比，支持向量机，或者简称SVM，在训练复杂的非线性方程时提供了一种更为清晰，更加强大的方式。 在逻辑回归中我们已经熟悉了这里的假设函数形式，和下边的S型激活函数。下面依然用z表示$\theta^Tx$ 现在考虑下我们想要逻辑回归做什么：如果有一个 y = 1 的样本，不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 y = 1，现在我们希望 $h_\theta(x)​$ 趋近1。因为我们想要正确地将此样本分类，这就意味着当 $h_\theta(x)​$ 趋近于1时，$\theta^Tx​$ 应当远大于0。这是因为由于 z 表示 $\theta^Tx​$，当 z 远大于0时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于1。相反地，如果我们有另一个样本，即 y = 0。我们希望假设函数的输出值将趋近于0，这对应于 $\theta^Tx​$，或者就是 z 会远小于0，因为对应的假设函数的输出值趋近0。 如果你进一步观察逻辑回归的代价函数公式，你会发现每个样本 (x, y) 都会为总代价函数，增加上面的一项，因此，对于总代价函数通常会有对所有的训练样本的代价函数求和。 接下来，考虑逻辑回归的两种情况： 一种是y = 1 的情况， 一种是 y = 0的情况 在第一种情况中，假设 y = 1， 此时在目标函数中只需要第一项起作用， 因为y = 1时， (1-y)项等于零，因此，在y = 1的样本中， 即(x, y) 中， 我们得到$-log({1\over1+e^{-z}})$ 这一项。。如果画出关于 z 的函数，你会看到下面的这条曲线，我们同样可以看到，当 Z 增大时，也就是相当于 $\theta^Tx$ 增大时，z 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本时，试图将$\theta^Tx$设置得非常大。因为，在代价函数中的这一项会变的非常小。 现在开始建立支持向量机，我们从这里开始： 我们会从这个代价函数开始，也就是 $-log({1\over1+e^{-z}})$ 一点一点修改，让我取这里的 z = 1 点，我先画出将要用的代价函数。 新的代价函数(当y=1)和逻辑回归的代价函数类似，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在y=1的前提下的。你也许能想到，这应该能做同逻辑回归中类似的事情，但事实上，在向量机的优化问题中，这会变得更坚定，并且为支持向量机，带来计算上的优势。 另外一种情况是当 y = 1时，此时如果你仔细观察逻辑回归的代价函数只留下了第二项，因为第一项被消除了。如果当 y = 0时，那么这一项也就是0了。所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一项表示。并且，如果你将这一项作为z的函数，那么，这里就会得到横轴 z 。同样地，我们要替代逻辑回归的代价函数这一条蓝色的线，用相似的方法， 画出当y = 0的时候向量机的代价函数。 如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，第一个函数，我称之为cost1(z)，同时，第二个函数函数我称它为cost0(z)。这里的下标是指在代价函数中，对应的y=1和y=0 的情况，拥有了这些定义后，下面开始构建支持向量机。 这是我们在逻辑回归中使用代价函数 J(θ)。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要其表达式的两个部分替换为 cost1(z)，也就是前面构建的代价函数$cost_1(\theta^Tx)$, 和cost0(z)，也就是$cost_0(\theta^Tx)$。因此，对于支持向量机，我们得到了这里的最小化问题，即是将上面的两部分代价函数替换为下边蓝色的支持向量机代价函数的公式$cost_1(\theta^Tx)$ 和 $cost_0(\theta^Tx)$。然后再加上正则化参数。 但是在一般的时候，对支持向量机公式的参数，会有一些不同，下面尝试对向量机的代价函数进行改造： 首先，我们要除去1/m这一项，当然，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致，因为这里仅仅除去1/m这一项，也会得出同样的 θ 最优值，因为 仅是个常量，因此，这个最小化问题中，无论前面是否有 1/m 这一项，最终我所得到的最优值都是一样的。 第二点概念上的变化，对于逻辑回归，在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们不得不去用有 λ 这一项来平衡，这就相当于我们想要最小化加上正则化参数。我们所做的是通过设置不同正则参数 λ 达到优化目的。这样，我们就能够权衡对应的项，是使得训练样本拟合的更好。即最小化。但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的 λ 来权衡这两项。就是使用一个不同的参数称为C， 因此，在逻辑回归中，如果给定 λ，一个非常大的值，意味着给予正则化更大的权重。而这里，就对应于将 C 设定为非常小的值，那么，同样的的将会给正则化项更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数 C 考虑成 1/λ。 最后有别于逻辑回归输出的概率。我们的代价函数，当最小化代价函数，获得参数θ时，支持向量机所做的是它来直接预测的值等于1，还是等于0。因此，当这个假设函数当$\theta^Tx$小于0时，会预测为0。当$\theta^Tx$大于或者等于0时，将预测为1。 2.大间距的理解大间距分类器优化约定向量机经常被看做是大间距分类器，接下来了解一下向量机的大边界的含义，并进一步了解SVM模型的假设。 这是我的支持向量机模型的代价函数，左边是cost1(z) 关于z的代价函数，此函数用于正样本，右边是cost0(x)关于z的代价函数，横轴表示z，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本y=1 ，则只有在z &gt;= 1时，代价函数cost1(z)才等于0。 或者是说，如果有一个正样本，我们希望$\theta^Tx$&gt;=1, 反之， 如果y = 0, 他只有在z &lt;= -1 的时候， 函数值才为0.这是支持向量机的一个有趣的性质。事实上，如果有一个正样本y = 1, 则其实我们仅仅要求$\theta^Tx$大于等于0，就能将该样本恰当的分出，这是因为如果$\theta^Tx&gt;&gt;0$的话，我们的模型代价函数值为0， 所以我们需要的是比0大很多的值，比如大于等于1。这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说是安全的间距因子。 接下来看看C参数对支持向量机的影响。假设将C设置为一个非常大的数，比如将C设置成10000或者其他非常大的数 如果 C 非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为0的最优解。因此，让我们尝试在代价项的第一项为0的情形下理解该优化问题。比如我们可以把设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。 假如输入一个训练样本标签为 y = 1，你想令第一项为0，你需要做的是找到一个 θ，使得$\theta^Tx &gt;= 1$，类似地，对于一个训练样本，标签为 y = 0，为了使 cost0(z) 函数的值为0，我们需要 $\theta^Tx &lt;= -1$。因此，现在考虑优化问题。选择参数，使得第一项等于0，就会导致下面的优化问题，因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是C乘以0加上二分之一乘以第二项。这里第一项是C乘以0，因此可以将其删去，因为我知道它是0。这将遵从以下的约束(注意，这事是当向量机被看做是大间距分类器时的约定)：如果 $y^{(i)}$ 是等于1 的， $\theta^Tx^{(i)} &gt;=1$ ，如果 $y^{(i)}$ 是等于0的，$\theta^Tx^{(i)} &lt;= -1$ ，这样当求解这个优化问题的时候，当你最小化这个关于变量θ的函数的时候，你会得到一个非常有趣的决策边界。 大间距分类器决策边界和大边界理解 如果有一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。 比如，上面的绿色和粉色的决策边界，可以完全将训练样本区分开。但是多多少少这个看起来并不是非常自然 正常的支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线与样本之间有更大的距离，这个距离叫做间距(margin)。 当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器。现在我们可以理解为，上面一小节提到的优化约定，即：如果 $y^{(i)}$ 是等于1 的， $\theta^Tx^{(i)} &gt;=1$ ，如果 $y^{(i)}$ 是等于0的，$\theta^Tx^{(i)} &lt;= -1$ ，就是为了获取具有最大间距的决策边界，来获取最好的优化结果。 那么在让代价函数最小化的过程中，我们希望找出在y=0和y=1两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成： 事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点(outlier) 的影响。比如我们加入一个额外的正样本。 在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类似粉色的决策界。仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数C，设置的非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果C 设置的小一点，如果你将C设置的不要太大，则你最终会得到这条黑线，当然数据如果不是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数C非常大的情形，同时，C的作用类似于1/λ。这只是C非常大的情形，或者等价地 λ 非常小的情形。你最终会得到类似粉线这样的决策界，但是实际上应用支持向量机的时候，当C不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。 3.通过向量内积理解目标函数我们先前给出的支持向量机模型中的目标函数。 4.核函数核函数功能回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题： 为了获取上图所示的判定边界，我们的模型可能是$\theta_0 + \theta_{1x1} + \theta_{2x2} + \theta_{3x1x2} + + \theta_{4x1^2} + …..$ 的形式 我们可以用一些列新的特征f来代替模型中的每一项。例如令： f1 = x1, f2 = x2, f3 = x1x2, f4 = x1^2 ….， 得到$h_\theta(x) = f1 + f2 + f3 + … + fn$, 然而， 除了对原有的特征进行组合之外，我们可以利用核函数计算出新的特征。 给定一个训练实例x， 我们利用x的各个特征与我们预先选定的地标(landmarks) $l^{(1)}, l^{(2)}, l^{(3)}$的近似成都来选取新的特征f1，f2, f3 这些地标 $l^{(i)}$ 的作用是什么？ 如果一个训练实例与地标之间的距离近似于0，则新特征 f 近似于 $e^{-0} = 1$，如果训练实例x与地标之间距离较远，则 f 近似于$e^{一个大数} = 0$ 假设我们的训练实例含有两个特征[x1, x2] , 给定地标$l^{(1)}$与不同的σ值，见下图： 图中水平面的坐标为 x1，x2 ， f代表垂直坐标轴。可以看出，只有当x与$l^{(1)}$重合时才具有最大值。随着x的改变 f 值改变的速率受到σ2的控制。 在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此 f1 接近1，而 f2, f3接近0。因此hθ（x） &gt; 0，因此预测y = 1。同理可以求出，对于离 $l^{(2)}$ 较近的绿色点，也预测 y = 1，但是对于蓝绿色的点，因为其离三个地标都较远，预测 y = 0。 这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征f1, f2, f3。 核函数使用如何选择地标 我们通常是根据训练集的数量选择地标的数量，即如果训练集中有m个实例，则我们选取m个地标，并且令:$l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, …., l^{(m)} = x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即： z 注意: 如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的 另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。 5.两个参数C, σ对支持向量机的影响C对支持向量机的影响 C = 1/λ C较大的时候， 相当于λ较小的时候， 可能会导致过拟合， 高方差； C较小的时候，相当于λ较大的时候， 可能会导致欠拟合， 高偏差； σ对支持向量机的影响 σ较大时，可能会导致低方差，高偏差； σ较小时时，可能会导致低偏差，高房差； 6.使用支持向量机不同的核函数 在高斯核函数之外我们还有其他一些选择，如：多项式核函数（Polynomial Kernel）字符串核函数（String kernel）卡方核函数（ chi-square kernel）直方图交集核函数（histogram intersection kernel） 多分类问题 假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有k个类，则我们需要k个模型，以及k个参数向量θ。我们同样也可以训练k个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。 选择参数 1、选择参数C。需要考虑高方差和高偏差的问题2、选择内核参数σ ， 和核函数。 除非使用线型和的SVM。 7.逻辑回归和SVM之间的选择n为特征数， m为训练样本数。 (1) 如果相较于m而言， n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 (2)如果n较小，而且m大小中等，例如在 1-1000 之间，而m在10-10000之间，使用高斯核函数的支持向量机。 (3)如果n较小，而m较大，例如在1-1000之间，而m大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 (4)神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型评估、优化、设计过程]]></title>
    <url>%2F2018%2F11%2F24%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[1.机器学习模型的评估和选择有啥方法能优化我们的模型整理了一下优化模型时，经常做的一些操作，优化模型无外乎以下几种方法: 获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度 尝试增加正则化程度 但是，也如我们所见，并不是任何模型都适用于这些优化方法，我们还需要对症下药，接下来可以看一下怎么确定，我们的模型需要什么优化服务 模型过拟合的判断 当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。此时很可能模型已经产生了过拟合。 为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差： 对于线性回归模型，我们利用测试集数据计算代价函数 J 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外还可以计算误分类比率 模型的选择和交叉验证集当我们的使用多项式模型的时候，经常会不确定多项式的项数该如何确定，下面是一种比较简单的处理思路: 假设我们要在10个不同次数的二项式模型之间进行选择： ​ 显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能适应我们的测试集或者推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 选择模型的方法：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集 使用训练集训练出10个模型 用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）Train/validation/test error Training error:$J{train}(\theta) = \frac{1}{2m}\sum\limits{i=1}{m}(h_{\theta}(x{(i)})-y{(i)})2$Cross Validation error:$J{cv}(\theta) = \frac{1}{2m{cv}}\sum\limits{i=1}^{m}(h{\theta}(x{(i)}{cv})-y{(i)}{cv})^2$Test error:$J{test}(\theta)=\frac{1}{2m{test}}\sum\limits{i=1}^{m{test}}(h{\theta}(x^{(i)}{cv})-y{(i)}_{cv})2​$ 模型的偏差和方差诊断偏差和方差当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？ 其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。 我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析： 通过训练误差和交叉验证集误差判断偏差和方差问题，总结如下: 训练集误差和交叉验证集误差近似时：偏差/欠拟合交叉验证集误差远大于训练集误差时：方差/过拟合 正则化力度与偏差/方差在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。 我们选择一系列的想要测试的 λ 值，通常是 0-10之间的呈现2倍关系的值（如 0， 0.01， 0.01， 0.04， 0.08， 0.015， 0.032， 0.064， 1.28, 2.56, 5.12, 10共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。 选择正则化系数的方法为： 使用训练集训练出12个不同程度正则化的模型 用12个模型分别对交叉验证集计算的出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上： 当 λ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大随着 λ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 通过学习曲线评估偏差和方差学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。 例如： 如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。 如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观 也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。 如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。 也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。 学习曲线的总结: 当Jtrain和Jcv都偏高的时候，处于高偏差(欠拟合)的情况，此时增加训练数据不会有更好的结果 当Jtrain偏低而Jcv偏高的时候，处于高方差(过拟合)的情况，此时增加训练数据往往会获得更好的训练结果 神经网络的方差和偏差 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。 通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。 解决高偏差和高方差的总结解决高方差问题: 获得更多的训练实例——解决高方差 尝试减少特征的数量——解决高方差 尝试增加正则化程度λ——解决高方差 解决高偏差的问题 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 类偏斜的误差度量类偏斜和查准率查全率的介绍类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例， 这时候很难用一般的误差度量方法来度量其真实的误差。 例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 因为，下面将引入查准率和查全率的概念。 我们将算法预测的结果分成四种情况： 正确肯定（True Positive,TP）：预测为真，实际为真 正确否定（True Negative,TN）：预测为假，实际为假 错误肯定（False Positive,FP）：预测为真，实际为假 错误否定（False Negative,FN）：预测为假，实际为真 查准率（Precision）和查全率（Recall）的公式为 ： 查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。 因此，可以看出用于度量偏斜类问题，查准率和查全率更能体现模型表现的好坏。 查准率和查全率之间的选择在很多的应用中，我们希望能够保证查准率和召回率的相对平衡。 继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。 查准率(Precision)=TP/(TP+FP) 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率(Recall)=TP/(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。 我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同： 我们希望有一个帮助我们选择这个阀值的方法。 一种方法是计算F1 值（F1 Score），其计算公式为： $F_1Score：2{PR\over{P+R}}$ 我们选择使得F1值最高的阀值, F1的想法就是尽量让查准率和查全率都不会太小。 2.机器学习系统的设计机器学习系统设计思路以一个垃圾邮件分类器算法为例 为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。 为了构建这个分类器算法，我们可以做很多事，例如： 收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本 基于邮件的路由信息开发一系列复杂的特征 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理 为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法 误差分析设计机器学习系统的首要任务 当设计一个模型的时候， 最好的方法是快速的将最简单版本的算法实现，一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。 这么做的原因是：因为通常你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。 除了画出学习曲线之外，一件非常有用的事是误差分析。比如在构造垃圾邮件分类器时，可以看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。 构建一个学习算法的推荐方法为： 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择 进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势 举个误差分析的栗子: 以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看： 是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。 发现是否缺少某些特征，记下这些特征出现的次数。例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。 误差分析需要交叉验证来验证 误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。 不要用测试集来做交叉验证 因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。 总结: 当你在研究一个新的机器学习问题时，推荐你实现一个较为简单快速、即便不是那么完美的算法。目前大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。 另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>设计过程</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras构建简单神经网络]]></title>
    <url>%2F2018%2F11%2F11%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fkeras%E6%9E%84%E5%BB%BA%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[Keras 构建神经网络该示例的一般流程是首先加载数据，然后定义网络，最后训练网络。 要使用 Keras，你需要知道以下几个核心概念。 创建神经网络序列模型123from keras.models import Sequential # Create the Sequential model model = Sequential() keras.models.Sequential 类是神经网络模型的封装容器。它会提供常见的函数，例如 fit()、evaluate() 和 compile()。我们将介绍这些函数（在碰到这些函数的时候）。我们开始研究模型的层级吧。 层级Keras 层级就像神经网络层级。有完全连接的层级、最大池化层级和激活层级。你可以使用模型的 add() 函数添加层级。例如，简单的模型可以如下所示： 1234567891011121314from keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten #创建序列模型 model = Sequential() #第一层级 - 添加有 32 个节点的输入层 model.add(Dense, input_dim=32) #第二层级 - 添加有 128 个节点的完全连接层级 model.add(Dense(128)) #第三层级 - 添加 softmax 激活层级 model.add(Activation('softmax')) #第四层级 - 添加完全连接的层级 model.add(Dense(10)) #第五层级 - 添加 Sigmoid 激活层级 model.add(Activation('sigmoid')) Keras 将根据第一层级自动推断后续所有层级的形状。这意味着，你只需为第一层级设置输入维度。 上面的第一层级 model.add(Flatten(input_dim=32)) 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。 模型编译和训练构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 categorical_crossentropy。我们还可以指定优化程序，暂时将使用 adam。最后，我们可以指定评估模型用到的指标。我们将使用准确率。 1model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics = [&apos;accuracy&apos;]) 我们可以使用以下命令来查看模型架构： model.summary() 然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。 model.fit(X, y, nb_epoch=1000, verbose=0) 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。 最后，我们可以使用以下命令来评估模型： model.evaluate() 很简单，对吧？我们实践操作下。 练习我们从最简单的示例开始。在此测验中，你将构建一个简单的多层前向反馈神经网络以解决 XOR 问题。 将第一层级设为 Flatten() 层级，并将 input_dim 设为 2。 将第二层级设为 Dense() 层级，并将输出宽度设为 8。 在第二层级之后使用 softmax 激活函数。 将输出层级宽度设为 2，因为输出只有 2 个类别。 在输出层级之后使用 softmax 激活函数。 对模型运行 10 个 epoch。 准确度应该为 50%。可以接受，当然肯定不是太理想！在 4 个点中，只有 2 个点分类正确？我们试着修改某些参数，以改变这一状况。例如，你可以增加 epoch 次数。如果准确率达到 75%，你将通过这道测验。能尝试达到 100% 吗？ 首先，查看关于模型和层级的 Keras 文档。 Keras 多层感知器网络示例和你要构建的类似。请将该示例当做指南，但是注意有很多不同之处。 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom keras.utils import np_utilsimport tensorflow as tftf.python.control_flow_ops = tf# Set random seednp.random.seed(42)# Our dataX = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')y = np.array([[0],[1],[1],[0]]).astype('float32')# Initial Setup for Kerasfrom keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten# One-hot encoding the outputy = np_utils.to_categorical(y)# Building the modelxor = Sequential()xor.add(Dense(32, input_dim=2))xor.add(Activation("sigmoid"))xor.add(Dense(2))xor.add(Activation("sigmoid"))xor.compile(loss="categorical_crossentropy", optimizer="adam", metrics = ['accuracy'])# Uncomment this line to print the model architecture# xor.summary()# Fitting the modelhistory = xor.fit(X, y, nb_epoch=100, verbose=0)# Scoring the modelscore = xor.evaluate(X, y)print("\nAccuracy: ", score[-1])# Checking the predictionsprint("\nPredictions:")print(xor.predict_proba(X)) 结果： 123456789Using TensorFlow backend.4/4 [==============================] - 0sAccuracy: 0.75Predictions:4/4 [==============================] - 0s[[0.6914389 0.6965836 ] [0.7073754 0.7086655 ] [0.6919555 0.68419015] [0.70766294 0.6967294 ]]]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络]]></title>
    <url>%2F2018%2F11%2F07%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1.非线性假设当我们使用线型回归或者逻辑回归的时候，有这样一个缺点，当特征太多的时候，计算的负荷会非常大。 下面是一个例子: 当我们使用 x1, x2的多次项式进行预测时，我们可以应用的很好。 使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合（x1x2 + x1x3 + x1x4 + …. + x99x100），，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。 假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB值），作为假设，我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车， 如下图所示： 假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约3百万个特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。 2.神经元和大脑介绍2 神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。 神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过让我和你分享，一些这方面的证据。 大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此，你才能明白我的话。神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。 下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。举几个例子： 这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA (美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。 第二个例子： 关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。 3.模型表示神经网络模型表示为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？ 每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多输入/树突，并且有一个输出/轴突。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。 下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。 轴突是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。 神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（weight）。 逻辑回归学习模型的神经元： 单个神经元的效果和逻辑回归的效果没有区别，但是神经网络会组成有神经元组成的网络。 一个简单的神经网络 其中 x1, x2, x3是输入单元（input units），我们将原始数据输入给它们。 a1, a2, a3是中间单元，代表三个神经元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算 hθ(x)。神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）： 下面引入一些标记法来帮助描述模型： $a_i^{(j)}$代表第 j 层的第 i 个激活单元(神经元)。 $\theta^{(j)}$代表从第 j 层映射到第 j+1 层时的权重的矩阵，例如 $\theta^{(1)}$ 代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 j + 1 层的激活单元数量为行数，以第 j 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中 $\theta^{(1)}$ 的尺寸为 3*4。 注: 每层权重矩阵的列数为 ( 神经元数 + 1 ), 行数为 ( 上一层神经元数 + 1 ) 对于上图所示的模型，激活单元和输出分别表达为： 上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型 我们可以知道：每一个 a 的输出都是由上一层所有的 x 和每一个 x 所对应的 θ 决定的（我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )）把 x , θ, a分别用矩阵表示： 神经网络模型向量化( FORWARD PROPAGATION ) 相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值： 为了更好了了解Neuron Networks的工作原理，我们先把左半部分遮住： 右半部分其实就是以a0, a1, a2, a3 , 按照Logistic Regression的方式输出 hθ(x)： 其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量 [x1 - x3] 变成了中间层的 [ $a_1^{(2)}$ - $a_3^{(2)}$] ,我们可以把a0, a1, a2, a3看成更为高级的特征值，也就是x0, x1, x2, x3 的进化体，并且它们是由 x 与 θ 决定的，因为是梯度下降的，所以 a 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 x 次方厉害，也能更好的预测新数据。 这就是神经网络相比于逻辑回归和线性回归的优势。 4. 特征的直观理解从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。举例说明： 逻辑与(AND)：下图中左半部分是神经网络的设计与output层表达式，右边上部分是sigmod函数，下半部分是真值表。 其中 θ0 = -30, θ1 = 20, θ2 = 20 我们的输出函数hθ(x) 即为：hθ(x) = g(-30 + 20x1 + 20x2), g(x) 的图像是： 所以我们有：hθ(x) ≈ x1 AND x2 逻辑或(OR): 下图是神经网络的设计与output层表达式和真值表。 逻辑非(NOT)： 异或(XNOR): 5.多分类神经网络当我们有不止两种分类时（也就是 y = 1, 2, 3, … ），比如以下这种情况，该怎么办？ 如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。假如输入向量 x 有三个维度，两个中间层，输出层4个神经元分别用来表示4类，也就是每一个数据在输出层都会出现 [a b c d] ^T, 且a, b, c, d 中仅有一个为1，表示当前类。下面是该神经网络的可能结构示例： 6. 神经网络常用的激活函数什么是激活函数在神经网络中，我们经常可以看到对于某一个隐藏层的节点，该节点的激活值计算一般分为两步： （1）输入该节点的值为 x1,x2时，在进入这个隐藏节点后，会先进行一个线性变换，计算出值 $z^{[1]}=w_1x_1+w_2x_2+b^{[1]}=W^{[1]}x+b^{[1]}$，上标 1表示第 1 层隐藏层。 （2）再进行一个非线性变换，也就是经过非线性激活函数，计算出该节点的输出值(激活值) $a^{(1)}=g(z^{(1)})$ ，其中 g(z)为非线性函数。 常用的激活函数在深度学习中，常用的激活函数主要有：sigmoid函数，tanh函数，ReLU函数。 1.sigmoid函数 在逻辑回归中我们介绍过sigmoid函数，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下： sigmoid激活函数缺点： （1）当 z 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 g′(z) 将接近 0 。这会导致权重 W 的梯度将接近 0 ，使得梯度更新十分缓慢，即梯度消失。 （2）函数的输出不是以0为均值，将不便于下层的计算。sigmoid函数可用在网络最后一层，作为输出层进行二分类，尽量不要使用在隐藏层。 (3) 指数计算消耗资源 2.tanh函数 tanh函数相较于sigmoid函数要常见一些，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1)之间，其公式与图形为： tanh函数的优点 (1) tanh解决了sigmoid的输出非“零为中心”的问题。 tanh函数的缺点 （1）同sigmoid函数的第一个缺点一样，当 z 很大或很小时，g′(z) 接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。 (2) 依旧是指数计算 3.ReLU函数 ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下： ReLU函数的优点：（1）在输入为正数的时候（对于大多数输入 zz 空间来说），不存在梯度消失问题。（2） 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）ReLU函数的缺点： （1）当输入为负时，梯度为0，会产生梯度消失问题。 4.Leaky ReLU 函数 这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下： Leaky ReLU函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题。 优点: 1.神经元不会出现死亡的情况。 2.对于所有的输入，不管是大于等于0还是小于0，神经元不会饱和。 3.由于Leaky ReLU线性、非饱和的形式，在SGD中能够快速收敛。 4.计算速度要快很多。Leaky ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。 7.神经网络代价函数假设神经网络的训练样本有 m 个，每个包含一组输入x 和一组输出信号 y ，L 表示神经网络层数，Si 表示每层的neuron个数( Sl表示输出层神经元个数)，SL 代表最后一层中处理单元的个数。 将神经网络的分类定义为两种情况：二类分类和多类分类， 二类分类: SL = 1, y = 0 or 1 表示哪一类； K类分类: SL = k, yi = 1 表示分到第i类； (k &gt; 2) 这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。正则化的那一项只是排除了每一层 θ0 后，每一层的 θ 矩阵的和。最里层的循环 j 循环所有的行（由 sl +1 层的激活单元数决定），循环 i 则循环所有的列，由该层（ sl 层）的激活单元数所决定。即：hθ(x) 与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。 8.反向传播算法反向传播算法介绍在计算神经网络预测结果的时候采用了正向传播的算法，即从第一层开始向正向一层的神经元一层一层进行计算，知道最后一层的hθ(x). 为了计算代价函数的偏导数${\delta\over\delta\theta_{ij}^{(i)}} J(\theta)$ , 现在需要用到反向传播算法，以极具是首先计算最后一层的误差，然后再一层一层的反向求出各层的误差，直到倒数第二层。下面举例说明反向传播算法： 假设我们的训练集只有一个实例(x^(1), y^(1)), 我们的神经网络是一个四层的神经网络，其中: K = 4, SL = 4, L = 4 其前向传播算法为: 反向传播算法推导过程 即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。 9. 模型构建常用技巧1.梯度检验 2. 随机初始化任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的， 会导致梯度消失的情况。我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：Theta1 = rand(10, 11) * (2*eps) – ep 10. 梯度小时和梯度爆照(1)简介梯度消失与梯度爆炸 层数比较多的神经网络模型在训练的时候会出现梯度消失(gradient vanishing problem)和梯度爆炸(gradient exploding problem)问题。梯度消失问题和梯度爆炸问题一般会随着网络层数的增加变得越来越明显。 例如，对于图1所示的含有3个隐藏层的神经网络，梯度消失问题发生时，靠近输出层的hidden layer 3的权值更新相对正常，但是靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。梯度爆炸的情况是：当初始的权值过大，靠近输入层的hidden layer 1的权值变化比靠近输出层的hidden layer 3的权值变化更快，就会引起梯度爆炸的问题。 (2)梯度不稳定问题 在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题。 梯度不稳定的原因：前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸。 (3)产生梯度消失的根本原因 我们以图2的反向传播为例，假设每一层只有一个神经元且对于每一层都可以用公式1表示，其中σ为sigmoid函数，C表示的是代价函数，前一层的输出和后一层的输入关系如公式1所示。我们可以推导出公式2。 图2：简单的深度神经网络 而sigmoid函数的导数σ’(x)如图3所示。 图3：sigmoid函数导数图像 可见，σ’(x)的最大值为 1/4，而我们一般会使用标准方法来初始化网络权重，即使用一个均值为0标准差为1的高斯分布。因此，初始化的网络权值通常都小于1，从而有 |σ’(z)w &lt;= 1/4|。对于2式的链式求导，层数越多，求导结果越小，最终导致梯度消失的情况出现。 (4)产生梯度爆炸的根本原因 当，也就是w比较大的情况。则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题。 (5)如何解决梯度消失和梯度爆炸 梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑以下三种方案解决： 用ReLU、Leaky ReLU、PReLU、RReLU、Maxout等替代sigmoid函数。 用Batch Normalization。 LSTM的结构设计也可以改善RNN中的梯度消失问题。 11.构建神经网络的综合步骤网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。第一层的单元数即我们训练集的特征数量。最后一层的单元数是我们训练集的结果的类的数量。如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。我们真正要决定的是隐藏层的层数和每个中间层的单元数。训练神经网络： 参数的随机初始化 利用正向传播方法计算所有的hθ(x) 编写计算代价函数 J 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数 一句话总结神经网络我认为神经网络就是通过各层神经元将变量的重新计算， 导致特征的维度被大大放大， 总之经过神经网络瞎搞之后，特征已不是简单的特征，而原理从根本还是逻辑回归，当特征数量较多，而且模型不能满足线型要求， 可以考虑使用神经网络]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 逻辑回归]]></title>
    <url>%2F2018%2F11%2F03%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归作用可以做概率预测，也可用于分类，仅能用于线性问题。通过计算真实值与预测值的概率，然后变换成损失函数，求损失函数最小值来计算模型参数，从而得出模型。 sklearn调用接口1class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, *random_state=None*, solver=’liblinear‘, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1) 参数** =&gt; penalty : str, ‘l1’ or ‘l2’ LogisticRegression和LogisticRegressionCV默认就带了正则化项。penalty参数可选择的值为”l1”和”l2”，分别对应L1的正则化和L2的正则化，默认是L2的正则化。 在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。 另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。 penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。 但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。 =&gt; dual : bool 对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False =&gt; tol : float, optional 迭代终止判据的误差范围。 =&gt; C : float, default: 1.0 C为正则化系数λ的倒数，通常默认为1。设置越小则对应越强的正则化。 =&gt; fit_intercept : bool, default: True 是否存在截距，默认存在 =&gt; intercept_scaling : float, default 1. 仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。 =&gt; class_weight : dict or ‘balanced’, default: None class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重， 或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。 如果class_weight选择**balanced**，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低；样本量越少，则权重越高。 当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y)) n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3] 0,1分别出现2次和三次 那么**class_weight**有什么作用呢？ ​ 在分类模型中，我们经常会遇到两类问题： ​ 第一种是误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。 ​ 第二种是样本是高度失衡的，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。 这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。 =&gt; random_state : int, RandomState instance or None, optional, default: None 随机数种子，默认为无，仅在正则化优化算法为sag,liblinear时有用。 =&gt; solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} solver参数决定了我们对逻辑回归损失函数的优化方法，有4种算法可以选择，分别是： a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候，SAG是一种线性收敛算法，这个速度远比SGD快。 从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear则既可以用L1正则化也可以用L2正则化。 同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了 =&gt; max_iter : int, optional 仅在正则化优化算法为newton-cg, sag and lbfgs 才有用，算法收敛的最大迭代次数。 =&gt; multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’ OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。 其他类的分类模型获得以此类推。 ​ 而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归， 得到模型参数。我们一共需要T(T-1)/2次分类。 可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。 但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。 =&gt; verbose : int, default: 0 =&gt; warm_start : bool, default: False =&gt; n_jobs : int, default: 1 如果multi_class =’ovr’“，并行数等于CPU内核数量。当“solver”设置为“liblinear”时，无论是否指定“multi_class”，该参数将被忽略。如果给定值-1，则使用所有内核。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[过拟合、正则化(Regularization)]]></title>
    <url>%2F2018%2F11%2F02%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.过拟合问题过拟合问题描述常用的机器学习优化算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(over-fitting)的问题，可能会导致它们效果很差。使用正则化技术可以减少过拟合问题。如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。下图是一个回归问题的例子： 第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。分类问题中也存在这样的问题： 就以多项式理解， 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 如果我们发现了过拟合问题，应该如何处理？ 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA） 正则化。 保留所有的特征，但是减少参数的大小（magnitude）。 2.代价函数上面的回归问题中如果我们的模型是：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3 + θ_4x_4^4$ 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数 θ 的值，这就是正则化的基本方法。 我们决定要减少θ3和 θ4的大小，我们要做的便是修改代价函数，在其中 θ3和 θ4 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 θ3和θ4。 修改后的代价函数如下： $min_θ {1\over 2m}[h_θ(x^{(i)} - y^{(i)})^2 + 1000θ_3^2 + 10000θ_4^2]$ 通过这样的代价函数选择出的 θ3和 θ4 对预测结果的影响就比之前要小许多， 因为为了θ3和θ4的系数很大，为了优化代价函数， 其值一定会特别的小。 假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设： $J_θ = {1\over 2m}[\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)}) + λ\sum_{j=1}^nθ_j^2]$其中又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对 θ0 进行惩罚。 经过正则化处理的模型与原模型的可能对比如下图所示： 如果选择的正则化参数λ过大，则会把所有的参数都最小化了，导致模型变成 $h_θ(x) = θ_0$，也就是上图中红色直线所示的情况，造成欠拟合。 那为什么增加的一项 $λ\sum_{j=1}^nθ_j^2$ 可以使的值减小呢？ 因为如果我们令 λ的值很大的话，为了使Cost Function 尽可能的小，所有的 θ 的值（不包括θ0） 都会在一定程度上减小。 但若λ的值太大了，那么不包括θ0）都会趋近于0，这样我们所得到的只能是一条平行于轴的直线。 所以对于正则化，我们要取一个合理的 λ 的值，这样才能更好的应用正则化。 回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。 3.正则化线性回归对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。正则化线性回归的代价函数为： $J_{(θ)} = {1\over 2m} \sum_{i=1}^m[(h_θ(x^{(i)})-y^{(i)})^2 + λ\sum_{j=1}^nθ_j^2]$ 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对θ进行正则化，所以梯度下降算法将分两种情形： 4.正规方程逻辑回归针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数J(θ)，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数J(θ)。 用Python实现 123456789import numpy as npdef costReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X*theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X*theta.T))) reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2)) return np.sum(first ‐ second) / (len(X)) + reg 要最小化该代价函数，通过求导，得出梯度下降算法为： 注：看上去同线性回归一样，但是知道 $h_θ(x) = g(θ^TX)$ ，所以与线性回归不同。 Octave 中，我们依旧可以用fminuc 函数来求解代价函数最小化的参数，值得注意的是θ0参数的更新规则与其他情况不同。 注意： 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者hθ(x)的不同所以还是有很大差别。 θ不参与其中的任何一个正则化。 5.其他防止过拟合的方法DropoutDropout是在深度学习中降低过拟合风险的常见方法，它是由大牛Hinton老爷子提出的。老爷子认为在神经网络产生过拟合主要是因为神经元之间的协同作用产生的。因此老爷子在神经网络进行训练的时候，让部分神经元失活，这样就阻断了部分神经元之间的协同作用，从而强制要求一个神经元和随机挑选出的神经元共同进行工作，减轻了部分神经元之间的联合适应性。Dropout的具体流程如下： 1.对 l 层第 j 个神经元产生一个随机数 $r^{(l)}_j∼Bernouli(p)$ 2.将 l 层第 j 个神经元的输入乘上产生的随机数作为这个神经元新的输入：$x^{(l)∗}_j=x^{(l)}_j∗r^{(l)}_j$ 3.此时第 l 层第 j 个神经元的输出为：$y{(l+1)}j=f(∑^k{j=1}(w^{(l+1)}_j ∗ x^{(l)∗}_j + b^{(l+1)}))$其中，k为第l层神经元的个数，f为该神经元的激活函数，b为偏置，w为权重向量。注意：当我们采用了Dropout之后再训练结束之后，应当将网络的权重乘上概率p得到测试网络的权重，或者可以在训练时，将样本乘上 1/P。 提前终止在对模型进行训练时，我们可以将我们的数据集分为三个部分，训练集、验证集、测试集。我们在训练的过程中，可以每隔一定量的step，使用验证集对训练的模型进行预测，一般来说，模型在训练集和验证集的损失变化如下图所示： 可以看出，模型在验证集上的误差在一开始是随着训练集的误差的下降而下降的。当超过一定训练步数后，模型在训练集上的误差虽然还在下降，但是在验证集上的误差却不在下降了。此时我们的模型就过拟合了。因此我们可以观察我们训练模型在验证集上的误差，一旦当验证集的误差不再下降时，我们就可以提前终止我们训练的模型。 增加样本量在实际的项目中，你会发现，上面讲述的那些技巧虽然都可以减轻过拟合的风险，但是却都比不上增加样本量来的更实在。 为什么增加样本可以减轻过拟合的风险呢？这个要从过拟合是啥来说。过拟合可以理解为我们的模型对样本量学习的太好了，把一些样本的特殊的特征当做是所有样本都具有的特征。举个简单的例子，当我们模型去训练如何判断一个东西是不是叶子时，我们样本中叶子如果都是锯齿状的话，如果模型产生过拟合了，会认为叶子都是锯齿状的，而不是锯齿状的就不是叶子了。如果此时我们把不是锯齿状的叶子数据增加进来，此时我们的模型就不会再过拟合了。因此其实上述的那些技巧虽然有用，但是在实际项目中，你会发现，其实大多数情况都比不上增加样本数据来的实在。]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 线型回归]]></title>
    <url>%2F2018%2F10%2F31%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[简单线性回归线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。 使用sklearn.linear_model.LinearRegression进行线性回归sklearn对Data Mining的各类算法已经有了较好的封装，基本可以使用fit、predict、score来训练、评价模型，并使用模型进行预测，一个简单的例子如下： 123456789from sklearn import linear_modelclf = linear_model.LinearRegression()X = [[0,0],[1,1],[2,2]]y = [0,1,2]clf.fit(X,y)print(clf.coef_)[ 0.5 0.5]print(clf.intercept_)1.11022302463e-16 LinearRegression已经实现了多元线性回归模型，当然，也可以用来计算一元线性模型，通过使用list[list]传递数据就行。下面是LinearRegression的具体说明。 使用方法实例化sklearn一直秉承着简洁为美得思想设计着估计器，实例化的方式很简单，使用clf = LinearRegression()就可以完成，但是仍然推荐看一下几个可能会用到的参数： fit_intercept：是否存在截距，默认存在 normalize：标准化开关，默认关闭 还有一些参数感觉不是太有用，就不再说明了，可以去官网文档中查看。 回归其实在上面的例子中已经使用了fit进行回归计算了，使用的方法也是相当的简单。 fit(X,y,sample_weight=None)：X,y以矩阵的方式传入，而sample_weight则是每条测试数据的权重，同样以array格式传入。 predict(X)：预测方法，将返回预测值y_pred score(X,y,sample_weight=None)：评分函数，将返回一个小于1的得分，可能会小于0 方程LinearRegression将方程分为两个部分存放，coef_存放回归系数，intercept_则存放截距，因此要查看方程，就是查看这两个变量的取值。 多项式回归其实，多项式就是多元回归的一个变种，只不过是原来需要传入的是X向量，而多项式则只要一个x值就行。通过将x扩展为指定阶数的向量，就可以使用LinearRegression进行回归了。sklearn已经提供了扩展的方法——sklearn.preprocessing.PolynomialFeatures。利用这个类可以轻松的将x扩展为X向量，下面是它的使用方法： 123456789&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; X_train = [[1],[2],[3],[4]]&gt;&gt;&gt; quadratic_featurizer = PolynomialFeatures(degree=2)&gt;&gt;&gt; X_train_quadratic = quadratic_featurizer.fit_transform(X_train)&gt;&gt;&gt; print(X_train_quadratic)[[ 1 1 1] [ 1 2 4] [ 1 3 9] [ 1 4 16]] 经过以上处理，就可以使用LinearRegression进行回归计算了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>线型回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归 & 分类问题]]></title>
    <url>%2F2018%2F10%2F31%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1. 分类问题在分类问题中，如果想要预测的变量y是离散的值，那么可以只用一种叫做逻辑回归(logistic regression)的算法,这是在目前最流行的一种学习算法之一。 在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。 我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈0,1 ，其中 0 表示负向类，1 表示正向类。 线型回归不适合分类问题如果我们要用线性回归算法来解决一个分类问题，对于分类， y 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 y 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。 逻辑回归算法逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y 取值离散的情况，如：1 0 0 1。 2. 逻辑回归表达式在分类问题中，要用什么样的函数来表示我们的假设。在解决分类问题的时候，我们希望分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。 为什么线型回归不适合分类问题?回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线： 根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测： 当 $h_θ(x) &gt;= 0.5$ 时, 预测y= 1; 当$h_θ(x) &lt; 0.5$ 时，预测y = 0 对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。但是， 假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。 这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了， 所以使用线型回归模型，很难确定断定一个问题分类的边界。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决分类这样的问题。 逻辑回归模型我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是： $h_θ(x) = g(θ^TX)$ 函数g的表达式为: $g(z) = {1\over1+e^{-z}}$ 其中: X 代表特征向量, g代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function） g(z) 的函数图像为: 对逻辑回归模型理解 $h_θ(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算出输出的变量=1 的可能性, 即$h_θ(x) =P(y=1|x; θ)$ 例如, 如果对于给定的x， 通过已经确定的参数计算出$h_θ(x) = 0.7$ ， 则表示有70%的几率y为正向类，响应地y为负向类的几率为1-0.7=0.3 g(z) （Sigmoid function）函数的作用就是将$θ^TX$的结果映射到0-1之间， 3. 决策边界对决策边界的理解 决策边界(decision boundary)，能够更好地帮助我们理解逻辑回归的假设函数中计算什么 在逻辑回归中， 我们预测到: 当$h_θ(x) &gt;= 0.5$ 时， 预测 y= 1; 当 $h_θ(x) &gt;= 0.5$ 时，预测 y = 0； 根据上面绘制的S形函数图像，我们知道当 z = 0 时, g(z) = 0.5 z &gt; 时, g(z) &gt; 0.5 z &lt; 0 时, g(z) &lt; 0.5 又 $z = θ^Tx$, 即: $θ^TX &gt;= 0$ 时, 预测 y = 1 , $θ^TX &lt; 0$ 时， 预测 y = 0 现在假设我们有一个模型: 并且参数θ是向量[-3 1 1]. 则当 -3 + x1 + x2 &gt;= 0, 即x1 + x2 &gt;= 3时， 模型将预测 y = 1. 我们可以绘制直线x1 + x2 = 3, 这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。 复杂形状的决策边界假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？ 因为需要用曲线才能分隔 y = 0 的区域和 y = 1 的区域，我们需要二次方特征： 所以按照假设函数应该趋近， $h_θ(x) = g(θ_0 + θ_1x_1 + θ_2x_2 + θ_1x_1^2 + θ_2x_2^2 )$ θ最终结果趋近 [-1 0 0 1 1], 我们最终得到的判定边界恰好是圆点在原点且半径为1的圆形 , 综上我们可以用复杂的模型来适应飞象复杂形状的判定边界 4. 逻辑回归代价函数和梯度下降逻辑回归代价函数及简化对于线性回归模型，我们定义的代价函数是所有模型误差的平方和和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_θ(x) = g(θ^TX)$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）如下图，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 线型回归代价函数为: $J_{(θ)} = {1\over m} \sum_{i=1}^m{1\over2}(h_θ(x^{(i)})-y^{(i)})^2​$ 重新定义逻辑回归代价函数，使代价函数在逻辑回归算法中为凸函数, 过程如下: 根据推倒可以证明, 我们推倒的的代价值函数会给我们一个凸优化问题。代价函数 J(θ) 会是一个凸函数，并且没有局部最优值。 python代码实现 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X* theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X* theta.T))) return np.sum(first ‐ second) / (len(X)) 梯度下降算法推倒及简化在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为： Repeat { $θ_j : =θ_j − α{∂\over∂θ_j }J(θ)$ (simultaneously update all ) } 求导后得到: Repeat { $θ_j : =θj − α{1\over m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update all ) } 注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的 $h_θ(x) = g(θ^TX)$ 与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。 5. 高级优化算法一些高级算法的介绍现在我们换个角度来看，什么是梯度下降，我们有个代价函数 J(θ)，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数θ时，它们会计算出两样东西：J(θ) 以及 J 等于 0、1直到 n 时的偏导数项。 假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。 另一种考虑梯度下降的思路是：我们需要写出代码来计算 J(θ) 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。 对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数J(θ)。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些 J(θ)的收敛性，那么我们就需要自己编写代码来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$。所以，在写完能够计算这两者的代码之后，我们就可以使用梯度下降。 然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轭梯度法、 BFGS (变尺度法)、 L-BFGS (限制变尺度法) 就是其中一些更高级的优化算法，它们需要有一种方法来计算 J(θ)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。 这三种算法的优点： 一个是使用这其中任何一个算法，你通常不需要手动选择学习率 α ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 α ，并自动选择一个好的学习速率 α，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。 Octave 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，因此，如果你正在你的机器学习程序中使用一种不同的语言，比如如果你正在使用C、C++、Java等等，你可能会想尝试一些不同的库，以确保你找到一个能很好实现这些算法的库。 如何使用这些算法比方说，你有一个含两个参数的问题，这两个参数是θ1和θ2，因此，通过这个代价函数，你可以得到 θ1 和 θ2 的值，如果你将 J(θ) 最小化的话，那么它的最小值将是 θ1 = 5 ，θ2 = 5 。代价函数 J(θ) 的导数推出来就是这两个表达式： $α{∂\over∂θ_1}J(θ) = 2(θ_1 - 5)$ $α{∂\over∂θ_2}J(θ) = 2(θ_2 - 5)$ 如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的Octave 函数： 123456function [jVal, gradient]=costFunction(theta) jVal=(theta(1)‐5)^2+(theta(2)‐5)^2; gradient=zeros(2,1); gradient(1)=2*(theta(1)‐5); gradient(2)=2*(theta(2)‐5);end 这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个costFunction 函数后，你就可以调用高级的优化函数，这个函数叫fminunc，它表示Octave 里无约束最小化函数。调用它的方式如下： 123options=optimset('GradObj','on','MaxIter',100);initialTheta=zeros(2,1);[optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options); 你要设置几个options，这个 options 变量作为一个数据结构可以存储你想要的options，所以 GradObj 和On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说100，我们给出一个 的猜测初始值，它是一个2×1的向量，那么这个命令就调用fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的值。 实际运行过程示例 6. 多分类问题多分类的介绍一些多分类的例子: 例一: 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用 y=1, y=2, y=3, y=4 来表示 例二: 如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用 y=1 这个类别来代表；或者患了感冒，用 y=2 来代表；或者得了流感用 y=3 来代表. 然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样： 对于一个多类分类问题，我们的数据集或许看起来像这样： 一对于多分类思路我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为“一对余”方法。 现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 y = 1 ，方框表示 y = 2，叉叉表示 y = 3。下面要做的就是使用一个训练集，将其分成3个二元分类问题。我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。 这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。 为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作$h_θ^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作$h_θ^{(2)}(x)$ ,依此类推。 最后我们得到一系列的模型简记为：$h_θ^{(i)}(x) = P(y=i|x; θ)$ 其中：i = (1, 2, 3, 4, 5, …, k) k为总分类数量最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_θ^{(i)}(x)$ ， 其中 i 对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 x ，然后我们选择一个让 $h_θ^{(i)}(x)$ 最大的 i 作为最终的分类结果。现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论值是多少，我们都有最高的概率值，我们预测就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Octave基础操作]]></title>
    <url>%2F2018%2F10%2F26%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2Foctave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[基础功能命令修改命令行的提示1PS1('&gt;&gt; ') % &gt;&gt; 就是修改后的提示符号 变量赋值语句1A = 5 + 6 ; % 后面加上一个可以不在终端打印变量的值 显示工作空间的所有变量12who % 显示工作空间的所有变量whos % 显示工作空间的所有变量和详细信息 删除变量12clear A % 删除变量Aclear % 删除所有变量 打印变量12A % 直接再终端输入变量名称就可以将变量的值打印出来disp(A) % 通过disp函数将变量打印出来 修改全局的输出内容的长短12format long % 将输出数值的长度定义为long类型format short % 将输出数值的长度定义为short类型 查看命令的帮助信息123helo randhelp eyehelp help 添加搜索路径1addpath path % 添加路径到函数和数据等的某人搜索路径 基础运算数值运算13-2； 5*8； 1/2； % 基础运算 逻辑运算12341 &amp;&amp; 0 % 逻辑与1 || 0 % 逻辑或~ 1 % 逻辑费XOR(1, 0) % 异或运算 判断语句121 == 2 % 相等判断1 ~= 2 % 不等于判断 矩阵运算创建矩阵1234567891011A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵v = [ 1 2 3 ] % 创建一个行向量v = 1:6 % 创建一个从1到6的行向量v = 1:0.1:2 % 创建一个从1开始，以0.1为步长，直到2的行向量v = ones(2, 3) % 创建一个2行3列的元素都是1的矩阵v = 2 * ones(2, 3) % 创建一个2行3列的元素都是2的矩阵v = zeros(2, 3) % 创建一个2行3列的元素都是0的矩阵v = rand(2, 3) % 创建一个2行3列的元素都是0-1之间的随机数的矩阵v = randn(2, 3) % 创建一个2行3列的元素都符合平局值为0,1为标准差的正态分布随机数的矩阵I = eye(6) % 创建一个大小为6的单位矩阵v = type(3) % 返回一个3*3的随机矩阵 矩阵运算12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵B = [ 11, 12; 13 14; 15 16 ]2 * A % A矩阵中的每个元素都乘以2A .* B % 将A矩阵和B矩阵中的每一个对应位置的元素相乘,同样适用于+ - / ^等A .* % A矩阵的每个元素取二次方log(v) % 矩阵的每个院对对数运算exp(v) % 矩阵的每个元素进行以为底，以这些元素为幂的运算abs(v) % 对v矩阵的每个元素取绝对值A + 1 % 将A矩阵的每个元素加上1A&apos; % 取A矩阵的转置矩阵 获取矩阵尺寸12345A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵size(A) % 返回A矩阵的尺寸,返回的内容同样是行向量size(A，1) % 返回矩阵的行数size(A，2) % 返回矩阵的列数lengh(A) % 返回A矩阵的维度大小，即矩阵的长宽中比较大的维度，矩阵A的length即为3 矩阵的索引12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵A(3, 2) % 取A矩阵的第三行第二列的元素A(2, :) % 返回第二行的所有元素A(:, 2) % 返回第二列的所有元素A([1 3], :) % 取第1行和第3行的所有元素A(:, 2) = [10; 11; 12] % 将矩阵的第二列重新赋值A = [A, [100, 101, 102]] % 给矩阵附加了一列矩阵，给矩阵增加一列。带，和不带，相同[A B] % 两个矩阵直接连在一起，矩阵A在左边，矩阵B在右边[A;B] % 两个矩阵直接连在一起，矩阵A在上边, 矩阵B在下边A(:) % 将矩阵的所有元素导向一个单独的列向量排列起来 矩阵的计算123456789101112131415A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵val = max(a) % 返回a矩阵中的最大值, 当a为多维矩阵的时候返回的是每一列的最大值val = max(A, a) % 两个矩阵所有元素逐个比较返回较大的值max(A,[],1) % 得到矩阵每一列元素的最大值max(A,[],2) % 得到矩阵每一行元素的最大值[val, ind] = max(a) % 返回a矩阵中的最大值和对应的索引sum（a) % 把a矩阵的所有元素相加起来, 如果是多维矩阵默认为每一列的总和sum（a,1) % 求多维矩阵每一列的总和sum(a, 3) % 求多维矩阵每一行的总和a&lt;3 % 对矩阵每一个元素进行&lt;3判断，判断为真返回1, 否则返回0find(a&lt;3) % 返回哪些元素小于3(是索引值)prod（a) % 将a矩阵的所有元素相乘floor(a) % 将a矩阵的所有元素进行向下取舍ceil(a) % 将a矩阵的所有元素进行向上取整pinv(v) % 求v矩阵的逆矩阵 绘制图像绘制直方图123w = -6 + sqrt(10) * (randn(1, 10000))hist(w) % 绘制w矩阵的直方图hist(w, 50) % 绘制w矩阵的直方图，并指定50个长方形 绘制曲线图1234567891011121314t = [0:0.1:1];y1 = sin(2*pi*4t);plot(t, y1);hold on % 新绘制的图不会覆盖之前的图片，会在之前的图片上生成新的曲线y2 = cos(2*pi*4*t)plot(t, y2, &apos;r&apos;) % 绘制新的直线图，r表示线的颜色为红色xlable(&apos;time&apos;) % 添加x轴名称ylable(&apos;value&apos;) % 给y轴添加名称legend(&apos;sin, &apos;cos&apos;) % 给线命名title(&apos;myplot&apos;) % 给图片一个标题名称print -dpng &apos;myplot.png&apos; % 输出图片plot clos % 关掉图片axis([0.5 1 ‐1 1]) % 改变坐标返回，前两个参数控制横坐标，后两参数控制纵坐标Clf % 清除一个图像 在一张画纸上绘制两张直线图123456789t = [0:0.1:1];y1 = sin(2*pi*4t);y2 = cos(2*pi*4*t)；figure(1); plot(t, y1); % 绘制第一张图片figure(2); plot(t, y2); % 绘制第二张图片subplot(1,2,1) %将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。plot(t,y1) % 将图片绘制到第一个格子suplot(1,2,2) % 使用第二个格子plot(t,y2) % 将图片绘制到第二个格子 彩色格图绘制12imagesc(A ) % 绘制彩色格子图imagesc(A)，colorbar，colormap gray % 生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。 移动数据导入数据1load('featureD.dat') % 加载featureD.dat中的所有数据，并将其复制给变量featureD 导出数据12save hello.mat v % 将变量A导出为一个叫hello.mat文件 二进制形式save hello.mat v -ascii % 将变量A导出为一个叫hello.mat文件 ascii形式 控制语句for循环1234v = zeros(10,1)for i=1:10, v(i) = 2^1;end while 循环123456v = zeros(10,1)i = 1while i &lt;= 5, v(i) = 100; i = i+1;end if - else - elif 语句12345678v = zeros(10,1)if v(1) == 1, disp('1');elseif v(1) == 2, disp('2');else disp('3');end 自定义函数定义函数1.先创建一个文件​ squarethisnumber.m # .m前定义的就是函数名2.编写函数文件 12function y = squareThisNumber(x)y = x^2; 第一行写着 function y = squareThisNumber(x) ，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 里。另外，它告诉了Octave这个函数有一个参数，就是参数 ，还有定义的函数体，y = x^2 使用自定义函数1.切换到函数文件所在目录2.直接通过函数名squareThisNumber() 调用函数]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线型回归 & 梯度下降]]></title>
    <url>%2F2018%2F10%2F20%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.模型表示问题的概述 在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。 模型引入假使我们回归问题的训练集（Training Set）如下表所示： 我们将要用来描述这个回归问题的标记如下:m 代表训练集中实例的数量x 代表特征/输入变量y 代表目标变量/输出变量(x, y) 代表训练集中的实例(x^i, y^i)代 表第 个观察实例h 代表学习算法的解决方案或函数也称为假设（hypothesis） 这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。 h代表hypothesis(假设)， 表h示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h根据输入的 值来得出 y 值， y 值对应房子的价格因此， h 是一个从 x 到 y 的函数映射。 我将选择最初的使用规则代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 h？一种可能的表达方式为：hθ(x)=θ0+θ1∗x ，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。 2.代价函数什么是代价函数 在线性回归中我们有一个像这样的训练集， m代表了训练样本的数量 。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：hθ(x)=θ0+θ1∗x 。我们通过训练获得的预测函数在预测时必然会和实际值有一些偏差，下面就给出了这部分预测偏差的定义 代价函数的定义: 我们选择的参数决定了我们得到的模型相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）,下图蓝色线段变为预测和实际的误差。 平方误差代价函数: 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。 怎么优化线型回归模型 优化回归模型的目标: 便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数最小。 代价函数公式: 代价函数坐标图 可以看出在三维空间中存在一个使得J(θ_0,θ_1)代价函数最小的点， 当代价函数取得最小值的时候，我们会获得最符合训练集的预测模型。 3.梯度下降算法算法介绍 梯度下降是一个用来求函数最小值的算法。其思想是利用迭代的思想来逼近最低点，沿着梯度的负方向更新模型权重。我们将使用梯度下降算法来求出代价函数 J(θ_0,θ_1) 的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0, θ1, …., θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（globalminimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 梯度下降算法公式:公式介绍 repeat until convergence{$$θ_j=θ_j−α∂/∂θ_j J(θ0,θ1) (for j=0 and j=1)$$} 参数α: 学习率, 它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。当到达损失函数的最低点的时候，其导数为0，已经到达了局部最优点，即θ不会再发生改变 注意: 在梯度下降算法中，一般都会采用同步更新的方式来优化模型, 即上面公式的θj同步更新。同步更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。 学习率的选择对算法的影响 学习率过小的影响: 则达到收敛所需的迭代次数会非常高 学习率过大的影响: 每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛 怎么确定模型是否收敛 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛, 如上图在300步之后基本接近收敛。 也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较 梯度下降算法分类批量梯度下降 在上面的线性回归场景中的梯度下降算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。 批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点： （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 随机梯度下降公式: 随机梯度下降指在梯度下降的每一步中只用到了训练集的一个样本或者一部分样本，最后得到的可能是全局最优解，也可能是局部最优解。 优点： （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。缺点： （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。 （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。 （3）不易于并行实现。 4.梯度下降线型回归模型单变量线型回归梯度下降梯度下降、线型回归算法比较 单变量梯度下降公式代价函数计算 参数θ的计算 多变量线型回归梯度下降多变量特征现在对回归模型增加更多的特征，例如房价预测问题，增加房间数楼层等，构成一个含有多个变量的模型，模型中的特征如下图。 增添更多特征后，引入一系列新的注释： n 代表特征的数量 x^(i)^ 代表第i个训练实例，是特征矩阵的第i行，是一个向量(vector). x~j~^(i)^ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征 如: $x_2^{(2)} = 3, x_3^{(2)} = 2$ 支持多个变量的h表示为: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 这个公式中有个n+1参数和n个变量，为了使得公式能够简化一些，引入$x_0=1$，则公式转化为:$h_θ(x) = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m* (n+1), 因此变量的假设公式可以表示为: $h_θ(x) = θ^TX$ 多变量梯度下降公式与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即： $J_{(θ_0, .., θn)} = {1\over2m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 , 其中: h_θ(x) = θ^TX = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度梯度下降下降公式： 求导数后得到: 5.特征和多项式回归特征选择 有的时候的们的特征并不是很好的可以直接用来训练模型，比如房子价格预测模型的两个特征 特征: x1 房子的临街宽度， x2 房子的纵向深度 此时很明显房子的价格用 特征一和特征二的乘积，即房子的占地面积来预测更合适 $x=x_1 * x_2 = area (面积)$ 多项式回归 很多时候，我们并没有那么幸运，正好所有的数据之间都是线性关系，能用一条直线，一个平面就描述地出来，大多数时候，我们可能需要画成曲线、曲面或者曲啥更高维度的东西 比如下图数据集，线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，用二次方程（蓝色的线）和三次方程（绿色的线）来拟合这些点，但是二次方程有个缺点，到了后面，反而会下降，这和我们的认知不符合，而三次方程不会遇到这个问题，和二次方程相加又会和上升的部分相互抵消一部分，所以最后选择了三次方程加上二次方程。 二次方模型公式：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2$ 三次方模型公式: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3$ 通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以把多项式回归看成是另类的多变量线性回归，我们可以令：$x_2 = x_2^2$ , $x_3 = x_3^3$ , 从而将模型转化为线型回归模型 注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要, 因为比如x的范围是1-100的话，$x^2$的范围就是1-10000，$x^3$的范围就是1-1000000。 6.特征缩放为什么要特征缩放 在使用梯度下降的时候会遇到因为特征大小不统一，导致收敛速度过慢的问题，在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。 以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。 解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。 特征缩放的两种方法线型归一化 原理： 通过对原始数据进行变换把数据映射到(默认为[0,1])之间 公式 归一化的弊端 使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景 特征标准化 原理： 通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内 公式： 标准化的有点 如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。 7.正规方程线型回归正规方程算法介绍 对于线型回归问题经常都是在使用梯度下降算法，但是对于某些线型回归问题，正规方程是更好的解决办法。如下面的代价函数: 梯度下降和正规方程比较 梯度下降 正规方程 需要选择学习率 不需要 需要多次迭代 一次运算得出 当特征数量n特别大时能比较适用 需要计算$(X^TX)^{-1}$如果特征数量n较大则运算代价大因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型等其他模型 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习介绍]]></title>
    <url>%2F2018%2F10%2F19%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[机器学习的发展 机器学习被应用到越来越多的领域这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。 再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。 手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。 一个比较好的机器学习定义 一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升 类比于下棋，经验E 就是程序上万次的自我练习的经验而任务T 就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。 机器学习基本算法最常用的两个算法监督学习算法 我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，即我们可以根据正确答案来不断的优化我们的模型。 监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。 无监督学习算法 根据未知的训练样本(未被标记分类), 从数据中找出不同的结构，将数据识别为不同的聚集簇。 无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？ 还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。 监督学习算法常见问题回归问题 通过像预测房子的价格一样，通过回归模型通过输入值来推出一个连续的输出值 分类问题 通过数学模型来判断输入数据所属的类别，可以是二类别问题（是/不是），也可以是多类别问题（在多个类别中判断输入数据具体属于哪一个类别） 无监督学习常年问题聚类问题 依据研究对象（样品或指标）的特征，将其分为不同的分类。 ###]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Registry 私有库]]></title>
    <url>%2F2018%2F10%2F17%2F11.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2F%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.关于Registry仓库官方的Docker hub是一个用于管理公共镜像的好地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景需要我们拥有一个私有的镜像仓库用于管理我们自己的镜像。这个可以通过开源软件Registry来达成目的。 Registry在github上有两份代码：老代码库和新代码库。老代码是采用python编写的，存在pull和push的性能问题，出到0.9.1版本之后就标志为deprecated，不再继续开发。从2.0版本开始就到在新代码库进行开发，新代码库是采用go语言编写，修改了镜像id的生成算法、registry上镜像的保存结构，大大优化了pull和push镜像的效率。 官方在Docker hub上提供了registry的镜像（详情），我们可以直接使用该registry镜像来构建一个容器，搭建我们自己的私有仓库服务。Tag为latest的registry镜像是0.9.1版本的，我们直接采用2.1.1版本。 2.Registry的部署运行下面命令获取registry镜像 1$ sudo docker pull registry:2.1.1 然后启动一个容器 1$ sudo docker run -d -v /opt/data/registry/:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1 验证服务是否启动成功 说明我们已经启动了registry服务，打开浏览器输入http://127.0.0.1:5000/v2 3.验证向仓库中push镜像 现在我们通过将镜像push到registry来验证一下。我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库， 1$ sudo docker tag hello-world 127.0.0.1:5000/hello-world 接下来，我们运行docker push将hello-world镜像push到我们的私有仓库中， 1$ sudo docker push 127.0.0.1:5000/hello-world 1234567The push refers to a repository [127.0.0.1:5000/hello-world] (len: 1)975b84d108f1: Image successfully pushed3f12c794407e: Image successfully pushedlatest: digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b size: 2744 现在我们可以查看我们本地/opt/registry目录下已经有了刚推送上来的hello-world。我们也在浏览器中输入http://127.0.0.1:5000/v2/_catalog，如下图所示， 从镜像库中拉取镜像 现在我们可以先将我们本地的127.0.0.1:5000/hello-world和hello-world先删除掉， 123$ sudo docker rmi hello-world$ sudo docker rmi 127.0.0.1:5000/hello-world 然后使用docker pull从我们的私有仓库中获取hello-world镜像， 1$ sudo docker pull 127.0.0.1:5000/hello-world 123456789101112131415161718192021Using default tag: latestlatest: Pulling from hello-worldb901d36b6f2f: Pull complete0a6ba66e537a: Pull completeDigest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4bStatus: Downloaded newer image for 127.0.0.1:5000/hello-world:latestlienhua34@lienhua34-Compaq-Presario-CQ35-Notebook-PC ~ $ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEregistry 2.1.1 b91f745cd233 5 days ago 220.1 MBubuntu 14.04 a5a467fddcb8 6 days ago 187.9 MB127.0.0.1:5000/hello-world latest 0a6ba66e537a 2 weeks ago 960 B 4.查询镜像库查询镜像库中的镜像 1http://10.0.110.218:5000/v2/_catalog 5.错误排查错误描述 在push 到docker registry时，可能会报错： 123The push refers to a repository [192.168.1.100:5000/registry]Get https://192.168.1.100:5000/v1/_ping: http: server gave HTTP response to HTTPS client 这个问题可能是由于客户端采用https，docker registry未采用https服务所致。一种处理方式是把客户对地址“192.168.1.100:5000”请求改为http。 目前很多文章都是通过修改docker的配置文件“etc/systemconfig/docker”，重启docker来解决这个问题。但发现docker1.12.3版本并无此文件，根据网上创建此文件，并填入相应内容，重启docker无效果，仍然报此错误。 解决办法 在”/etc/docker/“目录下，创建”daemon.json“文件。在文件中写入： { “insecure-registries”:[“192.168.1.100:5000”] } 保存退出后，重启docker。]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[githubpage + hexo + yilia 搭建个人博客]]></title>
    <url>%2F2018%2F10%2F17%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2F%E5%8D%9A%E5%AE%A2%2Fgithub%2Bhexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[0.本博客的由来本来感觉写博客很费时间，但是最近感觉这两年攒手里的笔记太多了，不方便整理和分享 所以打算以后就干脆直接将笔记整理到github，这样也比自己维护一个博客省心 下面就将搭建本博客的步骤也同步上来，当做一个hello-world吧。 1.环境准备电脑环境是Windows，安装好git后，所有搭建操作均在git bash内完成 1）安装hexo(首先要安装git, node.js, npm) 注意：首次安装git 要配置user信息 123$git config --global user.name "yourname" #（yourname是git的用户名）$git config --global user.email email） 2）使用npm安装hexo 1$npm install -g hexo 3）创建hexo文件夹 12$mkdir hexo_blog$cd hexo_lobg 4）初始化框架 1234567$hexo init #hexo #会自动创建网站所需要的文件$npm install #安装依赖包$hexo generate $hexo server #现在可以用127.0.0.1:4000访问hexo默认的hello world界面 ,hexo s = hexo server 2.部署到github1）首次使用github需要配置密钥 1ssh-keygen -t rsa -C "email" 生成ssh密钥，按三次回车键，密码为空,这边会生成id_rsa和_rsa.pub文件 打开id_rsa.pub，复制全文添加到GitHub 的Add SSH key中。 2）创建Respository， 并开启githubPage 首先注册登录github,然后创建页面仓库，Repository name 命名应该是 youname.github.io 在setting界面， 配置 3）安装hexo-deployer-git 1$npm install hexo-deployer-git --save 用来推送项目到github 4）生成博客，并push到github 123$hexo generate$hexo deploy 5）验证结果 通过https://youname.github.io 进行访问 3.更换博客模板目前访问的博客模板比较简略，下面介绍使用：yilia模板 1）拉取模板文件 1$git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 2）更改配置文件修改模板为yilia 打开项目目录下的_config.yml文件，更改主题theme; theme: yilia然后配置yilia文件下的_config.yml（目录：hexo/themes/yilia/_config.yml） 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127# Headermenu: 主页: / 归档: /archives #分类: /categories #标签: /tags# SubNavsubnav: github: &quot;https://github.com/KyleAdultHub&quot; #weibo: &quot;#&quot; #rss: &quot;#&quot; #zhihu: &quot;#&quot; qq: &quot;/information&quot; #weixin: &quot;#&quot; #jianshu: &quot;#&quot; #douban: &quot;#&quot; #segmentfault: &quot;#&quot; #bilibili: &quot;#&quot; #acfun: &quot;#&quot; mail: &quot;/information&quot; #facebook: &quot;#&quot; #google: &quot;#&quot; #twitter: &quot;#&quot; #linkedin: &quot;#&quot; rss: /atom.xml# 是否需要修改 root 路径# 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，# 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。root: /# Content# 文章太长，截断按钮文字excerpt_link: more# 文章卡片右下角常驻链接，不需要请设置为falseshow_all_link: &apos;展开全文&apos;# 数学公式mathjax: false# 是否在新窗口打开链接open_in_new: false# 打赏# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏reward_type: 0# 打赏wordingreward_wording: &apos;谢谢你&apos;# 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpgalipay: # 微信二维码图片地址weixin: # 目录# 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录toc: 1# 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为falsetoc_hide_index: true# 目录为空时的提示toc_empty_wording: &apos;目录，不存在的…&apos;# 是否有快速回到顶部的按钮top: true# Miscellaneousbaidu_analytics: &apos;&apos;google_analytics: &apos;&apos;favicon: /favicon.png#你的头像urlavatar: /img/header.jpg#是否开启分享share_jia: true#评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment#不需要使用某项，直接设置值为false，或注释掉#具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/#1、多说duoshuo: false#2、网易云跟帖wangyiyun: false#3、畅言changyan_appid: *** #这个畅言id和conf写自己的changyan_conf: ***#4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的disqus: false#5、Gitmentgitment_owner: false #你的 GitHub IDgitment_repo: &apos;&apos; #存储评论的 repogitment_oauth: client_id: &apos;&apos; #client ID client_secret: &apos;&apos; #client secret# 样式定制 - 一般不需要修改，除非有很强的定制欲望…style: # 头像上面的背景颜色 header: &apos;#4d4d4d&apos; # 右滑板块背景 slider: &apos;linear-gradient(200deg,#a0cfe4,#e8c37e)&apos;# slider的设置slider: # 是否默认展开tags板块 showTags: false# 智能菜单# 如不需要，将该对应项置为false# 比如#smart_menu:# friends: falsesmart_menu: innerArchive: &apos;所有文章&apos; friends: &apos;友链&apos; aboutme: &apos;关于我&apos;friends: #友情链接1: http://localhost:4000/ aboutme: 程序猿一枚&lt;br&gt;]]></content>
      <categories>
        <category>开发工具</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Appium手机爬虫]]></title>
    <url>%2F2017%2F01%2F10%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAppium%E6%89%8B%E6%9C%BA%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[Appium的介绍和安装Appium的简介Appium是移动端的自动化测试工具，类似于前面所说的Selenium，利用它可以驱动Android、iOS等设备完成自动化测试，比如模拟点击、滑动、输入等操作Appium负责驱动移动端来完成一系列操作，对于iOS设备来说，它使用苹果的UIAutomation来实现驱动；对于Android来说，它使用UIAutomator和Selendroid来实现驱动。同时Appium也相当于一个服务器，我们可以向它发送一些操作指令，它会根据不同的指令对移动设备进行驱动，以完成不同的动作。Appium相关链接GitHub：https://github.com/appium/appium官方网站：http://appium.io官方文档：http://appium.io/introduction.html下载链接：https://github.com/appium/appium-desktop/releasesPython Client：https://github.com/appium/python-clientAppium Api：https://testerhome.comm/topics/3711Appium的安装通过Appium Desktop安装Appium Desktop介绍Appium Desktop支持全平台的安装，我们直接从GitHub的Releases里面安装即可链接为https://github.com/appium/appium-desktop/releases。目前的最新版本各平台的安装方法下载exe安装包appium-desktop-Setup-1.1.0.exeMac平台可以下载dmg安装包如appium-desktop-1.1.0.dmgLinux平台可以选择下载源码通过Node.js安装1.首先安装node.js2.npm install -g appiumpython-api安装pip install Appium-Python-Client开发环境的配置Android开发环境配置下载和配置Android Studio，下载地址为https://developer.android.com/studio/index.html?hl=zh-cn。下载后直接安装即可。打开Android Studio，直接打开首选项里面的Android SDK设置页面，勾选要安装的SDK版本，点击OK按钮即可下载和安装勾选的SDK版本。添加环境变量，添加ANDROID_HOME为Android SDK所在路径，然后再添加SDK文件夹下的tools和platform-tools文件夹到PATH中。更详细的配置可以参考Android Studio的官方文档：https://developer.android.com/studio/intro/index.html。Appium的使用(有界面版示例)启动Appium服务点击Appium输入主机和端口号，点击Start Server按钮即可启动Appium服务将手机连接到PC端将手机通过数据线和PC相连打开USB调试功能，确保PC可以连接到手机可以在PC端测试是否连接成功adb devices -l 查看连接的设备列表如果找不到adb命令，请检查Android开发环境和环境变量是否配置成功如果adb命令不显示设备信息，检查手机和PC的连接情况启动App并操作通过Appium内置驱动来操作打开app点击Appium中的Start New Session配置启动App时的参数platformName: 它是平台名称，需要区分Android或iOs，此处填AndroiddeviceName: 它是设备名称， 此处是手机的具体类型appPackage： 它是App程序的包名AppActivity: 它是入口Activity名，这里通常需要.开头点击保存按钮，可以将此配置进行保存点击Start Session，便会启动Android手机上的App，同时PC上会弹出一个调试窗口，里面包含了页面源码操作应用App选择元素和浏览器的调试一样，只要点击作业页面中的元素，对应的元素就会高亮，中间栏会显示对应的源码右侧是和元素包含的属性和事件等信息操作元素点击右侧元素的属性，便可以实现对元素的操作操作的录制点击中间栏的眼睛按钮，Appium会开始录制操作的动作，这时我们可以在窗口中操作APp的行为都会记录下来Recorder处可以自动生成对应的语言代码。通过Python代码来操作打开app(初始化应用)打开方法用字典来配置Desired Capabilities 参数，新建一个Session示例代码server = &quot;http://localhost:4723/wd/hub&quot;desired_caps = {&quot;platformName&quot;: &quot;Android&quot;,&quot;deviceName&quot;: &quot;MI_NOTE_Pro&quot;,&quot;appPackage&quot;: &quot;com.tencent.mm&quot;,&quot;appActivity&quot;: &quot;.ui.LauncherUI&quot;}from appium import Webdriverfrom selenium.webdriver.support.ui import WebDriverWaitdriver = wevdriver.Remote(server, desired_caps) # 启动app查找元素element = driver.find_element_by_android_uiautomator('new UiSelector().description(&quot;Animation&quot;)')elements = driver.find_elements_by_android_uiautomator('new UiSelector().description(&quot;Animation&quot;)') 点击操作多点点击方法driver.tap(self, positions, duration=None)positions: 它是点击的位置组成的列表duration: 它是点击持续的时间通过tap方法实现触摸点击该方法可以模拟手指点击，最多五个手指，也可以设置触摸的时间长短单点点击方法element.click() 对元素进行点击示例代码driver.tap([(100, 20), (100, 60), (100, 100)], 500)屏幕拖动元素间拖动方法driver.scroll(self, origin_el, destination_el)original_el: 拖动的起始元素destination_el:拖动的终止元素两点间的拖动方法driver.swipe(self, start_x, start_y, end_x, end_y, duration=None)start_x: 它是开始位置的横坐标start_y: 它是开始位置的纵坐标end_x: 它是终止位置的横坐标end_y: 它是终止位置的纵坐标duration: 它是持续时间，单位是毫秒两点间的快速的滑动driver.swipe()start_x: 它是开始位置的横坐标start_y: 它是开始位置的纵坐标end_x: 它是终止位置的横坐标end_y: 它是终止位置的纵坐标示例代码driver.scroll(el1, el2)driver.swipe(100, 100, 100, 400, 5000)driver.flick(100, 100, 100, 400)元素的拖拽拖拽方法driver.drag_and_drop(self, origin_el, destination_el)origin_el: 被拖拽的元素destination_el：拖拽的目标元素示例代码driver.drag_and_drop(el1, el2)文本的输入输入的方法ele.set_text('Hello')示例代码element = driver.find_element_by_id('name')element.set_text(&quot;Hello&quot;)动作链使用动作链的方法from appium import TouchAction()action = TouchAction()action.tap(element).perfoem()常用的动作链还有: tap、press、long_press、release、move_to、wait、cancel示例代码from appium import TouchAction()action = TouchAction()a_ele = self.driver.find_element_by_class_name(&quot;listView&quot;)action.press(a_ele).move_to(x=10, y=0), move_to(x=10, y=-600).release()action.perform]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mitmproxy&mitmdump手机爬虫]]></title>
    <url>%2F2017%2F01%2F06%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2Fmitmproxy%26mitmdump%E6%89%8B%E6%9C%BA%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[mitmproxy的介绍和安装配置mitmproxy的作用mitmproxy是一个支持HTTP和HTTPS的抓包程序，类似Fiddler、Charles的功能，只不过它通过控制台的形式操作。mitmproxy还有两个关联组件，一个是mitmdump，它是mitmproxy的命令行接口，利用它可以对接Python脚本，实现监听后的处理；另一个是mitmweb，它是一个Web程序，通过它以清楚地观察到mitmproxy捕获的请求。可以保存Http会话并进行分析模拟客户端发起请求，模拟服务器端返回的响应利用反向代理将流量发给指定的服务器支持Mac和Linux上的透明代理利用Python对Http请求和响应进行实时的处理mitmproxy和mitmdump的区别mitmproxy起到了代理服务的功能，手机和PC在同一个局域网内，可以将mitmproxy设置为手机的代理，这样数据都是通过mitmproxy妆发出去的，起到了中间人的作用mtmdump可以实现将抓取到的请求和响应直接交给某个Python程序进行处理，比如提取和入库操作mitmproxy相关链接GitHub：https://github.com/mitmproxy/mitmproxy官方网站：https://mitmproxy.orgPyPI：https://pypi.python.org/pypi/mitmproxy官方文档：http://docs.mitmproxy.orgmitmdump脚本：http://docs.mitmproxy.org/en/stable/scripting/overview.html下载地址：https://github.com/mitmproxy/mitmproxy/releasesDockerHub：https://hub.docker.com/r/mitmproxy/mitmproxymitmproxy的安装linux下使用pip安装pip3 install mitmproxy这是最简单和通用的安装方式，执行完毕之后即可完成mitmproxy的安装，另外还附带安装了mitmdump和mitmweb这两个组件。windows下安装获取安装包，下载地址: https://github.com/mitmproxy/mitmproxy/releases/在Windows上不支持mitmproxy的控制台接口，但是可以使用mitmdump和mitmweb。Linux下源码安装下载源码包，下载地址: https://github.com/mitmproxy/mitmproxy/releases/它包含了最新版本的mitmproxy和内置的Python 3环境，以及最新的OpenSSL环境tar -zxvf mitmproxy-2.0.2-linux.tar.gzsudo mv mitmproxy mitmdump mitmweb /usr/bin证书配置证书配置的说明对于mitmproxy来说，如果想要截获HTTPS请求，就需要设置证书。mitmproxy在安装后会提供一套CA证书，只要客户端信任了mitmproxy提供的证书，就可以通过mitmproxy获取HTTPS请求的具体内容，否则mitmproxy是无法解析HTTPS请求的。证书配置步骤启动mitmdumpmitmdump找到家目录.mitmproxy目录里面的CA证书mitmproxy-ca.pem PEM格式的证书私钥mitmproxy-ca-cert.pem PEM格式证书，适用于大多数非Windows平台mitmproxy-ca-cert.p12 PKCS12格式的证书，适用于Windows平台mitmproxy-ca-cert.cer 与mitmproxy-ca-cert.pem相同，只是改变了后缀，适用于部分Android平台mitmproxy-dhparam.pem PEM格式的秘钥文件，用于增强SSL安全性在各平台上配置证书的过程Windows平台1.双击mitmproxy-ca.p12，会出现导入证书的页面向导2.直接点击下一步，会出现密码设置的提示，这里不需要设置密码，直接点击下一步按钮即可3.接下来选择证书的存储区域。这里点击第二选项，将所有的证书都放入下列存储，然后点击浏览按钮，选择证书的存储位置为收信人的根证书颁发急购，接着点击确定按钮，点击下一步4.过程中出现安全警告直接点击是Android1.在Android手机上，需要将证书mitmproxy-ca-cert.pem文件发送到手机上，例如直接复制文件。2.接下来，点击证书，便会出现一个提示窗口，这时输入证书的名称，然后点击“确定”按钮即可完成安装。iOS1.将mitmproxy-ca-cert.pem文件发送到iPhone上，推荐使用邮件方式发送，然后在iPhone上可以直接点击附件并识别安装。点击“安装”按钮之后，会跳到安装描述文件的页面，点击“安装”按钮，此时会有警告提示，继续点击右上角的“安装”按钮，安装成功之后会有已安装的提示。如果你的iOS版本是10.3及以上版本，还需要在“设置”→“通用”→“关于本机”→“证书信任设置”将mitmproxy的完全信任开关打开。mitmproxy的使用启动mitmproxy服务mitmproxy 这样就会在8080端口上运行一个代理服务将mitmproxy设置为手机端的代理将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings发送请求在手机端进行网络请求，便可以在mitmproxy的界面上看到对应的请求查看/处理请求和响应查看请求的详情光标移动到对应的请求位置，点击ENTER在请求中查看Request/Response/Detail将光标移动到对应的分栏上，点击Tab重新编辑请求1.在请求中，点击e键进行编辑2.按照高亮的部分，选择想要编辑的内容(比如: q：修改请求方参数，m：修改请求方式)进入修改页面后,可以直接对内容进行修改点击a可以增加一行参数修改完成后点击Esc退出修改，q返回上一级页面3.敲击a进行修改的保存4.对修改后的请求重新发起请求mitdump的使用编写请求和响应的处理脚本日志输出介绍ctx模块提供了不同等级的log将会打印不一样的颜色使用示例from mitmproxy import ctx def request(flow):flow.request.headers['User-Agent'] = 'MitmProxy'ctx.log.info(&quot;info&quot;)ctx.log.warn(&quot;warn&quot;)ctx.log.error(&quot;error&quot;)Request对象处理介绍我们可以通过request() 方法实现对请求进行修改request对象包含的属性: url、headers、cookies、host、method、scheme、port使用示例def request(flow):request = flow.requestprint(request.url)Response对象处理介绍可以通过response() 方法实现对响应的操作，比如入库等操作response对象包含的属性: status_code、deaders、cookies、text使用示例def response(flow):response = flow.responseprint(response.text)启动mitmproxy服务mitmdump [OPTIONS]-w: 可以指定将接货的数据都保存到此文件中-s: 可以指定scripts.py 脚本文件，用来处理请求和响应，它需要放在当前命令的执行目录下将mitmdump设置为手机端的代理将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings发送请求在手机端进行网络请求，便可以在mitmdump的日志中便可以看见对应的请求]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据去重方法]]></title>
    <url>%2F2017%2F01%2F02%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2F%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%E6%96%B9%E6%B3%95-HashBloom%2F</url>
    <content type="text"><![CDATA[hash序列化过滤使用hash哈希方法去重的场景当数据量不大的时候，并且数据所占内存不多的时候当只有几万条url去重的时候，可以直接使用hash+redis的set类型进行数据的去重使用sha1-redis去重的实例 import hashlib import redis from redis import * class Filter_hash(object): # 创建redis客户端对象 sr = StrictRedis(host='localhost', port=6379, db=0) # 定义存储hash后数据的key_name key = 'hash_list' def add_hash_num(self, key_name，url): # 创建一个哈希对象 fp = hashlib.sha1() # 对url进行哈希序列化 fp.update(url) fp_num = fp.hexdiget() # 将十六进制后的序列化的hash数值进行存储 added = self.server.sadd(self.key, fp_num) return added # 如果插入成功， 返回1，表示数据不重复插入成功，否则返回0 filter_cur = Filter_hash() if __name__ == '__main__': filter_cur.add_hash_num('url'， 'https://www.baidu.com')bloom-布隆过滤什么是布隆过滤器是一种space efficient的概率模型数据结构，用于判断一个元素是否在集合中。一个空的布隆过滤器是一个m bit的bitmap，每一位都初始化为0。布隆过滤器定义有k个hash函数，对输入的数据生成k个hash值，定义一个map函数将k个hash值映射到bitmap的k个位。bitmap数据类型bitmap介绍Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32长度的位数组。位操作方法可以被分为两组：一、对单一位的操作，比如设置某一位为1或0，或者得到这一位的值；二、对一组位的操作，比方说计算一定范围内的1的个数（比如计数） bitmap的应用场景bitmap一个最大的优势是它通常能在存储信息的时候节省大量空间。比方说一个用增量ID来辨别用户的系统，可以用仅仅512MB的空间来标识40亿个用户是否想要接受通知。使用SETBIT和GETBIT命令来对位进行置数和检索（redis中实现的bitmap类型数据的操作）&gt; setbit key 10 1(integer) 1&gt; getbit key 10(integer) 1&gt; getbit key 11(integer) 0返回的是该位上之前的数值SETBIT 如上所示，意思是将第10位置位为1，第二个参数可为0或1。如果设置的位超出了当前String的长度，那么会自动增长。（最长2^32，下同） GETBIT 如上所示，返回第10位和第11位的数据，分别是1和0。如果查找的位超出了当前String的长度，那么会返回0。接下来是三个对一组位进行操作的命令: BITOP 执行不同字符串之间的逐位操作。所提供的操作有AND，OR，XOR和NOT。BITCOUNT BITCOUNT 计数,返回bitmap里值为1的位的个数. BITPOS 返回第一个0或1的位置 BITPOS和BITCOUNT不仅可以作用于整个bitmap，还可以作用于一定的范围,下面是一个BITCOUNT的例子布隆过滤的原理布隆过滤器需要的是一个位数组(和位图类似)和K个映射函数(和Hash表类似)，在初始状态时，对于长度为m的位数组array，它的所有位被置0 对于有n个元素的集合S={S1,S2...Sn},通过k个映射函数{f1,f2,......fk}，将集合S中的每个元素Sj(1&lt;=j&lt;=n)映射为K个值{g1,g2...gk}，然后再将位数组array中相对应的array[g1],array[g2]......array[gk]置为1： 如果要查找某个元素item是否在S中，则通过映射函数{f1,f2,...fk}得到k个值{g1,g2...gk}，然后再判断array[g1],array[g2]...array[gk]是否都为1，若全为1，则item在S中，否则item不在S中。这个就是布隆过滤器的实现原理。布隆过滤优点相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。布隆过滤器可以表示全集，其它任何数据结构都不能；k 和 m 相同，使用同一组 Hash 函数的两个布隆过滤器的交并差运算可以使用位操作进行。缺点误算率（False Positive），随着存入的元素数量增加，错判“在集合内”的概率就越大了，误算率随之增加。一般情况下不能从布隆过滤器中删除元素. 我们很容易想到把位列阵变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全的删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。布隆过滤器的应用场景在对大量的数据进行去重的时候， 可以使用布隆过滤器判断元素是否已经在集合中，通过判断的结果，来对数据进行相应的操作布隆过滤实现（包在下方）构造HashMap类这里新建了一个HashMap类。构造函数传入两个值，一个是m位数组的位数，另一个是种子值seed。不同的散列函数需要有不同的seed，这样可以保证不同的散列函数的结果不会碰撞。在hash()方法的实现中，value是要被处理的内容。这里遍历了value的每一位，并利用ord()方法取到每一位的ASCII码值，然后混淆seed进行迭代求和运算，最终得到一个数值。这个数值的结果就由value和seed唯一确定。我们再将这个数值和m进行按位与运算，即可获取到m位数组的映射结果，这样就实现了一个由字符串和seed来确定的散列函数。当m固定时，只要seed值相同，散列函数就是相同的，相同的value必然会映射到相同的位置。所以如果想要构造几个不同的散列函数，只需要改变其seed就好了。以上内容便是一个简易的散列函数的实现。构造BloomFilterBloom Filter里面需要用到k个散列函数，这里要对这几个散列函数指定相同的m值和不同的seed值由于我们需要亿级别的数据的去重，即前文介绍的算法中的n为1亿以上，散列函数的个数k大约取10左右的量级实现判断元素是否重复和添加元素到集合的方法insert方法Bloom Filter算法会逐个调用散列函数对放入集合中的元素进行运算，得到在m位位数组中的映射位置，然后将位数组对应的位置置1。这里代码中我们遍历了初始化好的散列函数，然后调用其hash()方法算出映射位置offset，再利用Redis的setbit()方法将该位置1。exit方法我们要实现判定是否重复的逻辑，方法参数value为待判断的元素。我们首先定义一个变量exist，遍历所有散列函数对value进行散列运算，得到映射位置，用getbit()方法取得该映射位置的结果，循环进行与运算。这样只有每次getbit()得到的结果都为1时，最后的exist才为True，即代表value属于这个集合。如果其中只要有一次getbit()得到的结果为0，即m位数组中有对应的0位，那么最终的结果exist就为False，即代表value不属于这个集合。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fiddler Android配置]]></title>
    <url>%2F2016%2F12%2F28%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2Ffiddler%E9%85%8D%E7%BD%AEAndroid%E6%89%8B%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Fiddler for Android / Google Nexus 7Configure FiddlerClick Tools &gt; Fiddler Options &gt; Connections.Ensure that the checkbox by Allow remote computers to connect is checked.If you check the box, restart Fiddler.Hover over the Online indicator at the far right of the Fiddler toolbar to display the IP address of the Fiddler server.Configure Nexus DeviceSwipe down from the top of the screen and tap the Settings icon.Tap Wi-Fi.Tap and hold your current Wi-Fi network. Select Modify Network.Tap the Show advanced options box.Tap the Proxy settings dropdown and select Manual.Type the IP address and port (usually 8888) of the Fiddler server.Tap Save.To verify this configuration, go to http://ipv4.fiddler:8888/. Chrome should display the Fiddler Echo Service webpage, and the traffic should appear in Fiddler.Disable the proxyAfter using Fiddler, return to the Proxy Settings screen above and remove the proxy.Decrypt HTTPSOn the Fiddler Echo Service Webpage, click the FiddlerRoot Certificate link.If the download doesn't open automatically, swipe down from the top and tap the Settings icon.Tap Personal &gt; Security.Under Credential Storage, tap Install from storage.Tap the FiddlerRoot.cer file.(Optional) Type a name for the certificate.To verify this configuration, tap Trusted credentials &gt; User. This should display the Fiddler certificate.Disable HTTPS DecryptionTo delete the FiddlerRoot certificate, tap Trusted credentials &gt; User and delete the certificate.]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fiddler ios配置]]></title>
    <url>%2F2016%2F12%2F28%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2Ffiddler%E9%85%8D%E7%BD%AEios%E6%89%8B%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[Capture Traffic from iOS Device Configure Fiddler Click Tools &gt; Fiddler Options &gt; Connections. Click the checkbox by Allow remote computers to connect. Restart Fiddler. Ensure your firewall allows incoming connections to the Fiddler process. Hover over the Online indicator at the far right of the Fiddler toolbar to display the IP addresses assigned to Fiddler's machine. Verify client iOS device can reach Fiddler by navigating in the browser to http://FiddlerMachineIP:8888. This address should return the Fiddler Echo Service page. For iPhone: Disable the 3g/4g connection. Set the iOS Device Proxy Tap Settings &gt; General &gt; Network &gt; Wi-Fi. Tap the settings for the Wi-Fi network. Tap the Manual option in the HTTP Proxy section. In the Server box, type the IP address or hostname of your Fiddler instance. In the Port box, type the port Fiddler is listening on (usually 8888). Ensure the Authentication slider is set to Off. Decrypt HTTPS Traffic from iOS Devices Download the Certificate Maker plugin for Fiddler. Install the Certificate Maker plugin. Restart Fiddler. Configure the device where Fiddler is installed to trust Fiddler root certificate. On the iOS device, go to http://ipv4.fiddler:8888/ in a browser. From the bottom of the Fiddler Echo Service webpage, download the FiddlerRoot certificate. Open the FiddlerRoot.cer file. Tap the Install button. Tap the Install button again. Uninstall FiddlerRoot Certificate If you decide to uninstall the root certificate: Tap the Settings app. Tap General. Scroll to Profiles. Tap the DO_NOT_TRUST_FiddlerRoot* profile. Tap Remove.]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fiddler 抓包工具安装、配置]]></title>
    <url>%2F2016%2F12%2F28%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FFiddler%20%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[使用前配置安装证书(配置HTTPS网站的抓取证书)点击工具栏tools点击Telerlk FiddlerOptions点击HTTPS选项点击右上角的Actions--&gt;Trust Root Certificate 软件界面手机app抓包方法将Fiddler设置为代理点击工具栏tools----&gt;Connections勾选 Allow remote computers to connect端口号设置为8888（可以设置其他）配置手机端的证书配置方法见其他两篇介绍配置移动端的官方文档：http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureForiOS使用手机访问网页]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Splash爬取动态HTML]]></title>
    <url>%2F2016%2F12%2F26%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2FSplash%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81HTML%2F</url>
    <content type="text"><![CDATA[Splash的功能介绍和运行Splash的简介Splash是一个JavaScript渲染服务，是一个带有HTTP API的轻量级浏览器，同时它对接了Python中的Twisted和QT库。利用它，我们同样可以实现动态渲染页面的抓取。Splash文档Splash官方文档https://splash.readthedocs.io/en/stable/scripting-ref.htmlSplash-api官方文档https://splash.readthedocs.io/en/stable/api.htmlSplash的功能异步方式处理多个网页渲染过程；获取渲染后的页面的源代码或截图；通过关闭图片渲染或者使用Adblock规则来加快页面渲染速度；可执行特定的JavaScript脚本；可通过Lua脚本来控制页面渲染过程；获取渲染的详细过程并通过HAR（HTTP Archive）格式呈现。Splash服务的运行docker run -d -p 8050:8050 scrapinghub/splash打开http://localhost:8050/即可看到其Web页面Splash-Lua脚本介绍Splash的运行机制Splash加载和操作渲染过程，可以理解是由一段Lua代码来操控Splash留出来了一个main() 函数的接口，我们可以通过编写该段函数，操作其传入的splash对象，Splash会调用其指令并执行对应的操作可以在浏览器界面编写并点击render me来启动，也可以通过调用splash-api的方式来调用Splash Lua脚本示例function main(splash, args)assert(splash:go(args.url)) 请求urlassert(splash:wait(0.5)) 等待0.5sreturn { 返回结果html = splash:html(), 获取页面源码png = splash:png(), 获取页面截图har = splash:har(), 获取请求的详细har信息}endSplash异步处理Splash异步的介绍在脚本内调用的wait()方法类似于Python中的sleep()，其参数为等待的秒数。当Splash执行到此方法时，它会转而去处理其他任务，然后在指定的时间过后再回来继续处理。另外，这里做了加载时的异常检测。go()方法会返回加载页面的结果状态，如果页面出现4xx或5xx状态码，ok变量就为空Splash异步脚本示例function main(splash, args)local example_urls = {&quot;www.baidu.com&quot;, &quot;www.taobao.com&quot;, &quot;www.zhihu.com&quot;}local urls = args.urls or example_urlslocal results = {}for index, url in ipairs(urls) dolocal ok, reason = splash:go(&quot;http://&quot; .. url)if ok thensplash:wait(2)results[url] = splash:png()endendreturn resultsendSplash负载均衡的配置搭建多个splash服务节点docker run -d -p 8050:8050 scrapinghub/splash配置负载均衡服务(使用nginx)nginx配置节点 http { upstream splash { least_conn; server 41.159.27.223:8050 weight=4; server 41.159.27.221:8050 weight=2; server 41.159.27.9:8050 weight=2; server 41.159.117.119:8050 weight=1; } server { listen 8050; location / { proxy_pass http://splash; } } }nginx配置认证现在Splash是可以公开访问的，如果不想让其公开访问，还可以配置认证，这仍然借助于Nginx。可以在server的location字段中添加auth_basic和auth_basic_user_file字段，具体配置如下：这里使用的用户名和密码配置放置在/etc/nginx/conf.d目录下，我们需要使用htpasswd命令创建。例如，创建一个用户名为admin的文件，相关命令如下htpasswd -c .htpasswd admin接下来就会提示我们输入密码，输入两次之后，就会生成密码文件，其内容如下：配置完成后，重启一下Nginx服务： sudo nginx -s reload server { listen 8050; location / { proxy_pass http://splash; auth_basic &quot;Restricted&quot;; auth_basic_user_file /etc/nginx/conf.d/.htpasswd; } }Splash对象的常用属性加载参数获取方法一 function main(splash, args) local url = args.url end获取方法二 function main(splash) local url = splash.args.url end开启关闭javascriptsplash.js_enabled = false/true设置加载的超时时间splash.resource_timeout = 2单位是秒。如果设置为0或nil（类似Python中的None），代表不检测超时。禁止加载图片splash.images_enabled = true/false禁止开启插件(flash等)splash.plugins_enabled = true/false滚动条splash.scroll_position = {x=100, y=400}滚动条向右滚动100像素，向下滚动400像素Splash对象常用方法请求某链接使用方法splash:go{url, baseurl=nil, headers=nil, http_method=&quot;GET&quot;, body=nil, formdata=nil}url：请求的URL。baseurl：可选参数，默认为空，表示资源加载相对路径。headers：可选参数，默认为空，表示请求头。http_method：可选参数，默认为GET，同时支持POST。body：可选参数，默认为空，发POST请求时的表单数据，使用的Content-type为application/json。formdata：可选参数，默认为空，POST的时候的表单数据，使用的Content-type为application/x-www-form-urlencoded。return: 结果ok和原因reason的组合，如果ok为空，代表网页加载出现了错误，此时reason变量中包含了错误的原因使用示例 function main(splash, args) local ok, reason = splash:go{&quot;http://httpbin.org/post&quot;, http_method=&quot;POST&quot;, body=&quot;name=Germey&quot;} if ok then return splash:html() end end设置定时任务使用示例(0.2s后获取网页截图) function main(splash, args) local snapshots = {} local timer = splash:call_later(function() snapshots[&quot;a&quot;] = splash:png() splash:wait(1.0) snapshots[&quot;b&quot;] = splash:png() end, 0.2) splash:go(&quot;https://www.taobao.com&quot;) splash:wait(3.0) return snapshots end模拟发送get请求使用方法response = splash:http_get{url, headers=nil, follow_redirects=true}url：请求URL。headers：可选参数，默认为空，请求头。follow_redirects：可选参数，表示是否启动自动重定向，默认为true使用示例 function main(splash, args) local treat = require(&quot;treat&quot;) local response = splash:http_get(&quot;http://httpbin.org/get&quot;) return { html=treat.as_string(response.body), url=response.url, status=response.status } end模拟发送post请求使用方法response = splash:http_post{url, headers=nil, follow_redirects=true, body=nil}url：请求URL。headers：可选参数，默认为空，请求头。follow_redirects：可选参数，表示是否启动自动重定向，默认为true。body：可选参数，即表单数据，默认为空。使用示例 function main(splash, args) local treat = require(&quot;treat&quot;) local json = require(&quot;json&quot;) local response = splash:http_post{&quot;http://httpbin.org/post&quot;, body=json.encode({name=&quot;Germey&quot;}), headers={[&quot;content-type&quot;]=&quot;application/json&quot;} } return { html=treat.as_string(response.body), url=response.url, status=response.status } end获取页面加载过程描述等信息使用方法splash:har() 获取加载过程详细信息splash:url() 获取正在访问的url页面cookie相关操作使用方法splash:get_cookies() 获取当前页面的cookie cookies = splash:add_cookie{name, value, path=nil, domain=nil, expires=nil, httpOnly=nil, secure=nil} 给页面添加cookiesplash:clear_cookies() 清空当前页面所有cookie设置页面大小使用方法splash:get_viewport_size() 获取页面宽高splash:set_viewport_size(width, height) 设置页面宽高splash:set_viewport_full() 设置页面铺满设置User-Agent使用示例 function main(splash) splash:set_user_agent('Splash') splash:go(&quot;http://httpbin.org/get&quot;) return splash:html() end设置请求头使用示例 function main(splash) splash:set_custom_headers({ [&quot;User-Agent&quot;] = &quot;Splash&quot;, [&quot;Site&quot;] = &quot;Splash&quot;, }) splash:go(&quot;http://httpbin.org/get&quot;) return splash:html() end节点操作使用方法element = splash:select(&quot;#kw&quot;) 通过css选择器选择元素, 返回选中的第一个元素element = splash:select_all(&quot;#kw&quot;) 通过css选择器选择元素, 返回选中所有元素element.send_text('test') 向节点输入文本element.mouse_click() 点击节点等待wait使用方法ok, reason = splash:wait{time, cancel_on_redirect=false, cancel_on_error=true}time：等待的秒数。cancel_on_redirect：可选参数，默认为false，表示如果发生了重定向就停止等待，并返回重定向结果。cancel_on_error：可选参数，默认为false，表示如果发生了加载错误，就停止等待。return: 结果ok和原因reason的组合。使用示例 function main(splash) splash:go(&quot;https://www.taobao.com&quot;) splash:wait(2) return {html=splash:html()} end调用JavaScript定义的方法使用示例 function main(splash, args) local get_div_count = splash:jsfunc([[ function () { var body = document.body; var divs = body.getElementsByTagName('div'); return divs.length; } ]]) splash:go(&quot;https://www.baidu.com&quot;) return (&quot;There are %s DIVs&quot;):format(get_div_count()) end执行JavaScript代码(表达式类)使用示例local title = splash:evaljs(&quot;document.title&quot;) 执行JavaScript代码并返回最后一条JavaScript语句的返回结果执行JavaScript代码(声明类)使用示例 function main(splash, args) splash:go(&quot;https://www.baidu.com&quot;) splash:runjs(&quot;foo = function() { return 'bar' }&quot;) local result = splash:evaljs(&quot;foo()&quot;) return result end加载js代码使用方法此方法只负责加载JavaScript代码或库，不执行任何操作。如果要执行操作，可以调用evaljs()或runjs()ok, reason = splash:autoload{source_or_url, source=nil, url=nil}source_or_url：JavaScript代码或者JavaScript库链接。source：JavaScript代码。url：JavaScript库链接使用示例 function main(splash, args) splash:autoload([[ function get_document_title(){ return document.title; } ]]) splash:go(&quot;https://www.baidu.com&quot;) return splash:evaljs(&quot;get_document_title()&quot;) end页面内容的操作方法使用方法splash:set_content(&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;hello&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&quot;) 添加页面内容splash:html() 获取当前页面源码获取png/jpeg格式的页面截图使用方法splash:png()splash:jpeg()设置代理使用示例 function main(splash, args) request = splash:on_request(function(request) request:set_proxy{host, port, username=nil, password=nil, type='HTTP/SOCKS5') end) endSplash-API的使用Splash API的介绍Splash Lua脚本的用法，但这些脚本是在Splash页面中测试运行的Splash API就是我们可以通过python程序使用Splash服务的方法Splash给我们提供了一些HTTP API接口，我们只需要请求这些接口并传递相应的参数即可常用Splash APIrender.png接口介绍通过width和height来控制宽高，它返回的是PNG格式的图片二进制数据。示例如下常用参数width、height、render_all使用示例 import requests url = 'http://localhost:8050/render.png?url=https://www.jd.com&amp;wait=5&amp;width=1000&amp;height=700' response = requests.get(url) with open('taobao.png', 'wb') as f: f.write(response.content)render.html接口介绍此接口用于获取JavaScript渲染的页面的HTML代码，接口地址就是Splash的运行地址加此接口名称常用参数url 、baseurl、timeout、resource_timeout、wait、proxy、js、js_source、filter、allowed_dominallowed_content_types、viewport、images、headers、body、heep_method、save_args、load_args使用示例 import requests url = 'http://localhost:8050/render.html?url=https://www.baidu.com' response = requests.get(url) print(response.text)render.har接口介绍此接口用于获取页面加载的HAR数据使用示例curl http://localhost:8050/render.har?url=https://www.jd.com&amp;wait=5render.json接口介绍此接口包含了前面接口的所有功能，返回结果是JSON格式以JSON形式返回了相应的请求数据可以通过传入不同参数控制其返回结果。比如，传入html=1，返回结果即会增加源代码数据；传入png=1，返回结果即会增加页面PNG截图数据；传入har=1，则会获得页面HAR数据使用示例curl http://localhost:8050/render.json?url=https://httpbin.orgexecute接口介绍用此接口便可实现与Lua脚本的对接可以用该接口，通过原生的Lua脚本来调用splash可以通过lua_source参数传递Lua脚本给splash服务器进行请求和渲染服务常用参数timeout、allowed_domains、proxy、filters、save_args、load_args使用示例 import requests from urllib.parse import quote lua = ''' function main(splash) return 'hello' end ''' url = 'http://localhost:8050/execute?lua_source=' + quote(lua) response = requests.get(url) print(response.text)]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Puppeteer]]></title>
    <url>%2F2016%2F12%2F25%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2FPuppeteer%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81HTML%2F</url>
    <content type="text"><![CDATA[Puppeteer简介Puppeteer介绍Puppeteer 是一个node库，他提供了一组用来操纵Chrome的API, 通俗来说就是一个 headless chrome浏览器 (当然你也可以配置成有UI的，默认是没有的)。既然是浏览器，那么我们手工可以在浏览器上做的事情 Puppeteer 都能胜任, 另外，Puppeteer 翻译成中文是”木偶”意思，所以听名字就知道，操纵起来很方便，你可以很方便的操纵她去实现：运行环境1. Nodejs 的版本不能低于 v7.6.0, 需要支持 async, await.2. 需要最新的 chrome driver, 这个你在通过 npm 安装 Puppeteer 的时候系统会自动下载的npm install puppeteer3.安装pyppeteerpip install pyppeteerPuppeteer的官方文档https://github.com/miyakogi/pyppeteer知识前提Puppeteer 几乎所有的操作都是 异步的, 为了使用大量的 then 使得代码的可读性降低，本文所有 demo 代码都是用 async, await 方式实现由于文档上大量使用async和await关键字, 需要提前了解Async/Await 异步编程Puppeteer的基本用法python示例1. 先通过launch() 创建一个浏览器实例 Browser 对象2. 然后通过 Browser 对象创建页面 Page 对象3. 然后 page.goto() 跳转到指定的页面4. 调用 page.screenshot() 对页面进行截图5. 关闭浏览器Puppeteer的常用属性/方法/对象launch(options)方法介绍使用launch 方法会返回一个browser对象, 创建browser对象的时候可以选择性的配置如下选项常用参数Browser对象介绍Browser对象相当于selenium的driver对象, 常用的有如下方法当返回值是promise的时候, 在python中返回的是异步函数, 需要这样的函数需要使用await关键字调用常用方法browser.userAgent() 设置browser的请求头Page对象介绍Page对象相当于selenium的每一个标签页我们对页面的元素进行操作就是操作page对象常用方法常用方法page.content() 获取页面的源码page.cookies(...urls) 获取当前的cookiespage.goto() 请求某个页面page.click(selector[, options]) 点击元素page.focus(selector) 聚焦某元素page.close(options) 关闭当前标签页page.url() 当前页面的urlpage.reload(options) 重新加载页面page.type(selector, text[, options]) 向聚焦的元素框中输入内容获取元素Page.quirySelector() 选择一个元素Page.querySelectorAll() 选择一组元素Page.xpath() 通过xpath选择元素evaluate()方法(运行js脚本/获取元素的属性)evaluate接收字符串为参数， 字符串的内容可以使javascript的原生函数， 或者是表达式， 当字符串的内容是表达式的时候我们通常可以用来获取元素的属性， 当字符串的内容是js函数的时候通常可以用来在当前页面执行js代码当字符串的内容是表达式的时候可以用force_expr属性来强调, 不然可能会识别失败报错示例：修改driver的配置1. Page.setViewport() 修改浏览器视窗大小2. Page.setUserAgent() 设置浏览器的 UserAgent 信息3. Page.emulateMedia() 更改页面的CSS媒体类型，用于进行模拟媒体仿真。 可选值为 “screen”, “print”, “null”, 如果设置为 null 则表示禁用媒体仿真。键盘keyboardkeyboard.down(key[, options]) 触发 keydown 事件keyboard.press(key[, options]) 按下某个键，key 表示键的名称，比如 ‘ArrowLeft’ 向左键，详细的键名映射请戳这里keyboard.sendCharacter(char) 输入一个字符keyboard.type(text, options) 输入一个字符串keyboard.up(key) 触发 keyup 事件鼠标mousemouse.click(x, y, [options]) 移动鼠标指针到指定的位置，然后按下鼠标，这个其实 mouse.move 和 mouse.down 或 mouse.up 的快捷操作mouse.down([options]) 触发 mousedown 事件，options 可配置:options.button 按下了哪个键，可选值为[left, right, middle], 默认是 left, 表示鼠标左键options.clickCount 按下的次数，单击，双击或者其他次数delay 按键延时时间mouse.move(x, y, [options]) 移动鼠标到指定位置， options.steps 表示移动的步长mouse.up([options]) 触发 mouseup 事件waitFor()等待方法page.waitFor(selectorOrFunctionOrTimeout[, options[, …args]]) 下面三个的综合 APIpage.waitForFunction(pageFunction[, options[, …args]]) 等待 pageFunction 执行完成之后page.waitForNavigation(options) 等待页面基本元素加载完之后，比如同步的 HTML, CSS, JS 等代码page.waitForSelector(selector[, options]) 等待某个选择器的元素加载之后，这个元素可以是异步加载的，这个 API 非常有用，你懂的。事件的监听使用方法Page.on(&quot;event&quot;, callback)event: 事件名称callback: 监听到事件后执行的回调函数所有可以监听的事件官方文档介绍的Python使用Puppeteer的区别Keyword arguments for optionsPuppeteer uses object (dictionary in python) for passing options to functions/methods. Pyppeteer accepts both dictionary and keyword arguments for options.Dictionary style option (similar to puppeteer):browser = await launch({'headless': True})Keyword argument style option (more pythonic, isn't it?):browser = await launch(headless=True)Element selector method name ($ -&gt; querySelector)In python, $ is not usable for method name. So pyppeteer usesPage.querySelector()/Page.querySelectorAll()/Page.xpath() instead of Page.$()/Page.$$()/Page.$x(). Pyppeteer also has shorthands for these methods, Page.J(), Page.JJ(), and Page.Jx().Arguments of Page.evaluate() and Page.querySelectorEval()Puppeteer's version of evaluate() takes JavaScript raw function or string of JavaScript expression, but pyppeteer takes string of JavaScript. JavaScript strings can be function or expression. Pyppeteer tries to automatically detect the string is function or expression, but sometimes it fails. If expression string is treated as function and error is raised, add force_expr=True option, which force pyppeteer to treat the string as expression.Example to get page content:content = await page.evaluate('document.body.textContent', force_expr=True)Example to get element's inner text:element = await page.querySelector('h1')title = await page.evaluate('(element) =&gt; element.textContent', element)python实现puppeteer同步使用库github ： https://github.com/issacLiuUp/puppeteer]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Crontab定时执行爬虫]]></title>
    <url>%2F2016%2F12%2F25%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FCrontab%E5%AE%9A%E6%97%B6%E6%89%A7%E8%A1%8C%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[Crontab的使用方法(ubuntu为例)安装cron软件apt-get install cron编辑crontab定时执行命令进入crontab编辑界面crontab -e 进入编辑界面crontab -l 查看当前的定时任务crontab -r 删除任务编辑需要被定时执行的命令编辑的格式分(0-59) 小时(0-23) 日(1-31) 月(1-12) 星期(0-6) 命令(command)示例30 7 8 * * ls 指定每月的8日的7:30执行ls命令*/15 * * * * ls 每15分钟执行一次ls命令0 */2 * * * ls 每隔两个小时执行一次ls注意点* /num 代表每隔多长时间的意思当一个位置使用每隔符号的时候，其前边的时间位置，不能为*星期中0表示周日使用Crontab定时爬虫编辑爬虫脚本先定义好爬虫的启动脚本， 以start.sh为例给.sh文件添加可执行权限chmod + start.sh在crontab中编辑脚本文件执行时间0 6 * * * ${SPIDER_DIR}/start.sh &gt;&gt; /dev/null 2&gt;&amp;1]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 爬虫工具列表]]></title>
    <url>%2F2016%2F12%2F23%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2FPython%20%E7%88%AC%E8%99%AB%E7%9A%84%E5%B7%A5%E5%85%B7%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Python 爬虫的工具列表爬虫网络库通用urllib -网络库(stdlib)。requests -网络库。grab – 网络库(基于pycurl)。pycurl – 网络库(绑定libcurl)。urllib3 – Python HTTP库，安全连接池、支持文件post、可用性高。httplib2 – 网络库。RoboBrowser – 一个简单的、极具Python风格的Python库，无需独立的浏览器即可浏览网页。MechanicalSoup – 一个与网站自动交互Python库。mechanize -有状态、可编程的Web浏览库。socket – 底层网络接口(stdlib)。Unirest for Python – Unirest是一套可用于多种语言的轻量级的HTTP库。hyper – Python的HTTP/2客户端。PySocks – SocksiPy更新并积极维护的版本，包括错误修复和一些其他的特征。作为socket模块的直接替换。异步treq – 类似于requests的API(基于twisted)。aiohttp – asyncio的HTTP客户端/服务器(PEP-3156)。网络爬虫框架功能齐全的爬虫grab – 网络爬虫框架(基于pycurl/multicur)。scrapy – 网络爬虫框架(基于twisted)。pyspider – 一个强大的爬虫系统。cola – 一个分布式爬虫框架。其他portia – 基于Scrapy的可视化爬虫。restkit – Python的HTTP资源工具包。它可以让你轻松地访问HTTP资源，并围绕它建立的对象。demiurge – 基于PyQuery的爬虫微框架。HTML/XML解析器通用lxml – C语言编写高效HTML/ XML处理库。支持XPath。cssselect – 解析DOM树和CSS选择器。pyquery – 解析DOM树和jQuery选择器。BeautifulSoup – 低效HTML/ XML处理库，纯Python实现。html5lib – 根据WHATWG规范生成HTML/ XML文档的DOM。该规范被用在现在所有的浏览器上。feedparser – 解析RSS/ATOM feeds。MarkupSafe – 为XML/HTML/XHTML提供了安全转义的字符串。xmltodict – 一个可以让你在处理XML时感觉像在处理JSON一样的Python模块。xhtml2pdf – 将HTML/CSS转换为PDF。untangle – 轻松实现将XML文件转换为Python对象。清理Bleach – 清理HTML(需要html5lib)。sanitize – 为混乱的数据世界带来清明。文本处理用于解析和操作简单文本的库。通用difflib – (Python标准库)帮助进行差异化比较。Levenshtein – 快速计算Levenshtein距离和字符串相似度。fuzzywuzzy – 模糊字符串匹配。esmre – 正则表达式加速器。ftfy – 自动整理Unicode文本，减少碎片化。转换unidecode – 将Unicode文本转为ASCII。字符编码uniout – 打印可读字符，而不是被转义的字符串。chardet – 兼容 Python的2/3的字符编码器。xpinyin – 一个将中国汉字转为拼音的库。pangu.py – 格式化文本中CJK和字母数字的间距。Slug化awesome-slugify – 一个可以保留unicode的Python slugify库。python-slugify – 一个可以将Unicode转为ASCII的Python slugify库。unicode-slugify – 一个可以将生成Unicode slugs的工具。pytils – 处理俄语字符串的简单工具(包括pytils.translit.slugify)。通用解析器PLY – lex和yacc解析工具的Python实现。pyparsing – 一个通用框架的生成语法分析器。人的名字python-nameparser -解析人的名字的组件。电话号码phonenumbers -解析，格式化，存储和验证国际电话号码。用户代理字符串python-user-agents – 浏览器用户代理的解析器。HTTP Agent Parser – Python的HTTP代理分析器。特定格式文件处理解析和处理特定文本格式的库。通用tablib – 一个把数据导出为XLS、CSV、JSON、YAML等格式的模块。textract – 从各种文件中提取文本，比如 Word、PowerPoint、PDF等。messytables – 解析混乱的表格数据的工具。rows – 一个常用数据接口，支持的格式很多(目前支持CSV，HTML，XLS，TXT – 将来还会提供更多!)。Officepython-docx – 读取，查询和修改的Microsoft Word2007/2008的docx文件。xlwt / xlrd – 从Excel文件读取写入数据和格式信息。XlsxWriter – 一个创建Excel.xlsx文件的Python模块。xlwings – 一个BSD许可的库，可以很容易地在Excel中调用Python，反之亦然。openpyxl – 一个用于读取和写入的Excel2010 XLSX/ XLSM/ xltx/ XLTM文件的库。Marmir – 提取Python数据结构并将其转换为电子表格。PDFPDFMiner – 一个从PDF文档中提取信息的工具。PyPDF2 – 一个能够分割、合并和转换PDF页面的库。ReportLab – 允许快速创建丰富的PDF文档。pdftables – 直接从PDF文件中提取表格。MarkdownPython-Markdown – 一个用Python实现的John Gruber的Markdown。Mistune – 速度最快，功能全面的Markdown纯Python解析器。markdown2 – 一个完全用Python实现的快速的Markdown。YAMLPyYAML – 一个Python的YAML解析器。CSScssutils – 一个Python的CSS库。ATOM/RSSfeedparser – 通用的feed解析器。SQLsqlparse – 一个非验证的SQL语句分析器。HTTPhttp-parser – C语言实现的HTTP请求/响应消息解析器。微格式opengraph – 一个用来解析Open Graph协议标签的Python模块。可移植的执行体pefile – 一个多平台的用于解析和处理可移植执行体(即PE)文件的模块。PSDpsd-tools – 将Adobe Photoshop PSD(即PE)文件读取到Python数据结构。自然语言处理处理人类语言问题的库。NLTK -编写Python程序来处理人类语言数据的最好平台。Pattern – Python的网络挖掘模块。他有自然语言处理工具，机器学习以及其它。TextBlob – 为深入自然语言处理任务提供了一致的API。是基于NLTK以及Pattern的巨人之肩上发展的。jieba – 中文分词工具。SnowNLP – 中文文本处理库。loso – 另一个中文分词库。genius – 基于条件随机域的中文分词。langid.py – 独立的语言识别系统。Korean – 一个韩文形态库。pymorphy2 – 俄语形态分析器(词性标注+词形变化引擎)。PyPLN – 用Python编写的分布式自然语言处理通道。这个项目的目标是创建一种简单的方法使用NLTK通过网络接口处理大语言库。浏览器自动化与仿真selenium – 自动化真正的浏览器(Chrome浏览器，火狐浏览器，Opera浏览器，IE浏览器)。Ghost.py – 对PyQt的webkit的封装(需要PyQT)。Spynner – 对PyQt的webkit的封装(需要PyQT)。Splinter – 通用API浏览器模拟器(selenium web驱动，Django客户端，Zope)。多重处理threading – Python标准库的线程运行。对于I/O密集型任务很有效。对于CPU绑定的任务没用，因为python GIL。multiprocessing – 标准的Python库运行多进程。celery – 基于分布式消息传递的异步任务队列/作业队列。concurrent-futures – concurrent-futures 模块为调用异步执行提供了一个高层次的接口。异步异步网络编程库asyncio – (在Python 3.4 +版本以上的 Python标准库)异步I/O，时间循环，协同程序和任务。Twisted – 基于事件驱动的网络引擎框架。Tornado – 一个网络框架和异步网络库。pulsar – Python事件驱动的并发框架。diesel – Python的基于绿色事件的I/O框架。gevent – 一个使用greenlet 的基于协程的Python网络库。eventlet – 有WSGI支持的异步框架。Tomorrow – 异步代码的奇妙的修饰语法。队列celery – 基于分布式消息传递的异步任务队列/作业队列。huey – 小型多线程任务队列。mrq – Mr. Queue – 使用redis &amp; Gevent 的Python分布式工作任务队列。RQ – 基于Redis的轻量级任务队列管理器。simpleq – 一个简单的，可无限扩展，基于Amazon SQS的队列。python-gearman – Gearman的Python API。云计算picloud – 云端执行Python代码。dominoup.com – 云端执行R，Python和matlab代码。电子邮件电子邮件解析库flanker – 电子邮件地址和Mime解析库。Talon – Mailgun库用于提取消息的报价和签名。网址和网络地址操作解析/修改网址和网络地址库。URLfurl – 一个小的Python库，使得操纵URL简单化。purl – 一个简单的不可改变的URL以及一个干净的用于调试和操作的API。urllib.parse – 用于打破统一资源定位器(URL)的字符串在组件(寻址方案，网络位置，路径等)之间的隔断，为了结合组件到一个URL字符串，并将“相对URL”转化为一个绝对URL，称之为“基本URL”。tldextract – 从URL的注册域和子域中准确分离TLD，使用公共后缀列表。网络地址netaddr – 用于显示和操纵网络地址的Python库。网页内容提取提取网页内容的库。HTML页面的文本和元数据newspaper – 用Python进行新闻提取、文章提取和内容策展。html2text – 将HTML转为Markdown格式文本。python-goose – HTML内容/文章提取器。lassie – 人性化的网页内容检索工具micawber – 一个从网址中提取丰富内容的小库。sumy -一个自动汇总文本文件和HTML网页的模块Haul – 一个可扩展的图像爬虫。python-readability – arc90 readability工具的快速Python接口。scrapely – 从HTML网页中提取结构化数据的库。给出了一些Web页面和数据提取的示例，scrapely为所有类似的网页构建一个分析器。视频youtube-dl – 一个从YouTube下载视频的小命令行程序。you-get – Python3的YouTube、优酷/ Niconico视频下载器。维基WikiTeam – 下载和保存wikis的工具。WebSocket用于WebSocket的库。Crossbar – 开源的应用消息传递路由器(Python实现的用于Autobahn的WebSocket和WAMP)。AutobahnPython – 提供了WebSocket协议和WAMP协议的Python实现并且开源。WebSocket-for-Python – Python 2和3以及PyPy的WebSocket客户端和服务器库。DNS解析dnsyo – 在全球超过1500个的DNS服务器上检查你的DNS。pycares – c-ares的接口。c-ares是进行DNS请求和异步名称决议的C语言库。计算机视觉OpenCV – 开源计算机视觉库。SimpleCV – 用于照相机、图像处理、特征提取、格式转换的简介，可读性强的接口(基于OpenCV)。mahotas – 快速计算机图像处理算法(完全使用 C++ 实现)，完全基于 numpy 的数组作为它的数据类型。代理服务器shadowsocks – 一个快速隧道代理，可帮你穿透防火墙(支持TCP和UDP，TFO，多用户和平滑重启，目的IP黑名单)。tproxy – tproxy是一个简单的TCP路由代理(第7层)，基于Gevent，用Python进行配置。其他Python工具列表awesome-pythonpycrumbspython-github-projectspython_referencepythonidae]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据提取-json/Xpath]]></title>
    <url>%2F2016%2F12%2F13%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%20jsonXpath%2F</url>
    <content type="text"><![CDATA[数据提取介绍什么是数据提取简单的来说，数据提取就是从响应中获取我们想要的数据的过程响应数据的分类非结构化数据：html中的数据没有结构没有规律的数据，避难提取，需要使用正则、XPATH等方法将数据提取出来结构化数据：json、xml等数据的格式比较有规律，转化和提取很方便数据的处理方法非结构化数据的提取正则表达式、xpath结构化数据将响应内容转化为python的字段数据类型，再进行数据的筛选数据提取的方法结构化的数据提取JSONJSON数据的介绍JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。Json在数据交换中起到了一个载体的作用，承载着相互传递的数据，比如网站前台与后台之间的数据交互。由于把json数据转化为python的字典格式的数据类型比较简单，所以在爬虫的时候，如果能找到返回json数据的url，尽量使用这用url的响应数据；json数据类型和python中一些数据类型的对应关系怎么找到响应json数据的响应请求手机版的网页，在响应中普遍会使用json数据的响应形式抓包手机app软件json字符串——python字典类型转化方法将python数据格式化打印pprint(json_str)将json字符串转化为python风格的字典json.loads(json_str) 返回json字符串转化后的字典json字符串转化为python的字典类型数据将python字典数据转化为json字符串json.dumps(python_dict, ensure_ascii=False, indent=4) 返回字典转化后的json字符串把python字典对象转化为json字符串ensure_ascii=Fale json数据的编码格式默认为ascii，该参数将json的编码格式变为utf-8indent 将json数据格式化，在打印和写入文件的时候，会格式化数据，并且按照规定缩进将json类文件对象转化为python字典类对象with open('douban.txt') as f:json.load(f) 返回转化后文件内容将python字典对象转化为json类文件对象：with open('douban.txt') as f:json.dump(f, f, ensure_ascii=False, indent=2) 返回转化后文件内容ensure_ascii=False json数据的编码格式默认为ascii，该参数将json的编码格式变为utf-8indent 将json数据格式化，在打印和写入文件的时候，会格式化数据，并且按照规定缩进备注：当我们读写数据到文件中的时候，具有read()和write()方法的对象就是类文件对象；处理json数据常见错误loads的不是纯json字符串处理方法：去数据中对应的位置去寻找错误类似字典的，键值对使用单引号的数据（误以为json数据）处理方法：将对应的单引号都替换成双引号，就变成了json字符串非结构化数据提取HTML怎么找到html的响应先确定我们想要查找的内容在带有html的的响应中进行查找需要寻找的内容，如果存在，我们就需要通过这个响应来提取数据在急速版经常存在html响应通过什么方式提取HTML内容中的数据通过正则表达式了获取HTML页面中的目标数据（这种方法将不进行详细记录，查看正则表达式的笔记）通过XPATH来获取目标数据XPATH/LXML/XML的介绍XML和HTML的区别XML的介绍Extensible Markup Language（可扩展标记语言），是一种标记语言，很类似 HTML，被设计为传输和存储数据用的一种语言，焦点是数据的内容；XML数据的节点结构示例：每个XML标签我们都称之为节点，所有节点之间都有他们之间的关系如上HTML的介绍HyperText Markup Language（超文本标记语言） ，显示数据以及如何更好的显示数据HTML DOM 介绍Document Object Model for HTML (文档对象模型)，通过 HTML DOM，可以访问所有的 HTML 元素，连同它们所包含的文本和属性。可以对其中的内容进行修改和删除，同时也可以创建新的元素。什么是XPATHXPath (XML Path Language) 是一门在 XML/HTMl 文档中查找信息的语言，可用来在 XML/HTML 文档中对元素和属性进行遍历，进行数据的查找。W3School官方文档：http://www.w3school.com.cn/xpath/index.asp什么是LXMLlxml是一款高性能的 Python HTML/XML 解析器，我们可以利用XPath，来快速的定位特定元素以及获取节点信息，我们在爬虫中会大量使用LXML来进行数据的提取等XPATH的节点选择常用节点选择工具：Chrome插件 XPath Helper开源的XPath表达式编辑工具:XMLQuire(XML格式文件可用)Firefox插件 XPath Checker常用节点选择语法介绍XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。通过路径选择节点/ 在路径最开始表示根节点，后面的/代表层级关系，可以选择当前层级的下一层元素// 在最开始表示根节点，后面//代表层级关系，可以选择当前层级的所有后代元素/. 选取当前节点/.. 选取当前节点的父节点/@attribute_name 获取节点的属性//@attribute_name 选取节点包含节点后代元素的所有属性/text() 获取文本中的内容（不包含文本中的标签）//text( ) 获取节点包含节点后代元素的所有文本内容选择未知节点/* 匹配当前节点下的所有节点元素/@* 匹配当前节点的所有属性/node() 匹配任意节点，用于过度，不明确目标节点和当前节点之间关系时候使用节点轴的选择/preceding-sibling::* 匹配节点之前的同级节点/following-sibling::* 匹配节点之后的同级节点/ancestor::* 匹配该节点的祖先节点/descendant::* 匹配节点的子孙节点/child::* 匹配节点的子节点/following::* 匹配节点之后的所有节点/preceding::* 匹配节点之前的所有节点/attribute::* 匹配节点的所有属性条件选择节点（条件放在括号中）----谓语/book[1] 选取结果集的第一个元素XPATH选取元素的开始索引为1，和列表的起始值不一样/book[last()] 选取结果集的最后一个元素/book[last() - 1] 选取结果集的倒数第二个元素/book[position() &lt; 3] 选取结果集的前两个元素//titie[@lang] 选取结果集中带lang元素的属性的元素//title[@lang='eng'] 选取结果集带lang属性， 且为eng的元素还可以加文本限制, 例如：//title[text()='下一页']//a[contains(text(), '下一页')] 选取结果集的文本内容包含'下一页'的元素还可以加属性的限制，例如：//a[contains(@class, '下一页')]//a[not (@class='1')] 表示不包含该属性的标签元素选取多路径节点节点表达式1 | 节点表达式2 选取节点表达式1和节点表达式2所选取的所有节点；备注使用chrome插件选择标签时候，选中时，选中的标签会添加属性class=&quot;xh-highlight&quot;在python中XPATH的使用（lxml库）通过lxml使用XPATH的方法 pip install lxml如果安装lxml失败，可以尝试安装anaconda软件，基本上可以解决所有环境问题from lxml import etree 到如lxml库中的etree模块html = etree.HTML(text) 将html字符串转化为Element对象返回的html是Element对象Element对象可以使用xpath的方法，对html字符串进行查询和修改；text可以是str类型也可以是bytes类型（当标签中存在encoding属性的时候，text一定要是bytes类型）list = html.xpath(' xpath语句 ') 使用xpath语句，返回查询结果组成的列表返回的是由Element节点对象组成的列表，每一个对象都可以使用Xpath方法；如果没有内容，返回的是一个空列表text() 获取文本内容的时候，注意，当一段文字被标签拆分成多段的时候，每遇到一个标签便会被拆分成一个列表元素etree.tostring(html).decode() 将Element对象转化为字符串etree.tostring(html) 返回值为bytes类型数据，如果需要str需要手动解码备注：lxml 会自动修正 html 代码，lxml会自动将html代码中的异常部分进行修改，但是有的时候，会有将代码修改错误的情况，所以这种情况我们需要查看lxml修改后的代码，再根据修改后的代码进行操作]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多线程爬虫]]></title>
    <url>%2F2016%2F12%2F13%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2Fpython%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[多线程爬虫所需应用Queue（对列对象）介绍QueueQueue是python中的标准库，可以直接import Queue引用;队列是线程间最常用的交换数据的形式。Queue的作用python下多线程对于资源的处理，加锁是个重要的环节，因为python原生的list,dict等，都是not thread safe的，所以使用Queue，可以不用手动添加资源锁就能解决资源竞争的问题。在使用Queue进行参数传递的时候，只有Queue列队中有数据，线程就可以获得并执行任务，不需要等待其他线程对数据的处理并传递，更方便数据的传递和处理；使用方法创建队列对象from queue import Queuequeue = Queue(maxsize = n) 创建一个最大容量为n的queue列队将一个数值放在队列中queue.put( ) 向列队中添加任务，列队满后会阻塞等待myqueue.put_nowait( ) 想列队中添加任务，列队满后会报错将一个数值从队列中取出queue.get( ) 从列队中取出任务queue.get([block[, timeout]]) 从列队中取出任务，超时后会报错备注：从列队取出任务后，列队计数并不会减一，需要标记任务task_done任务完成，列队技术减一queue.task_done( ) 列队计数减一列队任务计数不为0时，阻塞线程Queue.join( ) 列队如果不为空，阻塞主线程其他查询列队状态的方法Queue.qsize() 返回队列当前计数大小Queue.empty() 如果队列为空，返回True,反之FalseQueue.full() 如果队列满了，返回True,反之FalseQueue.full 与 maxsize 大小对应线程的使用（复习）创建子线程import threadingt1 = threading.Thread(target=function, args=(, ))将子线程设置为守护进程t1.setDeamon(True) 启动子线程t1.start( )线程的使用策略使用策略创建不同功能任务需求的列队在处理时间较长的功能函数处，使用比较多的线程在处理时间比较快的函数处，使用比较少的线程少创建一些线程，线程之间会进行资源的竞争，导致请求失败率增加多设置一些最大请求次数，增加请求成功的几率将失败的请求，任务，防止到一个列表/对列中，单独再对失败的进程进行统一执行断点续爬的思想场景在我们爬取数据的时候，想要先停止爬取，将爬取的状态和数据保存，并想要下次爬取的时候再接着当前的状态继续爬取实现方法可以保存当前的所有的queue列队，在程序结束之前将queue对象存储到redis中当下次启动程序的时候，从redis数据库中读取上次停止之前的queue对象程序判断queue对象的状态，是否全部为空，如果不为空，那么便不需要再重新获取参数想queue列队中添加，直接接着之前的queue的装填进行爬取多线程爬虫代码实例]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Selenium爬取动态HTML]]></title>
    <url>%2F2016%2F12%2F13%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2FSelenium%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81HTML%2F</url>
    <content type="text"><![CDATA[动态HTML相关技术 JavaScriptJavaScript 是网络上最常用也是支持者最多的客户端脚本语言。它可以收集 用户的跟踪数据,不需要重载页面直接提交表单，在页面嵌入多媒体文件，甚至运行网页游戏。jQueryjQuery 是一个十分常见的库,70% 最流行的网站(约 200 万)和约 30% 的其他网站(约 2 亿)都在使用。一个网站使用 jQuery 的特征,就是源代码里包含了 jQuery 入口。如果你在一个网站上看到了 jQuery，那么采集这个网站数据的时候要格外小心。jQuery 可 以动态地创建 HTML 内容,只有在 JavaScript 代码执行之后才会显示。如果你用传统的方 法采集页面内容,就只能获得 JavaScript 代码执行之前页面上的内容。Ajax / DHTML我们与网站服务器通信的唯一方式，就是发出 HTTP 请求获取新页面。如果提交表单之后，或从服务器获取信息之后，网站的页面不需要重新刷新，那么你访问的网站就在用Ajax 技术。Ajax 其实并不是一门语言,而是用来完成网络任务(可以认为 它与网络数据采集差不多)的一系列技术。Ajax 全称是 Asynchronous JavaScript and XML(异步 JavaScript 和 XML)，网站不需要使用单独的页面请求就可以和网络服务器进行交互 (收发信息)。动态HTML技术的影响动态技术的效果对爬虫的影响（如下情况我们最好使用selenium来解决）动态生成HTML内容，在获取内容的时候，只能获取到js处理之前的内容（可能是js再次发起异步请求，或者是对内容传输过程加密，js解密显示）动态的生成url地址，在请求的时候，不知道其js的处理逻辑，不能解析js处理后的url地址动态的生成请求参数，在请求的时候 ，不知道其js的处理逻辑，不知道js处理后的请求参数服务器端动态生成的验证码（请求同一验证码url地址，返回图片不同），这样无法通过请求url下载图片来获取当前请求的验证码怎么解决直接从 JavaScript 代码里采集内容（费时费力）分析js代码的功能逻辑用python来重新实现其功能，完成参数或者内容的解析用 Python 的 第三方库运行 JavaScript，直接模拟浏览器操作，并获取element中的js渲染后的代码内容。通过使用selenium库来实现直接在浏览器上运行的效果，可以获取浏览器中elements中的源码（js解析后的内容）通过模拟浏览器进行请求，不需要我们构建请求的url地址和请求参数等内容获取模拟登录后的cookie等，保持登录状态Selenium的使用所需工具介绍selenium的介绍Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏使用selenium的场景在模拟登录的时候，可以模拟获取到动态的url、请求参数、等进行登录，还可以获取到cookie获取动态HTML页面内容，即请求页面中的内容是经过js修改过动态计算生成的内容各种driver的下载chromedriver官网网站https://sites.google.com/a/chromium.org/chromedriverchromedriver下载地址https://chromedriver.storage.googleapis.com/index.htmlgeckodriver官方网址https://github.com/mozilla/geckodrivergeckodriver下载地址https://github.com/mozilla/geckodriver/releasesselenium官方文档http://selenium-python.readthedocs.io/api.html创建driver对象、并设置窗口大小from selenium import webdriver from selenium.webdriver import ActionChainsfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WevDriverWaitfrom selenium.webdriver.support import export expected_conditions as ECfrom selenium.webdriver.remote.command import Commanddriver = webdriver.PhantomJS( )driver.set_window_size(width, height) 设置浏览器的窗口大小，截图可以根据窗口大小来截取driver.max_windiw( ) 让窗口显示最大加载网页driver = webdriver.PhantomJS( ) 加载浏览器driverdriver.get(&quot;http://www.baidu.com/&quot;) 请求url地址寻找元素driver.find_element_by_id() 通过id找到元素driver.find_element_by_name() 通过name属性寻找元素driver.find_element_by_xpath() 通过xpath来寻找元素driver.find_element_by_link_text() 通过文本内容寻找a标签driver.find_element_by_partial_link_text() 通过文本内容包含内容寻找a标签 driver.find_element_by_tag_name() 通过标签名获取元素driver.find_element_by_class_name() 通过类型获取元素driver.find_element_by_css_selector() 通过css选择器选择元素备注：find_element 和find_elements的区别：find_elements用来寻找多个元素, 返回一个和返回一个列表by_css_selector的用法示例： #food span.dairy.aged元素的交互element.send_keys(“长城”) 在input元素value中输入内容element.clear() 清空input标签中输入的内容element.click( ) 触发元素的事件动作链的使用source_ele = driver.find_element_by_css_selector('#draggable')target_ele = driver.find_element_by_css_selector('#droppable')actions = ActionChains(driver) 创建动作链actions.drag_and_drop(source_ele, target_ele) 调用动作链的方法actions.persorm() 执行动作链指定原生Js代码js_code = &quot;window.scrollTo(0, document.body.scrollHeight)&quot; 定义js代码driver.execute_script(js_code) 指定js代码获取节点元素的属性和文本值等element.get_attribute('属性名') 可以获取元素的属性element.text 可以获取文本内容element.id 获取元素的id属性element.size 获取元素的长宽element.size['width'] 获取元素的宽element.size['height'] 获取元素的高element.tag_name 获取元素的标签名称element.location 获取元素的坐标切换Framedriver.switch_to.frame('frame_id' ) 切换到iframe标签页面中driver.switch_to.parent_frame() 切换到父frame中driver.switch_to.default_content() 切换到默认的frame中等待隐式等待等待规则如果selenium没有在DOM中找到节点元素，将等待一段时间时间到达设定的时候后, 再次查找元素，若找不到元素则抛出找不到节点的异常，默认时间是0等待设置方法driver = webdriver.Chrome()driver.implicity_wait(10) 给driver设置了10秒的隐式等待时间driver.get('https://www.baidu.com')ele = driver.find_element_by_class_name('kw') 寻找元素, 找不到元素的话会等待10秒钟, 若还找不到元素则抛出异常显示等待等待规则规定了一个最长的等待时间，如果在规定的时间内加载出来了这个节点, 就返回找到节点如果没有找点节点则抛出超时异常等待设置方法driver = webdriver.Chrome()driver.get('https://www.baidu.com')wait = WebDriverWait(driver, 10) 设置了10秒的最大显示等待时间ele = wait.until(EC.presence_of_element_located((By.ID, 'kw'))) 设置等待条件，并等待寻找到节点ele_1 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#bt'))) 设置等待条件，并等待寻找到节点常用的等待条件title_is 标题是某内容title_contains 标题包含某内容presence_of_element_located 节点加载出来visibility_of_element_located 节点课件, 传入定位元组visibility_of 节点可见，传入节点对象presence_of_all_elements_located 所有节点加载出来text_to_be_present_in_element 某个节点中包含某文字text_to_be_present_in_element_value 某个节点值包含某文字frame_to_be_available_and_switch_to_it 加载并切换invisibility_of_element_located 节点不可见element_to_be_clickable 节点可点击element_to_be_seleted 节点可选择，传入节点对象element_located_to_be_selected 节点可选择，传入定位元组element_selection_state_to_be 传入节点对象以及状态，相等返回True, 否则返回falseelement_located_selection_state_to_be 传入定位元组以及状态，相等返回True，否则返回Falsealert_is_present 是否出现警告其他的等待driver.set_page_load_timeout(10) 设置页面加载的超时时间driver.set_script_timeout(10) 设置执行script脚本的超时时间鼠标的移动和点击鼠标的移动actions = ActionChains(driver)actions.move_to_element(element).move_by_offset( lx , ly )actions.perform()element 移动到哪个元素里面lx 相对元素左边界的距离ly 相对元素上边界的距离鼠标的点击from selenium.webdriver.remote.command import Commanddriver.execute(Command.MOUSE_DOWN, pamas)第一个参数为时间类型第二个参数为命令的附加命令，为字典类型操作driver页面请求参数相关driver.page_source 获取浏览器中element中的内容（即页面的源码）driver.current_url() 获取当前页面url地址driver.back() 退回上一个页面driver.forward() 返回到前面一个页面driver.flush() 刷新当前页面driver.get_cookies() 获取所有的cookie，返回的是列表嵌套字典（存储的每个cookie信息）driver.delete_cookie(&quot;key&quot;) 删除一条cookiedriver.delete_all_cookies() 删除所有的cookie内容{cookie['name']: cookie['value'] for cookie in driver.get_cookies( )} 将cookie构造请requests请求参数driver.execute_script('window.open()') 开启一个浏览器的选项卡driver.switch_to_window(driver.window_handles[1]) 切换到第二个选项卡窗口窗口截图相关from PIL import Imagedriver.save_screenshot(&quot;长城.png&quot;) 截图浏览器，没有图形界面可以通过截图来观察界面img = Image.open(io.BytesIO(driver.get_screenshot_as_png())) 获取截图并直接读取到内存img_small = img.crop((x0, y0, x1, y1)).convert('L') 对图片进行截图，四个参数代表(左、上、右、下)坐标，并将彩图转化为灰度图img_small.save('--path--') 将图片进行保存width = img.size[0] 获取图片的宽height = img.size[1] 获取图片的高img1.load( )[ i, j ] 获取图片该像素点的rgb值浏览器开关相关driver.close() 关闭当前页面，当页面全部都关闭后，退出浏览器driverdriver.quit() 退出浏览器driverselenium 的driver配置代理的设置phantomjs设置代理ipbrowser=webdriver.PhantomJS(PATH_PHANTOMJS)proxy=webdriver.Proxy()proxy.proxy_type=ProxyType.MANUALproxy.http_proxy='1.9.171.51:800'proxy.add_to_capabilities(webdriver.DesiredCapabilities.PHANTOMJS)browser.start_session(webdriver.DesiredCapabilities.PHANTOMJS)browser.get('http://1212.ip138.com/ic.asp';)Chrome设置代理ipfirefox设置代理ip方法一方法二禁用插件和无头浏览器的设置chrome配置方法设置无头浏览器设置禁用图片和javascriptfirefox配置方法firefox_options = webdriver.FirefoxOptions()firefox_options.set_preference(u&quot;permissions.default.stylesheet&quot;, 2) # 禁用cssfirefox_options.set_preference(u&quot;permissions.default.image&quot;, 2) # 禁用图片firefox_options.set_preference(u&quot;dom.ipc.plugins.enabled.libflashplayer.so&quot;, &quot;false&quot;) # 禁用flash插件firefox_options.set_headless() # 设置为无头浏览器driver = webdriver.Firefox(firefox_options=firefox_options)设置请求头的方法]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Requests模块-获取页面响应]]></title>
    <url>%2F2016%2F12%2F11%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2FRequest%E6%A8%A1%E5%9D%97-%E8%AF%B7%E6%B1%82%E8%8E%B7%E5%8F%96%E5%93%8D%E5%BA%94%2F</url>
    <content type="text"><![CDATA[Request模块的介绍request 库的特点requests的底层实现就是urllibrequests在python2 和python3中通用，方法完全一样requests简单易用requests能够自动帮助我们解压(gzip压缩的等)网页内容requests的作用发送网络请求，返回响应数据中文文档 API：http://docs.python-requests.org/zh_CN/latest/user/quickstart.htmlrequest/response的常用方法和属性requests.get() get请求使用方法：response = requests.get(url, headers=headers， params=params) 返回响应对象get方法的参数url：为请求的url地址headers：字典形式的命名参数，传递请求头。模拟浏览器获取想获取的数据，防止反扒检测；示例：headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;}params：字典形式命名参数，传递请求参数，获取想要获取的数据示例：params = {'wd':'长城'}request.post() post请求使用方法：response = requests.post(url, headers=headers, data= data)post方法的参数url：为请求的url地址（不包含锚点）headers：字典形式的命名参数，传递请求头。模拟浏览器获取想获取的数据，防止反扒检测；示例：headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;}data：当时普通的post请求的时候，传递的参数是字典形式命名参数Formdata参数示例：data = {'wd':'长城'}当时ajax异步post请求的时候，传递的的字符串形式的参数Payload参数示例：data = json.dumps({'wd': '长城'})还要加上请求头 ： 'Content-Type': 'application/json'requests.utils 工具requests.utils.qoute(url) 将url编码requests.utils.unqoute(url) 将url进行解码parsed = requests.utils.urlparse(url) 将url进行 协议、ip+端口、文件路径、参数等的拆分parsed.scheme 获取协议名parsed.netloc 获取ip+端口（或者是域名+端口）parsed.path 获取路径parsed.params 获取问号后面的参数response 响应的常用属性和方法response.text 获取响应内容html字符串获取html字符串，结果是`str`类型其编码方式，是requests根据响应头做出的有根据的推测，尝试使用这个编码方式来解码response.encoding 规定解码的格式可以在使用text属性获取响应字符串之前先规定编码格式，按照设定的编码格式进行解码respones.content 获取响应内容的二进制(bytes)数据通常我们在获取到内容的时候尝试对二进制数据进行解码，下面三种方法能够解决后续我们100%的编解码的需求response.content.deocde() 获取二进制响应内容并解码成为utf-8编码格式的字符串response.content.deocde(&quot;gbk&quot;) 获取二进制响应内容并解码成为gbk编码格式的字符串response.text 在使用content解码失败的时候，可以尝试让request去根据推测进行解码response.status_code 查看响应状态码response.request.headers 查看请求头response.headers 查看响应头response.request.url 查看响应urlresponse.url 查看请求url响应的url可能与请求url不同，比如304重定向，响应的url为重定向的url***找到想要请求url和请求参数的方法***寻找想要的url第一种情况：表单提交请求，并且表单有action属性通过form表单的action属性/a标签的href属性来找到对应的请求urlform表单提交时获取表单中所有的name、value键值对构造成字典，通过requests函数的data参数进行请求第二种情况：通过ajax异步发出的请求（非action表单提交请求）查看抓包结果，存在我们想要最多的数据会是我们想要的响应，其url为主要的请求url当有一些数据不存在该主要的请求中，那说明有js进行了其他异步请求获取了数据我们可以先找到其他请求的响应，找到其url地址，其url地址一般有两种构建途径1.一般的情况下和url相关的信息会存在主请求的响应中，我们可以通过在主响应中获取其他请求的请求url关键字，进行url的构建2.有的时候url地址或参数是通过js动态生成的，这时候，我们需要去寻找对应的js文件来观察js是怎样生成的动态url和参数等（参考下面）寻找想要的js文件（当不是通过form表单的action/a标签href属性提交请求的时候）找到进行url请求的js文件第一种方法使用开发者选择工具，点击会触发请求js的元素点击右边的event Listener，会获取到对应的点击事件点击事件所在的js文件第二种方法先了解js文件中会出现的关键字，一般可以查找url中动态参数名称；通过点击右上角的菜单search all file查找关键字，找到后点击进入对应的js文件会跳转到source界面，点击{}展开代码，显示文件全部的代码找到想要找的事件函数，并在想要查看实现方法的函数前边加上断点通过点击右边的调试工具，可以查看函数的具体执行步骤和实现方法了解了实现的方法后，可以在爬虫请求的时候，实现同样的方法，然后生成参数和url地址，进行请求使用代理获取响应什么叫代理在对网站爬取的时候，通过访问代理服务器，让代理服务器帮我们对目标服务器进行请求，然后通过代理服务器将响应返回给我们；代理的作用在需要大量的进行数据的爬取的时候防止在同一时间，使用同一个ip地址对同一个服务器进行大量的访问，被反扒出来在爬取网站的时候，通过代理服务器可以隐藏我们的真实ip地址，隐藏我们的身份，但是有的代理服务器不能隐匿我们的mac地址，如果想要隐匿我们的mac地址需要使用高匿代理服务器代理服务器的原理正向代理/反向代理正向代理在我们请求代理服务器的时候，我们知道我们的最终目标ip地址，比如我们使用代理服务器爬取百度服务器的内容反向代理我们在请求服务器的时候，不知道最终目标ip地址，比如我们在访问nginx反向代理的服务器的时候，我们只是在访问nginx服务器，我们并不知道nginx去访问哪一个ip地址requests使用代理使用方法：requests.get(url, proxies = proxies)需要参数proxies：字典类型命名参数，指定代理服务器支持的协议，和代理服务器的请求地址示例：proxies = {&quot;协议&quot;:&quot;协议://ip:port&quot;}，传递代理服务器支持协议类型（http/https），和代理服务器的ip和端口备注：proxies字典参数最多只能接受两个键值对，一个键是http，一个键是https，定义在进行http/https请求的时候分别使用的代理服务器代理服务器注意点：http的url地址要使用http的代理，https的要使用https的代理透明度低的代理能够被对方服务器找到我们的真实的ip，可能会导致代理的效果不明显cookie与session的请求cookie和session的区别cookie存在浏览器本地，session在服务端cookie不安全，session不会将数据暴露在客户端，比较安全session占用性能，会加长请求的时间cookie存储是有上限的，session没有请求带上cookies的好处能够请求登陆后的页面带上cookie反反扒，用登录成功的cookie来进行伪造伪造请求带上cookie的不好的地方使用同一个cookie，不间断的访问同一个服务器的时候，可以被对方识别为爬虫解决方法：使用多个用户名密码，多账号，随机选用账号进行服务器的访问，模拟多人访问模拟登录cookie/session请求的方法第一种session请求登录接口实例化一个session对象session = requests.session()使用session请求登录接口，session对象会将响应中的cookie进行保存session.post(url, data=data, headers=headers)也可以使用get请求再使用session对象请求其他需要登录的url地址，会自动带上cookie进行访问response =session.get(url, params=params, headers=headers)session对象的作用对响应的cookie进行保存，后面的访问会自动携带cookie第二种要获取了登录后的cookie请求字符串headers中放cookie请求头字符串第三种把cookie的每一个name和value组成一个字典将字典传给requests请求中的cookies参数接收获取response中的cookie的方法获取response中的cookie对象response.cookies 获取respone的cookie对象response.cookies只能获取服务器主动设置的cookie，不能获取我们手动创建的cookie返回的数据类型是列表嵌套字典，每一个字典包含一个cookie的所有信息将cookie对象转化为字典类型的方法requests.utils.dict_from_cookiejar(response.cookies)将python字典类型转化为cookie对象类型requests.utils.cookiejar_from_dict( {'key': value } )requests请求常见问题 SSl证书验证问题问题产生原因请求协议为https的网站需要向机构申请证书，这样用户才能直接通过https访问，但是有的网站（比如12306）的整数是自己研发的，这样浏览器不会在进行请求的时候，会产生一个证书异常，我们需要点击浏览器上的继续访问，来请求服务器，当我们使用爬虫来进行访问的时候，会直接报出异常解决办法：response = requests.get(&quot;https://www.12306.cn/mormhweb/ &quot;, verify=False)在请求非机构证书的网站的时候，我们需要在请求方法中加上verify=False 的参数，就不会报异常请求超时问题问题产生原因当我们进行请求的时候，我们可能因为在请求摸一个url的时候产生一些异常，导致长时间没有请求成功，由于请求一直在进行，导致后面的程序无法正常执行，验证影响程序的效率或者导致程序终止解决办法应用到的方法from retrying import retry @retry(stop_max_attempt_number=n) 装饰函数，表示函数如果报错将会再次执行，直到第n次如果依然报错，那将抛出异常 response = requests.get(url,timeout=10) 设置请求的超时时间，如果限定时间内没有请求成功，将会抛出异常assert response.status_code == m assert为断言关键字，如果后面的条件表达式为false将会排出异常示例：数据处理的技巧字符串的格式化&quot;abc{}abc&quot;.format()不同于%号的格式化，{ }方式的格式化，可以接收任意类型的格式化format 接收的参数与格式化{ }的数量相等在通过字符串格式化构建url进行requests请求时候，尽量使用{ }来格式化，因为在浏览器会将url格式化，%有时候会按照特殊字符处理； json数据与字符串之间的转化import jsonjson.dumps( dict) 把python类型字典数据转化为json字符串json.loads('json' ) 把json数据转化为python的字典类型扁平化赋值表达式name = &quot;a&quot; if lang==b else &quot;c&quot; if后面的条件如果成立，那么就把if前的值赋给str否则if否面的条件不成立，就把else后的值赋给strname = a and 'b' or 'c' 如果a为true，name等于b如果a为false，name等于c列表推导式和字典推导式列表推倒式[i for i in range(10) if i%2==0]字典推导式{i+1:i for i in range(10) if i%3==0}注意：在列表推导式中可以使用if判断，但是不能够使用elsetipslinux命令重命名作用可以将一段的linux命令，重名为其他比较短而且容易记忆的命令，方便我们的调用设置方法修改家目录下的.bashrc文件添加 alias 新的命令='原命令 -选项/参数'保存退出 source .bashrc已经可以使用新的命令了 将数据写入文件文件存储内容的方法with open('文件路径', encoding='utf-8') as f:f.write(content)encoding 如果文件不存在，相当于我们创建一个文件进行写入，我们可以通过encoding来指定写入内容的编码格式，如果不指定，可能会报错常见的反扒思路尽量减少请求的次数能抓列表页不抓详细页保存html页面，有利于重复使用多分析一个网站的不同类型页面手机极速版页面wap手机版页面web网页app抓包软件进行请求伪装多带一些请求头，有的时候请求头带的不同，服务器返回的结果不同代理ip，设置代理ip池，定期更新代理ip池在浏览器会根据cookie内容判断爬虫的时候，携带cookie（但是注意不要一直携带同一个cookie）需要获取登录之后的数据的时候，要进行模拟登录，获取cookie后，在进行数据的获取利用多线程/分布式（尽可能的快速抓取）在可能的情况下尽可能的使用多线程和分布式爬虫]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫基础介绍]]></title>
    <url>%2F2016%2F12%2F10%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%E7%9B%B8%E5%85%B3-%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[爬虫的介绍爬虫的定义 模拟浏览器其发送请求，获取到和浏览器一模一样的数据 浏览器能够看到的，我们爬虫才能够获取到，否则是没有办法获取到的，所以，只要浏览器能做的事情，原则上，爬虫都能做爬虫获取的数据的用途呈现数据，呈现在app或者在网站上伪造网站请求，进行自动的访问网站进行数据分析，获得结论爬虫的分类通用爬虫：搜索引擎的爬虫，通常指搜索引擎的爬虫，通用网络爬虫利用种子url从互联网中搜集网页，采集信息，这些网页信息用于为搜索引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。聚焦爬虫：针对特定网站的爬虫，指定url进行爬取，有明确的爬取目标通用爬虫工作原理第一步：数据抓取首先选取一部分的种子URL，将这些URL放入待抓取URL队列；取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环....第二步：数据存储搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。第三步：预处理搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。提取文字中文分词消除噪音（比如版权声明文字、导航条、广告等……）索引处理链接关系计算特殊文件处理第四步：提供检索服务，网站排名搜索引擎在对信息进行组织和处理后，为用户提供关键字检索服务，将用户检索相关的信息展示给用户。同时会根据页面的PageRank值（链接的访问量排名）来进行网站排名，这样Rank值高的网站在搜索结果中会排名较前，当然也可以直接使用 Money 购买搜索引擎网站排名，简单粗暴。通用爬虫的局限性通用搜索引擎所返回的网页里90%的内容无用。图片、音频、视频多媒体的内容通用搜索引擎无能为力不同用户搜索的目的不全相同，但是返回内容相同聚焦爬虫的工作流程爬虫爬取哪些数据教育机构：其他教育机构的开班，招生，就业，口碑资讯公司：特定领域的新闻数据的爬虫金融公司：关于各个公司的动态的信息，酒店/旅游：携程，去哪儿的酒店价格信息/机票，景点价格，其他旅游公司价格信息房地产、高铁：10大房地产楼盘门户网站，政府动态等强生保健医药：医疗数据，价格，目前的市场的行情Robots协议：网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。这个协议之是道德层面的协议，这个协议并不能从技术上阻止去对其网站进行爬取。例如：https://www.taobao.com/robots.txtHTTP/HTTPS协议url的形式url表现形式：scheme://host[:port#]/path/…/[?query-string][#anchor]scheme：协议(例如：http, https, ftp)host：服务器的IP地址或者域名port：服务器的端口（如果是走协议默认端口，80 or 443）path：访问资源的路径query-string：参数，发送给http服务器的数据anchor：锚（跳转到网页的指定锚点位置）示例：http://localhost:4000/file/part01/1.2.htmlhttp://item.jd.com/11936238.html#product-detailHTTP/HTTPS的区别HTTP超文本传输协议 默认端口号:80HTTPSHTTP + SSL(安全套接字层)默认端口号：443HTTP和HTTPS区别浏览器默认请求服务器是以HTTP协议进行请求，如果服务器支持HTTPS协议，会返回给浏览器端一个协议相关的响应，浏览器会重新发起HTTPS协议的请求； HTTP请求报文的形式http的请求过程域名---&gt;dns（拿ip）---&gt;浏览器请求ip---&gt;服务器---&gt;返回资源HTTP常见的请求头1. Host (主机和端口号)对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的Host部分2. Connection (链接类型)keep-alive 客户端携带表示支持长连接keep-alive 服务器端如果回复keep-alive， 代表允许双方建立长连接close： 服务器端回复close，代表不允许建立长连接，浏览器接收到响应后会主动断开连接3. Upgrade-Insecure-Requests (浏览器支持升级为HTTPS请求)升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。4. User-Agent (浏览器名称)对方的服务器通过User-Agent判断出来我们是一个手机版的浏览器还是电脑版的，同时能够判断出来浏览器的平台，型号，版本，内核版本5. Accept (传输文件类型)Accept: */*：表示什么都可以接收。Accept：image/gif：表明客户端希望接受GIF图像格式的资源；Accept：text/html：表明客户端希望接受html文本。Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。text：用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；application用于传输应用程序数据或者二进制数据。6. Referer (页面跳转处)表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。防盗链：有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载。7. Accept-Encoding（文件编解码格式）指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=08. Cookie （Cookie）cookies 能够记录用户信息，会在请求的时候一起传递给对方的服务器，对方的服务器能够根据cookie判断出来是否登陆过（用户的状态）9. x-requested-with :XMLHttpRequest (是Ajax 异步请求)10.Accept-Language (接受语言)指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。11.Accept-Charset（字符编码）指出浏览器可以接受的字符编码。举例：Accept-Charset:iso-8859-1,gb2312,utf-812.Content-Type (POST数据类型)Content-Type：POST请求里用来表示的内容类型。举例：Content-Type = Text/XML; charset=gb2312：0.请求体常见的请求方法GET特点：请求所带参数包含在url中，显示在地址栏，请求数据可以被缓存，请求参数有长度限制，请求速度快POST特点：请求所带参数在请求体中，请求数据不可以被缓存，请求数据没有长度限制，请求速度慢常用的响应报头1. Cache-Control：must-revalidate, no-cache, private。告诉客户端，对资源的缓存建议；2. Connection：keep-alive作为回应客户端的Connection：keep-alive，告诉客户端是否同意建立长连接；3. Content-Encoding:gzip告诉客户端，服务端发送的资源是采用gzip编码的；4. Content-Type：text/html;charset=UTF-8告诉客户端，资源文件的类型，还有字符编码；5. Date：Sun, 21 Sep 2016 06:18:21 GMT这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。6. Expires:Sun, 1 Jan 2000 01:00:00 GMT告诉客户端在这个时间前，缓存的到期时间7. Pragma:no-cache这个含义与Cache-Control等同。8.Server：Tengine/1.4.6这个是服务器和相对应的版本，只是告诉客户端服务器的信息。9. Transfer-Encoding：chunked这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。HTTP常见响应状态码100~199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。200：表示服务器成功接收请求并已完成整个处理过程200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。302：临时转移至新的url300~399表示请求转307：临时转移至新的url404：not found400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。500：服务器内部错误为什么浏览器渲染出来的页面和爬虫请求的页面不一样爬虫请求结果爬虫只会请求当前url地址，不会主动请求js，所以往往当前URL对应的响应和element的内容不一样浏览器浏览器请求服务器渲染出来的界面，以及最终的在终端element显示的内容，和爬虫爬取到的不一样，因为浏览器会对页面中的静态文件的url进行请求，将请求结果渲染到浏览器中，导致最终的网页代码和显示的结果，已经是被js文件进行修改过；在哪里查看当前url地址对应的响应（不包括对静态文件请求的响应）：抓包（network），network下的第一个url地址，当前url地址的response右键显示网页源码字符串和字节（str/ bytes）python3 字符串应用的字符集str ：unicode的呈现形式（python3应用的unicode的子集utf-8的形式对字符串进行呈现）bytes：二进制互联网上数据的都是以二进制的方式传输的，bytes是二进制格式的字符串对服务器的请求，要将请求数据先转化为bytes格式；获取到服务器的响应要将获取到的bytes类型的数据，进行字符串的转化Unicode/UTF8/ASCII字符集的介绍字符(Character)各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等字符集(Character set)多个字符的集合字符集包括：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集ASCII编码是1个字节，而Unicode编码通常是2个字节，UTF-8是Unicode的实现方式之一，它是一种变长的编码方式，可以是1，2，3个字节（一句字符的内容而定）str到bytes之间的转化（str、bytes）python3中字节和字符之间转化的方法从str---&gt; bytes：str使用encode方法转化为 bytes从bytes---&gt; str：bytes通过decode转化为str注意：编码方式解码方式必须一样，否则就会出现乱码]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2016%2F10%2F12%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式介绍什么是正则表达式 Regular Expression, 又称规则表达式。正则表达式就是用事先定义好的一些特定字符（组合），组成一个“规则字符串”，这个“规则字符串”用来描述一种字符串的匹配模式（pattern）； 正则的作用 可以用来检查一个字串是否包含某种子串、将匹配的子串替换或者取出 正则的特点 灵活性、逻辑性和功能性非常强大 正则表达式的使用方法re模块常用函数和方法 import re result_obj = re.search(正则表达式， 数据，flag=0） —-查找数据中第一个符合匹配规则的字符串 search()函数从数据中只能查找到第一个符合正则数据放到result_obj中， 如果没有匹配到想要匹配的结果会返回None result_obj.group() —-查看正则匹配的结果内容 result_obj.group(1， 2/ 组名) 返回需要组的匹配的结果，返回一个包含多个组匹配结果的元组； result_obj.group() == result_obj.group(0) == 正个正则表达式所有匹配的字符 re模块其他常用方法 compile 编译 作用 对正则表达式匹配规则进行预编译，在大量使用到正则的时候，可以提高匹配的速度 使用方法 p = re.compile(‘匹配规则’, re.DATALL) p.search(‘字符串’) 按照编译的规则对字符串进行匹配正则表达式中的特殊字符 匹配单个字符 正则表达式常用匹配方式匹配单个字符 空白字符\s == [ \f\n\r\t\v] 非空白字符 \S == [^\f\t\v\n\r] 在正则表达式中若只是想要匹配一个像特殊字符的普通字符需要在特殊字符前面加转义字符“\” 例如“.” 特殊字符在[ ]中例如：[. | * + ？等 ]没有特殊功能只代表普通字符 在[ ]中若是想使用“-”普通字符要加上转义字符\ 匹配多个字符 常用定位符 正则表达式的分组 注意: 在使用（|）的时候尽量使特殊的或者通用的变量放在前边 在引用分组的时候注意：\num表示八进制数num所表示的普通ASCII码字符，所以在引用的时候会默认表示ascii码字符，所以要注意转义或者使用原生字符串 匹配所有的汉字的方法 re.compole(r’[\u4e00-\u9fa5]’) 备注：匹配所有unicode编码的中文 正则表达式的贪婪与懒惰概念： 贪婪-尽可能多的匹配 懒惰-尽可能少的匹配默认为贪婪模式的匹配模式 在python中 +/*/{m,n}默认情况下总是贪婪的如何让贪婪模式变为懒惰模式 在量词后加上一个? 例子： 原生字符串的应用特殊字符的转义在表达式中如果包含“\”表示转义\后面的字符为八进制数字代表的ascii对应的特殊字符，在python中会对ascii包含的数字或者字符进行转义，这种情况会导致会将匹配规则的字符进行转义，结果不能匹配到想要匹配的字符串内容 解决办法： 取消转义 在每一个’\’字符前加上’\’，对”\’进行转义，这样会取消\的转义功能，将\只代表一个\字符，不会对后边的字符进行转义 ascii不包括的字符，如果前边有转义字符\，不需要加以转义，python会自动转义 原生字符串 如果在表达式或字符串前边加上r“”对字符串中的\字符自动转义 在使用的时候，匹配规则可以和想要匹配的内容写法相同，r会自动帮我们转义 示例： re.search(r’abc\nabc’, ‘abc\nabc’) 正则表达式的常见问题 如何让 . 特殊符号可以匹配所有内容（包括\n） 解决办法： 使用re.DOTALL 参数 示例： re.findall(r’abc.’, ‘abc\n\nsfgs’, re.DOTALL) 备注：也可以使用re.S 代替re.DOTALL 效果上是一样的Ascii码对应关系 在字符串，或者正则表达式中，\n\t等控制字符 或者 \数字（表示八进制的num所表示的普通ascii码）等显示字符，在应用的时候会默认为在调用ascii码的控制字符或者是显示字符，所以如果只是想表达单纯的字符 需要用\n或者r””这种形式进行转意，可以使用chr（八进制数）来查询对应的ascii字符 ascii码表]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Charles抓包工具安装、配置]]></title>
    <url>%2F2016%2F01%2F16%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FCharles%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Charles介绍Charles的作用Charles是一个网络抓包工具，相比Fiddler，其功能更为强大，而且跨平台支持得更好，可以选用它来作为主要的移动端抓包工具。相关链接官方网站：https://www.charlesproxy.com下载链接：https://www.charlesproxy.com/downloadCharles的配置HTTPS证书的配置Windows系统的证书配置1.首先打开Charles，点击Help→SSL Proxying→Install Charles Root Certificate，即可进入证书的安装页面。2.接下来，会弹出一个安装证书的页面，点击“安装证书”按钮，就会打开证书导入向导。3.直接点击“下一步”按钮，此时需要选择证书的存储区域，点击第二个选项“将所有的证书放入下列存储”，然后点击“浏览”按钮，从中选择证书存储位置为“受信任的根证书颁发机构”，再点击“确定”按钮，然后点击“下一步”按钮。4.再继续点击“下一步”按钮完成导入。Mac系统的证书配置1.如果你的PC是Mac系统，可以按照下面的操作进行证书配置。2.同样是点击Help→SSL Proxying→Install Charles Root Certificate，即可进入证书的安装页面。3.接下来，找到Charles的证书并双击，将“信任”设置为“始终信任”即可，如图1-48所示。代理配置具体操作是点击Proxy→Proxy Settings，打开代理设置页面，确保当前的HTTP代理是开启的，如图1-49所示。这里的代理端口为8888，也可以自行修改。将手机连接到Charles并安装证书ios系统1.将手机和电脑连在同一个局域网下，可以将PC设置为热点，手机连接其热点2.将PC的ip设置为手机的代理ip，Settings &gt; General &gt; Network &gt; Wi-Fi.3.设置完毕后，电脑上会出现一个提示窗口，询问是否信任此设备，此时点击Allow按钮即可。这样手机就和PC连在同一个局域网内了，而且设置了Charles的代理，即Charles可以抓取到流经App的数据包了。4.安装Charles的HTTPS证书，在电脑上打开Help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser5.在手机浏览器中打开chls.pro/ssl下载证书，点击“设置”→“通用”→“关于本机”→“证书信任设置”中将证书的完全信任开关打开。 Android系统1.将手机和电脑连在同一个局域网下，可以将PC设置为热点，手机连接其热点2.将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings3.设置完毕后，电脑上就会出现一个提示窗口，询问是否信任此设备, 此时直接点击Allow按钮即可。4.安装Charles的HTTPS证书，在电脑上打开Help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser5. 在手机浏览器上打开chls.pro/ssl，这时会出现一个提示框，为证书添加一个名称，然后点击“确定”按钮即可完成证书的安装。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据快速写入Csv文件]]></title>
    <url>%2F2016%2F01%2F05%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2F%E6%95%B0%E6%8D%AE%E5%BF%AB%E9%80%9F%E5%86%99%E5%85%A5Csv%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[csv文件的写入以列表的方式写入import csv 导入csv模块with open('data.csv', 'w', encoding='utf-8') as csvfile:writer = csv.writer(csvfile, delimiter=',') 创建阅读器writer.writerow(['id', 'name', 'age']) 向csv文件中写入一行数据writer.writerows([ ['1', '2', '3'], ['2', '3', '4'] ]) 向csv文件中写入多行数据以字典的方式写入import csv 导入csv模块with open('data.csv', 'w', encoding='utf-8') as csvfile:fieldnames = ['id', 'name', 'age']writer = csv.DictWriter(csvfile, fieldnames=fieldnames) 创建阅读器， 并指定列索引writer.writeheader() 向文件中写入列的索引writer.writerow({'id': 1, 'name': 2, 'age': 3}) 向csv文件中写入一行数据csv文件的读取csv文件按行读取import csvwith open('data.csv', 'r', encoding='utf-8') as csvfile:reader = csv.reader(csvfile) 创建阅读器for row in reader:print(row)&gt;&gt; ['id', 'name', 'age']使用pandas进行csv文件的写入和读取读取csv文件数据的方法df_obj = pd.read_csv('filename', [usecols=['col1', 'col2'], skiprows=n1, skipfooter=n2, index_col=n3, engine='python']) 返回的是DataFrame对象filename：表示读取的文件名usecols：表示读取文件的哪些列skiprows: 表示跳过文件的前几行skipfooter： 表示跳过文件的后几行index_col：表示使用那一列作为索引列engine： 使用的解释器引擎， 当读取中文文件的时候需要指定写入csv数据的方法df_obj.to_csv(&quot;filename&quot;)filename：表示读取的文件名,要加上.csv后缀；]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫扩展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
