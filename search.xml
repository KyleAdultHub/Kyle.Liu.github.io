<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>sklearn 逻辑回归</title>
      <link href="/2018/11/03/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/11/03/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h3 id="逻辑回归作用"><a href="#逻辑回归作用" class="headerlink" title="逻辑回归作用"></a>逻辑回归作用</h3><p>可以做概率预测，也可用于分类，仅能用于线性问题。通过计算真实值与预测值的概率，然后变换成损失函数，求损失函数最小值来计算模型参数，从而得出模型。</p><h3 id="sklearn调用接口"><a href="#sklearn调用接口" class="headerlink" title="sklearn调用接口"></a>sklearn调用接口</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">linear_model</span>.<span class="title">LogisticRegression</span><span class="params">(penalty=’l2’, dual=False, tol=<span class="number">0.0001</span>, C=<span class="number">1.0</span>, fit_intercept=True, intercept_scaling=<span class="number">1</span>, class_weight=None, *random_state=None*, solver=’liblinear‘, max_iter=<span class="number">100</span>, multi_class=’ovr’, verbose=<span class="number">0</span>, warm_start=False, n_jobs=<span class="number">1</span>)</span></span></span><br></pre></td></tr></table></figure><a id="more"></a><p>参数**</p><p>　　=&gt; <strong>penalty</strong> : str, ‘l1’ or ‘l2’　　</p><p>　　　　LogisticRegression和LogisticRegressionCV默认就带了正则化项。penalty参数可选择的值为”l1”和”l2”，分别对应L1的正则化和L2的正则化，默认是L2的正则化。</p><p>　　　　在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。</p><p>　　　　另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。</p><p>　　　　penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。</p><p>　　　　但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。</p><p>　　=&gt; <strong>dual</strong> : bool</p><p>　　　　对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False</p><p>　　=&gt; <strong>tol</strong> : float, optional</p><p>　　迭代终止判据的误差范围。</p><p>　　=&gt; <strong>C</strong> : float, default: 1.0</p><p>　　　　C为正则化系数λ的倒数，通常默认为1。设置越小则对应越强的正则化。</p><p>　　=&gt; <strong>fit_intercept</strong> : bool, default: True</p><p>　　　　是否存在截距，默认存在</p><p>　　=&gt; <strong>intercept_scaling</strong> : float, default 1.</p><p>　　　　仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。</p><p>　　=&gt; <strong>class_weight</strong> : dict or ‘balanced’, default: None</p><p>　　　　class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重，</p><p>　　　　或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。</p><p>　　　　如果class_weight<strong>选择**</strong>balanced**，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低；样本量越少，则权重越高。</p><p>　　　　当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))</p><p>　　　　n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3]  0,1分别出现2次和三次</p><p>　　　　<strong>那么**</strong>class_weight<strong>**有什么作用呢？</strong></p><p>​        　　在分类模型中，我们经常会遇到两类问题：</p><p>​       　　 第一种是<strong>误分类的代价很高</strong>。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。</p><p>​      　　 第二种是<strong>样本是高度失衡的</strong>，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。</p><p>　　　　这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。</p><p>　　=&gt; <strong>random_state</strong> : int, RandomState instance or None, optional, default: None</p><p>　　　　随机数种子，默认为无，仅在正则化优化算法为sag,liblinear时有用。</p><p>　　=&gt; <strong>solver</strong> : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}</p><p>　　　　solver参数决定了我们对逻辑回归损失函数的优化方法，有4种算法可以选择，分别是：</p><p>　　　　　　a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。</p><p>　　　　　　b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</p><p>　　　　　　c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</p><p>　　　　　　d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候，SAG是一种线性收敛算法，这个速度远比SGD快。</p><p>　　　　从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear则既可以用L1正则化也可以用L2正则化。</p><p>　　　　同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了</p><p>　　=&gt; <strong>max_iter</strong> : int, optional</p><p>　　　　仅在正则化优化算法为newton-cg, sag and lbfgs 才有用，算法收敛的最大迭代次数。</p><p>　　=&gt; <strong>multi_class</strong> : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’　　　　</p><p> 　  　　OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。</p><p>　　　　其他类的分类模型获得以此类推。</p><p>​         　 而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，</p><p>　　　   得到模型参数。我们一共需要T(T-1)/2次分类。</p><p>　　　   可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。</p><p>　　　   但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。</p><p>　　=&gt; <strong>verbose</strong> : int, default: 0</p><p>　　=&gt; <strong>warm_start</strong> : bool, default: False</p><p>　　=&gt; <strong>n_jobs</strong> : int, default: 1</p><p>　　　　如果multi_class =’ovr’“，并行数等于CPU内核数量。当“solver”设置为“liblinear”时，无论是否指定“multi_class”，该参数将被忽略。如果给定值-1，则使用所有内核。</p><p>　　</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
            <tag> sklearn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>过拟合、正则化(Regularization)</title>
      <link href="/2018/11/02/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
      <url>/2018/11/02/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h3 id="1-过拟合问题"><a href="#1-过拟合问题" class="headerlink" title="1.过拟合问题"></a>1.过拟合问题</h3><h4 id="过拟合问题描述"><a href="#过拟合问题描述" class="headerlink" title="过拟合问题描述"></a>过拟合问题描述</h4><p>常用的机器学习优化算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(over-fitting)的问题，可能会导致它们效果很差。使用正则化技术可以减少过拟合问题。<br>如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。<br>下图是一个回归问题的例子：</p><p><img src="/img/1541172957837.png" alt="1541172957837"></p><a id="more"></a><p>第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。<br>分类问题中也存在这样的问题：</p><p><img src="/img/1541173045662.png" alt="1541173045662"></p><p>就以多项式理解， 的次数越高，拟合的越好，但相应的预测的能力就可能变差。</p><blockquote><p>如果我们发现了过拟合问题，应该如何处理？</p></blockquote><ol><li>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA）</li><li>正则化。 保留所有的特征，但是减少参数的大小（magnitude）。</li></ol><h3 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2.代价函数"></a>2.代价函数</h3><p>上面的回归问题中如果我们的模型是：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3 + θ_4x_4^4$ 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数 θ 的值，这就是正则化的基本方法。</p><p>我们决定要减少θ3和 θ4的大小，我们要做的便是修改代价函数，在其中 θ3和 θ4 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 θ3和θ4。 修改后的代价函数如下：</p><p>$min_θ {1\over 2m}[h_θ(x^{(i)} - y^{(i)})^2 + 1000θ_3^2 + 10000θ_4^2]$</p><p>通过这样的代价函数选择出的 θ3和 θ4 对预测结果的影响就比之前要小许多， 因为为了θ3和θ4的系数很大，为了优化代价函数， 其值一定会特别的小。</p><p>假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p><blockquote><p>$J_θ = {1\over 2m}[\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)}) + λ\sum_{j=1}^nθ_j^2]$<br>其中又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对 θ0 进行惩罚。</p></blockquote><p>经过正则化处理的模型与原模型的可能对比如下图所示：</p><p><img src="/img/1541174343216.png" alt="1541174343216"></p><p>如果选择的正则化参数λ过大，则会把所有的参数都最小化了，导致模型变成 $h_θ(x) = θ_0$，也就是上图中红色直线所示的情况，造成欠拟合。 </p><p>那为什么增加的一项 $λ\sum_{j=1}^nθ_j^2$  可以使的值减小呢？ </p><p>因为如果我们令 λ的值很大的话，为了使Cost Function 尽可能的小，所有的 θ 的值（不包括θ0） 都会在一定程度上减小。 但若λ的值太大了，那么不包括θ0）都会趋近于0，这样我们所得到的只能是一条平行于轴的直线。 所以对于正则化，我们要取一个合理的 λ 的值，这样才能更好的应用正则化。 回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。</p><h3 id="3-正则化线性回归"><a href="#3-正则化线性回归" class="headerlink" title="3.正则化线性回归"></a>3.正则化线性回归</h3><p>对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。<br>正则化线性回归的代价函数为：</p><p> $J_{(θ)} = {1\over 2m} \sum_{i=1}^m[(h_θ(x^{(i)})-y^{(i)})^2 + λ\sum_{j=1}^nθ_j^2]$</p><p>如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对θ进行正则化，所以梯度下降算法将分两种情形：</p><p><img src="/img/1541174923066.png" alt="1541174923066"></p><h3 id="4-正规方程逻辑回归"><a href="#4-正规方程逻辑回归" class="headerlink" title="4.正规方程逻辑回归"></a>4.正规方程逻辑回归</h3><p>针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数J(θ)，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数J(θ)。</p><p><img src="/img/1541175045035.png" alt="1541175045035"></p><p><strong>用Python实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costReg</span><span class="params">(theta, X, y, learningRate)</span>:</span></span><br><span class="line">theta = np.matrix(theta)</span><br><span class="line">X = np.matrix(X)</span><br><span class="line">y = np.matrix(y)</span><br><span class="line">first = np.multiply(‐y, np.log(sigmoid(X*theta.T)))</span><br><span class="line">second = np.multiply((<span class="number">1</span> ‐ y), np.log(<span class="number">1</span> ‐ sigmoid(X*theta.T)))</span><br><span class="line">reg = (learningRate / (<span class="number">2</span> * len(X))* np.sum(np.power(theta[:,<span class="number">1</span>:theta.shape[<span class="number">1</span>]],<span class="number">2</span>))</span><br><span class="line"><span class="keyword">return</span> np.sum(first ‐ second) / (len(X)) + reg</span><br></pre></td></tr></table></figure><p><strong>要最小化该代价函数，通过求导，得出梯度下降算法为：</strong></p><p><img src="/img/1541175163753.png" alt="1541175163753"></p><p>注：看上去同线性回归一样，但是知道 $h_θ(x) = g(θ^TX)$ ，所以与线性回归不同。 Octave 中，我们依旧可以用fminuc 函数来求解代价函数最小化的参数，值得注意的是θ0参数的更新规则与其他情况不同。 </p><blockquote><p>注意：</p><ol><li>虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者hθ(x)的不同所以还是有很大差别。</li><li>θ不参与其中的任何一个正则化。</li></ol></blockquote><h3 id="5-其他防止过拟合的方法"><a href="#5-其他防止过拟合的方法" class="headerlink" title="5.其他防止过拟合的方法"></a>5.其他防止过拟合的方法</h3><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>Dropout是在深度学习中降低过拟合风险的常见方法，它是由大牛Hinton老爷子提出的。老爷子认为在神经网络产生过拟合主要是因为神经元之间的协同作用产生的。因此老爷子在神经网络进行训练的时候，让部分神经元失活，这样就阻断了部分神经元之间的协同作用，从而强制要求一个神经元和随机挑选出的神经元共同进行工作，减轻了部分神经元之间的联合适应性。<br>Dropout的具体流程如下：</p><p>1.对 l 层第 j 个神经元产生一个随机数 $r^{(l)}_j∼Bernouli(p)$</p><p>2.将 l 层第 j 个神经元的输入乘上产生的随机数作为这个神经元新的输入：$x^{(l)∗}_j=x^{(l)}_j∗r^{(l)}_j$</p><p>3.此时第 l 层第 j 个神经元的输出为：<br>$y{(l+1)}<em>j=f(∑^k</em>{j=1}(w^{(l+1)}_j  ∗  x^{(l)∗}_j  +  b^{(l+1)}))$<br>其中，k为第l层神经元的个数，f为该神经元的激活函数，b为偏置，w为权重向量。<br>注意：当我们采用了Dropout之后再训练结束之后，应当将网络的权重乘上概率p得到测试网络的权重，或者可以在训练时，将样本乘上 1/P。</p><h4 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h4><p>在对模型进行训练时，我们可以将我们的数据集分为三个部分，训练集、验证集、测试集。我们在训练的过程中，可以每隔一定量的step，使用验证集对训练的模型进行预测，一般来说，模型在训练集和验证集的损失变化如下图所示： </p><p><img src="/img/1541249965141.png" alt="1541249965141"></p><p>可以看出，模型在验证集上的误差在一开始是随着训练集的误差的下降而下降的。当超过一定训练步数后，模型在训练集上的误差虽然还在下降，但是在验证集上的误差却不在下降了。此时我们的模型就过拟合了。因此我们可以观察我们训练模型在验证集上的误差，一旦当验证集的误差不再下降时，我们就可以提前终止我们训练的模型。</p><h4 id="增加样本量"><a href="#增加样本量" class="headerlink" title="增加样本量"></a>增加样本量</h4><p>在实际的项目中，你会发现，上面讲述的那些技巧虽然都可以减轻过拟合的风险，但是却都比不上增加样本量来的更实在。</p><p>为什么增加样本可以减轻过拟合的风险呢？这个要从过拟合是啥来说。过拟合可以理解为我们的模型对样本量学习的太好了，把一些样本的特殊的特征当做是所有样本都具有的特征。举个简单的例子，当我们模型去训练如何判断一个东西是不是叶子时，我们样本中叶子如果都是锯齿状的话，如果模型产生过拟合了，会认为叶子都是锯齿状的，而不是锯齿状的就不是叶子了。如果此时我们把不是锯齿状的叶子数据增加进来，此时我们的模型就不会再过拟合了。因此其实上述的那些技巧虽然有用，但是在实际项目中，你会发现，其实大多数情况都比不上增加样本数据来的实在。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>sklearn 线型回归</title>
      <link href="/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h2 id="简单线性回归"><a href="#简单线性回归" class="headerlink" title="简单线性回归"></a>简单线性回归</h2><p>线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。</p><a id="more"></a><h3 id="使用sklearn-linear-model-LinearRegression进行线性回归"><a href="#使用sklearn-linear-model-LinearRegression进行线性回归" class="headerlink" title="使用sklearn.linear_model.LinearRegression进行线性回归"></a>使用sklearn.linear_model.LinearRegression进行线性回归</h3><p>sklearn对Data Mining的各类算法已经有了较好的封装，基本可以使用<code>fit</code>、<code>predict</code>、<code>score</code>来训练、评价模型，并使用模型进行预测，一个简单的例子如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">clf = linear_model.LinearRegression()</span><br><span class="line">X = [[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">y = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">clf.fit(X,y)</span><br><span class="line">print(clf.coef_)</span><br><span class="line">[ <span class="number">0.5</span> <span class="number">0.5</span>]</span><br><span class="line">print(clf.intercept_)</span><br><span class="line"><span class="number">1.11022302463e-16</span></span><br></pre></td></tr></table></figure><p><code>LinearRegression</code>已经实现了多元线性回归模型，当然，也可以用来计算一元线性模型，通过使用list[list]传递数据就行。下面是<code>LinearRegression</code>的具体说明。</p><h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><h5 id="实例化"><a href="#实例化" class="headerlink" title="实例化"></a>实例化</h5><p>sklearn一直秉承着简洁为美得思想设计着估计器，实例化的方式很简单，使用<code>clf = LinearRegression()</code>就可以完成，但是仍然推荐看一下几个可能会用到的参数：</p><ul><li><code>fit_intercept</code>：是否存在截距，默认存在</li><li><code>normalize</code>：标准化开关，默认关闭</li></ul><p>还有一些参数感觉不是太有用，就不再说明了，可以去官网文档中查看。</p><h5 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h5><p>其实在上面的例子中已经使用了<code>fit</code>进行回归计算了，使用的方法也是相当的简单。</p><ul><li><code>fit(X,y,sample_weight=None)</code>：<code>X</code>,<code>y</code>以矩阵的方式传入，而<code>sample_weight</code>则是每条测试数据的权重，同样以<code>array</code>格式传入。</li><li><code>predict(X)</code>：预测方法，将返回预测值<code>y_pred</code></li><li><code>score(X,y,sample_weight=None)</code>：评分函数，将返回一个小于1的得分，可能会小于0</li></ul><h5 id="方程"><a href="#方程" class="headerlink" title="方程"></a>方程</h5><p><code>LinearRegression</code>将方程分为两个部分存放，<code>coef_</code>存放回归系数，<code>intercept_</code>则存放截距，因此要查看方程，就是查看这两个变量的取值。</p><h4 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h4><p>其实，多项式就是多元回归的一个变种，只不过是原来需要传入的是X向量，而多项式则只要一个x值就行。通过将x扩展为指定阶数的向量，就可以使用<code>LinearRegression</code>进行回归了。sklearn已经提供了扩展的方法——<code>sklearn.preprocessing.PolynomialFeatures</code>。利用这个类可以轻松的将x扩展为X向量，下面是它的使用方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures</span><br><span class="line">&gt;&gt;&gt; X_train = [[1],[2],[3],[4]]</span><br><span class="line">&gt;&gt;&gt; quadratic_featurizer = PolynomialFeatures(degree=2)</span><br><span class="line">&gt;&gt;&gt; X_train_quadratic = quadratic_featurizer.fit_transform(X_train)</span><br><span class="line">&gt;&gt;&gt; print(X_train_quadratic)</span><br><span class="line">[[ 1  1  1]</span><br><span class="line"> [ 1  2  4]</span><br><span class="line"> [ 1  3  9]</span><br><span class="line"> [ 1  4 16]]</span><br></pre></td></tr></table></figure><p>经过以上处理，就可以使用<code>LinearRegression</code>进行回归计算了。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sklearn </tag>
            
            <tag> 线型回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>逻辑回归 &amp; 分类问题</title>
      <link href="/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/10/31/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h3 id="1-分类问题"><a href="#1-分类问题" class="headerlink" title="1. 分类问题"></a>1. 分类问题</h3><p>在分类问题中，如果想要预测的变量y是离散的值，那么可以只用一种叫做逻辑回归(logistic regression)的算法,这是在目前最流行的一种学习算法之一。</p><p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。  </p><a id="more"></a><p><img src="/img/1540997286168.png" alt="1540997286168"></p><p>我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈0,1 ，其中 0 表示负向类，1 表示正向类。</p><p><img src="/img/1540997531192.png" alt="1540997531192"></p><h4 id="线型回归不适合分类问题"><a href="#线型回归不适合分类问题" class="headerlink" title="线型回归不适合分类问题"></a>线型回归不适合分类问题</h4><p>如果我们要用线性回归算法来解决一个分类问题，对于分类， y 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 y 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。</p><h4 id="逻辑回归算法"><a href="#逻辑回归算法" class="headerlink" title="逻辑回归算法"></a>逻辑回归算法</h4><p>逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y 取值离散的情况，如：1 0 0 1。</p><h3 id="2-逻辑回归表达式"><a href="#2-逻辑回归表达式" class="headerlink" title="2. 逻辑回归表达式"></a>2. 逻辑回归表达式</h3><p>在分类问题中，要用什么样的函数来表示我们的假设。在解决分类问题的时候，我们希望分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。</p><h4 id="为什么线型回归不适合分类问题"><a href="#为什么线型回归不适合分类问题" class="headerlink" title="为什么线型回归不适合分类问题?"></a>为什么线型回归不适合分类问题?</h4><p>回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线：</p><p><img src="/img/1541078937881.png" alt="1541078937881"></p><p>根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：</p><p>当 $h_θ(x) &gt;= 0.5$ 时, 预测y= 1;   当$h_θ(x) &lt; 0.5$ 时，预测y = 0</p><p>对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。但是， 假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。</p><p><img src="/img/1541079100064.png" alt="1541079100064"></p><p>这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了， 所以使用线型回归模型，很难确定断定一个问题分类的边界。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决分类这样的问题。</p><h4 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h4><p>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是：</p><p>$h_θ(x) = g(θ^TX)$      函数g的表达式为:  $g(z) = {1\over1+e^{-z}}$</p><blockquote><p> 其中: X 代表特征向量, g代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function）</p></blockquote><p>g(z) 的函数图像为:</p><p><img src="/img/1541080097808.png" alt="1541080097808"></p><blockquote><p>对逻辑回归模型理解</p><p>$h_θ(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算出输出的变量=1 的可能性, 即$h_θ(x) =P(y=1|x; θ)$ </p><p>例如, 如果对于给定的x， 通过已经确定的参数计算出$h_θ(x) = 0.7$ ， 则表示有70%的几率y为正向类，响应地y为负向类的几率为1-0.7=0.3</p><p>g(z) （Sigmoid function）函数的作用就是将$θ^TX$的结果映射到0-1之间，</p></blockquote><h3 id="3-决策边界"><a href="#3-决策边界" class="headerlink" title="3. 决策边界"></a>3. 决策边界</h3><h4 id="对决策边界的理解"><a href="#对决策边界的理解" class="headerlink" title="对决策边界的理解"></a>对决策边界的理解</h4><blockquote><p>决策边界(decision boundary)，能够更好地帮助我们理解逻辑回归的假设函数中计算什么</p></blockquote><p><img src="/img/1541080767309.png" alt="1541080767309"></p><p>在逻辑回归中， 我们预测到: </p><p>当$h_θ(x) &gt;= 0.5$ 时， 预测 y= 1;     当 $h_θ(x) &gt;= 0.5$ 时，预测 y  = 0；</p><p>根据上面绘制的S形函数图像，我们知道当</p><p>z = 0 时, g(z) = 0.5</p><p>z &gt; 时, g(z) &gt; 0.5</p><p>z &lt; 0 时, g(z) &lt; 0.5</p><p>又 $z = θ^Tx$, 即: $θ^TX &gt;= 0$ 时, 预测 y = 1 , $θ^TX &lt; 0$ 时， 预测 y = 0</p><p>现在假设我们有一个模型: </p><p><img src="/img/1541081223807.png" alt="1541081223807"></p><p>并且参数θ是向量[-3 1 1]. 则当 -3 + x1 + x2 &gt;= 0, 即x1 + x2 &gt;= 3时， 模型将预测 y = 1. 我们可以绘制直线x1 + x2 = 3, 这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。</p><p><img src="/img/1541081375902.png" alt="1541081375902"></p><h4 id="复杂形状的决策边界"><a href="#复杂形状的决策边界" class="headerlink" title="复杂形状的决策边界"></a>复杂形状的决策边界</h4><p>假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？</p><p><img src="/img/1541081567959.png" alt="1541081567959"></p><p>因为需要用曲线才能分隔 y = 0 的区域和 y = 1 的区域，我们需要二次方特征：</p><p>所以按照假设函数应该趋近， $h_θ(x) = g(θ_0 + θ_1x_1 + θ_2x_2 + θ_1x_1^2 + θ_2x_2^2 )$  θ最终结果趋近 [-1 0 0 1 1], 我们最终得到的判定边界恰好是圆点在原点且半径为1的圆形 , 综上我们可以用复杂的模型来适应飞象复杂形状的判定边界</p><h3 id="4-逻辑回归代价函数和梯度下降"><a href="#4-逻辑回归代价函数和梯度下降" class="headerlink" title="4. 逻辑回归代价函数和梯度下降"></a>4. 逻辑回归代价函数和梯度下降</h3><h4 id="逻辑回归代价函数及简化"><a href="#逻辑回归代价函数及简化" class="headerlink" title="逻辑回归代价函数及简化"></a>逻辑回归代价函数及简化</h4><p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_θ(x) = g(θ^TX)$  带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）如下图，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p><p><img src="/img/1541082558471.png" alt="1541082558471"></p><p>线型回归代价函数为: $J_{(θ)} = {1\over m} \sum_{i=1}^m{1\over2}(h_θ(x^{(i)})-y^{(i)})^2​$</p><p>重新定义逻辑回归代价函数，使代价函数在逻辑回归算法中为凸函数, 过程如下: </p><p><img src="/img/1541083131135.png" alt="1541083131135"></p><blockquote><p>根据推倒可以证明, 我们推倒的的代价值函数会给我们一个凸优化问题。代价函数 J(θ) 会是一个凸函数，并且没有局部最优值。</p></blockquote><p><strong>python代码实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">   theta = np.matrix(theta)</span><br><span class="line">   X = np.matrix(X)</span><br><span class="line">   y = np.matrix(y)</span><br><span class="line">   first = np.multiply(‐y, np.log(sigmoid(X* theta.T)))</span><br><span class="line">   second = np.multiply((<span class="number">1</span> ‐ y), np.log(<span class="number">1</span> ‐ sigmoid(X* theta.T)))</span><br><span class="line">   <span class="keyword">return</span> np.sum(first ‐ second) / (len(X))</span><br></pre></td></tr></table></figure><h4 id="梯度下降算法推倒及简化"><a href="#梯度下降算法推倒及简化" class="headerlink" title="梯度下降算法推倒及简化"></a>梯度下降算法推倒及简化</h4><p>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：</p><p>Repeat {  $θ_j : =θ_j  −  α{∂\over∂θ_j }J(θ)$         (simultaneously update all ) }</p><p>求导后得到: </p><p>Repeat { $θ_j : =θ<em>j  −  α{1\over m}\sum</em>{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$    (simultaneously update all ) }</p><p><img src="/img/1541084166476.png" alt="1541084166476"></p><blockquote><p>注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的 $h_θ(x) = g(θ^TX)$ 与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。</p></blockquote><h3 id="5-高级优化算法"><a href="#5-高级优化算法" class="headerlink" title="5. 高级优化算法"></a>5. 高级优化算法</h3><h4 id="一些高级算法的介绍"><a href="#一些高级算法的介绍" class="headerlink" title="一些高级算法的介绍"></a>一些高级算法的介绍</h4><p>现在我们换个角度来看，什么是梯度下降，我们有个代价函数 J(θ)，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数θ时，它们会计算出两样东西：J(θ) 以及 J 等于 0、1直到 n 时的偏导数项。</p><p><img src="/img/1541166796206.png" alt="1541166796206"></p><p>假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。 另一种考虑梯度下降的思路是：我们需要写出代码来计算 J(θ) 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。 对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数J(θ)。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些 J(θ)的收敛性，那么我们就需要自己编写代码来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$。所以，在写完能够计算这两者的代码之后，我们就可以使用梯度下降。 </p><p>然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$两个项的话，那么这些算法就是为我们优化代价函数的不同方法，<strong>共轭梯度法、 BFGS (变尺度法)、 L-BFGS (限制变尺度法)</strong> 就是其中一些更高级的优化算法，它们需要有一种方法来计算 J(θ)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。</p><blockquote><p><strong>这三种算法的优点：</strong></p><p>一个是使用这其中任何一个算法，你通常不需要手动选择学习率 α ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 α ，并自动选择一个好的学习速率 α，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。</p></blockquote><p>Octave 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，因此，如果你正在你的机器学习程序中使用一种不同的语言，比如如果你正在使用C、C++、Java等等，你可能会想尝试一些不同的库，以确保你找到一个能很好实现这些算法的库。</p><h4 id="如何使用这些算法"><a href="#如何使用这些算法" class="headerlink" title="如何使用这些算法"></a>如何使用这些算法</h4><p>比方说，你有一个含两个参数的问题，这两个参数是θ1和θ2，因此，通过这个代价函数，你可以得到 θ1 和 θ2 的值，如果你将 J(θ) 最小化的话，那么它的最小值将是 θ1 = 5 ，θ2 = 5 。代价函数 J(θ) 的导数推出来就是这两个表达式：</p><p>$α{∂\over∂θ_1}J(θ) = 2(θ_1 - 5)$</p><p>$α{∂\over∂θ_2}J(θ) = 2(θ_2 - 5)$</p><p>如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的Octave 函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function [jVal, gradient]=costFunction(theta)</span><br><span class="line">jVal=(theta(1)‐5)^2+(theta(2)‐5)^2;</span><br><span class="line">gradient=zeros(2,1);</span><br><span class="line">gradient(1)=2*(theta(1)‐5);</span><br><span class="line">gradient(2)=2*(theta(2)‐5);</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个costFunction 函数后，你就可以调用高级的优化函数，这个函数叫fminunc，它表示Octave 里无约束最小化函数。调用它的方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">options=optimset(<span class="string">'GradObj'</span>,<span class="string">'on'</span>,<span class="string">'MaxIter'</span>,<span class="number">100</span>);</span><br><span class="line">initialTheta=zeros(<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">[optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></table></figure><p>你要设置几个options，这个 options 变量作为一个数据结构可以存储你想要的options，所以 GradObj 和On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说100，我们给出一个 的猜测初始值，它是一个2×1的向量，那么这个命令就调用fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的值。</p><p><strong>实际运行过程示例</strong></p><p><img src="/img/1541168153304.png" alt="1541168153304"></p><h3 id="6-多分类问题"><a href="#6-多分类问题" class="headerlink" title="6. 多分类问题"></a>6. 多分类问题</h3><h4 id="多分类的介绍"><a href="#多分类的介绍" class="headerlink" title="多分类的介绍"></a>多分类的介绍</h4><p><strong>一些多分类的例子:</strong></p><p>例一: 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用 y=1, y=2, y=3, y=4 来表示</p><p>例二: 如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用 y=1 这个类别来代表；或者患了感冒，用 y=2 来代表；或者得了流感用 y=3 来代表.</p><p>然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样：</p><p><img src="/img/1541168782303.png" alt="1541168782303"></p><p>对于一个多类分类问题，我们的数据集或许看起来像这样：</p><p><img src="/img/1541168809943.png" alt="1541168809943"></p><h4 id="一对于多分类思路"><a href="#一对于多分类思路" class="headerlink" title="一对于多分类思路"></a>一对于多分类思路</h4><p>我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为<strong>“一对余”</strong>方法。</p><p><img src="/img/1541169223823.png" alt="1541169223823"></p><p>现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 y = 1 ，方框表示 y = 2，叉叉表示 y = 3。下面要做的就是使用一个训练集，将其分成3个二元分类问题。我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。</p><p>这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。</p><p><img src="/img/1541169503523.png" alt="1541169503523"></p><p>为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作$h_θ^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作$h_θ^{(2)}(x)$ ,依此类推。 最后我们得到一系列的模型简记为：$h_θ^{(i)}(x) = P(y=i|x; θ)$ 其中：i = (1, 2, 3, 4, 5, …, k) k为总分类数量<br>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。<br>总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_θ^{(i)}(x)$ ， 其中 i 对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 x ，然后我们选择一个让 $h_θ^{(i)}(x)$ 最大的 i 作为最终的分类结果。<br>现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论值是多少，我们都有最高的概率值，我们预测就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> 分类问题 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Octave基础操作</title>
      <link href="/2018/10/26/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/octave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/10/26/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/octave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="基础功能命令"><a href="#基础功能命令" class="headerlink" title="基础功能命令"></a>基础功能命令</h2><h4 id="修改命令行的提示"><a href="#修改命令行的提示" class="headerlink" title="修改命令行的提示"></a>修改命令行的提示</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PS1('&gt;&gt; ')   % &gt;&gt; 就是修改后的提示符号</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="变量赋值语句"><a href="#变量赋值语句" class="headerlink" title="变量赋值语句"></a>变量赋值语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = 5 + 6 ; % 后面加上一个可以不在终端打印变量的值</span><br></pre></td></tr></table></figure><h4 id="显示工作空间的所有变量"><a href="#显示工作空间的所有变量" class="headerlink" title="显示工作空间的所有变量"></a>显示工作空间的所有变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">who  % 显示工作空间的所有变量</span><br><span class="line">whos   % 显示工作空间的所有变量和详细信息</span><br></pre></td></tr></table></figure><h4 id="删除变量"><a href="#删除变量" class="headerlink" title="删除变量"></a>删除变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clear A % 删除变量A</span><br><span class="line">clear  % 删除所有变量</span><br></pre></td></tr></table></figure><h4 id="打印变量"><a href="#打印变量" class="headerlink" title="打印变量"></a>打印变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A    % 直接再终端输入变量名称就可以将变量的值打印出来</span><br><span class="line">disp(A)    % 通过disp函数将变量打印出来</span><br></pre></td></tr></table></figure><h4 id="修改全局的输出内容的长短"><a href="#修改全局的输出内容的长短" class="headerlink" title="修改全局的输出内容的长短"></a>修改全局的输出内容的长短</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">format long   % 将输出数值的长度定义为long类型</span><br><span class="line">format short   % 将输出数值的长度定义为short类型</span><br></pre></td></tr></table></figure><h4 id="查看命令的帮助信息"><a href="#查看命令的帮助信息" class="headerlink" title="查看命令的帮助信息"></a>查看命令的帮助信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helo rand</span><br><span class="line">help eye</span><br><span class="line">help help</span><br></pre></td></tr></table></figure><h4 id="添加搜索路径"><a href="#添加搜索路径" class="headerlink" title="添加搜索路径"></a>添加搜索路径</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">addpath path % 添加路径到函数和数据等的某人搜索路径</span><br></pre></td></tr></table></figure><h2 id="基础运算"><a href="#基础运算" class="headerlink" title="基础运算"></a>基础运算</h2><h4 id="数值运算"><a href="#数值运算" class="headerlink" title="数值运算"></a>数值运算</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3-2； 5*8；  1/2； % 基础运算</span><br></pre></td></tr></table></figure><h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 &amp;&amp; 0  % 逻辑与</span><br><span class="line">1 || 0  % 逻辑或</span><br><span class="line">~ 1  % 逻辑费</span><br><span class="line">XOR(1, 0)   % 异或运算</span><br></pre></td></tr></table></figure><h4 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 == 2   % 相等判断</span><br><span class="line">1 ~= 2   % 不等于判断</span><br></pre></td></tr></table></figure><h2 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h2><h4 id="创建矩阵"><a href="#创建矩阵" class="headerlink" title="创建矩阵"></a>创建矩阵</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">v = [ 1 2 3 ]  % 创建一个行向量</span><br><span class="line">v = 1:6   % 创建一个从1到6的行向量</span><br><span class="line">v = 1:0.1:2    % 创建一个从1开始，以0.1为步长，直到2的行向量</span><br><span class="line">v = ones(2, 3)   % 创建一个2行3列的元素都是1的矩阵</span><br><span class="line">v = 2 * ones(2, 3)   % 创建一个2行3列的元素都是2的矩阵</span><br><span class="line">v = zeros(2, 3)   % 创建一个2行3列的元素都是0的矩阵</span><br><span class="line">v = rand(2, 3)   % 创建一个2行3列的元素都是0-1之间的随机数的矩阵</span><br><span class="line">v = randn(2, 3)   % 创建一个2行3列的元素都符合平局值为0,1为标准差的正态分布随机数的矩阵</span><br><span class="line">I = eye(6)    % 创建一个大小为6的单位矩阵</span><br><span class="line">v = type(3)   % 返回一个3*3的随机矩阵</span><br></pre></td></tr></table></figure><h4 id="矩阵运算-1"><a href="#矩阵运算-1" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">B = [ 11, 12; 13 14; 15 16 ]</span><br><span class="line">2 * A  % A矩阵中的每个元素都乘以2</span><br><span class="line">A .* B % 将A矩阵和B矩阵中的每一个对应位置的元素相乘,同样适用于+ - / ^等</span><br><span class="line">A .*   % A矩阵的每个元素取二次方</span><br><span class="line">log(v)     % 矩阵的每个院对对数运算</span><br><span class="line">exp(v)     % 矩阵的每个元素进行以为底，以这些元素为幂的运算</span><br><span class="line">abs(v)     % 对v矩阵的每个元素取绝对值</span><br><span class="line">A + 1    % 将A矩阵的每个元素加上1</span><br><span class="line">A&apos;     % 取A矩阵的转置矩阵</span><br></pre></td></tr></table></figure><h4 id="获取矩阵尺寸"><a href="#获取矩阵尺寸" class="headerlink" title="获取矩阵尺寸"></a>获取矩阵尺寸</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">size(A)     % 返回A矩阵的尺寸,返回的内容同样是行向量</span><br><span class="line">size(A，1)    % 返回矩阵的行数</span><br><span class="line">size(A，2)   % 返回矩阵的列数</span><br><span class="line">lengh(A)     % 返回A矩阵的维度大小，即矩阵的长宽中比较大的维度，矩阵A的length即为3</span><br></pre></td></tr></table></figure><h4 id="矩阵的索引"><a href="#矩阵的索引" class="headerlink" title="矩阵的索引"></a>矩阵的索引</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">A(3, 2)  % 取A矩阵的第三行第二列的元素</span><br><span class="line">A(2, :)    % 返回第二行的所有元素</span><br><span class="line">A(:, 2)     % 返回第二列的所有元素</span><br><span class="line">A([1 3], :)  % 取第1行和第3行的所有元素</span><br><span class="line">A(:, 2) = [10; 11; 12]   % 将矩阵的第二列重新赋值</span><br><span class="line">A = [A, [100, 101, 102]]   % 给矩阵附加了一列矩阵，给矩阵增加一列。带，和不带，相同</span><br><span class="line">[A B]    % 两个矩阵直接连在一起，矩阵A在左边，矩阵B在右边</span><br><span class="line">[A;B]    % 两个矩阵直接连在一起，矩阵A在上边, 矩阵B在下边</span><br><span class="line">A(:)     % 将矩阵的所有元素导向一个单独的列向量排列起来</span><br></pre></td></tr></table></figure><h4 id="矩阵的计算"><a href="#矩阵的计算" class="headerlink" title="矩阵的计算"></a>矩阵的计算</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">A = [ 1 2; 3 4; 5 6 ]  % 创建一个三行两列的矩阵</span><br><span class="line">val = max(a)    % 返回a矩阵中的最大值, 当a为多维矩阵的时候返回的是每一列的最大值</span><br><span class="line">val = max(A, a)    % 两个矩阵所有元素逐个比较返回较大的值</span><br><span class="line">max(A,[],1)    % 得到矩阵每一列元素的最大值</span><br><span class="line">max(A,[],2)   % 得到矩阵每一行元素的最大值</span><br><span class="line">[val, ind] = max(a)  % 返回a矩阵中的最大值和对应的索引</span><br><span class="line">sum（a)   % 把a矩阵的所有元素相加起来, 如果是多维矩阵默认为每一列的总和</span><br><span class="line">sum（a,1) % 求多维矩阵每一列的总和</span><br><span class="line">sum(a, 3)   % 求多维矩阵每一行的总和</span><br><span class="line">a&lt;3  % 对矩阵每一个元素进行&lt;3判断，判断为真返回1, 否则返回0</span><br><span class="line">find(a&lt;3)  % 返回哪些元素小于3(是索引值)</span><br><span class="line">prod（a)   % 将a矩阵的所有元素相乘</span><br><span class="line">floor(a)   % 将a矩阵的所有元素进行向下取舍</span><br><span class="line">ceil(a)   % 将a矩阵的所有元素进行向上取整</span><br><span class="line">pinv(v)   % 求v矩阵的逆矩阵</span><br></pre></td></tr></table></figure><h2 id="绘制图像"><a href="#绘制图像" class="headerlink" title="绘制图像"></a>绘制图像</h2><h4 id="绘制直方图"><a href="#绘制直方图" class="headerlink" title="绘制直方图"></a>绘制直方图</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = -6 + sqrt(10) * (randn(1, 10000))</span><br><span class="line">hist(w)    % 绘制w矩阵的直方图</span><br><span class="line">hist(w, 50)    % 绘制w矩阵的直方图，并指定50个长方形</span><br></pre></td></tr></table></figure><h4 id="绘制曲线图"><a href="#绘制曲线图" class="headerlink" title="绘制曲线图"></a>绘制曲线图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">t = [0:0.1:1];</span><br><span class="line">y1 = sin(2*pi*4t);</span><br><span class="line">plot(t,  y1);</span><br><span class="line">hold on % 新绘制的图不会覆盖之前的图片，会在之前的图片上生成新的曲线</span><br><span class="line">y2 = cos(2*pi*4*t)</span><br><span class="line">plot(t, y2, &apos;r&apos;)  % 绘制新的直线图，r表示线的颜色为红色</span><br><span class="line">xlable(&apos;time&apos;)    % 添加x轴名称</span><br><span class="line">ylable(&apos;value&apos;)    % 给y轴添加名称</span><br><span class="line">legend(&apos;sin, &apos;cos&apos;)   % 给线命名</span><br><span class="line">title(&apos;myplot&apos;)    % 给图片一个标题名称</span><br><span class="line">print  -dpng  &apos;myplot.png&apos;    % 输出图片</span><br><span class="line">plot clos   % 关掉图片</span><br><span class="line">axis([0.5 1 ‐1 1])   % 改变坐标返回，前两个参数控制横坐标，后两参数控制纵坐标</span><br><span class="line">Clf  % 清除一个图像</span><br></pre></td></tr></table></figure><h4 id="在一张画纸上绘制两张直线图"><a href="#在一张画纸上绘制两张直线图" class="headerlink" title="在一张画纸上绘制两张直线图"></a>在一张画纸上绘制两张直线图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t = [0:0.1:1];</span><br><span class="line">y1 = sin(2*pi*4t);</span><br><span class="line">y2 = cos(2*pi*4*t)；</span><br><span class="line">figure(1); plot(t, y1);    % 绘制第一张图片</span><br><span class="line">figure(2); plot(t, y2);    % 绘制第二张图片</span><br><span class="line">subplot(1,2,1) %将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。</span><br><span class="line">plot(t,y1)   % 将图片绘制到第一个格子</span><br><span class="line">suplot(1,2,2)   % 使用第二个格子</span><br><span class="line">plot(t,y2)  % 将图片绘制到第二个格子</span><br></pre></td></tr></table></figure><h4 id="彩色格图绘制"><a href="#彩色格图绘制" class="headerlink" title="彩色格图绘制"></a>彩色格图绘制</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">imagesc(A )   % 绘制彩色格子图</span><br><span class="line">imagesc(A)，colorbar，colormap gray   % 生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。</span><br></pre></td></tr></table></figure><h2 id="移动数据"><a href="#移动数据" class="headerlink" title="移动数据"></a>移动数据</h2><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load('featureD.dat')    % 加载featureD.dat中的所有数据，并将其复制给变量featureD</span><br></pre></td></tr></table></figure><h4 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">save hello.mat v   % 将变量A导出为一个叫hello.mat文件 二进制形式</span><br><span class="line">save hello.mat v -ascii  % 将变量A导出为一个叫hello.mat文件 ascii形式</span><br></pre></td></tr></table></figure><h2 id="控制语句"><a href="#控制语句" class="headerlink" title="控制语句"></a>控制语句</h2><h4 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = zeros(10,1)</span><br><span class="line">for i=1:10,</span><br><span class="line">v(i) = 2^1;</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h4 id="while-循环"><a href="#while-循环" class="headerlink" title="while 循环"></a>while 循环</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">v = zeros(10,1)</span><br><span class="line">i = 1</span><br><span class="line">while i &lt;= 5,</span><br><span class="line">v(i) = 100;</span><br><span class="line">i = i+1;</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h4 id="if-else-elif-语句"><a href="#if-else-elif-语句" class="headerlink" title="if - else - elif 语句"></a>if - else - elif 语句</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">v = zeros(10,1)</span><br><span class="line">if v(1) == 1,</span><br><span class="line">disp('1');</span><br><span class="line">elseif v(1) == 2,</span><br><span class="line">disp('2');</span><br><span class="line">else</span><br><span class="line">disp('3');</span><br><span class="line">end</span><br></pre></td></tr></table></figure><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><h4 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h4><p>1.先创建一个文件<br>​    squarethisnumber.m   # .m前定义的就是函数名<br>2.编写函数文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function y = squareThisNumber(x)</span><br><span class="line">y = x^2;</span><br></pre></td></tr></table></figure><blockquote><p>第一行写着 function y = squareThisNumber(x) ，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 里。另外，它告诉了Octave这个函数有一个参数，就是参数 ，还有定义的函数体，y  = x^2</p></blockquote><h4 id="使用自定义函数"><a href="#使用自定义函数" class="headerlink" title="使用自定义函数"></a>使用自定义函数</h4><p>1.切换到函数文件所在目录<br>2.直接通过函数名squareThisNumber() 调用函数</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> octave </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>线型回归 &amp; 梯度下降</title>
      <link href="/2018/10/20/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/10/20/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h2 id="1-模型表示"><a href="#1-模型表示" class="headerlink" title="1.模型表示"></a>1.模型表示</h2><h3 id="问题的概述"><a href="#问题的概述" class="headerlink" title="问题的概述"></a><strong>问题的概述</strong></h3><blockquote><p>在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。</p></blockquote><a id="more"></a><p><img src="/img/1539965168129.png" alt="1539965168129"></p><h3 id="模型引入"><a href="#模型引入" class="headerlink" title="模型引入"></a><strong>模型引入</strong></h3><p>假使我们回归问题的训练集（Training Set）如下表所示：</p><p><img src="/img/1539965299259.png" alt="1539965299259"></p><blockquote><p>我们将要用来描述这个回归问题的标记如下:<br>m 代表训练集中实例的数量<br>x 代表特征/输入变量<br>y 代表目标变量/输出变量<br>(x, y) 代表训练集中的实例<br>(x^i, y^i)代 表第 个观察实例<br>h 代表学习算法的解决方案或函数也称为假设（hypothesis）</p></blockquote><p><img src="/img/1539965543172.png" alt="1539965543172"></p><blockquote><p>这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。 h代表hypothesis(假设)， 表h示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h根据输入的 值来得出 y 值， y 值对应房子的价格因此， h 是一个从 x 到 y 的函数映射。</p><p>我将选择最初的使用规则代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 h？一种可能的表达方式为：<strong>hθ(x)=θ0+θ1∗x</strong> ，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</p></blockquote><h2 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2.代价函数"></a>2.代价函数</h2><h3 id="什么是代价函数"><a href="#什么是代价函数" class="headerlink" title="什么是代价函数"></a>什么是代价函数</h3><p><img src="/img/1539965938888.png" alt="1539965938888"></p><p>在线性回归中我们有一个像这样的训练集， m代表了训练样本的数量 。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：<strong>hθ(x)=θ0+θ1∗x</strong>  。<br>我们通过训练获得的预测函数在预测时必然会和实际值有一些偏差，下面就给出了这部分预测偏差的定义</p><blockquote><p><strong>代价函数的定义</strong>: 我们选择的参数决定了我们得到的模型相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是<strong>建模误差（modeling error）</strong>,下图蓝色线段变为预测和实际的误差。</p><p><strong>平方误差代价函数</strong>: 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。</p></blockquote><p><img src="/img/1540021643524.png" alt="1540021643524"></p><h3 id="怎么优化线型回归模型"><a href="#怎么优化线型回归模型" class="headerlink" title="怎么优化线型回归模型"></a>怎么优化线型回归模型</h3><blockquote><p>优化回归模型的目标: 便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数最小。</p></blockquote><blockquote><p>代价函数公式:</p><p><img src="/img/1540029941521.png" alt="1540029941521"></p></blockquote><h3 id="代价函数坐标图"><a href="#代价函数坐标图" class="headerlink" title="代价函数坐标图"></a>代价函数坐标图</h3><p><img src="/img/1540022174651.png" alt="1540022174651"></p><blockquote><p>可以看出在三维空间中存在一个使得J(θ_0,θ_1)代价函数最小的点， 当代价函数取得最小值的时候，我们会获得最符合训练集的预测模型。</p></blockquote><h2 id="3-梯度下降算法"><a href="#3-梯度下降算法" class="headerlink" title="3.梯度下降算法"></a>3.梯度下降算法</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><blockquote><p>梯度下降是一个用来求函数最小值的算法。其思想是利用迭代的思想来逼近最低点，沿着梯度的负方向更新模型权重。我们将使用梯度下降算法来求出代价函数 J(θ_0,θ_1) 的最小值。</p></blockquote><blockquote><p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0, θ1, …., θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（globalminimum），选择不同的初始参数组合，可能会找到不同的<strong>局部最小值</strong>。</p></blockquote><p><img src="/img/1540024168426.png" alt="1540024168426"></p><h3 id="梯度下降算法公式"><a href="#梯度下降算法公式" class="headerlink" title="梯度下降算法公式:"></a>梯度下降算法公式:</h3><p><strong>公式介绍</strong></p><p>repeat until convergence{<br>$$<br>θ_j=θ_j−α∂/∂θ_j J(θ0,θ1)　(for　j=0　and　j=1)<br>$$<br>}</p><p><img src="/img/1540026766508.png" alt="1540026766508"></p><blockquote><p>参数α: 学习率, 它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。当到达损失函数的最低点的时候，其导数为0，已经到达了局部最优点，即θ不会再发生改变</p></blockquote><blockquote><p>注意:  在梯度下降算法中，一般都会采用同步更新的方式来优化模型, 即上面公式的θj同步更新。同步更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。</p></blockquote><p><strong>学习率的选择对算法的影响</strong></p><ul><li><p>学习率过小的影响: 则达到收敛所需的迭代次数会非常高</p></li><li><p>学习率过大的影响: 每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛</p></li></ul><p><strong>怎么确定模型是否收敛</strong></p><p><img src="/img/1540568088758.png" alt="1540568088758"></p><ul><li><p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛, 如上图在300步之后基本接近收敛。</p></li><li><p>也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较</p></li></ul><h3 id="梯度下降算法分类"><a href="#梯度下降算法分类" class="headerlink" title="梯度下降算法分类"></a>梯度下降算法分类</h3><h4 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a><strong>批量梯度下降</strong></h4><blockquote><p>在上面的线性回归场景中的梯度下降算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。</p></blockquote><blockquote><p>批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。</p></blockquote><p><strong>优点：</strong><br>  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。<br>  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。</p><p><strong>缺点：</strong><br>  （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。</p><h4 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a><strong>随机梯度下降</strong></h4><p>公式: <img src="/img/1540042706219.png" alt="1540042706219"></p><blockquote><p>随机梯度下降指在梯度下降的每一步中只用到了训练集的一个样本或者一部分样本，最后得到的可能是全局最优解，也可能是局部最优解。</p></blockquote><p><strong>优点：</strong><br>  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。<br><strong>缺点：</strong><br>  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。<br>  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。<br>  （3）不易于并行实现。</p><h2 id="4-梯度下降线型回归模型"><a href="#4-梯度下降线型回归模型" class="headerlink" title="4.梯度下降线型回归模型"></a>4.梯度下降线型回归模型</h2><h3 id="单变量线型回归梯度下降"><a href="#单变量线型回归梯度下降" class="headerlink" title="单变量线型回归梯度下降"></a>单变量线型回归梯度下降</h3><h4 id="梯度下降、线型回归算法比较"><a href="#梯度下降、线型回归算法比较" class="headerlink" title="梯度下降、线型回归算法比较"></a><strong>梯度下降、线型回归算法比较</strong></h4><p><img src="/img/1540028773553.png" alt="1540028773553"></p><h4 id="单变量梯度下降公式"><a href="#单变量梯度下降公式" class="headerlink" title="单变量梯度下降公式"></a>单变量梯度下降公式</h4><p><strong>代价函数计算</strong></p><p><img src="/img/1540028837422.png" alt="1540028837422"></p><p><strong>参数θ的计算</strong></p><p><img src="/img/1540028905489.png" alt="1540028905489"></p><h3 id="多变量线型回归梯度下降"><a href="#多变量线型回归梯度下降" class="headerlink" title="多变量线型回归梯度下降"></a>多变量线型回归梯度下降</h3><h4 id="多变量特征"><a href="#多变量特征" class="headerlink" title="多变量特征"></a>多变量特征</h4><p>现在对回归模型增加更多的特征，例如房价预测问题，增加房间数楼层等，构成一个含有多个变量的模型，模型中的特征如下图。</p><p><img src="/img/1540563412916.png" alt="1540563412916"></p><p><strong>增添更多特征后，引入一系列新的注释</strong>：</p><blockquote><ul><li><p>n 代表特征的数量</p></li><li><p>x^(i)^ 代表第i个训练实例，是特征矩阵的第i行，是一个向量(vector).</p></li><li><p>x~j~^(i)^ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征  如:  $x_2^{(2)} = 3, x_3^{(2)} = 2$</p></li><li><p>支持多个变量的h表示为: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$</p><ul><li>这个公式中有个n+1参数和n个变量，为了使得公式能够简化一些，引入$x_0=1$，则公式转化为:$h_θ(x) = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$</li></ul></li><li><p>此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m* (n+1),</p><p>因此变量的假设公式可以表示为: $h_θ(x) = θ^TX$</p></li></ul></blockquote><h4 id="多变量梯度下降公式"><a href="#多变量梯度下降公式" class="headerlink" title="多变量梯度下降公式"></a>多变量梯度下降公式</h4><p>与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：</p><p>$J_{(θ_0, .., θ<em>n)} = {1\over2m}\sum</em>{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 , 其中: h_θ(x) = θ^TX = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$<img src="/img/1540635502085.png" alt="1540635502085"></p><p>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度<br><strong>梯度下降下降公式：</strong></p><p><img src="/img/1540566461561.png" alt="1540566461561"></p><p><strong>求导数后得到:</strong></p><p><img src="/img/1540566614915.png" alt="1540566614915"></p><h3 id="5-特征和多项式回归"><a href="#5-特征和多项式回归" class="headerlink" title="5.特征和多项式回归"></a>5.特征和多项式回归</h3><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a><strong>特征选择</strong></h3><blockquote><p>有的时候的们的特征并不是很好的可以直接用来训练模型，比如房子价格预测模型的两个特征</p><p>特征: x1 房子的临街宽度， x2 房子的纵向深度</p><p>此时很明显房子的价格用 特征一和特征二的乘积，即房子的占地面积来预测更合适  $x=x_1 * x_2 = area (面积)$</p></blockquote><p><img src="/img/1540616820528.png" alt="1540616820528"></p><h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a><strong>多项式回归</strong></h3><blockquote><p>很多时候，我们并没有那么幸运，正好所有的数据之间都是线性关系，能用一条直线，一个平面就描述地出来，大多数时候，我们可能需要画成曲线、曲面或者曲啥更高维度的东西</p><p>比如下图数据集，线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，用二次方程（蓝色的线）和三次方程（绿色的线）来拟合这些点，但是二次方程有个缺点，到了后面，反而会下降，这和我们的认知不符合，而三次方程不会遇到这个问题，和二次方程相加又会和上升的部分相互抵消一部分，所以最后选择了三次方程加上二次方程。</p><p>二次方模型公式：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2$      三次方模型公式: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3$ </p></blockquote><p><img src="/img/1540616493267.png" alt="1540616493267"></p><blockquote><p>通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以把多项式回归看成是另类的多变量线性回归，我们可以令：$x_2 = x_2^2$ , $x_3 = x_3^3$ , 从而将模型转化为线型回归模型</p><p>注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要, 因为比如x的范围是1-100的话，$x^2$的范围就是1-10000，$x^3$的范围就是1-1000000。</p></blockquote><h2 id="6-特征缩放"><a href="#6-特征缩放" class="headerlink" title="6.特征缩放"></a>6.特征缩放</h2><h3 id="为什么要特征缩放"><a href="#为什么要特征缩放" class="headerlink" title="为什么要特征缩放"></a>为什么要特征缩放</h3><blockquote><p>在使用梯度下降的时候会遇到因为特征大小不统一，导致收敛速度过慢的问题，在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p></blockquote><p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p><p><img src="/img/1540567672941.png" alt="1540567672941"></p><blockquote><p>解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。</p></blockquote><h3 id="特征缩放的两种方法"><a href="#特征缩放的两种方法" class="headerlink" title="特征缩放的两种方法"></a>特征缩放的两种方法</h3><p><strong>线型归一化</strong></p><ul><li>原理： <ul><li>通过对原始数据进行变换把数据映射到(默认为[0,1])之间</li></ul></li><li>公式<ul><li><img src="/img/1540644955305.png" alt="1540644955305"></li></ul></li><li>归一化的弊端<ul><li>使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景</li></ul></li></ul><p><strong>特征标准化</strong></p><ul><li><p>原理：</p><ul><li>通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内</li></ul></li><li><p>公式：</p><ul><li><img src="/img/1540645069496.png" alt="1540645069496"></li></ul></li><li>标准化的有点<ul><li>如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。</li></ul></li></ul><h2 id="7-正规方程线型回归"><a href="#7-正规方程线型回归" class="headerlink" title="7.正规方程线型回归"></a>7.正规方程线型回归</h2><h3 id="正规方程算法介绍"><a href="#正规方程算法介绍" class="headerlink" title="正规方程算法介绍"></a>正规方程算法介绍</h3><blockquote><p>对于线型回归问题经常都是在使用梯度下降算法，但是对于某些线型回归问题，正规方程是更好的解决办法。如下面的代价函数:</p></blockquote><p><img src="/img/1540619927547.png" alt="1540619927547"></p><p><img src="/img/1540620250525.png" alt="1540620250525"></p><h3 id="梯度下降和正规方程比较"><a href="#梯度下降和正规方程比较" class="headerlink" title="梯度下降和正规方程比较"></a>梯度下降和正规方程比较</h3><table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择学习率</td><td>不需要</td></tr><tr><td>需要多次迭代</td><td>一次运算得出</td></tr><tr><td>当特征数量n特别大时能比较适用</td><td>需要计算$(X^TX)^{-1}$如果特征数量n较大则运算代价大因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受</td></tr><tr><td>适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等其他模型</td></tr></tbody></table><blockquote><p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习介绍</title>
      <link href="/2018/10/19/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/10/19/10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h3 id="机器学习的发展"><a href="#机器学习的发展" class="headerlink" title="机器学习的发展"></a>机器学习的发展</h3><blockquote><p>机器学习被应用到越来越多的领域这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。</p><p>再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。</p><p>手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种<br>学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。<br>事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处<br>理和大部分的计算机视觉，都应用了机器学习。学习算法还广泛用于自定制程序。每次你去亚马逊或Net</p></blockquote><a id="more"></a><h3 id="一个比较好的机器学习定义"><a href="#一个比较好的机器学习定义" class="headerlink" title="一个比较好的机器学习定义"></a>一个比较好的机器学习定义</h3><blockquote><p>一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升</p><p>类比于下棋，经验E 就是程序上万次的自我练习的经验而任务T 就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。</p></blockquote><h3 id="机器学习基本算法"><a href="#机器学习基本算法" class="headerlink" title="机器学习基本算法"></a>机器学习基本算法</h3><h4 id="最常用的两个算法"><a href="#最常用的两个算法" class="headerlink" title="最常用的两个算法"></a>最常用的两个算法</h4><p><strong>监督学习算法</strong></p><blockquote><p>我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，即我们可以根据正确答案来不断的优化我们的模型。</p><p>监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。</p></blockquote><p><strong>无监督学习算法</strong></p><blockquote><p>根据未知的训练样本(未被标记分类), 从数据中找出不同的结构，将数据识别为不同的聚集簇。</p><p>无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？</p><p>还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。</p></blockquote><h4 id="监督学习算法常见问题"><a href="#监督学习算法常见问题" class="headerlink" title="监督学习算法常见问题"></a>监督学习算法常见问题</h4><p><strong>回归问题</strong></p><blockquote><p>通过像预测房子的价格一样，通过回归模型通过输入值来推出一个连续的输出值</p></blockquote><p> <strong>分类问题</strong></p><blockquote><p>通过数学模型来判断输入数据所属的类别，可以是二类别问题（是/不是），也可以是多类别问题（在多个类别中判断输入数据具体属于哪一个类别）</p></blockquote><h4 id="无监督学习常年问题"><a href="#无监督学习常年问题" class="headerlink" title="无监督学习常年问题"></a>无监督学习常年问题</h4><p><strong>聚类问题</strong></p><blockquote><p>依据研究对象（样品或指标）的特征，将其分为不同的分类。</p></blockquote><p>### </p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 基础内容 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA/</url>
      
        <content type="html"><![CDATA[<p>数据格式化输出<br>自动格式化输出文件<br>scrapy支持的格式化输出文件格式<br>json<br>jsonlines<br>csv<br>xml<br>通过配置setting中属性，来定义存储方式<br>FEED_URI   存储路径，必须配置，如果不配置，需要在启动时候指定路径<br>示例：file://tmp/export.csv<br>备注：<br>路径的名称可以格式化，例如：</p><p>FEED_FORMAT   用于序列化输出的文件格式，就是上面的几种<br>FEED_EXPORT_ENCODING   文件编码格式，一般默认utf-8，json默认是安全编码格式<br>FEED_EXPORT_FIELDS    定义输出文件中包含的字段，为列表格式<br>FEED_EXPORT_INDENT    定义输出内容的缩进，默认为0,<br>0和负数表示内容会放在一行上<br>None会选择最紧凑的方式，将数据进行摆放<br>其他，将会按照指定的缩进，对对象的成员进行格式化显示<br>FEED_STORE_EMPTY     如果没输出是否会导出文件，默认为False<br>FEED_STORAGES_BASE    字典类型，包含所有支持存储方式模板引擎， 一般不需要手动配置<br>默认为</p><p>FEED_STORAGES     一个字典类型，可以手动添加存储方式的模板引擎<br>默认为 { }<br>如果想要禁用某一个模板文件类型的引擎，可以将上面对应字段值设置为None<br>FEED_EXPORTERS_BASE    字典类型，包含了所有支持的格式文件处理引擎， 一般不需要手动配置<br>默认为</p><p>FEED_EXPORTERS      一个字典类型，可以手动添加文件处理引擎<br>默认为{ }<br>如果想要禁用某一个模板文件类型的引擎，可以将上面对应字段值设置为None<br>scrapy手动输出指定csv格式文件<br>目标：<br>获取如下格式的csv文件<br>配置的步骤</p><ol><li><p>在scrapy的spiders同层目录，新建my_project_csv_item_exporter.py文件内容如下（文件名可改，目录定死）<br>   ​    from scrapy.conf import settings<br>   ​    from scrapy.contrib.exporter import CsvItemExporter</p><pre><code>class MyProjectCsvItemExporter(CsvItemExporter):def __init__(self, *args, **kwargs):    delimiter = settings.get(&apos;CSV_DELIMITER&apos;, &apos;,&apos;)    kwargs[&apos;delimiter&apos;] = delimiter    fields_to_export = settings.get(&apos;FIELDS_TO_EXPORT&apos;, [])    if fields_to_export :        kwargs[&apos;fields_to_export&apos;] = fields_to_export    super(MyProjectCsvItemExporter, self).__init__(*args, **kwargs)</code></pre></li><li><p>在同层目录，settings.py文件新增如下内容（指定item,field顺序）<br>   ​    FEED_EXPORTERS = {<br>   ​        ‘csv’: ‘my_project.my_project_csv_item_exporter.MyProjectCsvItemExporter’,<br>   ​    } #这里假设你的project名字为my_project</p><pre><code>FIELDS_TO_EXPORT = [    &apos;id&apos;,    &apos;name&apos;,    &apos;email&apos;,    &apos;address&apos;</code></pre></li><li><p>在同层目录，settings.py文件指定分隔符<br>CSV_DELIMITER = “\t”<br>4.启动项目<br>全部设定完后，执行scrapy crawl spider -o spider.csv的时候，字段就按顺序来了。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>scrapy框架介绍<br>scrapy框架的介绍<br>什么是scrapy框架<br>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，我们只需要实现少量的代码，就能够快速的抓取<br>Scrapy 使用了 Twisted[‘twɪstɪd]异步网络框架，可以加快我们的下载速度和数据在模块中间的转载速度。<br><a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html</a><br>异步和非阻塞的概念<br>非阻塞<br>关注的是程序在等待调用结果（消息，返回值）时的状态，指在不能立刻得到结果之前，该调用不会阻塞当前线程。<br>异步<br>如果整个程序没有中介的等待过程，我们就说整个过程是一个异步的过程<br>多线程爬取数据的流程</p><p>scrapy的爬虫流程</p><p>scrapy框架各模块功能</p><p>创建爬虫项目（命令）<br>创建项目<br>命令<br>scrapy startproject +&lt;项目名字&gt;<br>创建后项目目录结构</p><p>创建爬虫<br>命令<br>scrapy genspider  +&lt;爬虫名字&gt; + &lt;允许爬取的域名&gt;          生成普通的spider爬虫<br>scrapy genspider -t crawl &lt;爬虫名字&gt;  &lt;允许爬取的域名&gt;        生成crawl_Spider爬虫<br>示例：<br>scrapy genspider itcast “itcast.cn”<br>爬虫应用创建的位置</p><p>启动爬虫应用<br>命令<br>scrapy crawl 爬虫名   执行单个爬虫<br>-o: 将爬虫的item输出存储到文件中.<br>quotes.jl : 将每一个item输出为一行的jsonline<br>quotes.csv: 将每一个item输出为每一行的csv格式文件<br>quotes.pickle: 存储为二进制文件<br>Scrapy–scrapy shell<br>什么是scrapy shell<br>Scrapy shell是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath表达式<br>查看实例属性/方法<br>在scrapy shell中查看<br>进行请求<br>scrapy shell ‘url’   对url地址进行请求获取响应<br>查看请求的属性<br>可使用任意scrapy实例 + TAB查看实例有哪些属性或者方法<br>response.url：当前响应的url地址<br>response.request.url：当前响应对应的请求的url地址<br>response.headers：响应头<br>response.body：响应体，也就是html代码，默认是byte类型<br>response.requests.headers：当前响应的请求头<br>response.meta: 下载延迟，请求深度等信息<br>response.xpath(***).extract()    验证xpath的正确性<br>在程序中<br>print（dir（实例对象））  查看实例对象的所有属性和方法</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/scrapy%E5%AF%B9%E6%8E%A5splash/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/scrapy%E5%AF%B9%E6%8E%A5splash/</url>
      
        <content type="html"><![CDATA[<p>scrapy对接splash<br>Scrapy 对接 Splash<br>环境准备<br>首先在这之前请确保已经正确安装好了Splash并正常运行，同时安装好了ScrapySplash库<br>Scrapy-Splash文档<br><a href="https://github.com/scrapy-plugins/scrapy-splash" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-splash</a><br>Scrapy-splash的配置<br>新建项目和spider<br>scrapy startproject scrapysplashtest     新建项目<br>scrapy genspider taobao <a href="http://www.taobao.com" target="_blank" rel="noopener">www.taobao.com</a>     新建spider<br>修改setting.py文件, 添加splash配置<br>SPLASH_URL = ‘<a href="http://localhost:8050&#39;" target="_blank" rel="noopener">http://localhost:8050&#39;</a>         添加splash服务的地址<br>DUPEFILTER_CLASS = ‘scrapy_splash.SplashAwareDupeFilter’       配置去重类<br>HTTPCACHE_STORAGE = ‘scrapy_splash.SplashAwareFSCacheStorage’     还需要配置一个Cache存储HTTPCACHE_STORAGE<br>添加splash中间件<br>DOWNLOADER_MIDDLEWARES = {<br>‘scrapy_splash.SplashCookiesMiddleware’: 723,<br>‘scrapy_splash.SplashMiddleware’: 725,<br>‘scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware’: 810,<br>}<br>SPIDER_MIDDLEWARES = {<br>‘scrapy_splash.SplashDeduplicateArgsMiddleware’: 100,<br>}<br>SplashRequest请求的使用<br>使用splash请求的说明<br>配置完成之后我们就可以利用Splash来抓取页面了，例如我们可以直接生成一个SplashRequest对象并传递相应的参数，Scrapy会将此请求转发给Splash<br>Splash对页面进行渲染加载，然后再将渲染结果传递回来，此时Response的内容就是渲染完成的页面结果了，最后交给Spider解析即可。<br>使用请求的方法<br>第一种方法<br>通过SplashRequest发送请求</p><p>第二种方法<br>scrapy.Request对象发送请求给splash服务器，只需将配置属性给meta参数即可</p><p>通过lua源码控制splash服务的示例<br>我们把Lua脚本定义成长字符串，通过SplashRequest的args来传递参数，同时接口修改为execute，另外args参数里还有一个lua_source字段用于指定Lua脚本内容，这样我们就成功构造了一个SplashRequest，对接Splash的工作就完成了。<br>​                                from scrapy import Spider<br>​                                from urllib.parse import quote<br>​                                from scrapysplashtest.items import ProductItem<br>​                                from scrapy_splash import SplashRequest</p><pre><code>script = &quot;&quot;&quot;function main(splash, args)splash.images_enabled = falseassert(splash:go(args.url))assert(splash:wait(args.wait))js = string.format(&quot;document.querySelector(&apos;#mainsrp-pager div.form &gt; input&apos;).value=%d;document.querySelector(&apos;#mainsrp-pager div.form &gt; span.btn.J_Submit&apos;).click()&quot;, args.page)splash:evaljs(js)assert(splash:wait(args.wait))return splash:html()end&quot;&quot;&quot;class TaobaoSpider(Spider):name = &apos;taobao&apos;allowed_domains = [&apos;www.taobao.com&apos;]base_url = &apos;https://s.taobao.com/search?q=&apos;def start_requests(self):for keyword in self.settings.get(&apos;KEYWORDS&apos;):for page in range(1, self.settings.get(&apos;MAX_PAGE&apos;) + 1):url = self.base_url + quote(keyword)yield SplashRequest(        url,        callback=self.parse,        endpoint=&apos;execute&apos;,        args={&apos;lua_source&apos;: script, &apos;page&apos;: page, &apos;wait&apos;: 7})</code></pre><p>使用scrapy-splash比使用selenium的优点<br>由于Splash和Scrapy都支持异步处理，我们可以看到同时会有多个抓取成功的结果，而Selenium的对接过程中每个页面渲染下载过程是在Downloader Middleware里面完成的，所以整个过程是堵塞式的，Scrapy会等待这个过程完成后再继续处理和调度其他请求，影响了爬取效率。<br>使用Splash，是在中间件中将请求和渲染等工作交给了splash服务器, 各请求之间是异步的，因此使用Splash爬取效率上比Selenium高出很多。<br>因此，在Scrapy中要处理JavaScript渲染的页面建议使用Splash，这样不会破坏Scrapy中的异步处理过程，会大大提高爬取效率，而且Splash的安装和配置比较简单，通过API调用的方式也实现了模块分离，大规模爬取时部署起来也更加方便。</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/scrapy_redis%E6%A1%86%E6%9E%B6/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/scrapy_redis%E6%A1%86%E6%9E%B6/</url>
      
        <content type="html"><![CDATA[<p>scrapy_redis框架<br>scrapy_redis的介绍<br>scrapy_redis的官方介绍<br>基于redis的scrapy组件<br>使用scrapy_redis要安装scrapy_redis组件（pip install scrapy_redis）<br>scrapy_redis使用场景<br>增量式抓取、分布式抓取、持久化抓取<br>scrapy_redis实现的功能及与原生scrapy的区别<br>request去重<br>scrapy：可以实现去重，但是scrapy去重不能实现持久化，即当下次如果再次运行程序，会再次存储之前存储过得内容<br>scrapy_redis：会将请求过的指纹集合保存起来，用来判断重复我请求，即便重新运行程序，已经请求过的请求，也不会进入待请求对列中<br>爬虫持久化<br>scrapy：每次运行爬虫都是一次全新的开始<br>scrapy_redis：会将待请求的序列化后的request对象保存起来，这样每次运行程序都会从待请求列队中读取request对象进行请求，即使终止程序，再次运行，依然会从待请求对列中获取请求对象<br>分布式爬虫<br>scrapy：不能实现分布式<br>scrapy_redis：可以通过redis存储爬取的状态（已爬取，和待爬取），让多台机器上的爬虫程序，共用一个redis服务器，这样数据是共享的，实现了分布式的爬虫<br>分布式和集群的区别（备注）：<br>分布式：一个业务拆分成多个子业务，部署在不同的服务器上<br>集群，同一个业务，部署在多个服务器上<br>scrapy_redis的爬虫工作流程</p><p>Scrapy_redis和scrapy框架使用的区别<br>克隆scrapy_redis的示例代码<br>克隆 github上的scrapy-redis源码文件<br>git clone <a href="https://github.com/rolando/scrapy-redis.git" target="_blank" rel="noopener">https://github.com/rolando/scrapy-redis.git</a><br>打开example-project的scrapy_redis示例源代码<br>mv scrapy-redis/example-project    项目目录<br>example-project项目目录结构如下</p><p>redis_scrapy代码上和普通scrapy爬虫的区别<br>一、spider爬虫文件<br>第一种Scrapy爬虫（如示例dmoz.py）<br>区别：<br>和普通的爬虫文件没有区别<br>文件主要内容：</p><p>第二种RedisSpider爬虫（如示例的myspider_redis.py）<br>区别<br>爬虫类继承自RedisSpider<br>要指定爬虫类的redis_key类属性，指定reids中存储start_url的位置<br>新增了爬虫类的方法<br>def make_request_from_data(self, data):   # data 为接收到额redis_key传递过来的url，我们可以自行拼接成想要的url进行请求<br>search_key = data<br>url = self.URL_MAIN % (search_key, 1)<br>result_item = SearchInfoResultItem()<br>result_item[“task_value”] = search_key<br>return Request(url, meta={“result_item”: result_item}, dont_filter=True)<br>作用<br>防止分布式爬虫在不同的程序中，都会去请求该start_url，这样会造成请求的重复，响应也会重复的交给解析函数去处理，从redis中取出的start_url第一次被爬虫取出之后便会删除掉，不会造成重复的请求<br>在持久化和去重的功能上实现了RedisSpider爬虫才实现了分布式<br>代码示例：</p><p>第三种RedisCrawSpider爬虫（如示例的myspider_redis.py）<br>区别<br>爬虫类继承自RedisCrawlSpider<br>比RedisSpider爬虫就多了一个rules过滤功能<br>作用<br>在持久化和去重的功能上实现了RedisSpider爬虫才实现了分布式<br>另外可以实现自动的对响应中的连接进行匹配，进行请求<br>代码示例</p><p>二、setting文件<br>区别：<br>设置了三个新的属性<br>DUPEFILTER_CLASS = “scrapy_redis.dupefilter.RFPDupeFilter”<br>指定爬虫给request对象去重的方法<br>SCHEDULER = “scrapy_redis.scheduler.Scheduler”<br>指定爬虫处理请求列队的方法<br>SCHEDULER_PERSIST = True<br>列队中的内容是否持久保存，为False的时候，在关闭程序的时候，清空redis<br>添加了一个处理item的pipeline<br> ‘scrapy_redis.pipelines.RedisPipeline’: 400<br>用来将items自动保存到reids中，我们可以取消，不用reids来保存<br>设置了redis的地址<br>REDIS_URL = ‘redis://192.168.207.130: 6379’<br>不写使用哪个数据库的话，默认使用0数据库<br>文件主要内容：</p><p>三、运行爬虫后，reids数据库的中多出的键（运行程序后）<br>redis数据库多个三个键<br>爬虫名：dupefilter   用来存储爬取过的request指纹集合，数据类型为set类型<br>爬虫名：requests  用来存储已经序列化的待请求对象，数据类型为zset类型<br>爬虫名：items    用来存储爬取出来的数据，数据类型为list类型<br>每个键存储的内容：</p><p>Scrapy_redis功能实现的方法(Scrapy_redis的源码)<br>scrapy_redis之redispipeline（实现item数据自动存储redis数据库）<br>文件位置<br>/python3.5/site-packages/scrapy_redis/pipelines/RedisPipeline<br>源码主要功能<br>实现将item存储在redis中<br>源码片段</p><p>scrapy_redis之RFPDupeFilter（主要实现去重）<br>文件位置<br>/python3.5/site-packages/scrapy_redis、dupefilter/RFPDpeFilter<br>源码主要功能<br>判断请求是否存在内存中，如果不存在<br>对请求进行一系列操作后，将其转化为指纹<br>将指纹存储在内存中<br>主要实现去重功能<br>源码片段</p><p>加密指纹生成过程<br>  import hashlib<br>  fp = hashlib.sha1()<br>  fp.update(url)<br>  fp.update(request.method)<br>  fp.update(request.body or b””)<br>  return fp.hexdiget()  #sha1结果的16进制的字符串<br>scrapy_redis之Scheduler（主要实现持久化）<br>文件位置<br>/python3.5/site-packages/scrapy_redis/scheduler/Scheduler<br>代码主要功能<br>判断取消过滤是否为true<br>判断url地址是否是第一次看到<br>如果过滤，且是第一次看到的请求，那么将会进入待请求的列队<br>主要实现持久化功能<br>源码片段</p><p>scrapy_redis的总结<br>scrapy_redis与scrapy的区别<br>本质区别<br>scrapy_redis_spider相比于之前的scrapy_spider多了持久化和持久化去重的功能<br>主要是因为scrapy_redis将指纹和请求进行了在redis中的存储,正是redis实现了持久化<br>代码区别<br>仅仅是在setting中增加了五行代码<br>拓展：<br>scrapy_redis实现了请求的增量式爬虫，还可以实现内容增量式爬虫<br>可以将dont_filter设置不进行过滤<br>会将每条数据根据发帖人，发帖时间，帖子更新时间等使用md5算法生成指纹<br>在进行存储的时候 进行判断是否已经存在，以及是否更新<br>如果不存在那么便插入<br>如果存在但是更新时间已经更新，那么便更新<br>工作场景常用的数据存储模式<br>优点<br>可以利用redis的读取快速的特点，进行数据的快速读取<br>可以实现读取和存储分离，防止锁表的产生<br>存储数据用mysql或者mongodb<br>先将爬取到的数据去重后存储到mysql或mongodb中<br>也可以先存储到数据库中，在对数据库进行数据的去重<br>redis应用其速度快的特点，用来展示数据<br>定期从mysql或者mongodb中将去重后的数据读取到redis中<br>读取数据从redis中读取，其速度远远优于mysql和mongodb</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/scrapyd%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/scrapyd%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>scrapyd部署总结<br>二、环境安装<br>安装scprayd，网址：<a href="https://github.com/scrapy/scrapyd" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd</a><br>安装scrapyd-client，网址：<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd-client</a><br>建议从github上下载最新源码，然后用python setup.py install安装，因为pip安装源有可能不是最新版的。</p><p>三、验证安装成功<br>在命令框中输入scrapyd，输出如下说明安装成功</p><p>打开<a href="http://localhost:6800/" target="_blank" rel="noopener">http://localhost:6800/</a> 可以看到</p><p>点击jobs可以查看爬虫运行情况。</p><p>接下来就是让人很头疼的scrapyd-deploy问题了，查看官方文档上说用<br>scrapyd-deploy -l<br>可以看到当前部署的爬虫项目，但是当我输入这段命令的时候显示这个命令不存在或者有错误、不合法之类的。<br>解决方案：<br>在你的python目录下的Scripts文件夹中，我的路径是“D:\program files\python2.7.0\Scripts”，增加一个scrapyd-deploy.bat文件。<br>内容为：<br>@echo off<br>“D:\program files\python2.7.0\python.exe” “D:\program files\python2.7.0\Scripts\scrapyd-deploy” %*<br>然后重新打开命令框，再运行scrapyd-deploy -l 就可以了。</p><p>四、发布工程到scrapyd<br>scrapyd-deploy <target> -p <project><br>target为你的服务器命令，project是你的工程名字。<br>首先对你要发布的爬虫工程的scrapy.cfg 文件进行修改，我这个文件的内容如下：<br>[deploy:scrapyd1]<br>url = <a href="http://localhost:6800/" target="_blank" rel="noopener">http://localhost:6800/</a><br>project = baidu</project></target></p><p>因此我输入的命令是：<br>scrapyd-deploy scrapyd1 -p baidu</p><p>输出如下</p><p>五、启动爬虫<br>使用如下命令启动一个爬虫<br>curl <a href="http://localhost:6800/schedule.json" target="_blank" rel="noopener">http://localhost:6800/schedule.json</a> -d project=PROJECT_NAME -d spider=SPIDER_NAME<br>PROJECT_NAME填入你爬虫工程的名字，SPIDER_NAME填入你爬虫的名字<br>我输入的代码如下：<br>curl <a href="http://localhost:6800/schedule.json" target="_blank" rel="noopener">http://localhost:6800/schedule.json</a> -d project=baidu -d spider=baidu</p><p>因为这个测试爬虫写的非常简单，一下子就运行完了。查看网站的jobs可以看到有一个爬虫已经运行完，处于Finished一列中</p><p>六、停止一个爬虫<br>curl <a href="http://localhost:6800/cancel.json" target="_blank" rel="noopener">http://localhost:6800/cancel.json</a> -d project=PROJECT_NAME -d job=JOB_ID<br>更多API可以查看官网：<a href="http://scrapyd.readthedocs.io/en/latest/api.html" target="_blank" rel="noopener">http://scrapyd.readthedocs.io/en/latest/api.html</a></p><p>七、远程开启服务器上的爬虫<br>前提: 已安装scrapyd-client<br>代码如下：<br>[cpp] view plain<br>copy</p><h1 id="coding-utf-8"><a href="#coding-utf-8" class="headerlink" title="coding=utf-8"></a>coding=utf-8</h1><p>import urllib<br>import urllib2  </p><h1 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h1><p>test_data = {‘project’:’baidu’, ‘spider’:’baidu’}<br>test_data_urlencode = urllib.urlencode(test_data)  </p><p>requrl = “<a href="http://localhost:6800/schedule.json&quot;" target="_blank" rel="noopener">http://localhost:6800/schedule.json&quot;</a>  </p><h1 id="以下是post请求"><a href="#以下是post请求" class="headerlink" title="以下是post请求"></a>以下是post请求</h1><p>req = urllib2.Request(url = requrl, data = test_data_urlencode)  </p><p>res_data = urllib2.urlopen(req)<br>res = res_data.read()  # res 是str类型<br>print res  </p><h1 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h1><h1 id="以下是get请求"><a href="#以下是get请求" class="headerlink" title="以下是get请求"></a>以下是get请求</h1><p>myproject = “baidu”<br>requrl = “<a href="http://localhost:6800/listjobs.json?project=&quot;" target="_blank" rel="noopener">http://localhost:6800/listjobs.json?project=&quot;</a> + myproject<br>req = urllib2.Request(requrl)  </p><p>res_data = urllib2.urlopen(req)<br>res = res_data.read()<br>print res  </p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-settinglog%E4%BF%A1%E6%81%AF/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-settinglog%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<p>Scrapy-setting/log信息<br>配置scrapy的setting文件<br>为什么需要配置文件<br>配置文件存放一些公共的变量（比如数据库的地址，账号密码等）方便自己和别人的使用和修改<br>为了方便识别，一般配置文件中变量的命名都为大写，例如：SQL_HOST = ‘192.168.0.1’<br>在pider和pipeline中访问setting内容<br>在spider中访问setting的内容<br>​         class Myspider(scrapy.Spider):<br>​            name = ‘myspider’<br>​            start_urls = [‘http://<strong>*</strong>‘;]<br>​            def parse(self, response):<br>​                # 可以直接通过访问实例属性的方法，访问setting文件中的内容<br>​                                # self.settings 返回的是一个字典，里面按照键值对存储所有的配置参数<br>​                key = self.settings.attributes.get(‘KEY’，None)<br>在pipeline文件中访问setting的内容<br>​         class Mypipeline(object):<br>​             def open_spider(self, spider):<br>​                  # 可以通过传递过来的spider实例来访问setting中的内容<br>​                  BOT_NAME = spider.setting.get(‘KEY’, None)<br>配置文件的常用的配置<br>​        BOT_NAME = ‘<strong><em>‘   项目名称<br>​        SPIDER_MOUDLES = [‘ys.spiders’]     # 爬虫创建的位置<br>​        NEWSPIDER_MOUDLE = ‘yg.spiders’    # 新建爬虫的位置<br>​        USER_AGENT =’</em></strong>‘      # 设置主句名称，用来告诉服务器请求的身份，注意，scrapy不能将USER_AGENT配置在        HEADERS中<br>​       ITEM_PIPLINES   # 管道<br>​        DOWNLOAD_MIDDLEWARES    # 下载中间件<br>​       OBEY_ROBOTFILES = False    # 是否遵守robots协议<br>​        SPIDER_MIDDLEWARES    # 爬虫中间件<br>​        DEAFAULT_REQUEAT_HEADERS = { ***}   # 配置scrapy 默认的请求头，不能包含USER-AGRNT和cookies、refer<br>​        #  使用增量式，或分布式爬虫需要配置<br>​        DUPEFILTER_CLASS = “scrapy_redis.dupefilter.RFPDupeFilter”<br>​        SCHEDULER = “scrapy_redis.scheduler.Scheduler”<br>​        SCHEDULER_PERSIST = True<br>​        ITEM_PIPELINES = {<br>​            ‘scrapy_redis.pipelines.RedisPipeline’: 400<br>​            }<br>​        REDIS_URL = ‘redis://127.0.0.1: 6379’<br>​<br>       RETRY_HTTP_CODES = []    # 设置会retry的状态码响应<br>        HTTPERROR_ALLOWED_CODES = [302,]    # 设置来指定spider能处理的response返回值<br>        LOG_LEVEL = “WARNING”     # 设置log提示的等级，log的从高到低的级别分别为：error、warning、info、debug<br>        RETRY_ENABLED = False     开启请求失败重新请求，默认是开启的<br>        RETRY_TIMES = 3        设置重新请求的次数<br>        DOWNLOAD_TIMEOUT = 6     设置请求最大等待时间<br>        COOKIES_ENABLE = True        #  是否启用cookie中间件，如果关闭是不会向服务器发送cookie的<br>        COOKIES_DEBUG = True        # 可以看带cookie在函数中传递的过程<br>        DOWNLOAD_DELAY = 3   # 请求延时<br>        CONCURRENT_REQUESTS = 16   # 并发数量，默认值为16<br>        CONCURRENT_REQUESTS_PRE_DOMAIN = 16   # 每个域名最大并发<br>        CONCURRENT_REQUESTS_PR_IP = 16    # 每个ip最大的并发<br>         AUTOTHROTTLE_ENABLED = True    # 动态调整下载延时<br>        CONCURRENT_REQUESTS_PRE_DOMAIN = 1  # 设置允许对同一域名发起请求的并发数量限制<br>        JOBDIR = ‘路径’     # 配置记录爬虫状态目录的文件，使爬虫中途停止再启动的时候会接着上一次的状态继续爬取<br>log的配置与scrapy的debug信息<br>scrapy-log相关的配置<br>log的作用<br>为了让我们自己希望输出到终端的内容能容易看一些<br>配置log的显示等级<br>在setting中设置log显示的级别<br>​            LOG_LEVEL = “WARNING”        #  在setting文件中添加一行<br>log的从高到低的级别分别为：error、warning、info、debug<br>自定义log日志的方法<br>在想要显示log的位置添加log的输出<br>第一种打印方式：（不带log产生的位置）</p><p>第二种打印方式：（带log产生的具体文件）</p><p>在普通的py文件中配置logger日志的方法<br>作用<br>我们配置完了logger的输出文件后，我们可以在任何其他的程序后，可以导入logger模块，帮助我们来可以看到错误产生的位置和时间等信息<br>我们还可以将log存储的文件命名为每日的日期，这样方便我们分别存储我们每日的bug<br>示例：<br>​                    import logging<br>​                           # 配置logger的输出格式，和输出内容<br>​                    logging.basicConfig(<br>​                                    level=logging.INFO,<br>​                                    format=’%(levelname)s : %(filename)s ‘<br>​                                           ‘[%(lineno)d] : %(message)s’<br>​                                           ‘ - %(asctime)s’, datefmt=’[%d-%b-%Y %H:%M:%S]’<br>​                                            filename=’./logdebug.log’)<br>​<br>                    if <strong>name</strong> == ‘<strong>main</strong>‘:<br>                        logger = logging.getLogger(<strong>name</strong>)<br>                        log_message = ‘error’<br>                        logger.warning(log_message）<br>输出的格式</p><p>详细示例</p><p>常见的scrapy的debug信息</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-pipeline/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-pipeline/</url>
      
        <content type="html"><![CDATA[<p>Scrapy-pipeline<br>pipeline的配置<br>pipeline类的作用<br>接收引擎传递过来的数据，对数据进行处理<br>配置pipeline类（在pipelines文件中）<br>​    class Myspiderpipline(object):<br>​        def process_item(self, item, spider):   # 实现数据处理的方法，方法名不能修改<br>​            with open(‘temp.txt’, ‘a’) as f:<br>​                json.dump( item, f, ensure_ascii=False, indent=2)<br>​                        # 将item打印，并将item传递给后面的pipeline，实现数据在管道pipeline之间的传递<br>​                    return item<br>注意：<br>process_item方法中的spider参数，表示传递参数过来的爬虫对象<br>spider： 传递数据的爬虫对象，spider.name可以返回爬虫的名称<br>item：传递过来的数据<br>setting文件中的设置开启pipeline<br>​    ITEM_PIPELINE = {<br>​        ‘myspider.pipelines.MyspiderPipeline’: 300,<br>​    }<br>注意：<br>‘muspider.pipelines.MyspiderPipeline’是pipeline文件的路径<br>300  是pipeline的权重，pipeline的权重越小，其优先级越高，可以理解为距离引擎的远近<br>如果权重相同的两个pipeline，scrapy会自动的将键值进行排序，根据排序结果定义访问顺序<br>pipeline的方法<br>​      form pymongo import MongoClient<br>​      class Myspiderpipline(object):<br>​            # open_spider在爬虫开启请求start_url前的时候只执行一次，一般用于导入数据库<br>​             def open_spider(self,  spider):<br>​                    # 实例化一个mongoclient<br>​                   con = MongoClient(spider.settings.get(‘HOST’), spider.settings.get(‘PORT’))<br>​                   db = con[spider.settings.get(‘DB’)]<br>​                   self.collection = db[spider.settings.get(‘COLLECTON’)]<br>​         </p><pre><code># close_spider在爬虫关闭的时候只执行 一次，一般用于需要关闭的数据库关闭def close_spider(self,  spider):        print(&apos;关闭pipeline&apos;)        return item      # from_crawler连接到setting.py的配置文件def from_crawler(cls, crawler):        return  cls(mongo_uri=crawler.settings.get(&apos;MGONGO_URI&apos;))</code></pre><p>备注：<br>一个项目会有多个spider，不同的pipeline处理不同的item的内容<br>一个spider的内容可能要做不同的操作，比如存入不同的数据库中<br>pipeline向mongoDB中插入数据<br>在pipeline类的外部定义使用的mongodb集合<br>​     from pymongo import MongoClient<br>​     client = MongoClient(host=’127.0.0.1’, port=27017)    # 实例化mongodb客户端<br>​     collection = client[‘myspider’][‘yangguang’]<br>在pipeline类open_spider中导入mongodb<br>​    form pymongo import MongoClient<br>​    class Myspiderpipline(object):<br>​        # open_spider在爬虫开启的时候只执行一次<br>​        def open_spider(self,  spider):<br>​            # 实例化一个mongoclient<br>​            con = MongoClient(spider.settings.get(‘HOST’), spider.settings.get(‘PORT’))<br>​            db = con[spider.settings.get(‘DB’)]<br>​            self.collection = db[spider.settings.get(‘COLLECTON’)]<br>向集合中插入数据<br>​            def process_item(self, item, spider):<br>​                item[‘collection_text’] = <em>**</em><br>​                collection.insert(dict(item))   # 向mongodb中插入数据<br>​                return item<br>向Mongodb中插入数据<br>要在setting中配置<br>MONGO_URI = ‘127.0.0.1’<br>MONGO_DATABASE = ‘数据库名’</p><p>pipeline向Mysql中存入数据<br>数据同步的方式写入Mysql</p><p>数据异步的方式写入Mysql</p><p>数据自动导出json文件<br>自定义形式导出</p><p>使用scrapy自带exportor导出</p><p>将爬取去到的图片地址对应的图片进行存储<br>setting文件中设置图片存储位置</p><p>pipeline中对图片进行下载</p><p>ArticleImagePipeline类的方法<br>get_media_requests(self, item, info)<br>该方法实现将item字段中的url字段取出来，然后直接生成Request对象，请求对象会加入到对列中，等待执行下载<br>file_path(self, request, response=None, info=None)<br>这个方法用来返回保存的文件名<br>item_completed(self, results, item, info)<br>当单个item完成下载后的处理方法<br>results参数就是该item对应的下载结果，它是一个列表形式，列表的每一个元素就是一个元组，其中包含了下载成功或者失败的信息<br>附件：<br>Desktop.zip</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-middleware/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-middleware/</url>
      
        <content type="html"><![CDATA[<p>Scrapy-middleware<br>中间件使用方法<br>使用方法<br>在middlewares文件中编写一个Downloader Middlewares的类，定义中间件<br>在setting中开启Downloader Middlewares，数值越小权限越大<br>request会先经过权限大的中间件，response会先经过权限小的中间件<br>​                SPIDER_MIDDLEWARES = {<br>​                    # 开启middleware，写清楚middleware所在的路径<br>​                    ‘login.middlewares.LoginSpiderMiddleware’: 543,<br>​                     }<br>常用自定义中间件<br>Downloader Middlewares（下载中间件）的方法<br>process_request(self, request, spider)<br>作用<br>当每个请求通过下载中间件时，该方法被调用,必须返回如下值的其中一个<br>返回内容<br>返回None<br>Scrapy将继续处理该请求，执行其他的中间件的相应方法，直到合适的下载器处理函数（下载处理程序）被调用，该请求被执行（其响应被下载）<br>返回Request对象<br>如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。<br>返回Response对象<br>Scrapy将不会调用任何其他的 process_request或 process_exception方法，或相应地下载函数； 将返回该response,已安装的中间件的 process_response() 方法则会在每个response返回时被调用。<br>抛出异常（包括抛出一个IgnoreRequest异常）<br>停止调用 process_request，下载中间件的 process_exception方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。<br>process_response（self, request, response, spider）<br>作用<br>当响应通过每个下载中间件的时候，调用方法，必须返回如下之一<br>返回内容<br>返回Response对象（可以与传入的响应相同，也可以是全新的对象）<br>该响应将被在链中的其他中间件的process_response方法处理。<br>返回Request对象<br>中间件链停止，返回的请求将被重新调度下载。处理类似于process_request()返回请求所做的那样。<br>产生异常（包括抛出一个IgnoreRequest异常）<br>停止调用 process_response，下载中间件链的 process_exception方法会被调用，如果没有任何一个方法处理该异常，则调用请求的errback（Request.errback）。如果没有代码处理抛出的异常，则该异常被忽略且不记录（不同于其他异常那样）。<br>process_exception(self, request，exception, spider)<br>作用：<br>当下载处理器（下载处理程序）、下载中间件的方法抛出异常（包括IgnoreRequest异常）时，Scrapy调用process_exception,返回如下几个参数之一<br>返回内容：<br>返回None：Scrapy将会继续处理该异常，调用接着已安装的其他中间件的process_exception方法，直到所有中间件都被调用完毕，则调用默认的异常处理。<br>返回Request对象，则返回的请求将被重新调用下载。这将停止中间件的process_exception方法执行，就如返回一个响应的那样。<br>常设置的中间件<br>给请求添加请求头的内容<br>​      from .settings import USER_AGENTS<br>​       class RandomUserAgent(object):    # 配置下载中间件<br>​           def process_request(self, request, spider):<br>​                ug_list = USER_AGENTS<br>​                user_agent = random.choice(ug_list)<br>​                # 在requests请求前添加自定义的USER_AGENT<br>​                request.headers[‘User-Agent’] = user_agent<br>给请求添加代理ip<br>​        class RandomProxy(object):<br>​               def process_request(self, request, spider):<br>​                    proxy = random.choice(PROXIES)</p><pre><code>if proxy[&apos;user_passwd&apos;] is None:        # 没有代理账户验证的代理使用方式        request.meta[&apos;proxy&apos;] = &quot;http://&quot; + proxy[&apos;ip_port&apos;]else:        # 对账户密码进行base64编码转换        base64_userpasswd = base64.b64encode(proxy[&apos;user_passwd&apos;])       # 对应到代理服务器的信令格式里       request.headers[&apos;Proxy-Authorization&apos;] = &apos;Basic &apos; + base64_userpasswdrequest.meta[&apos;proxy&apos;] = &quot;https://&quot; + proxy[&apos;ip_port&apos;]</code></pre><p>将跳转（302）等状态码不是200的响应，重新发起请求（只适用于scrapy，不适用与scrapy-redis）<br>​    class Forbidden302Middleware(object):<br>​        def process_response(self, request, response, spider):<br>​            if response.status != 200:<br>​                print(‘捕获到一个相应状态码不是200的请求:’, request.url, ‘: ‘, response.status_code)<br>​                return request<br>​            return response<br>给请求添加cookie的中间件<br>​    import random<br>​    class CookiesMiddleware(object):<br>​        def process_request(self,request,spider):<br>​            cookie = random.choice(cookie_pool)  # cookie_poll是通过其他模块定期爬取获得的COOKIE集合<br>​            request.cookies = cooki<br>在下载中间件中通过selenium实现模拟登录</p><p>所有内置下载中间件<br>关闭内置中间件的方法<br>示例：’scrapy.downloadermiddlewares.retry.RetryMiddleware’: None<br>​    DOWNLOADER_MIDDLEWARES = {<br>​        # Engine side<br>​        ‘scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware’: 100,<br>​        ‘scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware’: 300,<br>​        ‘scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware’: 350,<br>​        ‘scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware’: 400,<br>​        ‘scrapy.downloadermiddlewares.useragent.UserAgentMiddleware’: 500,<br>​        ‘scrapy.downloadermiddlewares.retry.RetryMiddleware’: 550,<br>​        ‘scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware’: 560,<br>​        ‘scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware’: 580,<br>​        # 对压缩(gzip, deflate)数据的支持<br>​        ‘scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware’: 590,<br>​        ‘scrapy.downloadermiddlewares.redirect.RedirectMiddleware’: 600,<br>​        ‘scrapy.downloadermiddlewares.cookies.CookiesMiddleware’: 700,<br>​        ‘scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware’: 750,<br>​        ‘scrapy.downloadermiddlewares.stats.DownloaderStats’: 850,<br>​        ‘scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware’: 900,<br>​<br>常用的伪造User-Agent<br>user_agent_list = [<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1”,<br>​    “Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6”,<br>​    “Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1”,<br>​    “Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5”,<br>​    “Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3”,<br>​    “Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3”,<br>​    “Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24”,<br>​    “Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/531.21.8 (KHTML, like Gecko) Version/4.0.4 Safari/531.21.10”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/533.17.8 (KHTML, like Gecko) Version/5.0.1 Safari/533.17.8”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/533.19.4 (KHTML, like Gecko) Version/5.0.2 Safari/533.18.5”,<br>​    “Mozilla/5.0 (Windows; U; Windows NT 6.1; en-GB; rv:1.9.1.17) Gecko/20110123 (like Firefox/3.x) SeaMonkey/2.0.12”,<br>​    “Mozilla/5.0 (Windows NT 5.2; rv:10.0.1) Gecko/20100101 Firefox/10.0.1 SeaMonkey/2.7.1”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_5_8; en-US) AppleWebKit/532.8 (KHTML, like Gecko) Chrome/4.0.302.2 Safari/532.8”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_4; en-US) AppleWebKit/534.3 (KHTML, like Gecko) Chrome/6.0.464.0 Safari/534.3”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_5; en-US) AppleWebKit/534.13 (KHTML, like Gecko) Chrome/9.0.597.15 Safari/534.13”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.186 Safari/535.1”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.54 Safari/535.2”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7”,<br>​    “Mozilla/5.0 (Macintosh; U; Mac OS X Mach-O; en-US; rv:2.0a) Gecko/20040614 Firefox/3.0.0 “,<br>​    “Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10.5; en-US; rv:1.9.0.3) Gecko/2008092414 Firefox/3.0.3”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5”,<br>​    “Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.14) Gecko/20110218 AlexaToolbar/alxf-2.0 Firefox/3.6.14”,<br>​    “Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10.5; en-US; rv:1.9.2.15) Gecko/20110303 Firefox/3.6.15”,<br>​    “Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1”<br>​    ]<br>代理ip代码示例<br>proxy_ip.pyrandomproxymiddleware.py<br>获取cookie_pool(cookie_list)的模块<br>get_cookie_list.py</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-items&amp;spider/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Scrapy-items&amp;spider/</url>
      
        <content type="html"><![CDATA[<p>Scrapy-items/spider</p><p>Items的配置<br>定义Item的作用<br>提前定义需要获取的数据，可以很清楚想要爬取的信息<br>没有定义的item属性不能被spider使用，键命名错误会报错，可以防止使用错误的键来存储数据<br>不同的爬虫使用不同的Item来存放不同的数据，在把数据交给pipeline的时候，可以通过isinstance(item, MyspiderItem) 来判断数据是属于哪个item，进行不同的数据(item)处理方式<br>item实例对象可以通过和字典相同的数据处理方式，来处理item对象的属性，所以我们可以直接将Myspider理解为一个字典<br>定义Item的方法（在item文件中）<br>​    class MyspiderItem(scrapy.Item):  # 创建一个item类<br>​        name = scrapy.Field()    # scrapy.Field( )只是占了一个位，并没有值<br>​                title = scrapy.Field( )<br>在Spider中使用item的方法（在spider文件中）<br>​    from Python import MyspiderItem<br>​    item = MyspiderItem()    # 创建一个自定义的item实例，item的操作和字典是一样的<br>​    item[‘title’] = ###     # 向item对象中插入数据<br>实现item对象和字典类型数据的转化<br>dict(item)      将item对象转化为dict类型数据<br>在将数据进行写入文件中的时候，一定要将其转化为json字符串，所以要先转化为字典<br>spider爬虫文件Spider类<br>scrapy_spider类的作用<br>传递给引擎request对象，或者需要处理的数据，接收引擎传递过来的response对象<br>配置spider类及类属性（在spider文件中）<br>​    class PythonSpider(scrapy.Spider)     # 自定义spider类，继承自scrapy.Spider类<br>​        name = ‘python’       # 创建爬虫时候的名字，不可以修改，和爬虫文件名称一致<br>​        # 创建爬虫时允许爬虫爬取范围，防止爬虫爬取到其他的网站，可以在一个列表中放置多个域名<br>​               #  从start_url后面爬取的url地址，必须要属于allow_domin域名下的地址<br>​        allowed_domain = [‘python.cn’]<br>​         # 定义开始爬取的网址，默认是可以被反复请求的<br>​        start_urls =  ‘<a href="http://www.itcast.cn/channel/teacher.shtml&#39;" target="_blank" rel="noopener">http://www.itcast.cn/channel/teacher.shtml&#39;</a>;<br>配置<strong>init</strong>方法（使用Redis_spider框架爬虫类继承crawlredisSpider的时候要动态获取allow_domin）<br>​        # 增加<strong>init</strong>()方法，根据从redis中获取的start_url动态设置allow_domin的方法<br>​    def <strong>init</strong>(self, <em>args, **kwargs):<br>​        domain = kwargs.pop(‘domain’, ‘’)<br>​        self.allowed_domains = filter(None, domain.split(‘,’))<br>​            # 要继承上方定义的爬虫类的原始的<strong>init</strong>方法<br>​        super(youyuanSpider, self).<strong>init</strong>(</em>args, **kwargs)<br>配置spider类的实例方法实现请求下一页<br>​    def parse(self,  response):  # 定义数据提取的方法，接收中间件传递过来的response对象（方法名不能改）<br>​       tr_list = response.xpath(“//table[@class=’tablelist’]/tr”)[1:-1]          # xpath分组提取<br>​              for tr in tr_list:<br>​                      item = {}<br>​                      item[“title”] = tr.xpath(“./td[1]/a/text()”).extract_first()<br>​                      item[“position_categary”] = tr.xpath(“./td[2]/text()”).extract_first()<br>​                      item[“url”] = response.url<br>​                      yield item        # 将item通过引擎传递给pipeline进行处理</p><pre><code>   next_url_temp = response.xpath(&quot;//a[@id=&apos;next&apos;]/@href&quot;).extract_first()   if next_url_temp is not None and next_url_temp != &quot;javascript:;&quot;:           next_url = &quot;http://hr.tencent.com/&quot;; + next_url_temp# scrapy.Request构造requests对象给引擎，callback表示response交给哪个函数处理yield scrapy.Request(next_page_url, callback=self.parse)  </code></pre><p>多个解析函数中如何传递参数示例<br>​    def parse(self, response):<br>​        tr_list = response.xpath(‘//div[@class=”greyframe”]/table//table/tr’)   # xpath分组提取<br>​        for i in tr_list:<br>​            item = MyspiderItem（）  # 实例化item对象<br>​            ……..<br>​            …….<br>​                        #   将item参数封装成request对象通过引擎传递给调度器，引擎传递回来的Response对象给get_content函数<br>​            yield scrapy.Request(item[“href”],  callback=self.get_content,  meta{“item”:item})   </p><pre><code>def get_content(self, response):    item = response.meta[&apos;item&apos;]    # 获取response.meta属性的item对象    item[&apos;text&apos;] = response.xpath(&quot;//div[@class=&apos;content_text14_2&apos;]//text( )&quot;).extract( )    yield  item    # 统一将数据传递给pipeline</code></pre><p>start_requests方法<br>作用：<br>我们定义的start_urls都是默认交给start_requests处理的，所以如果我们想要在处理在请求之前处理request对象，那么我们可以重写start_requests方法，实现指定其响应的解析函数等功能<br>示例：<br>​         import scrapy<br>​            def start_requests(self):<br>​               for url in self.start_urls:<br>​                    yield scrapy.Request(<br>​                        url,<br>​                        callback = self.parse<br>​                )<br>​            def parse(self, response)<br>​                self.settings.get(‘KEY’, ‘’)<br>​                pass<br>spider中用到的方法/属性：<br>response对象的属性<br>response.url：当前响应的url地址<br>response.request.url：当前响应对应的请求的url地址<br>response.status: 响应的状态码<br>response.headers：获取响应头<br>response.request.headers：当前响应的请求头<br>response.body：响应体，也就是html代码，默认是bytes类型<br>response.meta: 解析函数之间传递的数据，字典类型<br>response.request.headers.getlist(‘Cookie’)    请求的cookie<br>response.headers.getlist(‘Set-Cookie’)   响应的cookie<br>response.xpath( )   方法<br>xpath( )  response.xpath( )  返回的是一个含有多个匹配结果的selector对象的列表，其具有如下方法<br>extract( )返回一个包含字符串数据的列表，将列表中所有select对象中的data属性的值提取出来<br>如果xpath（）提取的数据不存在，返回一个空列表<br>extract_first  返回列表中的第一个字符串，将列表第一个对象元素的data属性值提取出来<br>如果xpath（）提取的数据不存在，返回一个None类型数据<br>scrapy.Request(url, callback, method=’GET’, headers, body, cookies, meta, dont_filter=False )   get请求方法<br>url必须传递，为请求的url，将请求的结果作为响应传递给callback解析函数<br>callback: 指定传入的参数交给那个解析函数去处理<br>meta: 在不同的解析函数中传递数据， meta默认还会携带部分信息， 比如下载延迟，请求深度等<br>cookiejar ： 给对应的request一个cookiejar表示，任意的数字，在之后的request也携带该标示，那么将会自动把request对象携带对应的request的获取到的所有cookie内容<br>dont_filter: 默认url会经过allow_domain过滤，如果dont_filter设置为True，则已经爬过的地址不会被过滤<br>scrapy默认有url去重的功能,该功能对需要重复请求的url有重要用途<br>如果请求的页面是实时在变的，在有需要要抓实时的页面内容的时候，需要设置为true<br>start_urls的请求，dont_filter参数默认为true<br>method： 请求的方法，当使用POST的时候要传递body参数，在scrapy中body为字符串的形式<br>cookies： 传递的cookie，注意，在scrapy中cookie不能放在header中进行传递<br>headers： 使用自定义的header，优先级高于在setting中设置的默认headers<br>body：请求体，当使用post请求，发送数据的时候使用，并且当发送payload类型参数的时候，一定要使用<br>并且还要加上请求头 ： ‘Content-Type’: ‘application/json’<br>注意：<br>当请求被重定向后，redict中间件会自动进行重定向请求，并返回重定向后的响应<br>yield<br>将数据返回给引擎，并将方法挂起；<br>原理上，让这个函数成为一个生成器，每次遍历的时候会将每个结果读取到内存中，不会导致没存的占用量瞬间变高<br>yield只能返回[BaseItem， dict， None， Request ]类型的数据；<br>yield 的不是Request对象，表示将变量的数据传递给pipeline，接scrapy.Request方法表示将request传递给调度器，进行请求返回给callback函数response<br>scrapy.FormRequest（）      post表单提交<br>formdata：携带的post请求的参数，字典类型，只能发送formdata类型的参数<br>其他参数和scrapy.Request（）方法的参数使用基本一致<br>scrapy.FormRequest.from_resopnse（）    对响应中的表单自动进行表单提交<br>response：   解析函数接收到的响应<br>formdata：  需要填写的表单内容，，只能发送formdata类型的参数<br>其他参数和scrapy.Request（）方法的参数使用基本一致<br>Scrapy中的CrawlSpider类<br>CrawlSpider的作用<br>是scrapy的一个子类，应用CrawlSpider，我们可以从start_url返回的response中自动提取url地址<br>scrapy会自动的构造requests请求，发送给引擎，进行请求<br>还可以指定response是否传递给解析函数，或者是否需要继续提取响应中的url进行请求<br>使用CrawlSpider的方法<br>CrawlSpider和普通爬虫的区别<br>CrawlSpider爬虫类继承自CrawlSpide<br>在爬虫中定义一个rules对象，scrapy会自动根据rules定义的规则，过滤出符合规则的url，自动进行请求，将响应传递给解析函数，或者继续经过rules过滤符合规律的url进行请求获取响应<br>不需要自定义parse方法，因为scrapy自动的使用parse方法去请求rules过滤出来的url地址<br>生成crawlspider的命令<br>scrapy genspider -t crawl &lt;爬虫名字&gt;  &lt;允许爬取的域名&gt;<br>配置CrawlSpider爬虫示例<br>​         form scrapy.linkextractors import LinkExtractor<br>​         from scrapy.spiders import CrawSpider, Rule</p><pre><code>class CsdnspiderSpider(CrawlSpider):      # CrawlSpider要继承CrawlSpider               name = &apos;csdnspider&apos;               allowed_domains = [&apos;suning.com&apos;]               start_urls = [&apos;http://snbook.suning.com/web/trd-fl/100301/46.htm&apos;]               # 提取url，自动构造请求，把请求交给引擎获取响应               rules = (                     Rule(linkExtractor(allow=r&apos;/web/trd-fl/\d+.htm$&apos;), callback=&apos;parse_next_url&apos; follow=True),                     Rule(linkExtractor(allow=r&apos;/web/prd/\d+.htm$&apos;), callback=&apos;parse_item&apos; follow=True),</code></pre><p>spiders.Rule常见参数<br>linkextractor：是 一个Link Extractor对象，用于定义需要提取的链接，交给parse方法；<br>linkextractor可以接收正则、xpath、css等匹配连接方式，可以查看linkextractor的源码来查看其可以接收的匹配方式<br>callback：从link_extractor中每获取到连接的时候，参数所指定的值作为回调函数<br>follow：是一个布尔值，指定了根据改规则从response提取的链接是否需要linkextractor继续跟进（如果callback为None，follow默认设置为True，否则默认为False）<br>process_links：指定该spider中哪个函数将会被调用，从link_extractor中获取到连接列表时将会调用该函数，该方法主要用来过滤url<br>process_request：指定该Spider中哪个函数将会被调用，当构造完request列表时都会调用该函数，用来过滤request，必须返回request\None<br>LinkExtracort常见的参数<br>allow：满足括号中正则匹配的url会被提取，如果为空，则全部匹配(也可以是xpath匹配)<br>可以使用其他参数，可以是xpath、css等匹配方法<br>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接，及xpath满足范围内的url地址会被提取<br>deny：满足括号中的正则表达式的url一定不提取（优先级高于allow）<br>allow_domains：会被提取的连接的domain<br>deny_domains: 一定不会被提取连接的domains<br>使用ScrawlSpider注意点<br>当LinkExtractor提取到一个满足匹配规则的URL地址，后面的规则讲不会匹配该url地址，所以，注意不要将前面url地址匹配规则写的太详细，会被前面的的规则提取完不会经过后面的规则提取；<br>scrapy会自动的将LinkExtractor提取到的所有url地址给补全协议和域名、端口等，进行请求，不需要我们手动补全；<br>scrapy模拟登录<br>scrapy模拟登录有两种方法<br>直接携带cookie<br>找到发送post请求的url地址，带上信息，发送请求<br>使用scrapy框架自动登录<br>使用scrapy模拟登录注意点<br>当需要获取登录后才能获取到数据的时候<br>一个cookie一般对一个网站访问的时候，服务器可能设置阈值，这时候我们要限制访问的延时<br>注意有时候服务器会对我们的ip和cookie对应结果做判断，所以我们最好不要频繁的更换ip和cookie的搭配关系<br>如果想要持续的状态保持可以在setting中设置COOKIES_ENABLE = True<br>如果想要使用多cookie，多ip对数据进行爬取，可以使用分布式，一个ip搭配一个cookie对数据进行请<br>scrapy携带cookie模拟登录<br>应用场景<br>1、cookie过期时间很长，常见于一些不规范的网站<br>2、能在cookie过期之前把搜有的数据拿到<br>3、配合其他程序使用，比如其使用selenium把登陆之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie<br>模拟登录的示例（使用start_request来使请求携带cookie）<br>​        import scrapy<br>​                import re<br>​                class RenrenSpider(scrapy.Spider):<br>​                        name = ‘renren’<br>​                        allowed_domains = [‘renren.com’]<br>​                        start_urls = [‘<a href="http://www.renren.com/327550029/profile&#39;;]" target="_blank" rel="noopener">http://www.renren.com/327550029/profile&#39;;]</a></p><pre><code>def start_requests(self):                # 0.在start_process中对start_urls进行请求        cookies_str = &quot;*******&quot;         # 1.获取到cookie,一般情况下我们也可以使用selenium进行获取        cookies = {i.split(&quot;=&quot;)[0]:i.split(&quot;=&quot;)[-1] for i in cookies_str.split(&quot;; &quot;)}        yield scrapy.Request(                self.start_urls[0],                callback=self.parse,                cookies=cookies            # 2.直接带上cookie进行页面的请求                )def parse(self, response):          # 接下来的请求scrapy会直接将其带上cookie    ret = re.findall(r&quot;你瞅啥&quot;,response.body.decode())    print(ret)    yield scrapy.Request(            &quot;http://www.renren.com/327550029/profile?v=info_timeline&quot;;,            callback=self.parse1</code></pre><p>scrapy发送post请求模拟登录<br>使用场景<br>模拟的POST请求可以获取到请求参数和实际请求的URL地址（可以在之前请求的响应找到，或者可以通过解析js等方式获取到post请求的所需参数）<br>发送post请求获取cookie的示例<br>​        import scrapy<br>​        import re<br>​        class GithubSpider(scrapy.Spider):<br>​            name = ‘github’<br>​            allowed_domains = [‘github.com’]<br>​            start_urls = [‘<a href="https://github.com/login&#39;;;]" target="_blank" rel="noopener">https://github.com/login&#39;;;]</a><br>​            def parse(self, response):<br>​                form_data = {}<br>​                # 1.获取并构建post请求需要的参数<br>​                form_data[“authenticity_token”] =      # 可以在之前的响应中获取，或js解析<br>​                   response.xpath(“//input[@name=’authenticity_token’]/@value”).extract_first()<br>​                form_data[“commit”] = “Sign in”<br>​                form_data[“utf8”] = “✓”<br>​                form_data[“login”] = “noobpythoner”<br>​                form_data[“password”] = “zhoudawei123”<br>​            </p><pre><code>    # 2.使用scrapy.FormRequest携带参数对登录的url地址进行请求    yield scrapy.FormRequest(       &quot;https://github.com/session&quot;,      #post请求的url地址        formdata = form_data,        callback = self.after_login    )def after_login(self,response):    ret = re.findall(r&quot;noobpythoner&quot;,response.body.decode())    print(ret)</code></pre><p>scrapy自动模拟登录<br>使用场景<br>在进行提交post请求的表单中，的form表单具有action属性<br>可以使用scrapy.FormRequest.from_resopnse方法，会自动帮我们获取form表单的提交地址<br>我们提供表单中需要填写的内容，可表单提交请求的响应交给哪个函数处理<br>scrapy自动模拟登录示例<br>​            import scrapy<br>​            import re<br>​            class RenrenSpider(scrapy.Spider):<br>​                 name = ‘renren’<br>​                 allowed_domains = [‘renren.com’]<br>​                 start_urls = [‘<a href="http://www.renren.com/327550029/profile&#39;]" target="_blank" rel="noopener">http://www.renren.com/327550029/profile&#39;]</a></p><pre><code>def parse(self, response):                # 使用FormRequest.from_resopnse对表单地址进行请求yield scrapy.FormRequest.from_resopnse(     # 传入包含post请求表单的响应，表单内容，回调函数     response,     formdata={&apos;email&apos;: &apos;user_name&apos;, &apos;password&apos;: &apos;password&apos;},     callback=self.parse_page     )def parse_page(self, response):    print(response.url, &apos;*&apos;, * 100, response.starus)    print(&apos;*&apos; * 100)    print(re.findall(r&apos;user_name&apos;, response.body.decode()))                    </code></pre><p>设置cookie允许在函数中传递<br>在setting中配置cookie的传递<br>​            COOKIES_ENABLE = True       # 设置允许cookie在不同的解析函数中传递，默认是允许的<br>​            COOKIES_DEBUG = True        # 可以看带cookie在函数中传递的过程</p><p>终端效果如下</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Gerapy%E7%9A%84%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/scrapy%E6%A1%86%E6%9E%B6/Gerapy%E7%9A%84%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>Gerapy的介绍<br>Gerapy的介绍<br>分布式的爬虫部署和管理工具，基于scrapy、scrapyd、scrapyd-API、Django、Vue.js<br>Gerapy的使用<br>Gerapy的安装<br>pip3 install gerapy<br>Gerapy应用步骤<br>1.初始化Gerapy<br>gerapy init<br>执行完毕之后，本地便会生成一个名字为 gerapy 的文件夹，接着进入该文件夹，可以看到有一个 projects 文件夹，我们后面会用到。<br>2.初始化数据库<br>cd gerapy<br>gerapy migrate<br>会在 gerapy 目录下生成一个 SQLite 数据库，同时建立数据库表。<br>3.启动Gerapy<br>gerapy runserver<br>这样我们就可以看到 Gerapy 已经在 8000 端口上运行了。接下来我们在浏览器中打开 <a href="http://localhost:8000/，就可以看到" target="_blank" rel="noopener">http://localhost:8000/，就可以看到</a> Gerapy 的主界面了<br>使用Gerapy管理项目<br>主机管理<br>我们可以点击左侧 Clients 选项卡，即主机管理页面，添加我们的 Scrapyd 远程服务，点击右上角的创建按钮即可添加我们需要管理的 Scrapyd 服务。<br>需要添加 IP、端口，以及名称，点击创建即可完成添加，点击返回即可看到当前添加的 Scrapyd 服务列表<br>添加主机前后<br>这样我们可以在状态一栏看到各个 Scrapyd 服务是否可用，同时可以一目了然当前所有 Scrapyd 服务列表，另外我们还可以自由地进行编辑和删除。</p><p>项目管理<br>Gerapy 的核心功能当然是项目管理，在这里我们可以自由地配置、编辑、部署我们的 Scrapy 项目，点击左侧的 Projects ，即项目管理选项，我们可以看到如下空白的页面：</p><p>将项目拖动到刚才 gerapy 运行目录的 projects 文件夹下，例如我这里写好了一个 Scrapy 项目，这时刷新页面，我们便可以看到 Gerapy 检测到了这个项目，同时它是不可配置、没有打包的</p><p>这时我们可以点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于 Git 的 commit 信息，然后点击打包按钮，即可发现 Gerapy 会提示打包成功，同时在左侧显示打包的结果和打包名称<br>打包成功之后，我们便可以进行部署了，我们可以选择需要部署的主机，点击后方的部署按钮进行部署，同时也可以批量选择主机进行部署，示例如下</p><p>监控任务<br>部署完毕之后就可以回到主机管理页面进行任务调度了，任选一台主机，点击调度按钮即可进入任务管理页面，此页面可以查看当前 Scrapyd 服务的所有项目、所有爬虫及运行状态<br>我们可以通过点击新任务、停止等按钮来实现任务的启动和停止等操作，同时也可以通过展开任务条目查看日志详情</p><p>编辑项目<br>同时 Gerapy 还支持项目编辑功能，有了它我们不再需要 IDE 即可完成项目的编写，我们点击项目的编辑按钮即可进入到编辑页面，如图所示</p><p>CrawlSpider代码自动生成<br>在 Scrapy 中，其实提供了一个可配置化的爬虫 CrawlSpider，它可以利用一些规则来完成爬取规则和解析规则的配置，这样可配置化程度就非常高，这样我们只需要维护爬取规则、提取逻辑就可以了。如果要新增一个爬虫，我们只需要写好对应的规则即可，这类爬虫就叫做可配置化爬虫。<br>Gerapy 可以做到：我们写好爬虫规则，它帮我们自动生成 Scrapy 项目代码。<br>我们可以点击项目页面的右上角的创建按钮，增加一个可配置化爬虫，接着我们便可以在此处添加提取实体、爬取规则、抽取规则了，例如这里的解析器，我们可以配置解析成为哪个实体，每个字段使用怎样的解析方式，如 XPath 或 CSS 解析器、直接获取属性、直接添加值等多重方式，另外还可以指定处理器进行数据清洗，或直接指定正则表达式进行解析等等，通过这些流程我们可以做到任何字段的解析。</p><p>再比如爬取规则，我们可以指定从哪个链接开始爬取，允许爬取的域名是什么，该链接提取哪些跟进的链接，用什么解析方法来处理等等配置。通过这些配置，我们可以完成爬取规则的设置。</p><p>最后点击生成按钮即可完成代码的生成。</p><p>生成的代码示例结果如图所示，可见其结构和 Scrapy 代码是完全一致的。</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>爬虫的介绍<br>爬虫的定义<br>  模拟浏览器其发送请求，获取到和浏览器一模一样的数据<br>  浏览器能够看到的，我们爬虫才能够获取到，否则是没有办法获取到的，所以，只要浏览器能做的事情，原则上，爬虫都能做<br>爬虫获取的数据的用途<br>呈现数据，呈现在app或者在网站上<br>伪造网站请求，进行自动的访问网站<br>进行数据分析，获得结论<br>爬虫的分类<br>通用爬虫：<br>搜索引擎的爬虫，通常指搜索引擎的爬虫，通用网络爬虫利用种子url从互联网中搜集网页，采集信息，这些网页信息用于为搜索引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。<br>聚焦爬虫：<br>针对特定网站的爬虫，指定url进行爬取，有明确的爬取目标<br>通用爬虫工作原理<br>第一步：数据抓取<br>首先选取一部分的种子URL，将这些URL放入待抓取URL队列；<br>取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。<br>分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环….<br>第二步：数据存储<br>搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。<br>搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。<br>第三步：预处理<br>搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。<br>提取文字<br>中文分词<br>消除噪音（比如版权声明文字、导航条、广告等……）<br>索引处理<br>链接关系计算<br>特殊文件处理<br>第四步：提供检索服务，网站排名<br>搜索引擎在对信息进行组织和处理后，为用户提供关键字检索服务，将用户检索相关的信息展示给用户。<br>同时会根据页面的PageRank值（链接的访问量排名）来进行网站排名，这样Rank值高的网站在搜索结果中会排名较前，当然也可以直接使用 Money 购买搜索引擎网站排名，简单粗暴。<br>通用爬虫的局限性<br>通用搜索引擎所返回的网页里90%的内容无用。<br>图片、音频、视频多媒体的内容通用搜索引擎无能为力<br>不同用户搜索的目的不全相同，但是返回内容相同<br>聚焦爬虫的工作流程</p><p>爬虫爬取哪些数据<br>教育机构：其他教育机构的开班，招生，就业，口碑<br>资讯公司：特定领域的新闻数据的爬虫<br>金融公司：关于各个公司的动态的信息，<br>酒店/旅游：携程，去哪儿的酒店价格信息/机票，景点价格，其他旅游公司价格信息<br>房地产、高铁：10大房地产楼盘门户网站，政府动态等<br>强生保健医药：医疗数据，价格，目前的市场的行情<br>Robots协议：<br>网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。<br>这个协议之是道德层面的协议，这个协议并不能从技术上阻止去对其网站进行爬取。<br>例如：<a href="https://www.taobao.com/robots.txt" target="_blank" rel="noopener">https://www.taobao.com/robots.txt</a><br>HTTP/HTTPS协议<br>url的形式<br>url表现形式：scheme://host[:port#]/path/…/[?query-string][#anchor]<br>scheme：协议(例如：http, https, ftp)<br>host：服务器的IP地址或者域名<br>port：服务器的端口（如果是走协议默认端口，80  or    443）<br>path：访问资源的路径<br>query-string：参数，发送给http服务器的数据<br>anchor：锚（跳转到网页的指定锚点位置）<br>示例：<a href="http://localhost:4000/file/part01/1.2.htmlhttp://item.jd.com/11936238.html#product-detail" target="_blank" rel="noopener">http://localhost:4000/file/part01/1.2.htmlhttp://item.jd.com/11936238.html#product-detail</a><br>HTTP/HTTPS的区别<br>HTTP<br>超文本传输协议<br>默认端口号:80<br>HTTPS<br>HTTP + SSL(安全套接字层)<br>默认端口号：443<br>HTTP和HTTPS区别<br>浏览器默认请求服务器是以HTTP协议进行请求，如果服务器支持HTTPS协议，会返回给浏览器端一个协议相关的响应，浏览器会重新发起HTTPS协议的请求；</p><p>HTTP请求报文的形式</p><p>http的请求过程<br>域名—&gt;dns（拿ip）—&gt;浏览器请求ip—&gt;服务器—&gt;返回资源<br>HTTP常见的请求头</p><ol><li>Host (主机和端口号)<br>对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的Host部分</li><li>Connection (链接类型)<br>keep-alive  客户端携带表示支持长连接<br>keep-alive  服务器端如果回复keep-alive， 代表允许双方建立长连接<br>close： 服务器端回复close，代表不允许建立长连接，浏览器接收到响应后会主动断开连接</li><li>Upgrade-Insecure-Requests (浏览器支持升级为HTTPS请求)<br>升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。</li><li>User-Agent (浏览器名称)<br>对方的服务器通过User-Agent判断出来我们是一个手机版的浏览器还是电脑版的，同时能够判断出来浏览器的平台，型号，版本，内核版本</li><li>Accept (传输文件类型)<br>Accept: <em>/</em>：表示什么都可以接收。<br>Accept：image/gif：表明客户端希望接受GIF图像格式的资源；<br>Accept：text/html：表明客户端希望接受html文本。<br>Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。<br>q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。<br>text：用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；<br>application用于传输应用程序数据或者二进制数据。</li><li>Referer (页面跳转处)<br>表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。<br>防盗链：有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载。</li><li>Accept-Encoding（文件编解码格式）<br>指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。<br>举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0</li><li>Cookie （Cookie）<br>cookies 能够记录用户信息，会在请求的时候一起传递给对方的服务器，对方的服务器能够根据cookie判断出来是否登陆过（用户的状态）</li><li>x-requested-with :XMLHttpRequest  (是Ajax 异步请求)<br>10.Accept-Language   (接受语言)<br>指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。<br>11.Accept-Charset（字符编码）<br>指出浏览器可以接受的字符编码。<br>举例：Accept-Charset:iso-8859-1,gb2312,utf-8<br>12.Content-Type (POST数据类型)<br>Content-Type：POST请求里用来表示的内容类型。<br>举例：Content-Type = Text/XML; charset=gb2312：<br>0.请求体<br>常见的请求方法<br>GET<br>特点：请求所带参数包含在url中，显示在地址栏，请求数据可以被缓存，请求参数有长度限制，请求速度快<br>POST<br>特点：请求所带参数在请求体中，请求数据不可以被缓存，请求数据没有长度限制，请求速度慢<br>常用的响应报头</li><li>Cache-Control：must-revalidate, no-cache, private。<br>告诉客户端，对资源的缓存建议；</li><li>Connection：keep-alive<br>作为回应客户端的Connection：keep-alive，告诉客户端是否同意建立长连接；</li><li>Content-Encoding:gzip<br>告诉客户端，服务端发送的资源是采用gzip编码的；</li><li>Content-Type：text/html;charset=UTF-8<br>告诉客户端，资源文件的类型，还有字符编码；</li><li>Date：Sun, 21 Sep 2016 06:18:21 GMT<br>这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。</li><li>Expires:Sun, 1 Jan 2000 01:00:00 GMT<br>告诉客户端在这个时间前，缓存的到期时间</li><li>Pragma:no-cache<br>这个含义与Cache-Control等同。<br>8.Server：Tengine/1.4.6<br>这个是服务器和相对应的版本，只是告诉客户端服务器的信息。</li><li>Transfer-Encoding：chunked<br>这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。<br>HTTP常见响应状态码<br>100~199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。<br>200：表示服务器成功接收请求并已完成整个处理过程<br>200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。<br>302：临时转移至新的url<br>300~399表示请求转<br>307：临时转移至新的url<br>404：not found<br>400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。<br>500：服务器内部错误<br>为什么浏览器渲染出来的页面和爬虫请求的页面不一样<br>爬虫请求结果<br>爬虫只会请求当前url地址，不会主动请求js，所以往往当前URL对应的响应和element的内容不一样<br>浏览器<br>浏览器请求服务器渲染出来的界面，以及最终的在终端element显示的内容，和爬虫爬取到的不一样，因为浏览器会对页面中的静态文件的url进行请求，将请求结果渲染到浏览器中，导致最终的网页代码和显示的结果，已经是被js文件进行修改过；<br>在哪里查看当前url地址对应的响应（不包括对静态文件请求的响应）：<br>抓包（network），network下的第一个url地址，当前url地址的response<br>右键显示网页源码</li></ol><p>字符串和字节（str/ bytes）<br>python3 字符串应用的字符集<br>str ：unicode的呈现形式（python3应用的unicode的子集utf-8的形式对字符串进行呈现）<br>bytes：二进制互联网上数据的都是以二进制的方式传输的，bytes是二进制格式的字符串<br>对服务器的请求，要将请求数据先转化为bytes格式；<br>获取到服务器的响应要将获取到的bytes类型的数据，进行字符串的转化<br>Unicode/UTF8/ASCII字符集的介绍<br>字符(Character)<br>各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等<br>字符集(Character set)<br>多个字符的集合字符集包括：<br>ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集<br>ASCII编码是1个字节，而Unicode编码通常是2个字节，UTF-8是Unicode的实现方式之一，它是一种变长的编码方式，可以是1，2，3个字节（一句字符的内容而定）<br>str到bytes之间的转化（str、bytes）<br>python3中字节和字符之间转化的方法<br>从str—&gt; bytes：str使用encode方法转化为 bytes<br>从bytes—&gt; str：bytes通过decode转化为str<br>注意：<br>编码方式解码方式必须一样，否则就会出现乱码</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E7%88%AC%E8%99%AB%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7%E5%88%97%E8%A1%A8/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E7%88%AC%E8%99%AB%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7%E5%88%97%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>Python 爬虫的工具列表<br>爬虫网络库<br>通用<br>urllib -网络库(stdlib)。<br>requests -网络库。<br>grab – 网络库(基于pycurl)。<br>pycurl – 网络库(绑定libcurl)。<br>urllib3 – Python HTTP库，安全连接池、支持文件post、可用性高。<br>httplib2 – 网络库。<br>RoboBrowser – 一个简单的、极具Python风格的Python库，无需独立的浏览器即可浏览网页。<br>MechanicalSoup – 一个与网站自动交互Python库。<br>mechanize -有状态、可编程的Web浏览库。<br>socket – 底层网络接口(stdlib)。<br>Unirest for Python – Unirest是一套可用于多种语言的轻量级的HTTP库。<br>hyper – Python的HTTP/2客户端。<br>PySocks – SocksiPy更新并积极维护的版本，包括错误修复和一些其他的特征。作为socket模块的直接替换。<br>异步<br>treq – 类似于requests的API(基于twisted)。<br>aiohttp – asyncio的HTTP客户端/服务器(PEP-3156)。<br>网络爬虫框架<br>功能齐全的爬虫<br>grab – 网络爬虫框架(基于pycurl/multicur)。<br>scrapy – 网络爬虫框架(基于twisted)。<br>pyspider – 一个强大的爬虫系统。<br>cola – 一个分布式爬虫框架。<br>其他<br>portia – 基于Scrapy的可视化爬虫。<br>restkit – Python的HTTP资源工具包。它可以让你轻松地访问HTTP资源，并围绕它建立的对象。<br>demiurge – 基于PyQuery的爬虫微框架。<br>HTML/XML解析器<br>通用<br>lxml – C语言编写高效HTML/ XML处理库。支持XPath。<br>cssselect – 解析DOM树和CSS选择器。<br>pyquery – 解析DOM树和jQuery选择器。<br>BeautifulSoup – 低效HTML/ XML处理库，纯Python实现。<br>html5lib – 根据WHATWG规范生成HTML/ XML文档的DOM。该规范被用在现在所有的浏览器上。<br>feedparser – 解析RSS/ATOM feeds。<br>MarkupSafe – 为XML/HTML/XHTML提供了安全转义的字符串。<br>xmltodict – 一个可以让你在处理XML时感觉像在处理JSON一样的Python模块。<br>xhtml2pdf – 将HTML/CSS转换为PDF。<br>untangle – 轻松实现将XML文件转换为Python对象。<br>清理<br>Bleach – 清理HTML(需要html5lib)。<br>sanitize – 为混乱的数据世界带来清明。<br>文本处理<br>用于解析和操作简单文本的库。<br>通用<br>difflib – (Python标准库)帮助进行差异化比较。<br>Levenshtein – 快速计算Levenshtein距离和字符串相似度。<br>fuzzywuzzy – 模糊字符串匹配。<br>esmre – 正则表达式加速器。<br>ftfy – 自动整理Unicode文本，减少碎片化。<br>转换<br>unidecode – 将Unicode文本转为ASCII。<br>字符编码<br>uniout – 打印可读字符，而不是被转义的字符串。<br>chardet – 兼容 Python的2/3的字符编码器。<br>xpinyin – 一个将中国汉字转为拼音的库。<br>pangu.py – 格式化文本中CJK和字母数字的间距。<br>Slug化<br>awesome-slugify – 一个可以保留unicode的Python slugify库。<br>python-slugify – 一个可以将Unicode转为ASCII的Python slugify库。<br>unicode-slugify – 一个可以将生成Unicode slugs的工具。<br>pytils – 处理俄语字符串的简单工具(包括pytils.translit.slugify)。<br>通用解析器<br>PLY – lex和yacc解析工具的Python实现。<br>pyparsing – 一个通用框架的生成语法分析器。<br>人的名字<br>python-nameparser -解析人的名字的组件。<br>电话号码<br>phonenumbers -解析，格式化，存储和验证国际电话号码。<br>用户代理字符串<br>python-user-agents – 浏览器用户代理的解析器。<br>HTTP Agent Parser – Python的HTTP代理分析器。<br>特定格式文件处理<br>解析和处理特定文本格式的库。<br>通用<br>tablib – 一个把数据导出为XLS、CSV、JSON、YAML等格式的模块。<br>textract – 从各种文件中提取文本，比如 Word、PowerPoint、PDF等。<br>messytables – 解析混乱的表格数据的工具。<br>rows – 一个常用数据接口，支持的格式很多(目前支持CSV，HTML，XLS，TXT – 将来还会提供更多!)。<br>Office<br>python-docx – 读取，查询和修改的Microsoft Word2007/2008的docx文件。<br>xlwt / xlrd – 从Excel文件读取写入数据和格式信息。<br>XlsxWriter – 一个创建Excel.xlsx文件的Python模块。<br>xlwings – 一个BSD许可的库，可以很容易地在Excel中调用Python，反之亦然。<br>openpyxl – 一个用于读取和写入的Excel2010 XLSX/ XLSM/ xltx/ XLTM文件的库。<br>Marmir – 提取Python数据结构并将其转换为电子表格。<br>PDF<br>PDFMiner – 一个从PDF文档中提取信息的工具。<br>PyPDF2 – 一个能够分割、合并和转换PDF页面的库。<br>ReportLab – 允许快速创建丰富的PDF文档。<br>pdftables – 直接从PDF文件中提取表格。<br>Markdown<br>Python-Markdown – 一个用Python实现的John Gruber的Markdown。<br>Mistune – 速度最快，功能全面的Markdown纯Python解析器。<br>markdown2 – 一个完全用Python实现的快速的Markdown。<br>YAML<br>PyYAML – 一个Python的YAML解析器。<br>CSS<br>cssutils – 一个Python的CSS库。<br>ATOM/RSS<br>feedparser – 通用的feed解析器。<br>SQL<br>sqlparse – 一个非验证的SQL语句分析器。<br>HTTP<br>http-parser – C语言实现的HTTP请求/响应消息解析器。<br>微格式<br>opengraph – 一个用来解析Open Graph协议标签的Python模块。<br>可移植的执行体<br>pefile – 一个多平台的用于解析和处理可移植执行体(即PE)文件的模块。<br>PSD<br>psd-tools – 将Adobe Photoshop PSD(即PE)文件读取到Python数据结构。<br>自然语言处理<br>处理人类语言问题的库。<br>NLTK -编写Python程序来处理人类语言数据的最好平台。<br>Pattern – Python的网络挖掘模块。他有自然语言处理工具，机器学习以及其它。<br>TextBlob – 为深入自然语言处理任务提供了一致的API。是基于NLTK以及Pattern的巨人之肩上发展的。<br>jieba – 中文分词工具。<br>SnowNLP – 中文文本处理库。<br>loso – 另一个中文分词库。<br>genius – 基于条件随机域的中文分词。<br>langid.py – 独立的语言识别系统。<br>Korean – 一个韩文形态库。<br>pymorphy2 – 俄语形态分析器(词性标注+词形变化引擎)。<br>PyPLN – 用Python编写的分布式自然语言处理通道。这个项目的目标是创建一种简单的方法使用NLTK通过网络接口处理大语言库。<br>浏览器自动化与仿真<br>selenium – 自动化真正的浏览器(Chrome浏览器，火狐浏览器，Opera浏览器，IE浏览器)。<br>Ghost.py – 对PyQt的webkit的封装(需要PyQT)。<br>Spynner – 对PyQt的webkit的封装(需要PyQT)。<br>Splinter – 通用API浏览器模拟器(selenium web驱动，Django客户端，Zope)。<br>多重处理<br>threading – Python标准库的线程运行。对于I/O密集型任务很有效。对于CPU绑定的任务没用，因为python GIL。<br>multiprocessing – 标准的Python库运行多进程。<br>celery – 基于分布式消息传递的异步任务队列/作业队列。<br>concurrent-futures – concurrent-futures 模块为调用异步执行提供了一个高层次的接口。<br>异步<br>异步网络编程库<br>asyncio – (在Python 3.4 +版本以上的 Python标准库)异步I/O，时间循环，协同程序和任务。<br>Twisted – 基于事件驱动的网络引擎框架。<br>Tornado – 一个网络框架和异步网络库。<br>pulsar – Python事件驱动的并发框架。<br>diesel – Python的基于绿色事件的I/O框架。<br>gevent – 一个使用greenlet 的基于协程的Python网络库。<br>eventlet – 有WSGI支持的异步框架。<br>Tomorrow – 异步代码的奇妙的修饰语法。<br>队列<br>celery – 基于分布式消息传递的异步任务队列/作业队列。<br>huey – 小型多线程任务队列。<br>mrq – Mr. Queue – 使用redis &amp; Gevent 的Python分布式工作任务队列。<br>RQ – 基于Redis的轻量级任务队列管理器。<br>simpleq – 一个简单的，可无限扩展，基于Amazon SQS的队列。<br>python-gearman – Gearman的Python API。<br>云计算<br>picloud – 云端执行Python代码。<br>dominoup.com – 云端执行R，Python和matlab代码。<br>电子邮件<br>电子邮件解析库<br>flanker – 电子邮件地址和Mime解析库。<br>Talon – Mailgun库用于提取消息的报价和签名。<br>网址和网络地址操作<br>解析/修改网址和网络地址库。<br>URL<br>furl – 一个小的Python库，使得操纵URL简单化。<br>purl – 一个简单的不可改变的URL以及一个干净的用于调试和操作的API。<br>urllib.parse – 用于打破统一资源定位器(URL)的字符串在组件(寻址方案，网络位置，路径等)之间的隔断，为了结合组件到一个URL字符串，并将“相对URL”转化为一个绝对URL，称之为“基本URL”。<br>tldextract – 从URL的注册域和子域中准确分离TLD，使用公共后缀列表。<br>网络地址<br>netaddr – 用于显示和操纵网络地址的Python库。<br>网页内容提取<br>提取网页内容的库。<br>HTML页面的文本和元数据<br>newspaper – 用Python进行新闻提取、文章提取和内容策展。<br>html2text – 将HTML转为Markdown格式文本。<br>python-goose – HTML内容/文章提取器。<br>lassie – 人性化的网页内容检索工具<br>micawber – 一个从网址中提取丰富内容的小库。<br>sumy -一个自动汇总文本文件和HTML网页的模块<br>Haul – 一个可扩展的图像爬虫。<br>python-readability – arc90 readability工具的快速Python接口。<br>scrapely – 从HTML网页中提取结构化数据的库。给出了一些Web页面和数据提取的示例，scrapely为所有类似的网页构建一个分析器。<br>视频<br>youtube-dl – 一个从YouTube下载视频的小命令行程序。<br>you-get – Python3的YouTube、优酷/ Niconico视频下载器。<br>维基<br>WikiTeam – 下载和保存wikis的工具。<br>WebSocket<br>用于WebSocket的库。<br>Crossbar – 开源的应用消息传递路由器(Python实现的用于Autobahn的WebSocket和WAMP)。<br>AutobahnPython – 提供了WebSocket协议和WAMP协议的Python实现并且开源。<br>WebSocket-for-Python – Python 2和3以及PyPy的WebSocket客户端和服务器库。<br>DNS解析<br>dnsyo – 在全球超过1500个的DNS服务器上检查你的DNS。<br>pycares – c-ares的接口。c-ares是进行DNS请求和异步名称决议的C语言库。<br>计算机视觉<br>OpenCV – 开源计算机视觉库。<br>SimpleCV – 用于照相机、图像处理、特征提取、格式转换的简介，可读性强的接口(基于OpenCV)。<br>mahotas – 快速计算机图像处理算法(完全使用 C++ 实现)，完全基于 numpy 的数组作为它的数据类型。<br>代理服务器<br>shadowsocks – 一个快速隧道代理，可帮你穿透防火墙(支持TCP和UDP，TFO，多用户和平滑重启，目的IP黑名单)。<br>tproxy – tproxy是一个简单的TCP路由代理(第7层)，基于Gevent，用Python进行配置。<br>其他Python工具列表<br>awesome-python<br>pycrumbs<br>python-github-projects<br>python_reference<br>pythonidae</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%20json%20%20Xpath/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%20json%20%20Xpath/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>多线程爬虫所需应用<br>Queue（对列对象）<br>介绍Queue<br>Queue是python中的标准库，可以直接import Queue引用;队列是线程间最常用的交换数据的形式。<br>Queue的作用<br>python下多线程对于资源的处理，加锁是个重要的环节，因为python原生的list,dict等，都是not thread safe的，所以使用Queue，可以不用手动添加资源锁就能解决资源竞争的问题。<br>在使用Queue进行参数传递的时候，只有Queue列队中有数据，线程就可以获得并执行任务，不需要等待其他线程对数据的处理并传递，更方便数据的传递和处理；<br>使用方法<br>创建队列对象<br>from queue import Queue<br>queue = Queue(maxsize = n)        创建一个最大容量为n的queue列队<br>将一个数值放在队列中<br>queue.put( )      向列队中添加任务，列队满后会阻塞等待<br>myqueue.put_nowait( )    想列队中添加任务，列队满后会报错<br>将一个数值从队列中取出<br>queue.get( )     从列队中取出任务<br>queue.get([block[, timeout]])        从列队中取出任务，超时后会报错<br>备注：从列队取出任务后，列队计数并不会减一，需要标记任务task_done<br>任务完成，列队技术减一<br>queue.task_done( )      列队计数减一<br>列队任务计数不为0时，阻塞线程<br>Queue.join( )                 列队如果不为空，阻塞主线程<br>其他查询列队状态的方法<br>Queue.qsize()              返回队列当前计数大小<br>Queue.empty()           如果队列为空，返回True,反之False<br>Queue.full()                  如果队列满了，返回True,反之False<br>Queue.full                    与 maxsize 大小对应<br>线程的使用（复习）<br>创建子线程<br>import threading<br>t1 = threading.Thread(target=function, args=(, ))<br>将子线程设置为守护进程<br>t1.setDeamon(True)<br>启动子线程<br>t1.start( )<br>线程的使用策略<br>使用策略<br>创建不同功能任务需求的列队<br>在处理时间较长的功能函数处，使用比较多的线程<br>在处理时间比较快的函数处，使用比较少的线程<br>少创建一些线程，线程之间会进行资源的竞争，导致请求失败率增加<br>多设置一些最大请求次数，增加请求成功的几率<br>将失败的请求，任务，防止到一个列表/对列中，单独再对失败的进程进行统一执行<br>断点续爬的思想<br>场景<br>在我们爬取数据的时候，想要先停止爬取，将爬取的状态和数据保存，并想要下次爬取的时候再接着当前的状态继续爬取<br>实现方法<br>可以保存当前的所有的queue列队，在程序结束之前将queue对象存储到redis中<br>当下次启动程序的时候，从redis数据库中读取上次停止之前的queue对象<br>程序判断queue对象的状态，是否全部为空，如果不为空，那么便不需要再重新获取参数想queue列队中添加，直接接着之前的queue的装填进行爬取<br>多线程爬虫代码实例</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Splash%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Splash%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<p>Splash的功能介绍和运行<br>Splash的简介<br>Splash是一个JavaScript渲染服务，是一个带有HTTP API的轻量级浏览器，同时它对接了Python中的Twisted和QT库。利用它，我们同样可以实现动态渲染页面的抓取。<br>Splash文档<br>Splash官方文档<br><a href="https://splash.readthedocs.io/en/stable/scripting-ref.html" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/scripting-ref.html</a><br>Splash-api官方文档<br><a href="https://splash.readthedocs.io/en/stable/api.html" target="_blank" rel="noopener">https://splash.readthedocs.io/en/stable/api.html</a><br>Splash的功能<br>异步方式处理多个网页渲染过程；<br>获取渲染后的页面的源代码或截图；<br>通过关闭图片渲染或者使用Adblock规则来加快页面渲染速度；<br>可执行特定的JavaScript脚本；<br>可通过Lua脚本来控制页面渲染过程；<br>获取渲染的详细过程并通过HAR（HTTP Archive）格式呈现。<br>Splash服务的运行<br>docker run -d -p 8050:8050 scrapinghub/splash<br>打开<a href="http://localhost:8050/即可看到其Web页面" target="_blank" rel="noopener">http://localhost:8050/即可看到其Web页面</a><br>Splash-Lua脚本介绍<br>Splash的运行机制<br>Splash加载和操作渲染过程，可以理解是由一段Lua代码来操控<br>Splash留出来了一个main() 函数的接口，我们可以通过编写该段函数，操作其传入的splash对象，Splash会调用其指令并执行对应的操作<br>可以在浏览器界面编写并点击render me来启动，也可以通过调用splash-api的方式来调用<br>Splash Lua脚本示例<br>function main(splash, args)<br>assert(splash:go(args.url))         请求url<br>assert(splash:wait(0.5))       等待0.5s<br>return {       返回结果<br>html = splash:html(),            获取页面源码<br>png = splash:png(),        获取页面截图<br>har = splash:har(),        获取请求的详细har信息<br>}<br>end<br>Splash异步处理<br>Splash异步的介绍<br>在脚本内调用的wait()方法类似于Python中的sleep()，其参数为等待的秒数。当Splash执行到此方法时，它会转而去处理其他任务，然后在指定的时间过后再回来继续处理。<br>另外，这里做了加载时的异常检测。go()方法会返回加载页面的结果状态，如果页面出现4xx或5xx状态码，ok变量就为空<br>Splash异步脚本示例<br>function main(splash, args)<br>local example_urls = {“<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">www.baidu.com&quot;</a>, “<a href="http://www.taobao.com&quot;" target="_blank" rel="noopener">www.taobao.com&quot;</a>, “<a href="http://www.zhihu.com&quot;}" target="_blank" rel="noopener">www.zhihu.com&quot;}</a><br>local urls = args.urls or example_urls<br>local results = {}<br>for index, url in ipairs(urls) do<br>local ok, reason = splash:go(“http://“ .. url)<br>if ok then<br>splash:wait(2)<br>results[url] = splash:png()<br>end<br>end<br>return results<br>end<br>Splash负载均衡的配置<br>搭建多个splash服务节点<br>docker run -d -p 8050:8050 scrapinghub/splash<br>配置负载均衡服务(使用nginx)<br>nginx配置节点<br>​            http {<br>​                upstream splash {<br>​                    least_conn;<br>​                    server 41.159.27.223:8050 weight=4;<br>​                    server 41.159.27.221:8050 weight=2;<br>​                    server 41.159.27.9:8050 weight=2;<br>​                    server 41.159.117.119:8050 weight=1;<br>​                }<br>​                server {<br>​                    listen 8050;<br>​                    location / {<br>​                        proxy_pass <a href="http://splash" target="_blank" rel="noopener">http://splash</a>;<br>​                    }<br>​                }<br>​            }<br>nginx配置认证<br>现在Splash是可以公开访问的，如果不想让其公开访问，还可以配置认证，这仍然借助于Nginx。可以在server的location字段中添加auth_basic和auth_basic_user_file字段，具体配置如下：<br>这里使用的用户名和密码配置放置在/etc/nginx/conf.d目录下，我们需要使用htpasswd命令创建。例如，创建一个用户名为admin的文件，相关命令如下<br>htpasswd -c .htpasswd admin<br>接下来就会提示我们输入密码，输入两次之后，就会生成密码文件，其内容如下：<br>配置完成后，重启一下Nginx服务： sudo nginx -s reload<br>​                server {<br>​                    listen 8050;<br>​                    location / {<br>​                        proxy_pass <a href="http://splash" target="_blank" rel="noopener">http://splash</a>;<br>​                        auth_basic “Restricted”;<br>​                        auth_basic_user_file /etc/nginx/conf.d/.htpasswd;<br>​                    }<br>​                }<br>Splash对象的常用属性<br>加载参数<br>获取方法一<br>​            function main(splash, args)<br>​                local url = args.url<br>​            end<br>获取方法二<br>​            function main(splash)<br>​                local url = splash.args.url<br>​            end<br>开启关闭javascript<br>splash.js_enabled = false/true<br>设置加载的超时时间<br>splash.resource_timeout = 2<br>单位是秒。如果设置为0或nil（类似Python中的None），代表不检测超时。<br>禁止加载图片<br>splash.images_enabled = true/false<br>禁止开启插件(flash等)<br>splash.plugins_enabled = true/false<br>滚动条<br>splash.scroll_position = {x=100, y=400}<br>滚动条向右滚动100像素，向下滚动400像素<br>Splash对象常用方法<br>请求某链接<br>使用方法<br>splash:go{url, baseurl=nil, headers=nil, http_method=”GET”, body=nil, formdata=nil}<br>url：请求的URL。<br>baseurl：可选参数，默认为空，表示资源加载相对路径。<br>headers：可选参数，默认为空，表示请求头。<br>http_method：可选参数，默认为GET，同时支持POST。<br>body：可选参数，默认为空，发POST请求时的表单数据，使用的Content-type为application/json。<br>formdata：可选参数，默认为空，POST的时候的表单数据，使用的Content-type为application/x-www-form-urlencoded。<br>return: 结果ok和原因reason的组合，如果ok为空，代表网页加载出现了错误，此时reason变量中包含了错误的原因<br>使用示例<br>​            function main(splash, args)<br>​                local ok, reason = splash:go{“<a href="http://httpbin.org/post&quot;" target="_blank" rel="noopener">http://httpbin.org/post&quot;</a>, http_method=”POST”, body=”name=Germey”}<br>​                if ok then<br>​                    return splash:html()<br>​                end<br>​            end<br>设置定时任务<br>使用示例(0.2s后获取网页截图)<br>​                    function main(splash, args)<br>​                             local snapshots = {}<br>​                             local timer = splash:call_later(function()<br>​                                 snapshots[“a”] = splash:png()<br>​                                 splash:wait(1.0)<br>​                                 snapshots[“b”] = splash:png()<br>​                                 end, 0.2)<br>​                             splash:go(“<a href="https://www.taobao.com&quot;" target="_blank" rel="noopener">https://www.taobao.com&quot;</a>)<br>​                             splash:wait(3.0)<br>​                             return snapshots<br>​                    end<br>模拟发送get请求<br>使用方法<br>response = splash:http_get{url, headers=nil, follow_redirects=true}<br>url：请求URL。<br>headers：可选参数，默认为空，请求头。<br>follow_redirects：可选参数，表示是否启动自动重定向，默认为true<br>使用示例<br>​            function main(splash, args)<br>​                local treat = require(“treat”)<br>​                local response = splash:http_get(“<a href="http://httpbin.org/get&quot;" target="_blank" rel="noopener">http://httpbin.org/get&quot;</a>)<br>​                    return {<br>​                        html=treat.as_string(response.body),<br>​                        url=response.url,<br>​                        status=response.status<br>​                    }<br>​            end<br>模拟发送post请求<br>使用方法<br>response = splash:http_post{url, headers=nil, follow_redirects=true, body=nil}<br>url：请求URL。<br>headers：可选参数，默认为空，请求头。<br>follow_redirects：可选参数，表示是否启动自动重定向，默认为true。<br>body：可选参数，即表单数据，默认为空。<br>使用示例<br>​            function main(splash, args)<br>​                local treat = require(“treat”)<br>​                local json = require(“json”)<br>​                local response = splash:http_post{“<a href="http://httpbin.org/post&quot;" target="_blank" rel="noopener">http://httpbin.org/post&quot;</a>,<br>​                    body=json.encode({name=”Germey”}),<br>​                    headers={[“content-type”]=”application/json”}<br>​                }<br>​                return {<br>​                    html=treat.as_string(response.body),<br>​                    url=response.url,<br>​                    status=response.status<br>​                }<br>​            end<br>获取页面加载过程描述等信息<br>使用方法<br>splash:har()        获取加载过程详细信息<br>splash:url()        获取正在访问的url<br>页面cookie相关操作<br>使用方法<br>splash:get_cookies()       获取当前页面的cookie<br>cookies = splash:add_cookie{name, value, path=nil, domain=nil, expires=nil, httpOnly=nil, secure=nil}    给页面添加cookie<br>splash:clear_cookies()    清空当前页面所有cookie<br>设置页面大小<br>使用方法<br>splash:get_viewport_size()    获取页面宽高<br>splash:set_viewport_size(width, height)    设置页面宽高<br>splash:set_viewport_full()    设置页面铺满<br>设置User-Agent<br>使用示例<br>​                    function main(splash)<br>​                            splash:set_user_agent(‘Splash’)<br>​                            splash:go(“<a href="http://httpbin.org/get&quot;" target="_blank" rel="noopener">http://httpbin.org/get&quot;</a>)<br>​                            return splash:html()<br>​                    end<br>设置请求头<br>使用示例<br>​                    function main(splash)<br>​                        splash:set_custom_headers({<br>​                                [“User-Agent”] = “Splash”,<br>​                                [“Site”] = “Splash”,<br>​                        })<br>​                        splash:go(“<a href="http://httpbin.org/get&quot;" target="_blank" rel="noopener">http://httpbin.org/get&quot;</a>)<br>​                        return splash:html()<br>​                    end<br>节点操作<br>使用方法<br>element = splash:select(“#kw”)    通过css选择器选择元素, 返回选中的第一个元素<br>element = splash:select_all(“#kw”)    通过css选择器选择元素, 返回选中所有元素<br>element.send_text(‘test’)     向节点输入文本<br>element.mouse_click()    点击节点<br>等待wait<br>使用方法<br>ok, reason = splash:wait{time, cancel_on_redirect=false, cancel_on_error=true}<br>time：等待的秒数。<br>cancel_on_redirect：可选参数，默认为false，表示如果发生了重定向就停止等待，并返回重定向结果。<br>cancel_on_error：可选参数，默认为false，表示如果发生了加载错误，就停止等待。<br>return: 结果ok和原因reason的组合。<br>使用示例<br>​            function main(splash)<br>​                splash:go(“<a href="https://www.taobao.com&quot;" target="_blank" rel="noopener">https://www.taobao.com&quot;</a>)<br>​                splash:wait(2)<br>​                return {html=splash:html()}<br>​            end<br>调用JavaScript定义的方法<br>使用示例<br>​                    function main(splash, args)<br>​                            local get_div_count = splash:jsfunc([[<br>​                                    function () {<br>​                                    var body = document.body;<br>​                                    var divs = body.getElementsByTagName(‘div’);<br>​                                    return divs.length;<br>​                                    }<br>​                                    ]])<br>​                            splash:go(“<a href="https://www.baidu.com&quot;" target="_blank" rel="noopener">https://www.baidu.com&quot;</a>)<br>​                            return (“There are %s DIVs”):format(get_div_count())<br>​                    end<br>执行JavaScript代码(表达式类)<br>使用示例<br>local title = splash:evaljs(“document.title”)  执行JavaScript代码并返回最后一条JavaScript语句的返回结果<br>执行JavaScript代码(声明类)<br>使用示例<br>​                    function main(splash, args)<br>​                         splash:go(“<a href="https://www.baidu.com&quot;" target="_blank" rel="noopener">https://www.baidu.com&quot;</a>)<br>​                          splash:runjs(“foo = function() { return ‘bar’ }”)<br>​                          local result = splash:evaljs(“foo()”)<br>​                          return result<br>​                    end<br>加载js代码<br>使用方法<br>此方法只负责加载JavaScript代码或库，不执行任何操作。如果要执行操作，可以调用evaljs()或runjs()<br>ok, reason = splash:autoload{source_or_url, source=nil, url=nil}<br>source_or_url：JavaScript代码或者JavaScript库链接。<br>source：JavaScript代码。<br>url：JavaScript库链接<br>使用示例<br>​                    function main(splash, args)<br>​                          splash:autoload([[<br>​                                  function get_document_title(){<br>​                                         return document.title;<br>​                            }<br>​                          ]])<br>​                          splash:go(“<a href="https://www.baidu.com&quot;" target="_blank" rel="noopener">https://www.baidu.com&quot;</a>)<br>​                          return splash:evaljs(“get_document_title()”)<br>​                    end<br>页面内容的操作方法<br>使用方法<br>splash:set_content(“<html><body><h1>hello</h1></body></html>“)     添加页面内容<br>splash:html()    获取当前页面源码<br>获取png/jpeg格式的页面截图<br>使用方法<br>splash:png()<br>splash:jpeg()<br>设置代理<br>使用示例<br>​                    function main(splash, args)<br>​                           request = splash:on_request(function(request)<br>​                                    request:set_proxy{host, port, username=nil, password=nil, type=’HTTP/SOCKS5’)<br>​                                    end)<br>​                    end<br>Splash-API的使用<br>Splash API的介绍<br>Splash Lua脚本的用法，但这些脚本是在Splash页面中测试运行的<br>Splash API就是我们可以通过python程序使用Splash服务的方法<br>Splash给我们提供了一些HTTP API接口，我们只需要请求这些接口并传递相应的参数即可<br>常用Splash API<br>render.png<br>接口介绍<br>通过width和height来控制宽高，它返回的是PNG格式的图片二进制数据。示例如下<br>常用参数<br>width、height、render_all<br>使用示例<br>​                        import requests<br>​                        url = ‘<a href="http://localhost:8050/render.png?url=https://www.jd.com&amp;wait=5&amp;width=1000&amp;height=700&#39;" target="_blank" rel="noopener">http://localhost:8050/render.png?url=https://www.jd.com&amp;wait=5&amp;width=1000&amp;height=700&#39;</a><br>​                        response = requests.get(url)<br>​                        with open(‘taobao.png’, ‘wb’) as f:<br>​                               f.write(response.content)<br>render.html<br>接口介绍<br>此接口用于获取JavaScript渲染的页面的HTML代码，接口地址就是Splash的运行地址加此接口名称<br>常用参数<br>url 、baseurl、timeout、resource_timeout、wait、proxy、js、js_source、filter、allowed_domin<br>allowed_content_types、viewport、images、headers、body、heep_method、save_args、load_args<br>使用示例<br>​            import requests<br>​            url = ‘<a href="http://localhost:8050/render.html?url=https://www.baidu.com&#39;" target="_blank" rel="noopener">http://localhost:8050/render.html?url=https://www.baidu.com&#39;</a><br>​            response = requests.get(url)<br>​            print(response.text)<br>render.har<br>接口介绍<br>此接口用于获取页面加载的HAR数据<br>使用示例<br>curl <a href="http://localhost:8050/render.har?url=https://www.jd.com&amp;wait=5" target="_blank" rel="noopener">http://localhost:8050/render.har?url=https://www.jd.com&amp;wait=5</a><br>render.json<br>接口介绍<br>此接口包含了前面接口的所有功能，返回结果是JSON格式<br>以JSON形式返回了相应的请求数据<br>可以通过传入不同参数控制其返回结果。比如，传入html=1，返回结果即会增加源代码数据；传入png=1，返回结果即会增加页面PNG截图数据；传入har=1，则会获得页面HAR数据<br>使用示例<br>curl <a href="http://localhost:8050/render.json?url=https://httpbin.org" target="_blank" rel="noopener">http://localhost:8050/render.json?url=https://httpbin.org</a><br>execute<br>接口介绍<br>用此接口便可实现与Lua脚本的对接<br>可以用该接口，通过原生的Lua脚本来调用splash<br>可以通过lua_source参数传递Lua脚本给splash服务器进行请求和渲染服务<br>常用参数<br>timeout、allowed_domains、proxy、filters、save_args、load_args<br>使用示例<br>​                        import requests<br>​                        from urllib.parse import quote<br>​                        lua = ‘’’<br>​                            function main(splash)<br>​                                return ‘hello’<br>​                            end<br>​                         ‘’’<br>​                        url = ‘<a href="http://localhost:8050/execute?lua_source=&#39;" target="_blank" rel="noopener">http://localhost:8050/execute?lua_source=&#39;</a> + quote(lua)<br>​                        response = requests.get(url)<br>​                        print(response.text)</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Selenium%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Selenium%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<p>动态HTML相关技术<br> JavaScript<br>JavaScript 是网络上最常用也是支持者最多的客户端脚本语言。它可以收集 用户的跟踪数据,不需要重载页面直接提交表单，在页面嵌入多媒体文件，甚至运行网页游戏。<br>jQuery<br>jQuery 是一个十分常见的库,70% 最流行的网站(约 200 万)和约 30% 的其他网站(约 2 亿)都在使用。一个网站使用 jQuery 的特征,就是源代码里包含了 jQuery 入口。如果你在一个网站上看到了 jQuery，那么采集这个网站数据的时候要格外小心。jQuery 可 以动态地创建 HTML 内容,只有在 JavaScript 代码执行之后才会显示。如果你用传统的方 法采集页面内容,就只能获得 JavaScript 代码执行之前页面上的内容。<br>Ajax  / DHTML<br>我们与网站服务器通信的唯一方式，就是发出 HTTP 请求获取新页面。如果提交表单之后，或从服务器获取信息之后，网站的页面不需要重新刷新，那么你访问的网站就在用Ajax 技术。<br>Ajax 其实并不是一门语言,而是用来完成网络任务(可以认为 它与网络数据采集差不多)的一系列技术。Ajax 全称是 Asynchronous JavaScript and XML(异步 JavaScript 和 XML)，网站不需要使用单独的页面请求就可以和网络服务器进行交互 (收发信息)。<br>动态HTML技术的影响<br>动态技术的效果对爬虫的影响（如下情况我们最好使用selenium来解决）<br>动态生成HTML内容，在获取内容的时候，只能获取到js处理之前的内容（可能是js再次发起异步请求，或者是对内容传输过程加密，js解密显示）<br>动态的生成url地址，在请求的时候，不知道其js的处理逻辑，不能解析js处理后的url地址<br>动态的生成请求参数，在请求的时候 ，不知道其js的处理逻辑，不知道js处理后的请求参数<br>服务器端动态生成的验证码（请求同一验证码url地址，返回图片不同），这样无法通过请求url下载图片来获取当前请求的验证码<br>怎么解决<br>直接从 JavaScript 代码里采集内容（费时费力）<br>分析js代码的功能逻辑<br>用python来重新实现其功能，完成参数或者内容的解析<br>用 Python 的 第三方库运行 JavaScript，直接模拟浏览器操作，并获取element中的js渲染后的代码内容。<br>通过使用selenium库来实现直接在浏览器上运行的效果，可以获取浏览器中elements中的源码（js解析后的内容）<br>通过模拟浏览器进行请求，不需要我们构建请求的url地址和请求参数等内容<br>获取模拟登录后的cookie等，保持登录状态<br>Selenium的使用<br>所需工具介绍<br>selenium的介绍<br>Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏<br>使用selenium的场景<br>在模拟登录的时候，可以模拟获取到动态的url、请求参数、等进行登录，还可以获取到cookie<br>获取动态HTML页面内容，即请求页面中的内容是经过js修改过动态计算生成的内容<br>各种driver的下载<br>chromedriver官网网站<br><a href="https://sites.google.com/a/chromium.org/chromedriver" target="_blank" rel="noopener">https://sites.google.com/a/chromium.org/chromedriver</a><br>chromedriver下载地址<br><a href="https://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">https://chromedriver.storage.googleapis.com/index.html</a><br>geckodriver官方网址<br><a href="https://github.com/mozilla/geckodriver" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver</a><br>geckodriver下载地址<br><a href="https://github.com/mozilla/geckodriver/releases" target="_blank" rel="noopener">https://github.com/mozilla/geckodriver/releases</a><br>selenium官方文档<br><a href="http://selenium-python.readthedocs.io/api.html" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html</a><br>创建driver对象、并设置窗口大小<br>from selenium import webdriver<br>from selenium.webdriver import ActionChains<br>from selenium.webdriver.common.by import By<br>from selenium.webdriver.support.ui import WevDriverWait<br>from selenium.webdriver.support import export expected_conditions as EC<br>from selenium.webdriver.remote.command import Command<br>driver = webdriver.PhantomJS( )<br>driver.set_window_size(width, height)      设置浏览器的窗口大小，截图可以根据窗口大小来截取<br>driver.max_windiw( )       让窗口显示最大<br>加载网页<br>driver = webdriver.PhantomJS( )      加载浏览器driver<br>driver.get(“<a href="http://www.baidu.com/&quot;" target="_blank" rel="noopener">http://www.baidu.com/&quot;</a>)    请求url地址<br>寻找元素<br>driver.find_element_by_id()                                  通过id找到元素<br>driver.find_element_by_name()                           通过name属性寻找元素<br>driver.find_element_by_xpath()                            通过xpath来寻找元素<br>driver.find_element_by_link_text()                       通过文本内容寻找a标签<br>driver.find_element_by_partial_link_text()            通过文本内容包含内容寻找a标签<br>driver.find_element_by_tag_name()                     通过标签名获取元素<br>driver.find_element_by_class_name()                  通过类型获取元素<br>driver.find_element_by_css_selector()                  通过css选择器选择元素<br>备注：<br>find_element 和find_elements的区别：find_elements用来寻找多个元素, 返回一个和返回一个列表<br>by_css_selector的用法示例： #food span.dairy.aged<br>元素的交互<br>element.send_keys(“长城”)            在input元素value中输入内容<br>element.clear()                                  清空input标签中输入的内容<br>element.click( )                                  触发元素的事件<br>动作链的使用<br>source_ele = driver.find_element_by_css_selector(‘#draggable’)<br>target_ele = driver.find_element_by_css_selector(‘#droppable’)<br>actions = ActionChains(driver)         创建动作链<br>actions.drag_and_drop(source_ele, target_ele)         调用动作链的方法<br>actions.persorm()     执行动作链<br>指定原生Js代码<br>js_code = “window.scrollTo(0, document.body.scrollHeight)”    定义js代码<br>driver.execute_script(js_code)          指定js代码<br>获取节点元素的属性和文本值等<br>element.get_attribute(‘属性名’)     可以获取元素的属性<br>element.text                                    可以获取文本内容<br>element.id                                        获取元素的id属性<br>element.size                                     获取元素的长宽<br>element.size[‘width’]                    获取元素的宽<br>element.size[‘height’]                  获取元素的高<br>element.tag_name                       获取元素的标签名称<br>element.location                           获取元素的坐标<br>切换Frame<br>driver.switch_to.frame(‘frame_id’ )            切换到iframe标签页面中<br>driver.switch_to.parent_frame()                      切换到父frame中<br>driver.switch_to.default_content()             切换到默认的frame中<br>等待<br>隐式等待<br>等待规则<br>如果selenium没有在DOM中找到节点元素，将等待一段时间<br>时间到达设定的时候后, 再次查找元素，若找不到元素则抛出找不到节点的异常，默认时间是0<br>等待设置方法<br>driver = webdriver.Chrome()<br>driver.implicity_wait(10)     给driver设置了10秒的隐式等待时间<br>driver.get(‘<a href="https://www.baidu.com&#39;" target="_blank" rel="noopener">https://www.baidu.com&#39;</a>)<br>ele = driver.find_element_by_class_name(‘kw’)      寻找元素, 找不到元素的话会等待10秒钟, 若还找不到元素则抛出异常<br>显示等待<br>等待规则<br>规定了一个最长的等待时间，如果在规定的时间内加载出来了这个节点, 就返回找到节点<br>如果没有找点节点则抛出超时异常<br>等待设置方法<br>driver = webdriver.Chrome()<br>driver.get(‘<a href="https://www.baidu.com&#39;" target="_blank" rel="noopener">https://www.baidu.com&#39;</a>)<br>wait = WebDriverWait(driver, 10)        设置了10秒的最大显示等待时间<br>ele = wait.until(EC.presence_of_element_located((By.ID, ‘kw’)))        设置等待条件，并等待寻找到节点<br>ele_1 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ‘#bt’)))    设置等待条件，并等待寻找到节点<br>常用的等待条件<br>title_is             标题是某内容<br>title_contains             标题包含某内容<br>presence_of_element_located               节点加载出来<br>visibility_of_element_located                 节点课件, 传入定位元组<br>visibility_of                    节点可见，传入节点对象<br>presence_of_all_elements_located          所有节点加载出来<br>text_to_be_present_in_element             某个节点中包含某文字<br>text_to_be_present_in_element_value          某个节点值包含某文字<br>frame_to_be_available_and_switch_to_it      加载并切换<br>invisibility_of_element_located             节点不可见<br>element_to_be_clickable     节点可点击<br>element_to_be_seleted    节点可选择，传入节点对象<br>element_located_to_be_selected     节点可选择，传入定位元组<br>element_selection_state_to_be          传入节点对象以及状态，相等返回True, 否则返回false<br>element_located_selection_state_to_be    传入定位元组以及状态，相等返回True，否则返回False<br>alert_is_present    是否出现警告<br>其他的等待<br>driver.set_page_load_timeout(10)         设置页面加载的超时时间<br>driver.set_script_timeout(10)          设置执行script脚本的超时时间<br>鼠标的移动和点击<br>鼠标的移动<br>actions = ActionChains(driver)<br>actions.move_to_element(element).move_by_offset( lx , ly )<br>actions.perform()<br>element   移动到哪个元素里面<br>lx  相对元素左边界的距离<br>ly 相对元素上边界的距离<br>鼠标的点击<br>from selenium.webdriver.remote.command import Command<br>driver.execute(Command.MOUSE_DOWN, pamas)<br>第一个参数为时间类型<br>第二个参数为命令的附加命令，为字典类型<br>操作driver<br>页面请求参数相关<br>driver.page_source                获取浏览器中element中的内容（即页面的源码）<br>driver.current_url()                获取当前页面url地址<br>driver.back()                          退回上一个页面<br>driver.forward()                     返回到前面一个页面<br>driver.flush()                          刷新当前页面<br>driver.get_cookies()               获取所有的cookie，返回的是列表嵌套字典（存储的每个cookie信息）<br>driver.delete_cookie(“key”)    删除一条cookie<br>driver.delete_all_cookies()      删除所有的cookie内容<br>{cookie[‘name’]: cookie[‘value’] for cookie in driver.get_cookies( )}   将cookie构造请requests请求参数<br>driver.execute_script(‘window.open()’)     开启一个浏览器的选项卡<br>driver.switch_to_window(driver.window_handles[1])         切换到第二个选项卡窗口<br>窗口截图相关<br>from PIL import Image<br>driver.save_screenshot(“长城.png”)     截图浏览器，没有图形界面可以通过截图来观察界面<br>img = Image.open(io.BytesIO(driver.get_screenshot_as_png()))            获取截图并直接读取到内存<br>img_small = img.crop((x0, y0, x1, y1)).convert(‘L’)     对图片进行截图，四个参数代表(左、上、右、下)坐标，并将彩图转化为灰度图<br>img_small.save(‘–path–’)    将图片进行保存<br>width  =  img.size[0]    获取图片的宽<br>height  =  img.size[1]    获取图片的高<br>img1.load( )[ i, j ]       获取图片该像素点的rgb值<br>浏览器开关相关<br>driver.close()    关闭当前页面，当页面全部都关闭后，退出浏览器driver<br>driver.quit()      退出浏览器driver<br>selenium 的driver配置<br>代理的设置<br>phantomjs设置代理ip<br>browser=webdriver.PhantomJS(PATH_PHANTOMJS)<br>proxy=webdriver.Proxy()<br>proxy.proxy_type=ProxyType.MANUAL<br>proxy.http_proxy=’1.9.171.51:800’<br>proxy.add_to_capabilities(webdriver.DesiredCapabilities.PHANTOMJS)<br>browser.start_session(webdriver.DesiredCapabilities.PHANTOMJS)<br>browser.get(‘<a href="http://1212.ip138.com/ic.asp&#39;;" target="_blank" rel="noopener">http://1212.ip138.com/ic.asp&#39;;</a>)<br>Chrome设置代理ip</p><p>firefox设置代理ip<br>方法一</p><p>方法二</p><p>禁用插件和无头浏览器的设置<br>chrome配置方法<br>设置无头浏览器</p><p>设置禁用图片和javascript</p><p>firefox配置方法<br>firefox_options = webdriver.FirefoxOptions()<br>firefox_options.set_preference(u”permissions.default.stylesheet”, 2)   # 禁用css<br>firefox_options.set_preference(u”permissions.default.image”, 2)       # 禁用图片<br>firefox_options.set_preference(u”dom.ipc.plugins.enabled.libflashplayer.so”, “false”)       # 禁用flash插件<br>firefox_options.set_headless()     # 设置为无头浏览器<br>driver = webdriver.Firefox(firefox_options=firefox_options)<br>设置请求头的方法</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Request%E6%A8%A1%E5%9D%97%E4%BD%BF%E7%94%A8/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Request%E6%A8%A1%E5%9D%97%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>Request模块的介绍<br>request 库的特点<br>requests的底层实现就是urllib<br>requests在python2 和python3中通用，方法完全一样<br>requests简单易用<br>requests能够自动帮助我们解压(gzip压缩的等)网页内容<br>requests的作用<br>发送网络请求，返回响应数据<br>中文文档 API：<a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html" target="_blank" rel="noopener">http://docs.python-requests.org/zh_CN/latest/user/quickstart.html</a><br>request/response的常用方法和属性<br>requests.get()    get请求<br>使用方法：<br>response = requests.get(url, headers=headers， params=params)    返回响应对象<br>get方法的参数<br>url：为请求的url地址<br>headers：字典形式的命名参数，传递请求头。模拟浏览器获取想获取的数据，防止反扒检测；<br>示例：headers = {“User-Agent”: “Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36”}<br>params：字典形式命名参数，传递请求参数，获取想要获取的数据<br>示例：params = {‘wd’:’长城’}<br>request.post()     post请求<br>使用方法：<br>response = requests.post(url, headers=headers, data= data)<br>post方法的参数<br>url：为请求的url地址（不包含锚点）<br>headers：字典形式的命名参数，传递请求头。模拟浏览器获取想获取的数据，防止反扒检测；<br>示例：headers = {“User-Agent”: “Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36”}<br>data：<br>当时普通的post请求的时候，传递的参数是字典形式命名参数<br>Formdata参数示例：data = {‘wd’:’长城’}<br>当时ajax异步post请求的时候，传递的的字符串形式的参数<br>Payload参数示例：data = json.dumps({‘wd’: ‘长城’})<br>还要加上请求头 ： ‘Content-Type’: ‘application/json’<br>requests.utils   工具<br>requests.utils.qoute(url)     将url编码<br>requests.utils.unqoute(url)     将url进行解码<br>parsed = requests.utils.urlparse(url)     将url进行  协议、ip+端口、文件路径、参数等的拆分<br>parsed.scheme       获取协议名<br>parsed.netloc    获取ip+端口（或者是域名+端口）<br>parsed.path    获取路径<br>parsed.params     获取问号后面的参数<br>response 响应的常用属性和方法<br>response.text               获取响应内容html字符串<br>获取html字符串，结果是<code>str</code>类型<br>其编码方式，是requests根据响应头做出的有根据的推测，尝试使用这个编码方式来解码<br>response.encoding               规定解码的格式<br>可以在使用text属性获取响应字符串之前先规定编码格式，按照设定的编码格式进行解码<br>respones.content         获取响应内容的二进制(bytes)数据<br>通常我们在获取到内容的时候尝试对二进制数据进行解码，下面三种方法能够解决后续我们100%的编解码的需求<br>response.content.deocde()           获取二进制响应内容并解码成为utf-8编码格式的字符串<br>response.content.deocde(“gbk”)         获取二进制响应内容并解码成为gbk编码格式的字符串<br>response.text      在使用content解码失败的时候，可以尝试让request去根据推测进行解码<br>response.status_code       查看响应状态码<br>response.request.headers     查看请求头<br>response.headers      查看响应头<br>response.request.url     查看响应url<br>response.url       查看请求url<br>响应的url可能与请求url不同，比如304重定向，响应的url为重定向的url</p><p><strong><em>找到想要请求url和请求参数的方法</em></strong><br>寻找想要的url<br>第一种情况：表单提交请求，并且表单有action属性<br>通过form表单的action属性/a标签的href属性来找到对应的请求url<br>form表单提交时获取表单中所有的name、value键值对<br>构造成字典，通过requests函数的data参数进行请求<br>第二种情况：通过ajax异步发出的请求（非action表单提交请求）<br>查看抓包结果，存在我们想要最多的数据会是我们想要的响应，其url为主要的请求url<br>当有一些数据不存在该主要的请求中，那说明有js进行了其他异步请求获取了数据<br>我们可以先找到其他请求的响应，找到其url地址，其url地址一般有两种构建途径<br>1.一般的情况下和url相关的信息会存在主请求的响应中，我们可以通过在主响应中获取其他请求的请求url关键字，进行url的构建<br>2.有的时候url地址或参数是通过js动态生成的，这时候，我们需要去寻找对应的js文件来观察js是怎样生成的动态url和参数等（参考下面）<br>寻找想要的js文件（当不是通过form表单的action/a标签href属性提交请求的时候）<br>找到进行url请求的js文件<br>第一种方法<br>使用开发者选择工具，点击会触发请求js的元素<br>点击右边的event Listener，会获取到对应的点击事件<br>点击事件所在的js文件<br>第二种方法<br>先了解js文件中会出现的关键字，一般可以查找url中动态参数名称；<br>通过点击右上角的菜单search all file<br>查找关键字，找到后点击进入对应的js文件<br>会跳转到source界面，点击{}展开代码，显示文件全部的代码<br>找到想要找的事件函数，并在想要查看实现方法的函数前边加上断点<br>通过点击右边的调试工具，可以查看函数的具体执行步骤和实现方法<br>了解了实现的方法后，可以在爬虫请求的时候，实现同样的方法，然后生成参数和url地址，进行请求<br>使用代理获取响应<br>什么叫代理<br>在对网站爬取的时候，通过访问代理服务器，让代理服务器帮我们对目标服务器进行请求，然后通过代理服务器将响应返回给我们；<br>代理的作用<br>在需要大量的进行数据的爬取的时候<br>防止在同一时间，使用同一个ip地址对同一个服务器进行大量的访问，被反扒出来<br>在爬取网站的时候，通过代理服务器可以隐藏我们的真实ip地址，隐藏我们的身份，但是有的代理服务器不能隐匿我们的mac地址，如果想要隐匿我们的mac地址需要使用高匿代理服务器<br>代理服务器的原理</p><p>正向代理/反向代理<br>正向代理<br>在我们请求代理服务器的时候，我们知道我们的最终目标ip地址，比如我们使用代理服务器爬取百度服务器的内容<br>反向代理<br>我们在请求服务器的时候，不知道最终目标ip地址，比如我们在访问nginx反向代理的服务器的时候，我们只是在访问nginx服务器，我们并不知道nginx去访问哪一个ip地址<br>requests使用代理<br>使用方法：<br>requests.get(url, proxies = proxies)<br>需要参数<br>proxies：字典类型命名参数，指定代理服务器支持的协议，和代理服务器的请求地址<br>示例：proxies = {“协议”:”协议://ip:port”}，传递代理服务器支持协议类型（http/https），和代理服务器的ip和端口<br>备注：proxies字典参数最多只能接受两个键值对，一个键是http，一个键是https，定义在进行http/https请求的时候分别使用的代理服务器<br>代理服务器注意点：<br>http的url地址要使用http的代理，https的要使用https的代理<br>透明度低的代理能够被对方服务器找到我们的真实的ip，可能会导致代理的效果不明显<br>cookie与session的请求<br>cookie和session的区别<br>cookie存在浏览器本地，session在服务端<br>cookie不安全，session不会将数据暴露在客户端，比较安全<br>session占用性能，会加长请求的时间<br>cookie存储是有上限的，session没有<br>请求带上cookies的好处<br>能够请求登陆后的页面<br>带上cookie反反扒，用登录成功的cookie来进行伪造<br>伪造请求带上cookie的不好的地方<br>使用同一个cookie，不间断的访问同一个服务器的时候，可以被对方识别为爬虫<br>解决方法：使用多个用户名密码，多账号，随机选用账号进行服务器的访问，模拟多人访问<br>模拟登录cookie/session请求的方法<br>第一种<br>session请求登录接口<br>实例化一个session对象<br>session = requests.session()<br>使用session请求登录接口，session对象会将响应中的cookie进行保存<br>session.post(url, data=data, headers=headers)<br>也可以使用get请求<br>再使用session对象请求其他需要登录的url地址，会自动带上cookie进行访问<br>response =session.get(url, params=params, headers=headers)<br>session对象的作用<br>对响应的cookie进行保存，后面的访问会自动携带cookie<br>第二种<br>要获取了登录后的cookie请求字符串<br>headers中放cookie请求头字符串<br>第三种<br>把cookie的每一个name和value组成一个字典<br>将字典传给requests请求中的cookies参数接收<br>获取response中的cookie的方法<br>获取response中的cookie对象<br>response.cookies      获取respone的cookie对象<br>response.cookies只能获取服务器主动设置的cookie，不能获取我们手动创建的cookie<br>返回的数据类型是列表嵌套字典，每一个字典包含一个cookie的所有信息<br>将cookie对象转化为字典类型的方法<br>requests.utils.dict_from_cookiejar(response.cookies)<br>将python字典类型转化为cookie对象类型<br>requests.utils.cookiejar_from_dict( {‘key’: value } )<br>requests请求常见问题<br> SSl证书验证问题<br>问题产生原因<br>请求协议为https的网站需要向机构申请证书，这样用户才能直接通过https访问，但是有的网站（比如12306）的整数是自己研发的，这样浏览器不会在进行请求的时候，会产生一个证书异常，我们需要点击浏览器上的继续访问，来请求服务器，当我们使用爬虫来进行访问的时候，会直接报出异常<br>解决办法：<br>response = requests.get(“<a href="https://www.12306.cn/mormhweb/" target="_blank" rel="noopener">https://www.12306.cn/mormhweb/</a> “, verify=False)<br>在请求非机构证书的网站的时候，我们需要在请求方法中加上verify=False 的参数，就不会报异常<br>请求超时问题<br>问题产生原因<br>当我们进行请求的时候，我们可能因为在请求摸一个url的时候产生一些异常，导致长时间没有请求成功，由于请求一直在进行，导致后面的程序无法正常执行，验证影响程序的效率或者导致程序终止<br>解决办法<br>应用到的方法<br>from retrying import retry<br>@retry(stop_max_attempt_number=n)      装饰函数，表示函数如果报错将会再次执行，直到第n次如果依然报错，那将抛出异常<br> response = requests.get(url,timeout=10)     设置请求的超时时间，如果限定时间内没有请求成功，将会抛出异常<br>assert response.status_code == m      assert为断言关键字，如果后面的条件表达式为false将会排出异常<br>示例：</p><p>数据处理的技巧<br>字符串的格式化<br>“abc{}abc”.format()<br>不同于%号的格式化，{ }方式的格式化，可以接收任意类型的格式化<br>format  接收的参数与格式化{ }的数量相等<br>在通过字符串格式化构建url进行requests请求时候，尽量使用{ }来格式化，因为在浏览器会将url格式化，%有时候会按照特殊字符处理；<br> json数据与字符串之间的转化<br>import json<br>json.dumps( dict)    把python类型字典数据转化为json字符串<br>json.loads(‘json’ )    把json数据转化为python的字典类型<br>扁平化赋值表达式<br>name = “a” if lang==b else “c”<br>if后面的条件如果成立，那么就把if前的值赋给str<br>否则if否面的条件不成立，就把else后的值赋给str<br>name = a and ‘b’ or ‘c’<br>如果a为true，name等于b<br>如果a为false，name等于c<br>列表推导式和字典推导式<br>列表推倒式<br>[i for i in range(10) if i%2==0]<br>字典推导式<br>{i+1:i for i in range(10) if i%3==0}<br>注意：<br>在列表推导式中可以使用if判断，但是不能够使用else<br>tips<br>linux命令重命名<br>作用<br>可以将一段的linux命令，重名为其他比较短而且容易记忆的命令，方便我们的调用<br>设置方法<br>修改家目录下的.bashrc文件<br>添加  alias  新的命令=’原命令 -选项/参数’<br>保存退出   source .bashrc<br>已经可以使用新的命令了<br>将数据写入文件<br>文件存储内容的方法<br>with open(‘文件路径’, encoding=’utf-8’) as f:<br>f.write(content)<br>encoding 如果文件不存在，相当于我们创建一个文件进行写入，我们可以通过encoding来指定写入内容的编码格式，如果不指定，可能会报错</p><p>常见的反扒思路<br>尽量减少请求的次数<br>能抓列表页不抓详细页<br>保存html页面，有利于重复使用<br>多分析一个网站的不同类型页面<br>手机极速版页面<br>wap手机版页面<br>web网页<br>app抓包软件<br>进行请求伪装<br>多带一些请求头，有的时候请求头带的不同，服务器返回的结果不同<br>代理ip，设置代理ip池，定期更新代理ip池<br>在浏览器会根据cookie内容判断爬虫的时候，携带cookie（但是注意不要一直携带同一个cookie）<br>需要获取登录之后的数据的时候，要进行模拟登录，获取cookie后，在进行数据的获取<br>利用多线程/分布式（尽可能的快速抓取）<br>在可能的情况下尽可能的使用多线程和分布式爬虫</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Puppeteer%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/Puppeteer%E5%8A%A8%E6%80%81%E7%88%AC%E5%8F%96/</url>
      
        <content type="html"><![CDATA[<p>Puppeteer简介<br>Puppeteer介绍<br>Puppeteer 是一个node库，他提供了一组用来操纵Chrome的API, 通俗来说就是一个 headless chrome浏览器 (当然你也可以配置成有UI的，默认是没有的)。既然是浏览器，那么我们手工可以在浏览器上做的事情 Puppeteer 都能胜任, 另外，Puppeteer 翻译成中文是”木偶”意思，所以听名字就知道，操纵起来很方便，你可以很方便的操纵她去实现：<br>运行环境</p><ol><li>Nodejs 的版本不能低于 v7.6.0, 需要支持 async, await.</li><li><p>需要最新的 chrome driver, 这个你在通过 npm 安装 Puppeteer 的时候系统会自动下载的<br>npm install puppeteer<br>3.安装pyppeteer<br>pip install pyppeteer<br>Puppeteer的官方文档<br><a href="https://github.com/miyakogi/pyppeteer" target="_blank" rel="noopener">https://github.com/miyakogi/pyppeteer</a><br>知识前提<br>Puppeteer 几乎所有的操作都是 异步的, 为了使用大量的 then 使得代码的可读性降低，本文所有 demo 代码都是用 async, await 方式实现<br>由于文档上大量使用async和await关键字,  需要提前了解Async/Await 异步编程<br>Puppeteer的基本用法<br>python示例</p></li><li><p>先通过launch() 创建一个浏览器实例 Browser 对象</p></li><li>然后通过 Browser 对象创建页面 Page 对象</li><li>然后 page.goto() 跳转到指定的页面</li><li>调用 page.screenshot() 对页面进行截图</li><li>关闭浏览器<br>Puppeteer的常用属性/方法/对象<br>launch(options)方法<br>介绍<br>使用launch 方法会返回一个browser对象, 创建browser对象的时候可以选择性的配置如下选项<br>常用参数</li></ol><p>Browser对象<br>介绍<br>Browser对象相当于selenium的driver对象, 常用的有如下方法<br>当返回值是promise的时候, 在python中返回的是异步函数, 需要这样的函数需要使用await关键字调用<br>常用方法</p><p>browser.userAgent()    设置browser的请求头<br>Page对象<br>介绍<br>Page对象相当于selenium的每一个标签页<br>我们对页面的元素进行操作就是操作page对象<br>常用方法<br>常用方法<br>page.content()     获取页面的源码<br>page.cookies(…urls)      获取当前的cookies<br>page.goto()     请求某个页面<br>page.click(selector[, options])     点击元素<br>page.focus(selector)    聚焦某元素<br>page.close(options)     关闭当前标签页<br>page.url()   当前页面的url<br>page.reload(options)     重新加载页面<br>page.type(selector, text[, options])    向聚焦的元素框中输入内容<br>获取元素<br>Page.quirySelector()      选择一个元素<br>Page.querySelectorAll()     选择一组元素<br>Page.xpath()     通过xpath选择元素<br>evaluate()方法(运行js脚本/获取元素的属性)<br>evaluate接收字符串为参数， 字符串的内容可以使javascript的原生函数， 或者是表达式， 当字符串的内容是表达式的时候我们通常可以用来获取元素的属性， 当字符串的内容是js函数的时候通常可以用来在当前页面执行js代码<br>当字符串的内容是表达式的时候可以用force_expr属性来强调, 不然可能会识别失败报错<br>示例：</p><p>修改driver的配置</p><ol><li>Page.setViewport() 修改浏览器视窗大小</li><li>Page.setUserAgent() 设置浏览器的 UserAgent 信息</li><li>Page.emulateMedia() 更改页面的CSS媒体类型，用于进行模拟媒体仿真。 可选值为 “screen”, “print”, “null”, 如果设置为 null 则表示禁用媒体仿真。<br>键盘keyboard<br>keyboard.down(key[, options]) 触发 keydown 事件<br>keyboard.press(key[, options]) 按下某个键，key 表示键的名称，比如 ‘ArrowLeft’ 向左键，详细的键名映射请戳这里<br>keyboard.sendCharacter(char) 输入一个字符<br>keyboard.type(text, options) 输入一个字符串<br>keyboard.up(key) 触发 keyup 事件<br>鼠标mouse<br>mouse.click(x, y, [options]) 移动鼠标指针到指定的位置，然后按下鼠标，这个其实 mouse.move 和 mouse.down 或 mouse.up 的快捷操作<br>mouse.down([options]) 触发 mousedown 事件，options 可配置:<br>options.button 按下了哪个键，可选值为[left, right, middle], 默认是 left, 表示鼠标左键<br>options.clickCount 按下的次数，单击，双击或者其他次数<br>delay 按键延时时间<br>mouse.move(x, y, [options]) 移动鼠标到指定位置， options.steps 表示移动的步长<br>mouse.up([options]) 触发 mouseup 事件<br>waitFor()等待方法<br>page.waitFor(selectorOrFunctionOrTimeout[, options[, …args]]) 下面三个的综合 API<br>page.waitForFunction(pageFunction[, options[, …args]]) 等待 pageFunction 执行完成之后<br>page.waitForNavigation(options) 等待页面基本元素加载完之后，比如同步的 HTML, CSS, JS 等代码<br>page.waitForSelector(selector[, options]) 等待某个选择器的元素加载之后，这个元素可以是异步加载的，这个 API 非常有用，你懂的。<br>事件的监听<br>使用方法<br>Page.on(“event”, callback)<br>event: 事件名称<br>callback: 监听到事件后执行的回调函数<br>所有可以监听的事件</li></ol><p>官方文档介绍的Python使用Puppeteer的区别<br>Keyword arguments for options<br>Puppeteer uses object (dictionary in python) for passing options to functions/methods. Pyppeteer accepts both dictionary and keyword arguments for options.<br>Dictionary style option (similar to puppeteer):<br>browser = await launch({‘headless’: True})<br>Keyword argument style option (more pythonic, isn’t it?):<br>browser = await launch(headless=True)<br>Element selector method name ($ -&gt; querySelector)<br>In python, $ is not usable for method name. So pyppeteer usesPage.querySelector()/Page.querySelectorAll()/Page.xpath() instead of Page.$()/Page.$$()/Page.$x(). Pyppeteer also has shorthands for these methods, Page.J(), Page.JJ(), and Page.Jx().<br>Arguments of Page.evaluate() and Page.querySelectorEval()<br>Puppeteer’s version of evaluate() takes JavaScript raw function or string of JavaScript expression, but pyppeteer takes string of JavaScript. JavaScript strings can be function or expression. Pyppeteer tries to automatically detect the string is function or expression, but sometimes it fails. If expression string is treated as function and error is raised, add force_expr=True option, which force pyppeteer to treat the string as expression.<br>Example to get page content:<br>content = await page.evaluate(‘document.body.textContent’, force_expr=True)<br>Example to get element’s inner text:<br>element = await page.querySelector(‘h1’)<br>title = await page.evaluate(‘(element) =&gt; element.textContent’, element)<br>python实现puppeteer同步使用库<br>github  ：  <a href="https://github.com/issacLiuUp/puppeteer" target="_blank" rel="noopener">https://github.com/issacLiuUp/puppeteer</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%BF%AB%E9%80%9F%E5%86%99%E5%85%A5Csv%E6%96%87%E4%BB%B6/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%BF%AB%E9%80%9F%E5%86%99%E5%85%A5Csv%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>csv文件的写入<br>以列表的方式写入<br>import  csv      导入csv模块<br>with open(‘data.csv’, ‘w’, encoding=’utf-8’) as csvfile:<br>writer = csv.writer(csvfile, delimiter=’,’)    创建阅读器<br>writer.writerow([‘id’, ‘name’, ‘age’])    向csv文件中写入一行数据<br>writer.writerows([ [‘1’, ‘2’, ‘3’], [‘2’, ‘3’, ‘4’] ])     向csv文件中写入多行数据<br>以字典的方式写入<br>import  csv      导入csv模块<br>with open(‘data.csv’, ‘w’, encoding=’utf-8’) as csvfile:<br>fieldnames = [‘id’, ‘name’, ‘age’]<br>writer = csv.DictWriter(csvfile, fieldnames=fieldnames)    创建阅读器， 并指定列索引<br>writer.writeheader()      向文件中写入列的索引<br>writer.writerow({‘id’: 1, ‘name’: 2, ‘age’: 3})    向csv文件中写入一行数据<br>csv文件的读取<br>csv文件按行读取<br>import csv<br>with open(‘data.csv’, ‘r’, encoding=’utf-8’) as csvfile:<br>reader = csv.reader(csvfile)      创建阅读器<br>for row in reader:<br>print(row)</p><blockquote><blockquote><p>[‘id’, ‘name’, ‘age’]<br>使用pandas进行csv文件的写入和读取<br>读取csv文件数据的方法<br>df_obj = pd.read_csv(‘filename’, [usecols=[‘col1’, ‘col2’], skiprows=n1, skipfooter=n2, index_col=n3, engine=’python’])     返回的是DataFrame对象<br>filename：表示读取的文件名<br>usecols：表示读取文件的哪些列<br>skiprows:  表示跳过文件的前几行<br>skipfooter： 表示跳过文件的后几行<br>index_col：表示使用那一列作为索引列<br>engine： 使用的解释器引擎， 当读取中文文件的时候需要指定<br>写入csv数据的方法<br>df_obj.to_csv(“filename”)<br>filename：表示读取的文件名,要加上.csv后缀；</p></blockquote></blockquote>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%E6%96%B9%E6%B3%95-HashBloom/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D%E6%96%B9%E6%B3%95-HashBloom/</url>
      
        <content type="html"><![CDATA[<p>hash序列化过滤<br>使用hash哈希方法去重的场景<br>当数据量不大的时候，并且数据所占内存不多的时候<br>当只有几万条url去重的时候，可以直接使用hash+redis的set类型进行数据的去重<br>使用sha1-redis去重的实例<br>​        import hashlib<br>​        import redis<br>​        from redis import *</p><pre><code>class Filter_hash(object):    # 创建redis客户端对象    sr = StrictRedis(host=&apos;localhost&apos;, port=6379, db=0)    # 定义存储hash后数据的key_name         key = &apos;hash_list&apos;            def add_hash_num(self, key_name，url):            # 创建一个哈希对象            fp = hashlib.sha1()            # 对url进行哈希序列化            fp.update(url)            fp_num = fp.hexdiget()            # 将十六进制后的序列化的hash数值进行存储            added = self.server.sadd(self.key, fp_num)            return   added   # 如果插入成功， 返回1，表示数据不重复插入成功，否则返回0            filter_cur = Filter_hash()if __name__ == &apos;__main__&apos;:      filter_cur.add_hash_num(&apos;url&apos;， &apos;https://www.baidu.com&apos;)</code></pre><p>bloom-布隆过滤<br>什么是布隆过滤器<br>是一种space efficient的概率模型数据结构，用于判断一个元素是否在集合中。<br>一个空的布隆过滤器是一个m bit的bitmap，每一位都初始化为0。布隆过滤器定义有k个hash函数，对输入的数据生成k个hash值，定义一个map函数将k个hash值映射到bitmap的k个位。<br>bitmap数据类型<br>bitmap介绍<br>Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32长度的位数组。<br>位操作方法可以被分为两组：<br>一、对单一位的操作，比如设置某一位为1或0，或者得到这一位的值；<br>二、对一组位的操作，比方说计算一定范围内的1的个数（比如计数）<br>bitmap的应用场景<br>bitmap一个最大的优势是它通常能在存储信息的时候节省大量空间。比方说一个用增量ID来辨别用户的系统，可以用仅仅512MB的空间来标识40亿个用户是否想要接受通知。<br>使用SETBIT和GETBIT命令来对位进行置数和检索（redis中实现的bitmap类型数据的操作）</p><blockquote><p>setbit key 10 1<br>(integer) 1<br>getbit key 10<br>(integer) 1<br>getbit key 11<br>(integer) 0<br>返回的是该位上之前的数值<br>SETBIT 如上所示，意思是将第10位置位为1，第二个参数可为0或1。如果设置的位超出了当前String的长度，那么会自动增长。（最长2^32，下同）<br>GETBIT 如上所示，返回第10位和第11位的数据，分别是1和0。如果查找的位超出了当前String的长度，那么会返回0。<br>接下来是三个对一组位进行操作的命令:<br>BITOP 执行不同字符串之间的逐位操作。所提供的操作有AND，OR，XOR和NOT。BITCOUNT<br>BITCOUNT 计数,返回bitmap里值为1的位的个数.<br>BITPOS 返回第一个0或1的位置<br>BITPOS和BITCOUNT不仅可以作用于整个bitmap，还可以作用于一定的范围,下面是一个BITCOUNT的例子<br>布隆过滤的原理<br>布隆过滤器需要的是一个位数组(和位图类似)和K个映射函数(和Hash表类似)，在初始状态时，对于长度为m的位数组array，它的所有位被置0　</p></blockquote><p>对于有n个元素的集合S={S1,S2…Sn},通过k个映射函数{f1,f2,……fk}，将集合S中的每个元素Sj(1&lt;=j&lt;=n)映射为K个值{g1,g2…gk}，然后再将位数组array中相对应的array[g1],array[g2]……array[gk]置为1：　</p><p>如果要查找某个元素item是否在S中，则通过映射函数{f1,f2,…fk}得到k个值{g1,g2…gk}，然后再判断array[g1],array[g2]…array[gk]是否都为1，若全为1，则item在S中，否则item不在S中。这个就是布隆过滤器的实现原理。<br>布隆过滤优点<br>相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数。<br>布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。<br>布隆过滤器可以表示全集，其它任何数据结构都不能；<br>k 和 m 相同，使用同一组 Hash 函数的两个布隆过滤器的交并差运算可以使用位操作进行。<br>缺点<br>误算率（False Positive），随着存入的元素数量增加，错判“在集合内”的概率就越大了，误算率随之增加。<br>一般情况下不能从布隆过滤器中删除元素. 我们很容易想到把位列阵变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全的删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。<br>布隆过滤器的应用场景<br>在对大量的数据进行去重的时候， 可以使用布隆过滤器判断元素是否已经在集合中，通过判断的结果，来对数据进行相应的操作<br>布隆过滤实现（包在下方）<br>构造HashMap类<br>这里新建了一个HashMap类。构造函数传入两个值，一个是m位数组的位数，另一个是种子值seed。不同的散列函数需要有不同的seed，这样可以保证不同的散列函数的结果不会碰撞。<br>在hash()方法的实现中，value是要被处理的内容。这里遍历了value的每一位，并利用ord()方法取到每一位的ASCII码值，然后混淆seed进行迭代求和运算，最终得到一个数值。这个数值的结果就由value和seed唯一确定。<br>我们再将这个数值和m进行按位与运算，即可获取到m位数组的映射结果，这样就实现了一个由字符串和seed来确定的散列函数。<br>当m固定时，只要seed值相同，散列函数就是相同的，相同的value必然会映射到相同的位置。<br>所以如果想要构造几个不同的散列函数，只需要改变其seed就好了。以上内容便是一个简易的散列函数的实现。</p><p>构造BloomFilter<br>Bloom Filter里面需要用到k个散列函数，这里要对这几个散列函数指定相同的m值和不同的seed值<br>由于我们需要亿级别的数据的去重，即前文介绍的算法中的n为1亿以上，散列函数的个数k大约取10左右的量级</p><p>实现判断元素是否重复和添加元素到集合的方法<br>insert方法<br>Bloom Filter算法会逐个调用散列函数对放入集合中的元素进行运算，得到在m位位数组中的映射位置，然后将位数组对应的位置置1。<br>这里代码中我们遍历了初始化好的散列函数，然后调用其hash()方法算出映射位置offset，再利用Redis的setbit()方法将该位置1。<br>exit方法<br>我们要实现判定是否重复的逻辑，方法参数value为待判断的元素。我们首先定义一个变量exist，遍历所有散列函数对value进行散列运算，得到映射位置，用getbit()方法取得该映射位置的结果，循环进行与运算。<br>这样只有每次getbit()得到的结果都为1时，最后的exist才为True，即代表value属于这个集合。如果其中只要有一次getbit()得到的结果为0，即m位数组中有对应的0位，那么最终的结果exist就为False，即代表value不属于这个集合。</p><p>布隆过滤源码包<br>bloomfilter.py<br>​            </p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/mitmproxy&amp;mitmdump%E6%89%8B%E6%9C%BA%E7%88%AC%E8%99%AB/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/mitmproxy&amp;mitmdump%E6%89%8B%E6%9C%BA%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>mitmproxy的介绍和安装配置<br>mitmproxy的作用<br>mitmproxy是一个支持HTTP和HTTPS的抓包程序，类似Fiddler、Charles的功能，只不过它通过控制台的形式操作。<br>mitmproxy还有两个关联组件，一个是mitmdump，它是mitmproxy的命令行接口，利用它可以对接Python脚本，实现监听后的处理；另一个是mitmweb，它是一个Web程序，通过它以清楚地观察到mitmproxy捕获的请求。<br>可以保存Http会话并进行分析<br>模拟客户端发起请求，模拟服务器端返回的响应<br>利用反向代理将流量发给指定的服务器<br>支持Mac和Linux上的透明代理<br>利用Python对Http请求和响应进行实时的处理<br>mitmproxy和mitmdump的区别<br>mitmproxy起到了代理服务的功能，手机和PC在同一个局域网内，可以将mitmproxy设置为手机的代理，这样数据都是通过mitmproxy妆发出去的，起到了中间人的作用<br>mtmdump可以实现将抓取到的请求和响应直接交给某个Python程序进行处理，比如提取和入库操作<br>mitmproxy相关链接<br>GitHub：<a href="https://github.com/mitmproxy/mitmproxy" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy</a><br>官方网站：<a href="https://mitmproxy.org" target="_blank" rel="noopener">https://mitmproxy.org</a><br>PyPI：<a href="https://pypi.python.org/pypi/mitmproxy" target="_blank" rel="noopener">https://pypi.python.org/pypi/mitmproxy</a><br>官方文档：<a href="http://docs.mitmproxy.org" target="_blank" rel="noopener">http://docs.mitmproxy.org</a><br>mitmdump脚本：<a href="http://docs.mitmproxy.org/en/stable/scripting/overview.html" target="_blank" rel="noopener">http://docs.mitmproxy.org/en/stable/scripting/overview.html</a><br>下载地址：<a href="https://github.com/mitmproxy/mitmproxy/releases" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy/releases</a><br>DockerHub：<a href="https://hub.docker.com/r/mitmproxy/mitmproxy" target="_blank" rel="noopener">https://hub.docker.com/r/mitmproxy/mitmproxy</a><br>mitmproxy的安装<br>linux下使用pip安装<br>pip3 install mitmproxy<br>这是最简单和通用的安装方式，执行完毕之后即可完成mitmproxy的安装，另外还附带安装了mitmdump和mitmweb这两个组件。<br>windows下安装<br>获取安装包，下载地址: <a href="https://github.com/mitmproxy/mitmproxy/releases/" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy/releases/</a><br>在Windows上不支持mitmproxy的控制台接口，但是可以使用mitmdump和mitmweb。<br>Linux下源码安装<br>下载源码包，下载地址: <a href="https://github.com/mitmproxy/mitmproxy/releases/" target="_blank" rel="noopener">https://github.com/mitmproxy/mitmproxy/releases/</a><br>它包含了最新版本的mitmproxy和内置的Python 3环境，以及最新的OpenSSL环境<br>tar -zxvf mitmproxy-2.0.2-linux.tar.gz<br>sudo mv mitmproxy mitmdump mitmweb /usr/bin<br>证书配置<br>证书配置的说明<br>对于mitmproxy来说，如果想要截获HTTPS请求，就需要设置证书。<br>mitmproxy在安装后会提供一套CA证书，只要客户端信任了mitmproxy提供的证书，就可以通过mitmproxy获取HTTPS请求的具体内容，否则mitmproxy是无法解析HTTPS请求的。<br>证书配置步骤<br>启动mitmdump<br>mitmdump<br>找到家目录.mitmproxy目录里面的CA证书<br>mitmproxy-ca.pem        PEM格式的证书私钥<br>mitmproxy-ca-cert.pem       PEM格式证书，适用于大多数非Windows平台<br>mitmproxy-ca-cert.p12        PKCS12格式的证书，适用于Windows平台<br>mitmproxy-ca-cert.cer           与mitmproxy-ca-cert.pem相同，只是改变了后缀，适用于部分Android平台<br>mitmproxy-dhparam.pem     PEM格式的秘钥文件，用于增强SSL安全性<br>在各平台上配置证书的过程<br>Windows平台<br>1.双击mitmproxy-ca.p12，会出现导入证书的页面向导<br>2.直接点击下一步，会出现密码设置的提示，这里不需要设置密码，直接点击下一步按钮即可<br>3.接下来选择证书的存储区域。这里点击第二选项，将所有的证书都放入下列存储，然后点击浏览按钮，选择证书的存储位置为收信人的根证书颁发急购，接着点击确定按钮，点击下一步<br>4.过程中出现安全警告直接点击是<br>Android<br>1.在Android手机上，需要将证书mitmproxy-ca-cert.pem文件发送到手机上，例如直接复制文件。<br>2.接下来，点击证书，便会出现一个提示窗口，这时输入证书的名称，然后点击“确定”按钮即可完成安装。<br>iOS<br>1.将mitmproxy-ca-cert.pem文件发送到iPhone上，推荐使用邮件方式发送，然后在iPhone上可以直接点击附件并识别安装。<br>点击“安装”按钮之后，会跳到安装描述文件的页面，点击“安装”按钮，此时会有警告提示，继续点击右上角的“安装”按钮，安装成功之后会有已安装的提示。<br>如果你的iOS版本是10.3及以上版本，还需要在“设置”→“通用”→“关于本机”→“证书信任设置”将mitmproxy的完全信任开关打开。<br>mitmproxy的使用<br>启动mitmproxy服务<br>mitmproxy    这样就会在8080端口上运行一个代理服务<br>将mitmproxy设置为手机端的代理<br>将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings<br>发送请求<br>在手机端进行网络请求，便可以在mitmproxy的界面上看到对应的请求<br>查看/处理请求和响应<br>查看请求的详情<br>光标移动到对应的请求位置，点击ENTER<br>在请求中查看Request/Response/Detail<br>将光标移动到对应的分栏上，点击Tab<br>重新编辑请求<br>1.在请求中，点击e键进行编辑<br>2.按照高亮的部分，选择想要编辑的内容(比如: q：修改请求方参数，m：修改请求方式)<br>进入修改页面后,可以直接对内容进行修改<br>点击a可以增加一行参数<br>修改完成后点击Esc退出修改，q返回上一级页面<br>3.敲击a进行修改的保存<br>4.对修改后的请求重新发起请求<br>mitdump的使用<br>编写请求和响应的处理脚本<br>日志输出<br>介绍<br>ctx模块提供了不同等级的log将会打印不一样的颜色<br>使用示例<br>from mitmproxy import ctx<br>def request(flow):<br>flow.request.headers[‘User-Agent’] = ‘MitmProxy’<br>ctx.log.info(“info”)<br>ctx.log.warn(“warn”)<br>ctx.log.error(“error”)<br>Request对象处理<br>介绍<br>我们可以通过request() 方法实现对请求进行修改<br>request对象包含的属性: url、headers、cookies、host、method、scheme、port<br>使用示例<br>def request(flow):<br>request = flow.request<br>print(request.url)<br>Response对象处理<br>介绍<br>可以通过response() 方法实现对响应的操作，比如入库等操作<br>response对象包含的属性: status_code、deaders、cookies、text<br>使用示例<br>def response(flow):<br>response = flow.response<br>print(response.text)<br>启动mitmproxy服务<br>mitmdump  [OPTIONS]<br>-w: 可以指定将接货的数据都保存到此文件中<br>-s: 可以指定scripts.py 脚本文件，用来处理请求和响应，它需要放在当前命令的执行目录下<br>将mitmdump设置为手机端的代理<br>将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings<br>发送请求<br>在手机端进行网络请求，便可以在mitmdump的日志中便可以看见对应的请求</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Fiddler%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Fiddler%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>使用前配置<br>安装证书(配置HTTPS网站的抓取证书)<br>点击工具栏tools<br>点击Telerlk FiddlerOptions<br>点击HTTPS选项<br>点击右上角的Actions–&gt;Trust Root Certificate<br>软件界面</p><p>手机app抓包方法<br>将Fiddler设置为代理<br>点击工具栏tools—-&gt;Connections<br>勾选 Allow remote computers to connect<br>端口号设置为8888（可以设置其他）<br>配置手机端的证书<br>配置方法见其他两篇介绍<br>配置移动端的官方文档：<a href="http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureForiOS" target="_blank" rel="noopener">http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureForiOS</a><br>使用手机访问网页</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Fiddler%E5%9C%A8ios%E4%B8%8A%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Fiddler%E5%9C%A8ios%E4%B8%8A%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>Capture Traffic from iOS Device<br>Configure Fiddler<br>Click Tools &gt; Fiddler Options &gt; Connections.</p><p>Click the checkbox by Allow remote computers to connect.</p><p>Allow remote computers to connect</p><p>Restart Fiddler.</p><p>Ensure your firewall allows incoming connections to the Fiddler process.</p><p>Hover over the Online indicator at the far right of the Fiddler toolbar to display the IP addresses assigned to Fiddler’s machine.</p><p>Online Tooltip</p><p>Verify client iOS device can reach Fiddler by navigating in the browser to <a href="http://FiddlerMachineIP:8888" target="_blank" rel="noopener">http://FiddlerMachineIP:8888</a>. This address should return the Fiddler Echo Service page.</p><p>For iPhone: Disable the 3g/4g connection.</p><p>Set the iOS Device Proxy<br>Tap Settings &gt; General &gt; Network &gt; Wi-Fi.</p><p>Tap the settings for the Wi-Fi network.</p><p>Tap the Manual option in the HTTP Proxy section.</p><p>In the Server box, type the IP address or hostname of your Fiddler instance.</p><p>In the Port box, type the port Fiddler is listening on (usually 8888).</p><p>Ensure the Authentication slider is set to Off.</p><p>iOS Proxy Settings</p><p>Decrypt HTTPS Traffic from iOS Devices<br>Download the Certificate Maker plugin for Fiddler.</p><p>Install the Certificate Maker plugin.</p><p>Restart Fiddler.</p><p>Configure the device where Fiddler is installed to trust Fiddler root certificate.</p><p>On the iOS device, go to <a href="http://ipv4.fiddler:8888/" target="_blank" rel="noopener">http://ipv4.fiddler:8888/</a> in a browser.</p><p>From the bottom of the Fiddler Echo Service webpage, download the FiddlerRoot certificate.</p><p>Download FiddlerRoot Certificate</p><p>Open the FiddlerRoot.cer file.</p><p>Tap the Install button.</p><p>Install Profile</p><p>Tap the Install button again.</p><p>Warning</p><p>Uninstall FiddlerRoot Certificate<br>If you decide to uninstall the root certificate:</p><p>Tap the Settings app.</p><p>Tap General.</p><p>Scroll to Profiles.</p><p>Tap the DO_NOT_TRUST_FiddlerRoot* profile.</p><p>Tap Remove.</p><p>是否将当前网页翻译成中文 网页翻译 中英对照 </p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Fiddler%E5%9C%A8Android%E4%B8%8A%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Fiddler%E5%9C%A8Android%E4%B8%8A%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>Fiddler for Android / Google Nexus 7</p><p>Configure Fiddler<br>​    1.<br>Click Tools &gt; Fiddler Options &gt; Connections.<br>​    2.<br>Ensure that the checkbox by Allow remote computers to connect is checked.<br>​    3.<br>If you check the box, restart Fiddler.<br>​    4.<br>Hover over the Online indicator at the far right of the Fiddler toolbar to display the IP address of the Fiddler server.</p><p>Configure Nexus Device<br>​    1.<br>Swipe down from the top of the screen and tap the Settings icon.<br>​    2.<br>Tap Wi-Fi.<br>​    3.<br>Tap and hold your current Wi-Fi network. Select Modify Network.</p><pre><code>4. </code></pre><p>Tap the Show advanced options box.</p><pre><code>5. </code></pre><p>Tap the Proxy settings dropdown and select Manual.</p><pre><code>6. </code></pre><p>Type the IP address and port (usually 8888) of the Fiddler server.</p><pre><code>7. </code></pre><p>Tap Save.</p><p>To verify this configuration, go to <a href="http://ipv4.fiddler:8888/" target="_blank" rel="noopener">http://ipv4.fiddler:8888/</a>. Chrome should display the Fiddler Echo Service webpage, and the traffic should appear in Fiddler.Disable the proxy<br>After using Fiddler, return to the Proxy Settings screen above and remove the proxy.Decrypt HTTPS<br>​    1.<br>On the Fiddler Echo Service Webpage, click the FiddlerRoot Certificate link.</p><pre><code>2. </code></pre><p>If the download doesn’t open automatically, swipe down from the top and tap the Settings icon.<br>​    3.<br>Tap Personal &gt; Security.<br>​    4.<br>Under Credential Storage, tap Install from storage.</p><pre><code>5. </code></pre><p>Tap the FiddlerRoot.cer file.<br>​    6.<br>(Optional) Type a name for the certificate.</p><p>To verify this configuration, tap Trusted credentials &gt; User. This should display the Fiddler certificate.Disable HTTPS Decryption<br>To delete the FiddlerRoot certificate, tap Trusted credentials &gt; User and delete the certificate.</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Crontab%E5%AE%9A%E6%97%B6%E6%89%A7%E8%A1%8C%E7%88%AC%E8%99%AB/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Crontab%E5%AE%9A%E6%97%B6%E6%89%A7%E8%A1%8C%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>Crontab的使用方法<br>安装cron软件<br>apt-get  install  cron<br>编辑crontab定时执行命令<br>进入crontab编辑界面<br>crontab  -e    进入编辑界面<br>crontab  -l     查看当前的定时任务<br>crontab -r    删除任务<br>编辑需要被定时执行的命令<br>编辑的格式<br>分(0-59)   小时(0-23)  日(1-31)   月(1-12)  星期(0-6)  命令(command)<br>示例<br>30  7  8  <em>  </em>  ls    指定每月的8日的7:30执行ls命令<br><em>/15  </em>  <em>  </em>  <em>  ls      每15分钟执行一次ls命令<br>0  </em>/2  <em>  </em>  *  ls     每隔两个小时执行一次ls<br>注意点</p><ul><li>/num 代表每隔多长时间的意思<br>当一个位置使用每隔符号的时候，其前边的时间位置，不能为<em><br>星期中0表示周日<br>使用Crontab定时爬虫<br>编辑python_spider命令<br>先把python的执行命令写入.sh脚本<br>#！/bin/sh     将脚本定义在可执行脚本目录中<br>cd <code>dirname</code> $0 || exit 1    cd到.sh文件所在目录(项目目录)，失败则退出，dirname两边的不是引号<br>所以我们要将.sh文件定义在项目目录中，执行的时候，会自动的执行cd命令<br>topython3    切换到python3的环境<br>scrapy crawl spider_name &gt;&gt; run.log 2&gt;&amp;1<br>将终端显示的内容重定向到log日志中，不会在终端显示错误、异常（2&gt;&amp;1，表示会将错误和异常也保存到日志中  ）<br>给.sh文件添加可执行权限<br>chmod  +  run.sh<br>在crontab中编辑脚本文件执行时间（注意些绝对路径）<br>0  6   </em>  <em>  </em>  /home/ubuntu/<strong><em>/myspider.sh &gt;&gt; /home/ubuntu/</em></strong> run_crontab.log 2&gt;&amp;1<br>将终端显示的内容重定向到log日志中，不会保存错误、异常（2&gt;&amp;1，表示会将错误和异常也保存到日志中  ）<br>动态查看log日志<br>tail  -f  log_name.log</li></ul>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Charles%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Charles%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>Charles介绍<br>Charles的作用<br>Charles是一个网络抓包工具，相比Fiddler，其功能更为强大，而且跨平台支持得更好，<br>可以选用它来作为主要的移动端抓包工具。<br>相关链接<br>官方网站：<a href="https://www.charlesproxy.com" target="_blank" rel="noopener">https://www.charlesproxy.com</a><br>下载链接：<a href="https://www.charlesproxy.com/download" target="_blank" rel="noopener">https://www.charlesproxy.com/download</a><br>Charles的配置<br>HTTPS证书的配置<br>Windows系统的证书配置<br>1.首先打开Charles，点击Help→SSL Proxying→Install Charles Root Certificate，即可进入证书的安装页面。<br>2.接下来，会弹出一个安装证书的页面，点击“安装证书”按钮，就会打开证书导入向导。<br>3.直接点击“下一步”按钮，此时需要选择证书的存储区域，点击第二个选项“将所有的证书放入下列存储”，然后点击“浏览”按钮，从中选择证书存储位置为“受信任的根证书颁发机构”，再点击“确定”按钮，然后点击“下一步”按钮。<br>4.再继续点击“下一步”按钮完成导入。<br>Mac系统的证书配置<br>1.如果你的PC是Mac系统，可以按照下面的操作进行证书配置。<br>2.同样是点击Help→SSL Proxying→Install Charles Root Certificate，即可进入证书的安装页面。<br>3.接下来，找到Charles的证书并双击，将“信任”设置为“始终信任”即可，如图1-48所示。</p><p>代理配置<br>具体操作是点击Proxy→Proxy Settings，打开代理设置页面，确保当前的HTTP代理是开启的，如图1-49所示。这里的代理端口为8888，也可以自行修改。</p><p>将手机连接到Charles并安装证书<br>ios系统<br>1.将手机和电脑连在同一个局域网下，可以将PC设置为热点，手机连接其热点<br>2.将PC的ip设置为手机的代理ip，Settings &gt; General &gt; Network &gt; Wi-Fi.</p><p>3.设置完毕后，电脑上会出现一个提示窗口，询问是否信任此设备，此时点击Allow按钮即可。这样手机就和PC连在同一个局域网内了，而且设置了Charles的代理，即Charles可以抓取到流经App的数据包了。<br>4.安装Charles的HTTPS证书，在电脑上打开Help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser</p><p>5.在手机浏览器中打开chls.pro/ssl下载证书，点击“设置”→“通用”→“关于本机”→“证书信任设置”中将证书的完全信任开关打开。<br> Android系统<br>1.将手机和电脑连在同一个局域网下，可以将PC设置为热点，手机连接其热点<br>2.将PC的ip设置为手机的代理ip，Settings &gt; Wi-Fi &gt; hold current Wi-Fi network &gt; Modify Network &gt; Show advanced options &gt; Proxy settings<br>3.设置完毕后，电脑上就会出现一个提示窗口，询问是否信任此设备, 此时直接点击Allow按钮即可。<br>4.安装Charles的HTTPS证书，在电脑上打开Help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser</p><ol start="5"><li>在手机浏览器上打开chls.pro/ssl，这时会出现一个提示框，为证书添加一个名称，然后点击“确定”按钮即可完成证书的安装。</li></ol>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Appium%E6%89%8B%E6%9C%BA%E7%88%AC%E8%99%AB/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD/Appium%E6%89%8B%E6%9C%BA%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<p>Appium的介绍和安装<br>Appium的简介<br>Appium是移动端的自动化测试工具，类似于前面所说的Selenium，利用它可以驱动Android、iOS等设备完成自动化测试，比如模拟点击、滑动、输入等操作<br>Appium负责驱动移动端来完成一系列操作，对于iOS设备来说，它使用苹果的UIAutomation来实现驱动；对于Android来说，它使用UIAutomator和Selendroid来实现驱动。<br>同时Appium也相当于一个服务器，我们可以向它发送一些操作指令，它会根据不同的指令对移动设备进行驱动，以完成不同的动作。<br>Appium相关链接<br>GitHub：<a href="https://github.com/appium/appium" target="_blank" rel="noopener">https://github.com/appium/appium</a><br>官方网站：<a href="http://appium.io" target="_blank" rel="noopener">http://appium.io</a><br>官方文档：<a href="http://appium.io/introduction.html" target="_blank" rel="noopener">http://appium.io/introduction.html</a><br>下载链接：<a href="https://github.com/appium/appium-desktop/releases" target="_blank" rel="noopener">https://github.com/appium/appium-desktop/releases</a><br>Python Client：<a href="https://github.com/appium/python-client" target="_blank" rel="noopener">https://github.com/appium/python-client</a><br>Appium Api：<a href="https://testerhome.comm/topics/3711" target="_blank" rel="noopener">https://testerhome.comm/topics/3711</a><br>Appium的安装<br>通过Appium Desktop安装<br>Appium Desktop介绍<br>Appium Desktop支持全平台的安装，我们直接从GitHub的Releases里面安装即可<br>链接为<a href="https://github.com/appium/appium-desktop/releases。目前的最新版本" target="_blank" rel="noopener">https://github.com/appium/appium-desktop/releases。目前的最新版本</a><br>各平台的安装方法<br>下载exe安装包appium-desktop-Setup-1.1.0.exe<br>Mac平台可以下载dmg安装包如appium-desktop-1.1.0.dmg<br>Linux平台可以选择下载源码<br>通过Node.js安装<br>1.首先安装node.js<br>2.npm install -g appium<br>python-api安装<br>pip install Appium-Python-Client<br>开发环境的配置<br>Android开发环境配置<br>下载和配置Android Studio，下载地址为<a href="https://developer.android.com/studio/index.html?hl=zh-cn。下载后直接安装即可。" target="_blank" rel="noopener">https://developer.android.com/studio/index.html?hl=zh-cn。下载后直接安装即可。</a><br>打开Android Studio，直接打开首选项里面的Android SDK设置页面，勾选要安装的SDK版本，点击OK按钮即可下载和安装勾选的SDK版本。<br>添加环境变量，添加ANDROID_HOME为Android SDK所在路径，然后再添加SDK文件夹下的tools和platform-tools文件夹到PATH中。<br>更详细的配置可以参考Android Studio的官方文档：<a href="https://developer.android.com/studio/intro/index.html。" target="_blank" rel="noopener">https://developer.android.com/studio/intro/index.html。</a><br>Appium的使用(有界面版示例)<br>启动Appium服务<br>点击Appium<br>输入主机和端口号，点击Start Server按钮即可启动Appium服务<br>将手机连接到PC端<br>将手机通过数据线和PC相连<br>打开USB调试功能，确保PC可以连接到手机<br>可以在PC端测试是否连接成功<br>adb  devices  -l   查看连接的设备列表<br>如果找不到adb命令，请检查Android开发环境和环境变量是否配置成功<br>如果adb命令不显示设备信息，检查手机和PC的连接情况<br>启动App并操作<br>通过Appium内置驱动来操作<br>打开app<br>点击Appium中的Start New Session<br>配置启动App时的参数<br>platformName: 它是平台名称，需要区分Android或iOs，此处填Android<br>deviceName: 它是设备名称， 此处是手机的具体类型<br>appPackage： 它是App程序的包名<br>AppActivity: 它是入口Activity名，这里通常需要.开头<br>点击保存按钮，可以将此配置进行保存<br>点击Start Session，便会启动Android手机上的App，同时PC上会弹出一个调试窗口，里面包含了页面源码<br>操作应用App<br>选择元素<br>和浏览器的调试一样，只要点击作业页面中的元素，对应的元素就会高亮，中间栏会显示对应的源码<br>右侧是和元素包含的属性和事件等信息<br>操作元素<br>点击右侧元素的属性，便可以实现对元素的操作<br>操作的录制<br>点击中间栏的眼睛按钮，Appium会开始录制操作的动作，这时我们可以在窗口中操作APp的行为都会记录下来<br>Recorder处可以自动生成对应的语言代码。</p><p>通过Python代码来操作<br>打开app(初始化应用)<br>打开方法<br>用字典来配置Desired Capabilities 参数，新建一个Session<br>示例代码<br>server = “<a href="http://localhost:4723/wd/hub&quot;" target="_blank" rel="noopener">http://localhost:4723/wd/hub&quot;</a><br>desired_caps = {<br>“platformName”: “Android”,<br>“deviceName”: “MI_NOTE_Pro”,<br>“appPackage”: “com.tencent.mm”,<br>“appActivity”: “.ui.LauncherUI”<br>}<br>from appium import Webdriver<br>from selenium.webdriver.support.ui import WebDriverWait<br>driver = wevdriver.Remote(server, desired_caps)    # 启动app<br>查找元素<br>element = driver.find_element_by_android_uiautomator(‘new UiSelector().description(“Animation”)’)<br>elements = driver.find_elements_by_android_uiautomator(‘new UiSelector().description(“Animation”)’)<br> 点击操作<br>多点点击方法<br>driver.tap(self, positions, duration=None)<br>positions: 它是点击的位置组成的列表<br>duration: 它是点击持续的时间<br>通过tap方法实现触摸点击<br>该方法可以模拟手指点击，最多五个手指，也可以设置触摸的时间长短<br>单点点击方法<br>element.click()    对元素进行点击<br>示例代码<br>driver.tap([(100, 20), (100, 60), (100, 100)], 500)<br>屏幕拖动<br>元素间拖动方法<br>driver.scroll(self, origin_el, destination_el)<br>original_el: 拖动的起始元素<br>destination_el:拖动的终止元素<br>两点间的拖动方法<br>driver.swipe(self, start_x, start_y, end_x, end_y, duration=None)<br>start_x: 它是开始位置的横坐标<br>start_y: 它是开始位置的纵坐标<br>end_x: 它是终止位置的横坐标<br>end_y: 它是终止位置的纵坐标<br>duration: 它是持续时间，单位是毫秒<br>两点间的快速的滑动<br>driver.swipe()<br>start_x: 它是开始位置的横坐标<br>start_y: 它是开始位置的纵坐标<br>end_x: 它是终止位置的横坐标<br>end_y: 它是终止位置的纵坐标<br>示例代码<br>driver.scroll(el1, el2)<br>driver.swipe(100, 100, 100, 400, 5000)<br>driver.flick(100, 100, 100, 400)<br>元素的拖拽<br>拖拽方法<br>driver.drag_and_drop(self, origin_el, destination_el)<br>origin_el: 被拖拽的元素<br>destination_el：拖拽的目标元素<br>示例代码<br>driver.drag_and_drop(el1, el2)<br>文本的输入<br>输入的方法<br>ele.set_text(‘Hello’)<br>示例代码<br>element = driver.find_element_by_id(‘name’)<br>element.set_text(“Hello”)<br>动作链<br>使用动作链的方法<br>from appium import TouchAction()<br>action = TouchAction()<br>action.tap(element).perfoem()<br>常用的动作链还有: tap、press、long_press、release、move_to、wait、cancel<br>示例代码<br>from appium import TouchAction()<br>action = TouchAction()<br>a_ele = self.driver.find_element_by_class_name(“listView”)<br>action.press(a_ele).move_to(x=10, y=0), move_to(x=10, y=-600).release()<br>action.perform<br>示例代码<br>Appium爬取微信<br>MomentsAppium.zip<br>Appium+ mitmdump 爬取京东<br>MitmAppiumJD.zip</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/%E7%A0%B4%E8%A7%A3%E8%AF%A1%E5%BC%82%E5%AD%97%E4%BD%93/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/%E7%A0%B4%E8%A7%A3%E8%AF%A1%E5%BC%82%E5%AD%97%E4%BD%93/</url>
      
        <content type="html"><![CDATA[<p>破解诡异字体</p><p>爬虫与诡异的字体-反爬与反反爬的奇技淫巧<br>从今天这篇文章开始，主要给大家过一些小众的反爬策略以及其中的利弊，所以一般来说都会有一个特定的示例，比如今天的反爬策略就来自反爬界大神之一的去哪儿（当然也是爬虫界的）手机版。<br>我们先来用chrome的手机模式开一下去哪儿的机票:</p><p>看着叫一个干净整洁，完全没毛病。但是！当我们掀开被子看源码的时候 震惊了：</p><p>不知道大家能不能看清图，我码字注解一下，显示的价格是560，而源码中的价格则是540！<br>我们这篇文章就来看看去哪儿手机版是怎么做到的（当然偷偷剧透下，下篇文章咱们主角还是去哪儿）<br>其实同学们只要智商在线，看到标题就知道，这是用字体实现，我们看右侧的css面板也可以看到，这个dom元素的字体是单独设置的。也就是去哪儿通过修改字体，让4显示成了6。<br>1.为什么字体可以反爬<br>感觉从这篇文章开始这个套路快进行不下去了。为什么字体可以反爬？ 首先这个迷惑性太大，很多马虎的工程师以为可以了，直接就爬下来，当然就是被老板骂的体无完肤了，对反反爬工程师的精神可以造成1万点伤害。同时呢，去哪儿这个字体是动态生成的字体，也就是说你也不知道下一次刷出来的4到底是几显示出来的。这个道行就比较高了。<br>2.怎么做好字体反爬？<br>这种反爬策略用在数字上确实天衣无缝，非常优雅。唯一值得提的就是一点，如何动态生成字体和页面来做好对应关系。其实这是一个工程性的问题，大部分编程语言都含有生成字体的库，如果对网站整体性能比较自信的话，完全可以每次请求都动态生成，当然这样确实会比较慢，比较推荐是通过定时任务，去更新一个字体池。每次有请求过来，从字体池中随机拿一个字体，换一个随机的名字（可以通过url rewrite来实现），并和现在的数字做一次映射，调整页面显示后整体输出，就可以在尽量不影响性能的情况下搞死反反爬。<br>当然使用字体也是有局限的，其中最大的问题莫过于@font-face的兼容性问题，所以去哪儿只在移动端采用这个反爬策略，可能也有兼容性的考虑吧，相比之下，去哪儿的pc端的反爬写法则丑陋很多。<br>3.遇到字体反爬怎么办？<br>好了，万一我就是要爬去哪儿机票了怎么办呢？其实字体反爬处理也并不复杂，就像前面说到的，大部分语言都有字体处理类库，而这种情况大概率来讲只有10个数字的字体，我们将字体解析后只要能找到对应关系，就简单了。这里如果是Java工程师的话，推荐用Apache.Fontbox，贴一段解析的代码，注意这段代码不保证现在可用，仅做参考：<br>int[] digits = null;<br>BASE64Decoder decoder = new BASE64Decoder();<br>TTFParser parser = new TTFParser();<br>try {<br>byte[] bytes = decoder.decodeBuffer(fontBase64);<br>InputStream inputStream = new ByteArrayInputStream(bytes);<br>TrueTypeFont ttf = parser.parse(inputStream);<br>if(ttf != null &amp;&amp; ttf.getCmap() != null &amp;&amp;<br>ttf.getCmap().getCmaps() != null &amp;&amp;<br>ttf.getCmap().getCmaps().length &gt; 0){<br>digits = new int[10];<br>CmapSubtable[] tables = ttf.getCmap().getCmaps();<br>CmapSubtable table = tables[0];// No matter what<br>for(int i =0;i&lt;10;i++){<br>digits[i] = table.getGlyphId(i+48)-1;<br>}<br>}<br>} catch (IOException e) {<br>digits = null;<br>}<br>return digits;</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/%E5%8F%82%E6%95%B0%E4%BC%AA%E9%80%A0/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/%E5%8F%82%E6%95%B0%E4%BC%AA%E9%80%A0/</url>
      
        <content type="html"><![CDATA[<p>参数伪造<br>携程反爬中的Eleven参数-反爬与反反爬的奇技淫巧<br>今天我们要聊点什么呢，之前说要聊去哪儿的，不过暂且咱们再放一放，先聊一聊去哪儿的干爹携程吧，上次我记得看了携程工程师霸气回应说懂爬虫的来去哪儿，懂反爬的来携程。我觉得特别棒，这种开放的心态和自信，正是一个开放的互联网环境所需要的。<br>所以今天这节课虽然咱们以携程为例，但是我们还是以学习的目的为主，因此我不会把完整的代码放出来，大家掌握思路，拿到渔网比直接copy代码有用的多。<br>上篇文章用邮箱加密给大家演示了爬虫中简单的JS对抗，今天这节课咱们就用携程的Eleven参数来演示下复杂的JS对抗。<br>对了，这个题图，主要是因为携程给他们这个反爬的JS起了一个名字叫oceanball-海洋球，不明觉厉啊。<br>好了，言归正传。做过携程酒店爬虫的朋友，估计都研究过这个eleven参数，这个参数到底是哪里的呢，我们先看下页面请求：</p><p>就是这样一个页面，打开一个酒店页面会发现实际的酒店房型列表是一个ajax请求，如下：<br><a href="http://hotels.ctrip.com/Domestic/tool/AjaxHote1RoomListForDetai1.aspx?psid=&amp;MasterHotelID=441351&amp;hotel=441351&amp;EDM=F&amp;roomId=&amp;IncludeRoom=&amp;city=2&amp;showspothotel=T&amp;supplier=&amp;IsDecoupleSpotHotelAndGroup=F&amp;contrast=0&amp;brand=0&amp;startDate=2017-08-28&amp;depDate=2017-08-29&amp;IsFlash=F&amp;RequestTravelMoney=F&amp;hsids=&amp;IsJustConfirm=&amp;contyped=0&amp;priceInfo=-1&amp;equip=&amp;filter=&amp;productcode=&amp;couponList=&amp;abForHuaZhu=&amp;defaultLoad=T&amp;TmFromList=F&amp;eleven=c4350e460862b69d9d76724e1325a0a54ef23c2e0648636c855a329418018a85&amp;callback=CASuBCgrghIfIUqemNE&amp;_=1503884369495" target="_blank" rel="noopener">http://hotels.ctrip.com/Domestic/tool/AjaxHote1RoomListForDetai1.aspx?psid=&amp;MasterHotelID=441351&amp;hotel=441351&amp;EDM=F&amp;roomId=&amp;IncludeRoom=&amp;city=2&amp;showspothotel=T&amp;supplier=&amp;IsDecoupleSpotHotelAndGroup=F&amp;contrast=0&amp;brand=0&amp;startDate=2017-08-28&amp;depDate=2017-08-29&amp;IsFlash=F&amp;RequestTravelMoney=F&amp;hsids=&amp;IsJustConfirm=&amp;contyped=0&amp;priceInfo=-1&amp;equip=&amp;filter=&amp;productcode=&amp;couponList=&amp;abForHuaZhu=&amp;defaultLoad=T&amp;TmFromList=F&amp;eleven=c4350e460862b69d9d76724e1325a0a54ef23c2e0648636c855a329418018a85&amp;callback=CASuBCgrghIfIUqemNE&amp;_=1503884369495</a><br>前面咱说过，出于对反爬工程师工作的尊重，我们今天的文章不去完整介绍整个携程爬虫的做法，其实除了这eleven参数，携程还是在代码了下了不少毒的。<br>一般来说，一个ajax请求，如果没有做cookie限制的话，最难的问题就是能把所有的参数拼接完整，这个请求中最难的就是这个eleven参数的获取，那咱们遇到这种问题，应该具体如何处理，同时对于反爬工作来说，又有什么可以借鉴的地方呢。<br>一、反反爬中遇到复杂JS请求的处理流程<br>凡是题目总有一个解题思路，反反爬也不例外，解题思路很重要。不然就像没头苍蝇一样到处乱撞，今天这篇文章最重要的就是说说这个解题思路。<br>1.查看发起这个请求的JS来源<br>首先，Ajax请求大多都是由JS发起的（今天我们不讨论flash或者其他情况，不常见），我们使用Chrome工具，将鼠标移动到这个请求的Initiator这一栏上，就可以看到完整的调用栈，非常清晰。</p><p>2.确认核心JS文件<br>可以看到调用栈中有非常多的JS，那具体我们要分析哪个呢？一般来说我们可以首先排除掉VM开头的和常见库如jQuery这类，当然也不绝对，有些JS也会把自己注册到VM中去，这个另说，携程这里的情况挺明显，一眼就看到了那个不寻常的oceanball，看着就不是一般人，就他了。<br>3.分析JS文件<br>点击这个oceanball咱们就可以看到完整了源码了，下面也是最难的一步，分析JS文件。上一篇文章已经简单说了一些，首先，可读性非常重要，咱们复制完整的JS，贴进Snippet中，咱们大概看下，文件太大，没办法贴到文章中来：<br>一看就是在代码里面下毒，携程反爬工程师不简单啊。</p><p>不过呢，咱们看到这种很多数字的其实不要害怕，特别是在看到eval函数和String.fromCharCode，因为这就是把代码加密了一下，这种加密就跟没加密一样，咱们把eval函数直接换成console.log函数，运行一下这个Snippet。在控制台就可以看到原始代码。解密后如下：</p><p>看来这个是毒中毒啊，解密后的代码虽然比刚刚的可读性大为增加，但是依然可读性不强，不过根据经验，这样的文件已经没有一个特别简单的方案可以一举解密，只能逐行分析加部分替换了。当然直接Debug也是一个最省力的方案。</p><p>通过Debug调试，我们可以看到很多变量的中间值，这中间最重要的莫过于这最后一行，我们可以看到Eleven参数这个时候已经算出来了，而向CASttwHNetheyMWqSJ这个函数中传入一串实现，而这个实现正式返回eleven的值（这段代码是要把人绕死的节奏吗），所以我们只需要构造一个假的CASttwHNetheyMWqSJ来接收这段实现即可，而CASttwHNetheyMWqSJ这个函数名也是通过url传入的，好了，基本上大体的分析搞定了，下面就是构造这个代码的运行环境了。<br>4.构造运行环境<br>重要的话要重复很多遍：写爬虫最好的语言就是JS，因为JS对抗是爬虫中最难的部分，而一个合适的JS环境则可以事半功倍。全球最好的JS爬虫框架-在线网络爬虫/大数据分析/机器学习开发平台-神箭手云。<br>有了JS运行环境，我们其实只需要构造一个函数接受Eleven参数，请求oceanball，并直接运行他就可以了。我们看下我们构造的CASttwHNetheyMWqSJ函数：<br>var callbackFunc = randomCallback(15); var getEleven; eval(“var “+callbackFunc+” = function(a){getEleven = a}”);<br>这里对于初学者可能还是有点绕，简单说明下，首先我们随机了一个函数名，然后我们定义了一个空函数来接受oceanball中的返回eleven参数的函数实现，然后我把这个函数名定义为接受new Function的实现，这样我们后面就可以用getEleven来直接获取eleven参数了。<br>这里还得说下普通JS环境的缺陷，由于在普通JS环境中（非浏览器中）缺少一些重要的内置变量，如window，document等等，导致很多JS是运行不了的，这里我们补上这些变量：<br>var Image = function(){}; var window = {}; window.document = {}; var document = window.document; window.navigator = {“appCodeName”:”Mozilla”, “appName”:”Netscape”, “language”:”zh-CN”, “platform”:”Win”}; window.navigator.userAgent = site.getUserAgent(); var navigator = window.navigator; window.location = {}; window.location.href = “<a href="http://hotels.ctrip.com/hotel/&quot;+hotel_id+&quot;.html&quot;" target="_blank" rel="noopener">http://hotels.ctrip.com/hotel/&quot;+hotel_id+&quot;.html&quot;</a>; var location = window.location;<br>JS如果比较熟悉的话，应该能看出来这里有一个变量不常规，就是第一行的Image变量。这又是携程下的毒啊，我们回到刚刚oceanball的代码看下：</p><p>代码中有一句尝试new一个Image对象，如果失败了，则把某一个参数+1，而这个参数正是eleven参数中的某一位，也就是说eleven参数中有一位记录了JS运行环境是否支持new Image，我们这种伪造的浏览器环境当然不支持，所以我们得补上。<br>5.大功告成<br>好了，解了这么多毒，咱们终于可以获取到了eleven参数，当然这只是漫漫长征第一步，后面请求到的结果中，还有更深更辣的毒等着大家：</p><p>二、反爬中可以借鉴的地方<br>首先非常感谢携程反爬工程师给了一个教科书般的JS反爬案例，从这个例子我们也可以看出反爬中使用JS代码加密混淆的威力，基本上如果不是JS熟手，或者不熟悉解题思路的话，就是束手无策。JS混淆的方案有很多，大家可以到网上搜一搜，这里还是很推荐携程这种ajax配合回调+js二次混淆的方案，可以说极大提升了反反爬难度的同时也做到了基本不影响性能。但是这个方案依然对于直接渲染JS页面效果一般，还是建议配合之前文章提到的css:content的方案，这样处理之后那对于反反爬工程师的酸爽绝对够得上100桶统一老坛。</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/CssContent%E5%AD%97%E4%BD%93%E7%A0%B4%E8%A7%A3/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/CssContent%E5%AD%97%E4%BD%93%E7%A0%B4%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>Css:Content字体破解</p><p>爬虫与汽车之家的Css:Content-反爬与反反爬的奇技淫巧<br>发布于</p><p>话说感觉这个系列的我起名字越来越不走心了。越写越像哈利波特的起名套路了-爬虫与混血王子。（嗯，一点都不违和）我这个爱扯犊子的性格真是很难收敛啊。<br>话说Css自从越来越强大之后，被很多反爬工程师看上了。上篇文章介绍的字体就需要用到Css中的font-face和font-family。这节我们通过汽车之家来看看Css在反爬中另外一个妙用-content。<br>本来网页上显示的字无非两种，一种就是文本，一种就是显示在图片上的。所以之前有一种常规的反爬，就是把字当做图片替换来显示，目前百度还经常喜欢这么处理，比如百度知道，百度指数（参考天坑），当然不少的电话号码和邮箱这类重要信息，依然也再沿用这个方案。<br>然后随着css的content兼容性越来越好之后，就又有了一个性能更好的反爬，就是用css的content来代替原来文本中的文字，辅以合适的随机生成的方法，确实也不错。算是一个性能和反爬折中的方案吧，比如我们今天要提到的 汽车之家论坛 就是采用这样一个反爬方案。<br>一.为什么Css:Content能反爬。<br>事实上任何能让原来html中文本隐藏混淆加密又不影响正常用户显示的方式都可以来做反爬，比如我们上一篇文章提到的字体方式混淆，比如我们后面要提到的JS加密。而这篇文章主要是采用Css的content属性来隐藏。<br>我们简单的看下content属性使用，大家可以尝试复制这段写入一个网页的Css中<br>div:before {<br>content: “神箭手”;<br>}<br>我们会发现每一个div的开头都加入的神箭手的文字，而且这些文字还是无法复制的。再加上并不像生成图片那样会严重拖慢服务器性能，所以算是一个挺折中的方案。<br>二.如何好好利用Css:Content反爬<br>我们一起来看看汽车之家的反爬学习一下：<br>我们打开一个汽车之家论坛网页，然后直接Ctrl+A全选，就可以很明显的看到用Css的Content插入进的字符，我们掀开被子仔细研究下：</p><p>可以看到，汽车之家是将一些常用的字，包括&lt;了&gt;&lt;，&gt;&lt;的&gt;&lt;九&gt;等等变换成Css，这样做优点是可以提前写好css，缺点是由于是固定的库，很容易先把映射的关系解析好，然后直接全文替换。因此比较推荐的，当然还是每次的动态生成，不过这依然产生一个工程性的优化问题，这里还是比较推荐用池的方案，或者用客户的IP做一个hash映射也不错。<br>不过汽车之家做的比较好的在于很好的隐藏了这段固定映射的Css，同时Css中的class也每次会替换一些名称部分，同时又不是每次都把所有的映射库输出出来，而是节选文章中有的文字，因此既缩小了输出大小，又隐藏了整体库，看得出工程师也是打得一手好牌啊。<br>三.如何应对Css:Content这种类型的反爬<br>我们就以汽车之家为例吧：<br>1.获取映射的Css<br>其实汽车之家这个例子来说，主要的问题并不是如何解决这种反爬，因为这种模式的反爬如果知道映射，搞起来太简单，直接正则替换就行了。这里最难的是怎么找到这个映射，汽车之家可以说隐藏的是教科书级的完美，解析方案我就不展开讲了，因为这个是Javascript对抗里的内容（后面我可能会拿出几篇文章单独讲JS对抗），重点就是解析跟在正文后面的这一段JS。</p><p>好的工具等于成功的一半，想要很好的解析JS，一定要有一个好的JS解释引擎。这样可以省掉大量的破解的工作而直接运行别人的JS，在代码中整合脚本引擎相当麻烦，最完美的莫过于找一个好的JS的爬虫框架。<br>具体的Javascript对抗我在后面再讲，这里简单说下，直接运行这段代码，我们可以看到在Nl_（可变的）这个变量中存储了完整的Css映射，因此我们想办法在运行中保存该变量，带入下一步即可。</p><p>2.根据映射进行内容替换<br>我们既然拥有了Css映射，那还有啥难的，直接读取CssRules，循环用正则替换把<br><span class="hs_kw4_mainYM"></span><br>这类标签直接替换成content中的文字：<br>九<br>好了，大功告成。<br>贴一下部分神箭手中运行的解析代码：<br>var code = extract(page.raw, “//div[@class=’conttxt’]/div/script”); if(code){ eval(code); var cssMapping = {}; if(rules.length &gt; 0){ for(var ri in rules){ var rule = rules[ri]; var matches1 = /content:\s*”([^”]+)”/.exec(rule) var matches2 = /.([^:]+)::before/.exec(rule) if(matches1 &amp;&amp; matches2){ cssMapping[matches2[1]] = matches1[1]; } } page.raw = page.raw.replace(/&lt;span\s+class=’(hs_kw\d+_[^’]+)’&gt;&lt;\/span&gt;/g, function(match,p1,p2){ return cssMapping[p1]; }); } } page.raw = exclude(page.raw, “//div[@class=’conttxt’]/div/script”);<br>————————————————最后说两句————————————————————<br>大家会发现，很多非常奇特的反爬措施，大多依赖Javascript加密的功力。当然有些Javascript加密可以直接通过JS渲染，如[Email Protection]，但是也有很多单纯通过渲染JS无法解决，如今天这篇文章中提到，因此也让Css的这种反爬措施有别于普通的JS加密形式，给反反爬工程师设置了更高的障碍，是我个人比较推荐的一种反爬手段。反过来对于反反爬来说：在有了好的工具前提下，熟练掌握JS则是一个必备内功之一。<br>文章导航</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/CloudFlare%E9%82%AE%E7%AE%B1%E5%8A%A0%E5%AF%86(cfemail)/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%89%B9%E6%AE%8A%E5%8F%8D%E5%8F%8D%E7%88%AC/CloudFlare%E9%82%AE%E7%AE%B1%E5%8A%A0%E5%AF%86(cfemail)/</url>
      
        <content type="html"><![CDATA[<p>CloudFlare邮箱加密(cfemail)</p><p>爬虫与CloudFlare邮箱加密(cfemail)-反爬与反反爬的奇技淫巧<br>发布于</p><p>由于Javascript这块的内容很多，难度也不一样。所以这一篇文章希望给大家讲一些入门级别的。本来找来找去感觉都是高段位玩家，然而今天在做邮箱地址爬虫的过程中一段代码突然跳到了我的面前-邮箱地址加密JS。恰好难度适中，就今天拿出来跟大家一起聊一聊：</p><p>下面我们就进入正题，先来看看这个文章标题上的内容跟我们这篇文章到底有啥关系：<br>CloudFlare是一家美国的跨国科技企业，总部位于旧金山，在英国伦敦亦设有办事处。CloudFlare以向客户提供网站安全管理、性能优化及相关的技术支持为主要业务。通过基于反向代理的内容传递网络(ContentDeliveryNetwork,CDN)及分布式域名解析服务(DistributedDomainNameServer)，CloudFlare可以帮助受保护站点抵御包括拒绝服务攻击(DenialofService)在内的大多数网络攻击，确保该网站长期在线，同时提升网站的性能、访问速度以改善访客体验。<br>看完了好像还是没关系，别急，我们今天要讲的是CloudFlare中的一个功能，叫做Email Obfuscation，也就是邮箱混淆。<br>当我们使用了 CloudFlare 的服务，如果开启了 Email Obfuscation ，页面里真正的 Email 地址会被隐藏，具体隐藏的代码如下：<br><a class="__cf_email__" href="/cdn-cgi/l/email-protection" data-cfemail="a89b9b9c919d9b909a989ae8d9d986cbc7c5">[email&#160;protected]</a><script data-cfhash="f9e31" type="text/javascript">/<em> &lt;![CDATA[ </em>/!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName(‘script’),e=t.length;e–;)if(t[e].getAttribute(‘data-cfhash’))return t[e]}();if(t&amp;&amp;(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute(‘data-cfemail’)){for(e=’’,r=’0x’+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+=’%’+(‘0’+(‘0x’+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/<em> ]]&gt; </em>/</script><br>做过爬虫的朋友应该都对这段代码不陌生，非常常见的一个邮箱混淆服务。<br>一.为什么要使用邮箱混淆服务。<br>事实上很多站长可能并不知道自己使用了这样的服务，因为CloudFlare主要还是以CDN为主，这个服务是附加的，而且CloudFlare也做的很贴心，几乎不用什么设置，就可以自动混淆。（实际上是识别了输出的页面里是否有邮箱，有邮箱就替换成[email protected]并在下面添加一段JS代码）。<br>无论邮箱和电话号码，即使主动留在互联网上，也不希望被人批量获取，所以邮箱混淆一直是件很重要的事情。包括用at代替@，用#代替@，还有生成图片的。而CloudFlare提供了一种完全不需要修改代码的方案，确实给了大家很大的方便。<br>二.CloudFlare邮箱混淆服务的优劣<br>那么我们使用这种方案有什么优点和缺点呢？<br>我们先说说优点：首先不用改代码，很方便；其次全局替换，不会有遗漏；最后JS混淆，比at和#更加彻底也对显示影响最小，毕竟at和#用的人多了，就和@一样了，有时候还会让真正想联系的客户摸不着头脑。<br>我们再说说缺点：我在验证码那篇文章中就提到过的成本回报比概念，当很多人用一个方案的时候，由于回报无限增大导致无论这个方案有多么好，他被破解的概率都会大大增加。当然CloudFlare混淆远远不止这个问题这么简单。他的这个混淆有时候恰恰使得这个页面中的邮箱标志更加明显，有点类似本来你把金子放在地上，很危险。现在把金子埋在地下，然后为了自己能找到，又画了一个此处有金子的标记。事实上在真正识别邮箱的爬虫中，CloudFlare反而降低获取Email的难度。所以先提前说反爬的结论，严重不推荐大家使用这个方案！<br>三.写爬虫时遇到CloudFlare邮箱混淆，如何解密？<br>说完了反爬，再说反反爬。这个我就不得不提工具的重要性了。有时候你遇到一个好工具，那真的一身轻松。这里需要再次强调，写爬虫最好的语言是JS，因为JS对抗是爬虫中最难的部分，框架本身就是JS的环境将使得事半功倍：<br>世界上最好的JS爬虫开发框架- 在线网络爬虫/大数据分析/机器学习开发平台-神箭手云。<br>好了，今天的盒饭有着落了。当然我们这篇文章还是得给爬虫工程师来点干货的。<br>我们今天就通过这个简单的例子来看看写爬虫时遇到复杂的JS到底怎么分析。<br>1.格式化代码<br>JS分析最重要的是格式，因为大部分JS代码都是混淆且压缩的。基本是不具备任何可读性，先格式化成可读的形式最重要。格式化的工具很多，这里我最推荐Chrome浏览其中的Snippet。因为格式化完了之后还可以调试，简直是神器。<br>我们把前面例子中HTML代码部分删除掉，留下JS部分，贴进Snippet，点击左下角的{}按钮格式化：</p><p>2.分析代码<br>是不是看着舒服太多了，已经到了人眼能看的级别了。我们贴出来看看：<br>!function(t, e, r, n, c, a, p) { try { t = document.currentScript || function() { for (t = document.getElementsByTagName(‘script’), e = t.length; e–; ) if (t[e].getAttribute(‘data-cfhash’)) return t[e] }(); if (t &amp;&amp; (c = t.previousSibling)) { p = t.parentNode; if (a = c.getAttribute(‘data-cfemail’)) { for (e = ‘’, r = ‘0x’ + a.substr(0, 2) | 0, n = 2; a.length - n; n += 2) e += ‘%’ + (‘0’ + (‘0x’ + a.substr(n, 2) ^ r).toString(16)).slice(-2); p.replaceChild(document.createTextNode(decodeURIComponent(e)), c) } p.removeChild(t) } } catch (u) {} }()<br>我们先大概看下整个代码，首先外层就是一个函数的定义及直接调用。大部分的JS库都会采用这种方法，既可以保证代码模块之间变量不相互污染，又可以通过传入参数实现内外部变量的传输。<br>再读代码第一段：获取变量t，这个过程我们结合前面的HTML代码可以看出，这个是在获取Email被加密后的Dom元素，为了后面获取data-cfemail做准备。<br>最后看第二段：显然就是从Dom元素中获取data-cfemail并解密出真实的Email并替换到页面显示中去。<br>3.整合进爬虫<br>一般来说，对于复杂的JS，我们还会有断点调试和其他分析的过程，这里的JS很简单，所以咱直接开始写代码。我们怎么在像神箭手这样的JS爬虫框架中处理呢，同样也非常简单：<br>var cfemails = extractList(content, “//*[@data-cfemail]/@data-cfemail”); for(var c in cfemails){ var a = cfemails[c]; for (e = ‘’, r = ‘0x’ + a.substr(0, 2) | 0, n = 2; a.length - n; n += 2) e += ‘%’ + (‘0’ + (‘0x’ + a.substr(n, 2) ^ r).toString(16)).slice(-2); var emailDecoded = decodeURIComponent(e); console.log(emailDecoded); }<br>可以看到，我们先通过xpath直接获取所有的data-cfemail的值，然后直接把CloudFlare这段解密JS复制过来就行了。运行后就可以直接获取该页面所有被CloudFlare混淆过的邮箱，简直比直接用正则提取邮箱还要简单还要准确。<br>————————————————最后再说两句——————————————————–<br>通过这篇文章中这个非常常见却又相对入门的例子，我们一起看了下Javascript在反爬与反反爬中扮演的角色，同时也看到了一个JS爬虫框架的重要性，如果是其他爬虫框架，要么需要自己整合JS引擎，要么就得读懂整段解密代码再翻译成那种语言，这个难度在我们后面会提到的一些JS加密中，将不敢想象。同时我们也可以看到类似CloudFlare这类通用邮箱混淆的脆弱性。<br>文章导航</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97Celery/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97Celery/</url>
      
        <content type="html"><![CDATA[<p>分布式任务队列Celery</p><p>分布式任务队列Celery<br>Celery （芹菜）是基于Python开发的分布式任务队列。它支持使用任务队列的方式在分布的机器／进程／线程上执行任务调度。<br>结构<br>核心部件</p><p>broker<br>消息队列，由第三方消息中间件完成<br>常见有RabbitMQ, Redis, MongoDB等</p><p>worker<br>任务执行器<br>可以有多个worker进程<br>worker又可以起多个queue来并行消费消息</p><p>backend<br>后端存储，用于持久化任务执行结果<br>功能部件</p><p>beat<br>定时器，用于周期性调起任务</p><p>flower<br>web管理界面<br>任务<br>基本用法是在程序里引用celery，并将函数方法绑定到task</p><p>from celery import Celery</p><p>app = Celery(‘tasks’, backend=’amqp’, broker=’amqp://guest@localhost//‘)<br>app.conf.CELERY_RESULT_BACKEND = ‘db+sqlite:///results.sqlite’</p><p>@app.task<br>def add(x, y):<br>return x + y<br>然后调用相应方法即可(delay与apply_async都是异步调用)<br>from tasks import add<br>import time<br>result = add.delay(4,4)<br>while not result.ready():<br>print “not ready yet”<br>time.sleep(5)</p><p>print result.get()<br>由于是采用消息队列，因此任务提交之后，程序立刻返回一个任务ID。<br>之后可以通过该ID查询该任务的执行状态和结果。<br>关联任务<br>执行1个任务，完成后再执行第2个，第一个任务的结果做第二个任务的入参<br>add.apply_async((2, 2), link=add.s(16))<br>结果：2+2+16=20<br>还可以做错误处理<br>@app.task(bind=True)<br>def error_handler(self, uuid):<br>result = self.app.AsyncResult(uuid)<br>print(‘Task {0} raised exception: {1!r}\n{2!r}’.format(<br>uuid, result.result, result.traceback))</p><p>add.apply_async((2, 2), link_error=error_handler.s())<br>定时任务<br>让任务在指定的时间执行，与下文叙述的周期性任务是不同的。<br>ETA, 指定任务执行时间,注意时区<br>countdown, 倒计时,单位秒<br>from datetime import datetime, timedelta<br>tomorrow = datetime.utcnow() + timedelta(seconds=3)<br>add.apply_async((2, 2), eta=tomorrow)<br>result = add.apply_async((2, 2), countdown=3)<br>tip<br>任务的信息是保存在broker中的，因此关闭worker并不会丢失任务信息<br>回收任务(revoke)并非是将队列中的任务删除，而是在worker的内存中保存回收的任务task-id，不同worker之间会自动同步上述revoked task-id。<br>由于信息是保存在内存当中的，因此如果将所有worker都关闭了，revoked task-id信息就丢失了，回收过的任务就又可以执行了。要防治这点，需要在启动worker时指定一个文件用于保存信息<br>celery -A app.celery worker –loglevel=info &amp;&gt; celery_worker.log –statedb=/var/tmp/celery_worker.state<br>过期时间<br>expires单位秒，超过过期时间还未开始执行的任务会被回收<br>add.apply_async((10, 10), expires=60)<br>重试<br>max_retries:最大重试次数<br>interval_start:重试等待时间<br>interval_step:每次重试叠加时长，假设第一重试等待1s，第二次等待1＋n秒</p><p>interval_max:最大等待时间</p><p>add.apply_async((2, 2), retry=True, retry_policy={<br>‘max_retries’: 3,<br>‘interval_start’: 0,<br>‘interval_step’: 0.2,<br>‘interval_max’: 0.2,<br>})</p><p>序列化<br>将任务结果按照一定格式序列化处理，支持pickle, JSON, YAML and msgpack<br>add.apply_async((10, 10), serializer=’json’)<br>压缩<br>将任务结果压缩<br>add.apply_async((2, 2), compression=’zlib’)<br>任务路由<br>使用-Q参数为队列(queue)命名，然后调用任务时可以指定相应队列<br>$ celery -A proj worker -l info -Q celery,priority.high</p><p>add.apply_async(queue=’priority.high’)<br>工作流<br>按照一定关系一次调用多个任务<br>group: 并行调度<br>chain: 串行调度<br>chord: 类似group，但分header和body2个部分，header可以是一个group任务，执行完成后调用body的任务<br>map: 映射调度，通过输入多个入参来多次调度同一个任务<br>starmap: 类似map，入参类似＊args<br>chunks:将任务按照一定数量进行分组<br>group<br>from celery import group</p><blockquote><blockquote><blockquote><p>res = group(add.s(i, i) for i in xrange(10))()<br>res.get(timeout=1)<br>[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]<br>chain<br>from celery import chain</p></blockquote></blockquote></blockquote><h1 id="2-2-4-8"><a href="#2-2-4-8" class="headerlink" title="2 + 2 + 4 + 8"></a>2 + 2 + 4 + 8</h1><blockquote><blockquote><blockquote><p>res = chain(add.s(2, 2), add.s(4), add.s(8))()<br>res.get()<br>16</p></blockquote></blockquote></blockquote><p>可以用｜来表示chain</p><h1 id="4-16-2-4-8"><a href="#4-16-2-4-8" class="headerlink" title="((4 + 16)  2 + 4)  8"></a>((4 + 16) <em> 2 + 4) </em> 8</h1><blockquote><blockquote><blockquote><p>c2 = (add.s(4, 16) | mul.s(2) | (add.s(4) | mul.s(8)))</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>res = c2()<br>res.get()<br>chord<br>from celery import chord</p><p>#1<em>2+2</em>2+…9*2<br>res = chord((add.s(i, i) for i in xrange(10)), xsum.s())()<br>res.get()<br>90<br>map<br>from proj.tasks import add</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>~xsum.map([range(10), range(100)])<br>[45, 4950]<br>starmap<br>~add.starmap(zip(range(10), range(10)))<br>[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]<br>chunks<br>from proj.tasks import add</p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>res = add.chunks(zip(range(100), range(100)), 10)()<br>res.get()<br>[[0, 2, 4, 6, 8, 10, 12, 14, 16, 18],<br>[20, 22, 24, 26, 28, 30, 32, 34, 36, 38],<br>[40, 42, 44, 46, 48, 50, 52, 54, 56, 58],<br>[60, 62, 64, 66, 68, 70, 72, 74, 76, 78],<br>[80, 82, 84, 86, 88, 90, 92, 94, 96, 98],<br>[100, 102, 104, 106, 108, 110, 112, 114, 116, 118],<br>[120, 122, 124, 126, 128, 130, 132, 134, 136, 138],<br>[140, 142, 144, 146, 148, 150, 152, 154, 156, 158],<br>[160, 162, 164, 166, 168, 170, 172, 174, 176, 178],<br>[180, 182, 184, 186, 188, 190, 192, 194, 196, 198]]<br>周期性任务<br>周期性任务就是按照一定的时间检查反复执行的任务。前面描述的定时任务值的是一次性的任务。<br>程序中引入并配置好周期性任务后，beat进程就会定期调起相关任务<br>beat进程是需要单独启动的<br>$ celery -A proj beat<br>或者在worker启动时一起拉起<br>$ celery -A proj worker -B<br>注意一套celery只能启一个beat进程<br>时区配置<br>由于python中时间默认是utc时间，因此最简便的方法是celery也用utc时区</p></blockquote></blockquote></blockquote><p>CELERY_TIMEZONE = ‘UTC’<br>这么配置可以保证任务调度的时间是准确的，但由于服务器一般都配置时区，因此flower、以及日志中的时间可能会有偏差<br>另外一种方法，就是配置正确的时区<br>CELERY_TIMEZONE = ‘Asia/Shanghai’<br>然后任务调起时，将时间带入时区配置<br>local_tz = pytz.timezone(app.config[‘CELERY_TIMEZONE’])<br>format_eta = local_tz.localize(datetime.strptime(eta.strip(), ‘%Y/%m/%d %H:%M:%S’))<br>add.apply_async((2, 2),eta=format_eta)<br>周期性任务配置<br>from datetime import timedelta</p><p>CELERYBEAT_SCHEDULE = {<br>‘add-every-30-seconds’: {<br>‘task’: ‘tasks.add’,<br>‘schedule’: timedelta(seconds=30),<br>‘args’: (16, 16)<br>},<br>}<br>周期性任务配置crontab<br>from celery.schedules import crontab</p><p>CELERYBEAT_SCHEDULE = {</p><h1 id="Executes-every-Monday-morning-at-7-30-A-M"><a href="#Executes-every-Monday-morning-at-7-30-A-M" class="headerlink" title="Executes every Monday morning at 7:30 A.M"></a>Executes every Monday morning at 7:30 A.M</h1><p>‘add-every-monday-morning’: {<br>‘task’: ‘tasks.add’,<br>‘schedule’: crontab(hour=7, minute=30, day_of_week=1),<br>‘args’: (16, 16),<br>},<br>}<br>本文参考官方文档</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/Celery%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%EF%BC%9A%E7%90%86%E8%AE%BA%E7%AF%87/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/Celery%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%EF%BC%9A%E7%90%86%E8%AE%BA%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>Celery构建一个分布式爬虫：理论篇</p><p>如何构建一个分布式爬虫：理论篇<br>前言<br>本系列文章计划分三个章节进行讲述，分别是理论篇、基础篇和实战篇。理论篇主要为构建分布式爬虫而储备的理论知识，基础篇会基于理论篇的知识写一个简易的分布式爬虫，实战篇则会以微博为例，教大家做一个比较完整且足够健壮的分布式微博爬虫。通过这三篇文章，希望大家能掌握如何构建一个分布式爬虫的方法；能举一反三，将celery用于除爬虫外的其它场景。目前基本上的博客都是教大家使用scrapyd或者scrapy-redis构建分布式爬虫，本系列文章会从另外一个角度讲述如何用requests+celery构建一个健壮的、可伸缩并且可扩展的分布式爬虫。<br>本系列文章属于爬虫进阶文章，期望受众是具有一定Python基础知识和编程能力、有爬虫经验并且希望提升自己的同学。小白要是感兴趣，也可以看看，看不懂的话，可以等有了一定基础和经验后回过头来再看。<br>另外一点说明，本系列文章不是旨在构建一个分布式爬虫框架或者分布式任务调度框架，而是利用现有的分布式任务调度工具来实现分布式爬虫，所以请轻喷。<br>分布式爬虫概览<br>何谓分布式爬虫？<br>通俗的讲，分布式爬虫就是多台机器多个 spider 对多个 url 的同时处理问题，分布式的方式可以极大提高程序的抓取效率。<br>构建分布式爬虫通畅需要考虑的问题<br>（1）如何能保证多台机器同时抓取同一个URL？<br>（2）如果某个节点挂掉，会不会影响其它节点，任务如何继续？<br>（3）既然是分布式，如何保证架构的可伸缩性和可扩展性？不同优先级的抓取任务如何进行资源分配和调度？<br>基于上述问题，我选择使用celery作为分布式任务调度工具，是分布式爬虫中任务和资源调度的核心模块。它会把所有任务都通过消息队列发送给各个分布式节点进行执行，所以可以很好的保证url不会被重复抓取；它在检测到worker挂掉的情况下，会尝试向其他的worker重新发送这个任务信息，这样第二个问题也可以得到解决；celery自带任务路由，我们可以根据实际情况在不同的节点上运行不同的抓取任务（在实战篇我会讲到）。本文主要就是带大家了解一下celery的方方面面(有celery相关经验的同学和大牛可以直接跳过了)<br>Celery知识储备<br>celery基础讲解<br>按celery官网的介绍来说<br>Celery 是一个简单、灵活且可靠的，处理大量消息的分布式系统，并且提供维护这样一个系统的必需工具。它是一个专注于实时处理的任务队列，同时也支持任务调度。<br>下面几个关于celery的核心知识点<br>broker：翻译过来叫做中间人。它是一个消息传输的中间件，可以理解为一个邮箱。每当应用程序调用celery的异步任务的时候，会向broker传递消息，而后celery的worker将会取到消息，执行相应程序。这其实就是消费者和生产者之间的桥梁。<br>backend: 通常程序发送的消息，发完就完了，可能都不知道对方时候接受了。为此，celery实现了一个backend，用于存储这些消息以及celery执行的一些消息和结果。<br>worker: Celery类的实例，作用就是执行各种任务。注意在celery3.1.25后windows是不支持celery worker的！<br>producer: 发送任务，将其传递给broker<br>beat: celery实现的定时任务。可以将其理解为一个producer，因为它也是通过网络调用定时将任务发送给worker执行。注意在windows上celery是不支持定时任务的！<br>下面是关于celery的架构示意图，结合上面文字的话应该会更好理解</p><p>由于celery只是任务队列，而不是真正意义上的消息队列，它自身不具有存储数据的功能，所以broker和backend需要通过第三方工具来存储信息，celery官方推荐的是 RabbitMQ和Redis，另外mongodb等也可以作为broker或者backend，可能不会很稳定，我们这里选择Redis作为broker兼backend。<br>关于redis的安装和配置可以查看这里<br>实际例子<br>先安装celery<br>pip install celery<br>我们以官网给出的例子来做说明，并对其进行扩展。首先在项目根目录下，这里我新建一个项目叫做celerystudy，然后切换到该项目目录下，新建文件tasks.py，然后在其中输入下面代码<br>from celery import Celery</p><p>app = Celery(‘tasks’, broker=’redis://:<a href="mailto:&#39;&#39;@223.129.0.190" target="_blank" rel="noopener">&#39;&#39;@223.129.0.190</a>:6379/2’, backend=’redis://:<a href="mailto:&#39;&#39;@223.129.0.190" target="_blank" rel="noopener">&#39;&#39;@223.129.0.190</a>:6379/3’)</p><p>@app.task<br>def add(x, y):<br>return x + y<br>这里我详细讲一下代码：我们先通过app=Celery()来实例化一个celery对象，在这个过程中，我们指定了它的broker，是redis的db 2,也指定了它的backend,是redis的db3, broker和backend的连接形式大概是这样<br>redis://:password@hostname:port/db_number<br>然后定义了一个add函数，重点是@app.task，它的作用在我看来就是<strong>将add()<br>注册为一个类似服务的东西，本来只能通过本地调用的函数被它装饰后，就可以通过网络来调用。这个tasks.py中的app就是一个worker。它可以有很多任务，比如这里的任务函数add。我们再通过在命令行切换到项目根目录</strong>，执行<br>celery -A tasks worker -l info<br>启动成功后就是下图所示的样子<br>celery的worker启动成功<br>这里我说一下各个参数的意思，-A指定的是app(即Celery实例)所在的文件模块，我们的app是放在tasks.py中，所以这里是 tasks；worker表示当前以worker的方式运行，难道还有别的方式？对的，比如运行定时任务就不用指定worker这个关键字; -l info表示该worker节点的日志等级是info，更多关于启动worker的参数(比如-c、-Q等常用的)请使用<br>celery worker –help<br>进行查看<br>将worker启动起来后，我们就可以通过网络来调用add函数了。我们在后面的分布式爬虫构建中也是采用这种方式分发和消费url的。在命令行先切换到项目根目录，然后打开python交互端<br>from tasks import add<br>rs = add.delay(2, 2) # 这里的add.delay就是通过网络调用将任务发送给add所在的worker执行<br>这个时候我们可以在worker的界面看到接收的任务和计算的结果。<br>[2017-05-19 14:22:43,038: INFO/MainProcess] Received task: tasks.add[c0dfcd0b-d05f-4285-b944-0a8aba3e7e61] # worker接收的任务<br>[2017-05-19 14:22:43,065: INFO/MainProcess] Task tasks.add[c0dfcd0b-d05f-4285-b944-0a8aba3e7e61] succeeded in 0.025274309000451467s: 4 # 执行结果<br>这里是异步调用，如果我们需要返回的结果，那么要等rs的ready状态true才行。这里add看不出效果，不过试想一下，如果我们是调用的比较占时间的io任务，那么异步任务就比较有价值了<br>rs #<br>rs.ready() # true 表示已经返回结果了<br>rs.status # ‘SUCCESS’ 任务执行状态，失败还是成功<br>rs.successful() # True 表示执行成功<br>rs.result # 4 返回的结果<br>rs.get() # 4 返回的结果<br>from tasks import add</p><p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>add.delay(5, 10)<br>这时候可以在celery的worker界面看到执行的结果<br>[2017-05-19 14:25:48,039: INFO/MainProcess] Received task: tasks.add[f5ed0d5e-a337-45a2-a6b3-38a58efd9760]<br>[2017-05-19 14:25:48,074: INFO/MainProcess] Task tasks.add[f5ed0d5e-a337-45a2-a6b3-38a58efd9760] succeeded in 0.03369094600020617s: 15<br>此外，我们还可以通过send_task()来调用，将excute_tasks.py改成这样<br>from tasks import app<br>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>app.send_task(‘tasks.add’, args=(10, 15),)<br>这种方式也是可以的。send_task()还可能接收到为注册（即通过@app.task装饰）的任务，这个时候worker会忽略这个消息<br>[2017-05-19 14:34:15,352: ERROR/MainProcess] Received unregistered task of type ‘tasks.adds’.<br>The message has been ignored and discarded.<br>定时任务<br>上面部分讲了怎么启动worker和调用worker的相关函数，这里再讲一下celery的定时任务。<br>爬虫由于其特殊性，可能需要定时做增量抓取，也可能需要定时做模拟登陆，以防止cookie过期，而celery恰恰就实现了定时任务的功能。在上述基础上，我们将tasks.py文件改成如下内容<br>from celery import Celery<br>app = Celery(‘add_tasks’, broker=’redis:’’//223.129.0.190:6379/2’, backend=’redis:’’//223.129.0.190:6379/3’)<br>app.conf.update(</p><h1 id="配置所在时区"><a href="#配置所在时区" class="headerlink" title="配置所在时区"></a>配置所在时区</h1><p>CELERY_TIMEZONE=’Asia/Shanghai’,<br>CELERY_ENABLE_UTC=True,</p><h1 id="官网推荐消息序列化方式为json"><a href="#官网推荐消息序列化方式为json" class="headerlink" title="官网推荐消息序列化方式为json"></a>官网推荐消息序列化方式为json</h1><p>CELERY_ACCEPT_CONTENT=[‘json’],<br>CELERY_TASK_SERIALIZER=’json’,<br>CELERY_RESULT_SERIALIZER=’json’,</p><h1 id="配置定时任务"><a href="#配置定时任务" class="headerlink" title="配置定时任务"></a>配置定时任务</h1><p>CELERYBEAT_SCHEDULE={<br>‘my_task’: {<br>‘task’: ‘tasks.add’, # tasks.py模块下的add方法<br>‘schedule’: 60, # 每隔60运行一次<br>‘args’: (23, 12),<br>}<br>}<br>)<br>@app.task<br>def add(x, y):<br>return x + y<br>然后先通过ctrl+c停掉前一个worker，因为我们代码改了，需要重启worker才会生效。我们再次以celery -A tasks worker -l info这个命令开启worker。<br>这个时候我们只是开启了worker，如果要让worker执行任务，那么还需要通过beat给它定时发送，我们再开一个命令行，切换到项目根目录，通过<br>celery beat -A tasks -l info<br>celery beat v3.1.25 (Cipater) is starting.<br><strong> - … </strong> - _<br>Configuration -&gt;<br>. broker -&gt; redis://223.129.0.190:6379/2<br>. loader -&gt; celery.loaders.app.AppLoader<br>. scheduler -&gt; celery.beat.PersistentScheduler<br>. db -&gt; celerybeat-schedule<br>. logfile -&gt; [stderr]@%INFO<br>. maxinterval -&gt; now (0s)<br>[2017-05-19 15:56:57,125: INFO/MainProcess] beat: Starting…<br>这样就表示定时任务已经开始运行了。<br>眼尖的同学可能看到我这里celery的版本是3.1.25，这是因为celery支持的windows最高版本是3.1.25。由于我的分布式微博爬虫的worker也同时部署在了windows上，所以我选择了使用 3.1.25。如果全是linux系统，建议使用celery4。<br>此外，还有一点需要注意，在celery4后，定时任务（通过schedule调度的会这样，通过crontab调度的会马上执行）会在当前时间再过定时间隔执行第一次任务，比如我这里设置的是60秒的间隔，那么第一次执行add会在我们通过celery beat -A tasks -l info启动定时任务后60秒才执行；celery3.1.25则会马上执行该任务。<br>关于定时任务更详细的请看官方文档celery定时任务</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/Celery%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AF%87/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/Celery%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<p>Celery构建一个分布式爬虫：基础篇</p><p>如何构建一个分布式爬虫：基础篇<br>首先，我们新建目录distributedspider，然后再在其中新建文件workers.py,里面内容如下<br>from celery import Celery<br>app = Celery(‘crawl_task’, include=[‘tasks’], broker=’redis://223.129.0.190:6379/1’, backend=’redis://223.129.0.190:6379/2’)</p><h1 id="官方推荐使用json作为消息序列化方式"><a href="#官方推荐使用json作为消息序列化方式" class="headerlink" title="官方推荐使用json作为消息序列化方式"></a>官方推荐使用json作为消息序列化方式</h1><p>app.conf.update(<br>CELERY_TIMEZONE=’Asia/Shanghai’,<br>CELERY_ENABLE_UTC=True,<br>CELERY_ACCEPT_CONTENT=[‘json’],<br>CELERY_TASK_SERIALIZER=’json’,<br>CELERY_RESULT_SERIALIZER=’json’,<br>)<br>上述代码主要是做Celery实例的初始化工作，include是在初始化celery app的时候需要引入的内容，主要就是注册为网络调用的函数所在的文件。然后我们再编写任务函数，新建文件tasks.py,内容如下<br>import requests<br>from bs4 import BeautifulSoup<br>from workers import app<br>@app.task<br>def crawl(url):<br>print(‘正在抓取链接{}’.format(url))<br>resp_text = requests.get(url).text<br>soup = BeautifulSoup(resp_text, ‘html.parser’)<br>return soup.find(‘h1’).text<br>它的作用很简单，就是抓取指定的url，并且把标签为h1的元素提取出来<br>最后，我们新建文件task_dispatcher.py，内容如下<br>from workers import app<br>url_list = [<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/introduction.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/introduction.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/next-steps.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/next-steps.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/getting-started/resources.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/getting-started/resources.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/application.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/application.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/tasks.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/tasks.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/canvas.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/canvas.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/workers.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/workers.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/daemonizing.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/daemonizing.html&#39;</a>,<br>‘<a href="http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html&#39;" target="_blank" rel="noopener">http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html&#39;</a><br>]<br>def manage_crawl_task(urls):<br>for url in urls:<br>app.send_task(‘tasks.crawl’, args=(url,))<br>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>manage_crawl_task(url_list)<br>这段代码的作用主要就是给worker发送任务，任务是tasks.crawl，参数是url(元祖的形式)<br>现在，让我们在节点A(hostname为resolvewang的主机)上启动worker<br>celery -A workers worker -c 2 -l info<br>这里 -c指定了线程数为2， -l表示日志等级是info。我们把代码拷贝到节点B(节点名为wpm的主机)，同样以相同命令启动worker，便可以看到以下输出<br>两个节点<br>可以看到左边节点(A)先是all alone，表示只有一个节点；后来再节点B启动后，它便和B同步了<br>sync with celery@wpm<br>这个时候，我们运行给这两个worker节点发送抓取任务<br>python task_dispatcher.py<br>可以看到如下输出<br>分布式抓取示意图<br>可以看到两个节点都在执行抓取任务，并且它们的任务不会重复。我们再在redis里看看结果<br>backend示意图<br>可以看到一共有11条结果，说明 tasks.crawl中返回的数据都在db2(backend)中了，并且以json的形式存储了起来，除了返回的结果，还有执行是否成功等信息。<br>到此，我们就实现了一个很基础的分布式网络爬虫，但是它还不具有很好的扩展性，而且貌似太简单了…下一篇我将以微博数据采集为例来演示如何构建一个稳健的分布式网络爬虫。</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%9A%E7%9B%91%E6%8E%A7%E7%95%8C%E9%9D%A2/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%9A%E7%9B%91%E6%8E%A7%E7%95%8C%E9%9D%A2/</url>
      
        <content type="html"><![CDATA[<p>celery分布式：监控界面</p><p>flower监控服务器<br>flower安装与运行的方法</p><p>通过浏览器打开地址</p><p>可能遇到的错误<br>pymongo.errors.NotMasterError: not master<br>关闭mongodb的数据集<br>其他<br>当服务器数量较多的时候，管理起来会很不方便，可以使用python的supervisor来管理后台进程，遗憾的是它并不支持python3，不过也可以装在python2的环境<br>虽然用了supervisor可以很方便的管理python程序，但是还是得一个个登陆不同的服务器的去管理，咋办捏？<br>我在github上找到一个工具supervisor-easy，可以批量管理supervisor，如图:</p><p>地址：<a href="https://github.com/trytofix/supervisor-easy" target="_blank" rel="noopener">https://github.com/trytofix/supervisor-easy</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/Celery&amp;flower%20%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/celery%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/Celery&amp;flower%20%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>Celery&amp;flower 后台运行部署<br>celery在后台运行<br>在生产环境中，你可能希望在后台运行worker.下面的文档中有详细的介绍：daemonization tutorial.<br>下面的脚本使用celery multi命令在后台启动一个或多个worker.<br>$ celery multi start w1 -A proj -l info<br>celery multi v4.0.0 (0today8)</p><blockquote><p>Starting nodes…<br>w1.halcyon.local: OK<br>你也可以重新启动:<br>$ celery multi restart w1 -A proj -l info<br>celery multi v4.0.0 (0today8)<br>Stopping nodes…<br>w1.halcyon.local: TERM -&gt; 64024<br>Waiting for 1 node…..<br>w1.halcyon.local: OK<br>Restarting node w1.halcyon.local: OK<br>celery multi v4.0.0 (0today8)<br>Stopping nodes…<br>w1.halcyon.local: TERM -&gt; 64052<br>或者停止它:<br>$ celery multi stop w1 -A proj -l info<br>stop命令是异步的，它不会等待所有的worker真正关闭。你可能需要使用stopwait命令，这个命令会保证当前所有的任务都执行完毕。<br>$ celery multi stopwait w1 -A proj -l info<br>注:celery multi命令不会保存workers的信息，所以当重新启动时你需要使用相同的命令行参数。当停止时，只有相同的pidfile和logfile参数是必须的。</p></blockquote><p>默认情况下，celery会在当前目录下创建pidfile和logfile.为了防止多个worker在启动时相互影响，你可以指定一个特定的目录。<br>$ mkdir -p /var/run/celery<br>$ mkdir -p /var/log/celery<br>$ celery multi start w1 -A proj -l info –pidfile=/var/run/celery/%n.pid \<br>–logfile=/var/log/celery/%n%I.log<br>通过multi命令你可以启动多个workers,另外这个命令还支持强大的命令行语法来为不同的workers指定不同的参数。<br>$ celery multi start 10 -A proj -l info -Q:1-3 images,video -Q:4,5 data \<br>-Q default -L:4,5 debug<br>更多关闭multi的举例请参考multi模块的API手册。</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/%E6%BB%91%E5%8A%A8%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/%E6%BB%91%E5%8A%A8%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>滑动验证码识别<br>极验三代验证码破解<br>破解思路<br>1.使用selenium的模拟点击和滑动等来实现破解<br>2.首先寻找快速识别按钮，如果找到进行点击验证<br>3.获取验证码的位置(4角坐标)<br>4.对验证码进行局域截图<br>5.点击滑块获取第二张的验证码缺口图片<br>6.遍历每一个像素点的位置，寻找缺口位置<br>7.构造运动轨迹，要注意避免规则的运动轨迹被识别出是人工构造轨迹<br>8.获取运动轨迹，并进行滑动<br>示例代码<br>crack.py<br>微博九宫格验证码识别<br>破解思路<br>1.使用selenium的模拟点击和滑动来实现破解<br>2.首先获取页面的验证码图片<br>3.获取图片根据所有可能的滑动轨迹模板库的图片进行对比，获取正确的滑动顺序<br>4.根据得到的拖动顺序，对其进行拖动<br>示例代码<br>crack.py</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/%E5%9B%BE%E7%89%87%E9%AA%8C%E8%AF%81%E7%A0%81-Tesseract%E7%A0%B4%E8%A7%A3/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/%E5%9B%BE%E7%89%87%E9%AA%8C%E8%AF%81%E7%A0%81-Tesseract%E7%A0%B4%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>图片验证码-Tesseract破解<br>图像的翻译-Tesseract<br>文字识别<br>文字识别-机器视觉的一个分支，将图像翻译成文字一般被称为 光学文字识别(Optical Character Recognition, OCR)。可以实现OCR的底层库并不多,目前很多库都是使用共同的几个底层 OCR 库,或者是在上面 进行定制<br>ORC库的概念<br>在读取和处理图像、图像相关的机器学习以及创建图像等任务中，Python 一直都是非常出色的语言。虽然有很多库可以进行图像处理，但在这里我们只重点介绍： Tesseract<br>作用<br>Tesseract是一个将图像翻译成文字的OCR库(光学文字识别，Optical Character Recognition)，可以翻译图片中的文字内容，目前由 Google 赞助(Google 也是一家以 OCR 和机器学习技术闻名于世的公司)。Tesseract 是目前公认最优秀、最精确的开源 OCR 系统，除了极高的精确度，Tesseract 也具有很高的灵活性。它可以通过训练识别出任何字体，也可以识别出任何 Unicode 字符。<br>tesseract处理的文字规范<br>tesseract处理的文字大都是格式规范的，格式规范的文字具有以下特点:<br>使用一个标准字体(不包含手写体、草书,或者十分“花哨的”字体)<br>即使被复印或拍照，字体还是很清晰，没有多余的痕迹或污点<br>排列整齐，没有歪歪斜斜的字<br>没有超出图片范围，也没有残缺不全，或紧紧贴在图片的边缘<br>怎么将图片格式化<br>文字的一些格式问题在图片预处理时可以进行解决。例如,可以把图片转换成灰度图，调整亮度和对比度，还可以根据需要进行裁剪和旋转（详情需要了解图像与信号处理）等<br>使用步骤<br>安装tesseract和tensor-ocr<br>windows下安装<br>先下载tesseract-ocr<br>下载地址: <a href="https://digi.bib.uni-mannheim.de/tesseract" target="_blank" rel="noopener">https://digi.bib.uni-mannheim.de/tesseract</a><br>下载 pytenseract<br>pip install pytesseract pillow<br>ubuntu下安装<br>sudo apt-get install -y tesseract-ocr libtesseract-dev libleptonica-dev<br>pip install pytesseract pillow<br>centos下安装<br>yum install -y tesseract<br>pip install pytesseract pillow<br>在终端中使用tesseract<br>tesseract  test.jpg  text      将翻译结果保存在text.txt中<br>tesseract chi_sim  text.png  paixu    将翻译结果保存在paixu.txt中（可以翻译中文简体）<br>tesseract –list-langs    查看当前支持的语言</p><p>在Python中使用tesesract<br>import pytesseract<br>from PIL import Image<br>image = Image.open(jpg)       获取图片<br>text = pytesseract.image_to_string(image, lang=’chi_sim’)       对图片的内容进行翻译<br>import subprocess<br>subprocess.call([‘tessetact’, ‘-1’, ‘chi_sim’, filePath, ‘paixu’])     使用终端命令进行翻译<br>pytesseract.file_to_text(‘image.png’)       直接对图片进行识别<br>使用tesseract识别代码(包含图片处理)<br>import pytesseract<br>from PIL import Image<br>image = Image.open(‘code2.jpg’)</p><h1 id="图片灰度化处理"><a href="#图片灰度化处理" class="headerlink" title="图片灰度化处理"></a>图片灰度化处理</h1><p>image = image.convert(‘L’) </p><h1 id="图片二值化处理"><a href="#图片二值化处理" class="headerlink" title="图片二值化处理"></a>图片二值化处理</h1><p>threshold = 127        设置二值化阈值<br>filter_func = lambda x: 0 if x &lt; threshold else 1<br>image = image.point(filter_func, ‘1’)<br>image.show()<br>result = pytesseract.image_to_text(image)     对处理后的图片进行识别<br>print(result)<br>对tesseract进行训练<br>训练的目的<br>tesseract一般只能识别符合文字规范的字体，其他的稍微复杂的字很难识别，tesseract可以通过训练对一种格式字体的反复训练，来实现字体的高精度识别<br>训练tesseract的方法<br>要训练 Tesseract 识别一种文字，无论是晦涩难懂的字体还是验证码，你都需要向 Tesseract 提供每个字符不同形式的样本。<br>首先要收集大量的验证码样本，样本的数量和复杂程度，会决定训练的效果。第二步是准确地告诉 Tesseract 一张图片中的每个字符是什么，以及每个字符的具体位置。<br>这里需要创建一些矩形定位文件(box file)，一个验证码图片生成一个矩形定位文件，也可以通过jTessBoxEditor软件来修改矩形的定位。<br>一个图片的矩形定位文件如下所示:</p><p>第一列符号是图片中的每个字符，后面的 4 个数字分别是包围这个字符的最小矩形的坐标 (图片左下角是原点 (0，0)，4 个数字分别对应每个字符的左下角 x 坐标、左下角 y 坐标、右上角 x 坐标和右上角 y 坐标)，最后一个数字“0”表示图片样本的编号。<br>矩形定位文件必须保存在一个 .box 后缀的文本文件中，(例如 4MmC3.box)。<br>tesseract训练教程<br><a href="http://www.cnblogs.com/mjorcen/p/3800739.html?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="noopener">http://www.cnblogs.com/mjorcen/p/3800739.html?utm_source=tuicool&amp;utm_medium=referral</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/%E4%BB%A3%E7%90%86%E6%B1%A0%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/%E4%BB%A3%E7%90%86%E6%B1%A0%E7%9A%84%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p>代理池的搭建<br>代理池的介绍<br>应用模块<br>aiohttp、requests、redis-py、pyquery、Flask<br>代理池模块<br>存储模块、获取代理模块、检测模块、接口模块、调度模块<br>各模块的功能<br>存储模块<br>负责存储抓取下来的代理，并对代理去重<br>获取模块<br>需要定时抓取代理，代理可以是免费抓取的代理，也可以是调用付费代理商购买的代理<br>检测模块<br>需要定时检测数据库中的代理，对效果不好的代理及时进行删除<br>接口模块<br>需要用API来提供对外的服务接口。外界通过访问接口的方式来获取代理<br>调度模块<br>调度前面的所有模块，使之协调工作<br>各模块的设计<br>存储模块<br>使用Redis有序集合，集合的每一个元素都是不重复的<br>每个元素都有一个分数字段，分数是可以重复的，数值小的排在前面，数值大的排在后面<br>每个代理100分为最高分，表示可用，0分为最低分，代表最不可用，自动进行移除<br>获取随机代理的逻辑，先随机获取最高分代理，如果不存在最高分代理，按照排名获取<br>获取模块<br>从各代理商处爬取或购买调用接口获得代理<br>调用存储模块，向数据库中添加代理，初始的分数是10<br>检测模块<br>不断的检测代理的可用性(检测的目标网站可以自定义)，进行代理的加分和减分或者移除<br>检测到代理可用，分数立即置为100分，检测到代理请求失败，则减少一分<br>接口模块<br>定义接口模块的随机获取代理，统计代理数量等方法<br>调度模块<br>通过配置每个模块的开关来设置调度模块的启动功能<br>通过多进程启动每一个模块<br>代码示例<br>ProxyPool.zip</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/Js%E5%8E%8B%E7%BC%A9%E3%80%81%E6%B7%B7%E6%B7%86%E3%80%81%E5%8A%A0%E5%AF%86%E5%8F%8A%E7%A0%B4%E8%A7%A3/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/Js%E5%8E%8B%E7%BC%A9%E3%80%81%E6%B7%B7%E6%B7%86%E3%80%81%E5%8A%A0%E5%AF%86%E5%8F%8A%E7%A0%B4%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>Js压缩、混淆、加密及破解<br>js压缩、混淆、加密介绍<br>三者的定义与区别<br>压缩：删除 Javascript 代码中所有注释、跳格符号、换行符号及无用的空格，从而压缩 JS 文件大小，优化页面加载速度。<br>混淆：经过编码将变量和函数原命名改为毫无意义的命名（如function(a,b,c,e,g)等），以防止他人窥视和窃取 Javascript 源代码，也有一定压缩效果。<br>加密：一般用eval方法加密，效果与混淆相似，也做到了压缩的效果。<br>三者的作用<br>压缩的主要目的是消除注释等无用字符，达到精简js代码，减小js文件大小的目的，这也是页面优化的一种方式；<br>混淆和加密的目的比较接近，都是为了防止他人直接查看源码，对代码（如重要的api等）起保护作用，但这也只是增加了阅读代码的代价，也就是所谓的防君子不防小人。<br>加密就是通过已经有的或者自己编写的加密方法，对js代码进行加密转换，但是当混淆和加密联合使用时，如先混淆在加密（或者先加密再混淆）时，破解时间就会增加。关于js的加密<br>浏览器是怎么解析混淆和加密后的js代码的<br>其实变量名只要是Unicode字符就行了，对于js引擎来说都是一样的，只是人类觉得他们不同而已。<br>是否可以破解压缩\混淆\加密后的代码<br>压缩和混淆后的js代码是不可以被还原的，但是我们可以将js加密的代码进行解密，后面的几篇是介绍如何进行js解密的。<br>常见的加密和解密方式介绍<br>加密方式的分类<br>一：最简单的加密解密<br>二：转义字符”\”的妙用<br>三：使用Microsoft出品的脚本编码器Script Encoder来进行编码   （自创简单解码）<br>四：任意添加NUL空字符（十六进制00H）    （自创）<br>五：无用内容混乱以及换行空格TAB大法<br>六：自写解密函数法<br>一、escape()和unescape()<br>加密方式的介绍<br>大家对于JAVASCRIPT函数escape()和unescape()想必是比较了解啦（很多网页加密在用它们），分别是编码和解码字符串<br>比如例子代码用escape()函数加密后变为如下格式：alert%28%22%u9ED1%u5BA2%u9632%u7EBF%22%29%3B<br>如果愿意我们可以写点JAVASCRIPT代码重新把它加密如下%61%6C%65%72%74%28%22%u9ED1%u5BA2%u9632%u7EBF%22%29%3B<br>这样加密后的代码是不能直接运行的，可以使用eval(codeString)，这个函数的作用就是检查JavaScript代码并执行，必选项 codeString 参数是包含有效 JavaScript 代码的字符串值，加上上面的解码unescape()<br>解密方式<br>直接将eval换成alert配合unescape便实现了解密<br>加密示例<br>加密方法<br>            <script language="JavaScript"><br>            varcode=unescape(“%61%6C%65%72%74%28%22%u9ED1%u5BA2%u9632%u7EBF%22%29%3B”);<br>            eval(varcode)<br>             </script><br>二、转义字符”\”加密<br>加密方式介绍<br>JavaScript提供了一些特殊字符如：\n （换行）、 \r （回车）、\’ （单引号 ）等应该是有所了解<br>其实”\”后面还可以跟八进制或十六进制的数字，如字符”a”则可以表示为：”\141”或”\x61”（注意是小写字符”x”），至于双字节字符如汉字”黑”则仅能用十六进制表示为”\u9ED1”（注意是小写字符”u”），其中字符”u”表示是双字节字符；<br>解密示例<br>直接将eval换成alert便实现了解密</p><pre><code>  # 八进制转义字符串如下:    &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt;    eval(&quot;\141\154\145\162\164\50\42\u9ED1\u5BA2\u9632\u7EBF\42\51\73&quot;)    &lt;/SCRIPT&gt;# 十六进制转义字符串如下:    &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt;    eval(&quot;\x61\x6C\x65\x72\x74\x28\x22\u9ED1\u5BA2\u9632\u7EBF\x22\x29\x3B&quot;)    &lt;/SCRIPT&gt;</code></pre><p>三：使用Microsoft出品的脚本编码器Script Encoder来进行编码<br>加密方式的介绍<br>直接使用Microsoft出品的JavaScript调用控件Scripting.Encoder完成的编码！<br>如果你觉得这样编码得到的代码LANGUAGE属性是JScript.Encode，很容易让人识破，那么还有一个几乎不为人知的window对象的方法execScript()，其原形为：<br>window.execScript( sExpression, sLanguage )<br>sExpression: 必选项。字符串(String)。要被执行的代码。<br>sLanguage　: 必选项。字符串(String)。指定执行的代码的语言。默认值为 Microsoft JScript<br>解密方式<br>编码后的代码运行前IE会先对其进行解码，如果我们先把加密后的代码放入一个自定义函数如上面的decode()中，然后对自定义函数decode调用toString()方法，得到的将是解码后的代码！<br>加密示例<br>加密方法</p><pre><code># 使用Scripting.Encoder &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt;   var Senc=new ActiveXObject(&quot;Scripting.Encoder&quot;);   var code=&apos;&lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt;\r\nalert(&quot;黑客防线&quot;);\r\n&lt;\/SCRIPT&gt;&apos;;   var Encode=Senc.EncodeScriptFile(&quot;.htm&quot;,code,0,&quot;&quot;);   alert(Encode);   &lt;/SCRIPT&gt; #  使用execScript() &lt;SCRIPT LANGUAGE=&quot;JavaScript&quot;&gt;   execScript(&quot;#@~^FgAAAA==@#@&amp;ls DD`J黑客防线r#p@#@&amp;FgMAAA==^#~@&quot;,&quot;JScript.Encode&quot;)   &lt;/SCRIPT&gt; # 加密结果  &lt;SCRIPT LANGUAGE=&quot;JScript.Encode&quot;&gt;#@~^FgAAAA==@#@&amp;ls DD`J黑客防线r#p@#@&amp;FgMAAA==^#~@&lt;/SCRIPT&gt; </code></pre><p>解密示例<br>解密方法<br>                        <script language="JScript.Encode"><br>                        function decode(){};<br>                        alert(decode.toString());<br>                        </script><br>四：任意添加NUL空字符（十六进制00H）<br>加密方式介绍<br>一次偶然的实验，使我发现在HTML网页中任意位置添加任意个数的”空字符”，IE照样会正常显示其中的内容，并正常执行其中的JavaScript 代码。<br>而添加的”空字符”我们在用一般的编辑器查看时，会显示形如空格或黑块，使得原码很难看懂，如用记事本查看则”空字符”会变成”空格”，利用这个原理加密结果如下：（其中显示的”空格”代表”空字符”）<br>如果不知道方法的人很难想到要去掉里面的”空字符”（00H）的<br>解密方式<br>去掉代码中的00H<br>加密示例<br>加密方法<br>            <s c="" ri="" p="" t="" l="" ang="" u="" a="" ge="    J   a    v a S    c r    i p t"><br>            a    l er    t   (“ 黑    客 防 线”)   ;    </s></p><pre><code>&lt;    /    SC   R    I    P    T&gt;  </code></pre><p>五：无用内容混乱以及换行空格TAB大法<br>加密方式介绍<br>在JAVASCRIPT代码中我们可以加入大量的无用字符串或数字，以及无用代码和注释内容等等<br>这使真正的有用代码埋没在其中，并把有用的代码中能加入换行、空格、TAB的地方加入大量换行、空格、TAB，并可以把正常的字符串用””来进行换行，这样就会使得代码难以看懂！如我加密后的形式如下：<br>解密方式<br>慢慢的分析吧<br>加密示例<br>加密方法<br>​            <script language="JavaScript"><br>​            “xajgxsadffgds”;1234567890<br>​            625623216;var $=0;alert//@$%%&amp;<em>()(&amp;(^%^<br>​            //cctv function//<br>​            (//hhsaasajx xc<br>​            /</em><br>​            asjgdsgu<em>/<br>​            “黑<br>​             客<br>​            防线”//ashjgfgf<br>​            /</em><br>​            @#%$^&amp;%$96667r45fggbhytjty<br>​            */<br>​            //window<br>​            )<br>​            ;”#@$#%@#432hu”;212351436<br>​            </script><br>六：自写解密函数法<br>加密方式介绍<br>这个方法和一、二差不多，只不过是自己写个函数对代码进行加密<br>很多VBS病毒使用这种方法对自身进行加密，来防止特征码扫描！下面是我写的一个简单的加密解密函数，<br>加密示例<br>加密方法<br>​                            # 加密方法<br>​                            <script language="JavaScript"><br>​                            function compile(code)<br>​                            {<br>​                              var c=String.fromCharCode(code.charCodeAt(0)+code.length);<br>​                               for(var i=1;i&lt;code.length;i++){<br>​                              c+=String.fromCharCode(code.charCodeAt(i)+code.charCodeAt(i-1));<br>​                               }<br>​                               alert(escape(c));<br>​                            }<br>​                            compile(’alert(“黑客防线”);’)<br>​                            </script><br>​                            # 加密结果<br>​                            o%CD%D1%D7%E6%9CJ%u9EF3%uFA73%uF1D4%u14F1%u7EE1Kd<br>解密示例<br>解密方法<br>​                    <script language="JavaScript"><br>​                    function uncompile(code)<br>​                    {<br>​                       code=unescape(code);<br>​                       var c=String.fromCharCode(code.charCodeAt(0)-code.length);<br>​                       for(var i=1;i&lt;code.length;i++){<br>​                      c+=String.fromCharCode(code.charCodeAt(i)-c.charCodeAt(i-1));<br>​                       }<br>​                       return c;<br>​                    }<br>​                    eval(uncompile(“o%CD%D1%D7%E6%9CJ%u9EF3%uFA73%uF1D4%u14F1%u7EE1Kd”));<br>​                    </script></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/ADSL%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA&amp;%E4%BB%A3%E7%90%86%E6%B1%A0/"/>
      <url>/2018/10/18/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%B8%B8%E8%A7%81%E5%8F%8D%E5%8F%8D%E7%88%AC/ADSL%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA&amp;%E4%BB%A3%E7%90%86%E6%B1%A0/</url>
      
        <content type="html"><![CDATA[<p>ADSL代理服务器搭建&amp;代理池<br>ADSL的介绍和使用<br>什么是ADSL<br>ADSL全称叫做Asymmetric Digital Subscriber Line，非对称数字用户环路，因为它的上行和下行带宽不对称。<br>它采用频分复用技术把普通的电话线分成了电话、上行和下行三个相对独立的信道，从而避免了相互之间的干扰。<br>有种主机叫做动态拨号VPS主机，这种主机在连接上网的时候是需要拨号的，只有拨号成功后才可以上网；<br>每拨一次号，主机就会获取一个新的IP，也就是它的IP并不是固定的，而且IP量特别大，几乎不会拨到相同的IP，如果我们用它来搭建代理，既能保证高度可用，又可以自由控制拨号切换。<br>连接ADSL服务器(推荐云代理)<br>ssh <a href="mailto:root@153.36.65.214" target="_blank" rel="noopener">root@153.36.65.214</a> -p 20063<br>ADSL服务器的使用<br>拨号初始化<br>进入之后，可以发现有一个可用的脚本文件，叫做ppp.sh，这是拨号初始化的脚本，运行它会让我们输入拨号的用户名和密码，然后它就会开始各种拨号配置，一次配置成功；<br>后面的拨号就不需要重复输入用户名和密码了；<br>都提示成功之后就可以进行拨号了。<br>拨号命令<br>adsl-start<br>停止拨号<br>adsl-stop<br>断线重播<br>先执行adsl-stop再执行adsl-start<br>将ADSL服务器设置为代理服务器<br>使用的工具<br>在Linux下搭建HTTP代理服务器，推荐TinyProxy和Squid，配置都非常简单，在这里我们以TinyProxy为例来讲解一下怎样搭建代理服务器。<br>配置步骤<br>安装TinyProxy<br>yum install -y epel-release<br>yum update -y<br>yum install -y tinyproxy<br>配置TinyProxy<br>vi /etc/tinyproxy/tinyproxy.conf<br>修改如下行<br>Port 8888      代理的端口<br>Allow 127.0.0.1     允许连接的主机，默认只允许本机连接，可以注释掉，允许所有主机连接<br>重启tinyproxy<br>service tinyproxy start<br>测试(在其他主机上操作)<br>curl -x 112.84.118.216:8888 httpbin.org/get<br>查看请求结果是不是设置的代理ip<br>动态获取IP<br>怎么动态获取ip<br>DDNS动态域名解析<br>我们需要使用一个域名来解析，也就是虽然IP是变的，但域名解析的地址可以随着IP的变化而变化。<br>它的原理其实是拨号主机向固定的服务器发出请求，服务器获取客户端的IP，然后再将域名解析到这个IP上就可以了。<br>国内比较有名的服务就是花生壳了，也提供了免费版的动态域名解析，另外DNSPOD也提供了解析接口来动态修改域名解析设置，DNSPOD，但是这样的方式都有一个通病，那就是慢！<br>自己配置ADSL代理池<br>所以根据花生壳的原理，可以完全自己实现一下动态获取IP的方法。<br>要实现这个需要两台主机，一台主机就是这台动态拨号VPS主机，另一台是具有固定公网IP的主机。<br>动态VPS主机拨号成功之后就请求远程的固定主机，远程主机获取动态VPS主机的IP，就可以得到这个代理，将代理保存下来，这样拨号主机每拨号一次，远程主机就会及时得到拨号主机的IP，如果有多台拨号VPS，也统一发送到远程主机，这样我们只需要从远程主机取下代理就好了，保准是实时可用，稳定高效的。<br>ADSL代理池远程主机和ADSL拨号服务器的功能划分<br>远程主机<br>监听主机请求，获取动态VPS主机IP<br>将VPS主机IP记录下来存入数据库，支持多个客户端<br>检测当前接收到的IP可用情况，如果不可用则删除<br>提供API接口，通过API接口可获取当前可用代理IP<br>ADSL拨号服务器<br>定时执行拨号脚本换IP<br>换IP后立即请求远程主机<br>拨号后检测是否拨号成功，如果失败立即重新拨号<br>远程主机实现<br>存储模块<br>功能设计<br>远程主机作为一台服务器，动态拨号VPS会定时请求远程主机，远程主机接收到请求后将IP记录下来存入数据库。<br>因为IP是一直在变化的，IP更新了之后，原来的IP就不能用了，所以对于一个主机来说我们可能需要多次更新一条数据。<br>另外我们不能仅限于维护一台拨号VPS主机，当然是需要支持多台维护的。在这里我们直接选用Key-Value形式的非关系型数据库存储更加方便，所以在此选用Redis数据库。<br>使用redis的Hash类型存储方式，Key就是拨号主机的名称，可以自己指定，Value就是代理的值。<br>接口模块<br>功能设计<br>使用Tornado来实现<br>实现random随机获取一个可用的动态代理功能<br>实现count获取代理池中的代理个数<br>ADSL拨号服务器实现<br>拨号模块<br>功能设计<br>定时拨号，每隔一段时间拨号一次，更新ip<br>更新前先清除远程主机数据库中的当前ip<br>更新后请求远程主机，将新的ip更新到redis散列里<br>示例代码<br>AdslProxy.zip</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>githubpage + hexo + yilia 搭建个人博客</title>
      <link href="/2018/10/17/20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/%E5%8D%9A%E5%AE%A2/github+hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/10/17/20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/%E5%8D%9A%E5%AE%A2/github+hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h4 id="0-本博客的由来"><a href="#0-本博客的由来" class="headerlink" title="0.本博客的由来"></a>0.本博客的由来</h4><p>本来感觉写博客很费时间，但是最近感觉这两年攒手里的笔记太多了，不方便整理和分享</p><p>所以打算以后就干脆直接将笔记整理到github，这样也比自己维护一个博客省心</p><p>下面就将搭建本博客的步骤也同步上来，当做一个hello-world吧。</p><a id="more"></a><h4 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1.环境准备"></a>1.环境准备</h4><p>电脑环境是Windows，安装好git后，所有搭建操作均在git bash内完成</p><p><strong>1）安装hexo(首先要安装git, node.js, npm)</strong></p><p>注意：首次安装git 要配置user信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>git config --global user.name "yourname"   #（yourname是git的用户名）</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>git config --global user.email email）</span><br></pre></td></tr></table></figure><p><strong>2）使用npm安装hexo</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>npm install -g hexo</span><br></pre></td></tr></table></figure><p><strong>3）创建hexo文件夹</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>mkdir hexo_blog</span><br><span class="line"><span class="meta">$</span>cd hexo_lobg</span><br></pre></td></tr></table></figure><p><strong>4）初始化框架</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>hexo init #hexo   #会自动创建网站所需要的文件</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>npm install    #安装依赖包</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>hexo generate </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>hexo server   #现在可以用127.0.0.1:4000访问hexo默认的hello world界面 ,hexo s = hexo server</span><br></pre></td></tr></table></figure><h4 id="2-部署到github"><a href="#2-部署到github" class="headerlink" title="2.部署到github"></a>2.部署到github</h4><p><strong>1）首次使用github需要配置密钥</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C "email"</span><br></pre></td></tr></table></figure><p>生成ssh密钥，按三次回车键，密码为空,这边会生成id_rsa和_rsa.pub文件</p><p>打开id_rsa.pub，复制全文添加到GitHub 的Add SSH key中。</p><p><strong>2）创建Respository， 并开启githubPage</strong></p><p>首先注册登录github,然后创建页面仓库，Repository name 命名应该是 youname.github.io </p><p>在setting界面， 配置</p><p><img src="/img/1539839479905-1539839725571.png" alt="1539839479905-1539839725571"></p><p><strong>3）安装hexo-deployer-git</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>npm install hexo-deployer-git --save     用来推送项目到github</span><br></pre></td></tr></table></figure><p><strong>4）生成博客，并push到github</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>hexo generate</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span>hexo deploy</span><br></pre></td></tr></table></figure><p><strong>5）验证结果</strong></p><p>通过<a href="https://youname.github.io" target="_blank" rel="noopener">https://youname.github.io</a> 进行访问</p><h4 id="3-更换博客模板"><a href="#3-更换博客模板" class="headerlink" title="3.更换博客模板"></a>3.更换博客模板</h4><p>目前访问的博客模板比较简略，下面介绍使用：yilia模板</p><p><strong>1）拉取模板文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span>git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br></pre></td></tr></table></figure><p><strong>2）更改配置文件修改模板为yilia</strong></p><p>打开项目目录下的_config.yml文件，更改主题theme;   <code>theme: yilia</code><br>然后配置yilia文件下的_config.yml（目录：<code>hexo/themes/yilia/_config.yml</code>） 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"># Header</span><br><span class="line">menu:</span><br><span class="line">  主页: /</span><br><span class="line">  归档: /archives</span><br><span class="line">  #分类: /categories</span><br><span class="line">  #标签: /tags</span><br><span class="line"></span><br><span class="line"># SubNav</span><br><span class="line">subnav:</span><br><span class="line">  github: &quot;https://github.com/KyleAdultHub&quot;</span><br><span class="line">  #weibo: &quot;#&quot;</span><br><span class="line">  #rss: &quot;#&quot;</span><br><span class="line">  #zhihu: &quot;#&quot;</span><br><span class="line">  qq: &quot;/information&quot;</span><br><span class="line">  #weixin: &quot;#&quot;</span><br><span class="line">  #jianshu: &quot;#&quot;</span><br><span class="line">  #douban: &quot;#&quot;</span><br><span class="line">  #segmentfault: &quot;#&quot;</span><br><span class="line">  #bilibili: &quot;#&quot;</span><br><span class="line">  #acfun: &quot;#&quot;</span><br><span class="line">  mail: &quot;/information&quot;</span><br><span class="line">  #facebook: &quot;#&quot;</span><br><span class="line">  #google: &quot;#&quot;</span><br><span class="line">  #twitter: &quot;#&quot;</span><br><span class="line">  #linkedin: &quot;#&quot;</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">rss: /atom.xml</span><br><span class="line"></span><br><span class="line"># 是否需要修改 root 路径</span><br><span class="line"># 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，</span><br><span class="line"># 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。</span><br><span class="line">root: /</span><br><span class="line"></span><br><span class="line"># Content</span><br><span class="line"># 文章太长，截断按钮文字</span><br><span class="line">excerpt_link: more</span><br><span class="line"># 文章卡片右下角常驻链接，不需要请设置为false</span><br><span class="line">show_all_link: &apos;展开全文&apos;</span><br><span class="line"># 数学公式</span><br><span class="line">mathjax: false</span><br><span class="line"># 是否在新窗口打开链接</span><br><span class="line">open_in_new: false</span><br><span class="line"></span><br><span class="line"># 打赏</span><br><span class="line"># 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏</span><br><span class="line">reward_type: 0</span><br><span class="line"># 打赏wording</span><br><span class="line">reward_wording: &apos;谢谢你&apos;</span><br><span class="line"># 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg</span><br><span class="line">alipay: </span><br><span class="line"># 微信二维码图片地址</span><br><span class="line">weixin: </span><br><span class="line"></span><br><span class="line"># 目录</span><br><span class="line"># 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录</span><br><span class="line">toc: 1</span><br><span class="line"># 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为false</span><br><span class="line">toc_hide_index: true</span><br><span class="line"># 目录为空时的提示</span><br><span class="line">toc_empty_wording: &apos;目录，不存在的…&apos;</span><br><span class="line"></span><br><span class="line"># 是否有快速回到顶部的按钮</span><br><span class="line">top: true</span><br><span class="line"></span><br><span class="line"># Miscellaneous</span><br><span class="line">baidu_analytics: &apos;&apos;</span><br><span class="line">google_analytics: &apos;&apos;</span><br><span class="line">favicon: /favicon.png</span><br><span class="line"></span><br><span class="line">#你的头像url</span><br><span class="line">avatar: /img/header.jpg</span><br><span class="line"></span><br><span class="line">#是否开启分享</span><br><span class="line">share_jia: true</span><br><span class="line"></span><br><span class="line">#评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment</span><br><span class="line">#不需要使用某项，直接设置值为false，或注释掉</span><br><span class="line">#具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/</span><br><span class="line"></span><br><span class="line">#1、多说</span><br><span class="line">duoshuo: false</span><br><span class="line"></span><br><span class="line">#2、网易云跟帖</span><br><span class="line">wangyiyun: false</span><br><span class="line"></span><br><span class="line">#3、畅言</span><br><span class="line">changyan_appid: *** #这个畅言id和conf写自己的</span><br><span class="line">changyan_conf: ***</span><br><span class="line"></span><br><span class="line">#4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的</span><br><span class="line">disqus: false</span><br><span class="line"></span><br><span class="line">#5、Gitment</span><br><span class="line">gitment_owner: false      #你的 GitHub ID</span><br><span class="line">gitment_repo: &apos;&apos;          #存储评论的 repo</span><br><span class="line">gitment_oauth:</span><br><span class="line">  client_id: &apos;&apos;           #client ID</span><br><span class="line">  client_secret: &apos;&apos;       #client secret</span><br><span class="line"></span><br><span class="line"># 样式定制 - 一般不需要修改，除非有很强的定制欲望…</span><br><span class="line">style:</span><br><span class="line">  # 头像上面的背景颜色</span><br><span class="line">  header: &apos;#4d4d4d&apos;</span><br><span class="line">  # 右滑板块背景</span><br><span class="line">  slider: &apos;linear-gradient(200deg,#a0cfe4,#e8c37e)&apos;</span><br><span class="line"></span><br><span class="line"># slider的设置</span><br><span class="line">slider:</span><br><span class="line">  # 是否默认展开tags板块</span><br><span class="line">  showTags: false</span><br><span class="line"></span><br><span class="line"># 智能菜单</span><br><span class="line"># 如不需要，将该对应项置为false</span><br><span class="line"># 比如</span><br><span class="line">#smart_menu:</span><br><span class="line">#  friends: false</span><br><span class="line">smart_menu:</span><br><span class="line">  innerArchive: &apos;所有文章&apos;</span><br><span class="line">  friends: &apos;友链&apos;</span><br><span class="line">  aboutme: &apos;关于我&apos;</span><br><span class="line"></span><br><span class="line">friends:</span><br><span class="line">  #友情链接1: http://localhost:4000/</span><br><span class="line">  </span><br><span class="line">aboutme: </span><br><span class="line">  程序猿一枚&lt;br&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>搭建Registry 私有库</title>
      <link href="/2018/10/17/11.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/Docker/%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93/"/>
      <url>/2018/10/17/11.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/Docker/%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h3 id="1-关于Registry仓库"><a href="#1-关于Registry仓库" class="headerlink" title="1.关于Registry仓库"></a><strong>1.关于Registry仓库</strong></h3><p>官方的<a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker hub</a>是一个用于管理公共镜像的好地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景需要我们拥有一个私有的镜像仓库用于管理我们自己的镜像。这个可以通过开源软件Registry来达成目的。</p><p> Registry在github上有两份代码：<a href="https://github.com/docker/docker-registry" target="_blank" rel="noopener">老代码库</a>和<a href="https://github.com/docker/distribution" target="_blank" rel="noopener">新代码库</a>。老代码是采用python编写的，存在pull和push的性能问题，出到0.9.1版本之后就标志为deprecated，不再继续开发。从2.0版本开始就到在新代码库进行开发，新代码库是采用go语言编写，修改了镜像id的生成算法、registry上镜像的保存结构，大大优化了pull和push镜像的效率。</p><p> 官方在Docker hub上提供了registry的镜像（<a href="https://hub.docker.com/_/registry/" target="_blank" rel="noopener">详情</a>），我们可以直接使用该registry镜像来构建一个容器，搭建我们自己的私有仓库服务。Tag为latest的registry镜像是0.9.1版本的，我们直接采用2.1.1版本。</p><a id="more"></a><h3 id="2-Registry的部署"><a href="#2-Registry的部署" class="headerlink" title="2.Registry的部署"></a>2.Registry的部署</h3><p><strong>运行下面命令获取registry镜像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker pull registry:2.1.1</span><br></pre></td></tr></table></figure><p><strong>然后启动一个容器</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker run -d -v /opt/data/registry/:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1</span><br></pre></td></tr></table></figure><p><strong>验证服务是否启动成功</strong></p><p>说明我们已经启动了registry服务，打开浏览器输入<a href="http://127.0.0.1:5000/v2" target="_blank" rel="noopener">http://127.0.0.1:5000/v2</a></p><h3 id="3-验证"><a href="#3-验证" class="headerlink" title="3.验证"></a>3.验证</h3><p><strong>向仓库中push镜像</strong></p><p>现在我们通过将镜像push到registry来验证一下。我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker tag hello-world 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><p>接下来，我们运行docker push将hello-world镜像push到我们的私有仓库中，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker push 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">The push refers to a repository [127.0.0.1:5000/hello-world] (len: 1)</span><br><span class="line"></span><br><span class="line">975b84d108f1: Image successfully pushed</span><br><span class="line"></span><br><span class="line">3f12c794407e: Image successfully pushed</span><br><span class="line"></span><br><span class="line">latest: digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b size: 2744</span><br></pre></td></tr></table></figure><p>现在我们可以查看我们本地/opt/registry目录下已经有了刚推送上来的hello-world。我们也在浏览器中输入<a href="http://127.0.0.1:5000/v2/_catalog，如下图所示，" target="_blank" rel="noopener">http://127.0.0.1:5000/v2/_catalog，如下图所示，</a></p><p><strong>从镜像库中拉取镜像</strong></p><p>现在我们可以先将我们本地的127.0.0.1:5000/hello-world和hello-world先删除掉，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker rmi hello-world</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo docker rmi 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><p>然后使用docker pull从我们的私有仓库中获取hello-world镜像，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker pull 127.0.0.1:5000/hello-world</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Using default tag: latest</span><br><span class="line"></span><br><span class="line">latest: Pulling from hello-world</span><br><span class="line"></span><br><span class="line">b901d36b6f2f: Pull complete</span><br><span class="line"></span><br><span class="line">0a6ba66e537a: Pull complete</span><br><span class="line"></span><br><span class="line">Digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b</span><br><span class="line"></span><br><span class="line">Status: Downloaded newer image for 127.0.0.1:5000/hello-world:latest</span><br><span class="line"></span><br><span class="line">lienhua34@lienhua34-Compaq-Presario-CQ35-Notebook-PC ~ $ sudo docker images</span><br><span class="line"></span><br><span class="line">REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE</span><br><span class="line"></span><br><span class="line">registry 2.1.1 b91f745cd233 5 days ago 220.1 MB</span><br><span class="line"></span><br><span class="line">ubuntu 14.04 a5a467fddcb8 6 days ago 187.9 MB</span><br><span class="line"></span><br><span class="line">127.0.0.1:5000/hello-world latest 0a6ba66e537a 2 weeks ago 960 B</span><br></pre></td></tr></table></figure><h3 id="4-查询镜像库"><a href="#4-查询镜像库" class="headerlink" title="4.查询镜像库"></a>4.查询镜像库</h3><p><strong>查询镜像库中的镜像</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://10.0.110.218:5000/v2/_catalog</span><br></pre></td></tr></table></figure><h3 id="5-错误排查"><a href="#5-错误排查" class="headerlink" title="5.错误排查"></a>5.错误排查</h3><p><strong>错误描述</strong></p><p>在push 到docker registry时，可能会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The push refers to a repository [192.168.1.100:5000/registry]</span><br><span class="line"></span><br><span class="line">Get https://192.168.1.100:5000/v1/_ping: http: server gave HTTP response to HTTPS client</span><br></pre></td></tr></table></figure><p>这个问题可能是由于客户端采用https，docker registry未采用https服务所致。一种处理方式是把客户对地址“192.168.1.100:5000”请求改为http。</p><p>目前很多文章都是通过修改docker的配置文件“etc/systemconfig/docker”，重启docker来解决这个问题。但发现docker1.12.3版本并无此文件，根据网上创建此文件，并填入相应内容，重启docker无效果，仍然报此错误。</p><p> <strong>解决办法</strong></p><p>在”/etc/docker/“目录下，创建”daemon.json“文件。在文件中写入：</p><p>{ “insecure-registries”:[“192.168.1.100:5000”] }</p><p>保存退出后，重启docker。</p>]]></content>
      
      
      <categories>
          
          <category> Docker容器技术 </category>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>正则表达式</title>
      <link href="/2018/01/12/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2018/01/12/4.python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="正则表达式介绍"><a href="#正则表达式介绍" class="headerlink" title="正则表达式介绍"></a>正则表达式介绍</h3><p><strong>什么是正则表达式</strong></p><p>Regular Expression, 又称规则表达式。<br>正则表达式就是用事先定义好的一些特定字符（组合），组成一个“规则字符串”，这个“规则字符串”用来描述一种字符串的匹配模式（pattern）；</p><p><strong>正则的作用</strong></p><p>可以用来检查一个字串是否包含某种子串、将匹配的子串替换或者取出</p><p><strong>正则的特点</strong></p><p>灵活性、逻辑性和功能性非常强大</p><a id="more"></a><h3 id="正则表达式的使用方法"><a href="#正则表达式的使用方法" class="headerlink" title="正则表达式的使用方法"></a>正则表达式的使用方法</h3><h4 id="re模块常用函数和方法"><a href="#re模块常用函数和方法" class="headerlink" title="re模块常用函数和方法"></a>re模块常用函数和方法</h4><ul><li>import  re</li><li>result_obj = re.search(正则表达式， 数据，flag=0）     —-查找数据中第一个符合匹配规则的字符串<ul><li>search()函数从数据中只能查找到第一个符合正则数据放到result_obj中，  如果没有匹配到想要匹配的结果会返回None</li></ul></li><li>result_obj.group()      —-查看正则匹配的结果内容<ul><li>result_obj.group(1， 2/ 组名) 返回需要组的匹配的结果，返回一个包含多个组匹配结果的元组；    result_obj.group() == result_obj.group(0) == 正个正则表达式所有匹配的字符</li></ul></li></ul><h4 id="re模块其他常用方法"><a href="#re模块其他常用方法" class="headerlink" title="re模块其他常用方法"></a>re模块其他常用方法</h4><p><img src="/img/1541300873462.png" alt="1541300873462"></p><h4 id="compile-编译"><a href="#compile-编译" class="headerlink" title="compile 编译"></a>compile 编译</h4><ul><li><p>作用</p><ul><li>对正则表达式匹配规则进行预编译，在大量使用到正则的时候，可以提高匹配的速度</li></ul></li><li><p>使用方法</p><ul><li>p = re.compile(‘匹配规则’,  re.DATALL)</li><li>p.search(‘字符串’)       按照编译的规则对字符串进行匹配正则表达式中的特殊字符<ul><li>匹配单个字符</li></ul></li></ul></li></ul><h3 id="正则表达式常用匹配方式"><a href="#正则表达式常用匹配方式" class="headerlink" title="正则表达式常用匹配方式"></a>正则表达式常用匹配方式</h3><h4 id="匹配单个字符"><a href="#匹配单个字符" class="headerlink" title="匹配单个字符"></a>匹配单个字符</h4><p><img src="/img/1541301141193.png" alt="1541301141193"></p><ul><li>空白字符\s == [ \f\n\r\t\v]   非空白字符 \S == [^\f\t\v\n\r]   </li><li>在正则表达式中若只是想要匹配一个像特殊字符的普通字符需要在特殊字符前面加转义字符“\” 例如“.”</li><li>特殊字符在[ ]中例如：[.    |   * + ？等 ]没有特殊功能只代表普通字符</li><li>在[ ]中若是想使用“-”普通字符要加上转义字符\</li></ul><h4 id="匹配多个字符"><a href="#匹配多个字符" class="headerlink" title="匹配多个字符"></a>匹配多个字符</h4><p><img src="/img/1541301215631.png" alt="1541301215631"></p><h4 id="常用定位符"><a href="#常用定位符" class="headerlink" title="常用定位符"></a>常用定位符</h4><p><img src="/img/1541301242792.png" alt="1541301242792"></p><h4 id="正则表达式的分组"><a href="#正则表达式的分组" class="headerlink" title="正则表达式的分组"></a>正则表达式的分组</h4><p><img src="/img/1541301315452.png" alt="1541301315452"></p><ul><li>注意: <ul><li>在使用（|）的时候尽量使特殊的或者通用的变量放在前边</li><li>在引用分组的时候注意：\num表示八进制数num所表示的普通ASCII码字符，所以在引用的时候会默认表示ascii码字符，所以要注意转义或者使用原生字符串</li></ul></li></ul><h4 id="匹配所有的汉字的方法"><a href="#匹配所有的汉字的方法" class="headerlink" title="匹配所有的汉字的方法"></a>匹配所有的汉字的方法</h4><ul><li>re.compole(r’[\u4e00-\u9fa5]’)<br>  备注：匹配所有unicode编码的中文</li></ul><h3 id="正则表达式的贪婪与懒惰"><a href="#正则表达式的贪婪与懒惰" class="headerlink" title="正则表达式的贪婪与懒惰"></a>正则表达式的贪婪与懒惰</h3><h4 id="概念："><a href="#概念：" class="headerlink" title="概念："></a>概念：</h4><ul><li>贪婪-尽可能多的匹配</li><li>懒惰-尽可能少的匹配<h4 id="默认为贪婪模式的匹配模式"><a href="#默认为贪婪模式的匹配模式" class="headerlink" title="默认为贪婪模式的匹配模式"></a>默认为贪婪模式的匹配模式</h4></li><li>在python中 +/*/{m,n}默认情况下总是贪婪的<h4 id="如何让贪婪模式变为懒惰模式"><a href="#如何让贪婪模式变为懒惰模式" class="headerlink" title="如何让贪婪模式变为懒惰模式"></a>如何让贪婪模式变为懒惰模式</h4></li><li>在量词后加上一个?</li><li>例子：<br><img src="/img/1541301444152.png" alt="1541301444152"></li></ul><h3 id="原生字符串的应用"><a href="#原生字符串的应用" class="headerlink" title="原生字符串的应用"></a>原生字符串的应用</h3><h4 id="特殊字符的转义"><a href="#特殊字符的转义" class="headerlink" title="特殊字符的转义"></a>特殊字符的转义</h4><p>在表达式中如果包含“\”表示转义\后面的字符为八进制数字代表的ascii对应的特殊字符，在python中会对ascii包含的数字或者字符进行转义，这种情况会导致会将匹配规则的字符进行转义，结果不能匹配到想要匹配的字符串内容</p><h4 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h4><ul><li>取消转义<br>  在每一个’\’字符前加上’\’，对”\’进行转义，这样会取消\的转义功能，将\只代表一个\字符，不会对后边的字符进行转义<br>  ascii不包括的字符，如果前边有转义字符\，不需要加以转义，python会自动转义</li><li><p>原生字符串<br>  如果在表达式或字符串前边加上r“”对字符串中的\字符自动转义<br>  在使用的时候，匹配规则可以和想要匹配的内容写法相同，r会自动帮我们转义<br>  示例： re.search(r’abc\nabc’, ‘abc\nabc’)</p><h3 id="正则表达式的常见问题"><a href="#正则表达式的常见问题" class="headerlink" title="正则表达式的常见问题"></a>正则表达式的常见问题</h3><blockquote><p>如何让 . 特殊符号可以匹配所有内容（包括\n）</p></blockquote></li><li><p>解决办法：<br>  使用re.DOTALL 参数</p></li><li>示例：<br>  re.findall(r’abc.’,  ‘abc\n\nsfgs’, re.DOTALL)<br>  备注：也可以使用re.S 代替re.DOTALL 效果上是一样的<h3 id="Ascii码对应关系"><a href="#Ascii码对应关系" class="headerlink" title="Ascii码对应关系"></a>Ascii码对应关系</h3></li><li><p>在字符串，或者正则表达式中，\n\t等控制字符 或者 \数字（表示八进制的num所表示的普通ascii码）等显示字符，在应用的时候会默认为在调用ascii码的控制字符或者是显示字符，所以如果只是想表达单纯的字符 需要用\n或者r””这种形式进行转意，可以使用chr（八进制数）来查询对应的ascii字符</p></li><li><p><strong>ascii码表</strong></p><p><img src="/img/1541301800714.png" alt="1541301800714"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> python爬虫 </category>
          
          <category> 爬虫基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则表达式 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
