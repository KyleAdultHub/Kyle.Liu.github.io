<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[模型评估、优化、设计过程]]></title>
    <url>%2F2018%2F11%2F24%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[1.机器学习模型的评估和选择有啥方法能优化我们的模型整理了一下优化模型时，经常做的一些操作，优化模型无外乎以下几种方法: 获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度 尝试增加正则化程度 但是，也如我们所见，并不是任何模型都适用于这些优化方法，我们还需要对症下药，接下来可以看一下怎么确定，我们的模型需要什么优化服务 模型过拟合的判断 当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。此时很可能模型已经产生了过拟合。 为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差： 对于线性回归模型，我们利用测试集数据计算代价函数 J 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外还可以计算误分类比率 模型的选择和交叉验证集当我们的使用多项式模型的时候，经常会不确定多项式的项数该如何确定，下面是一种比较简单的处理思路: 假设我们要在10个不同次数的二项式模型之间进行选择： ​ 显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能适应我们的测试集或者推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 选择模型的方法：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集 使用训练集训练出10个模型 用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）Train/validation/test error Training error:$J{train}(\theta) = \frac{1}{2m}\sum\limits{i=1}{m}(h_{\theta}(x{(i)})-y{(i)})2$Cross Validation error:$J{cv}(\theta) = \frac{1}{2m{cv}}\sum\limits{i=1}^{m}(h{\theta}(x{(i)}{cv})-y{(i)}{cv})^2$Test error:$J{test}(\theta)=\frac{1}{2m{test}}\sum\limits{i=1}^{m{test}}(h{\theta}(x^{(i)}{cv})-y{(i)}_{cv})2​$ 模型的偏差和方差诊断偏差和方差当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？ 其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。 我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析： 通过训练误差和交叉验证集误差判断偏差和方差问题，总结如下: 训练集误差和交叉验证集误差近似时：偏差/欠拟合交叉验证集误差远大于训练集误差时：方差/过拟合 正则化力度与偏差/方差在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。 我们选择一系列的想要测试的 λ 值，通常是 0-10之间的呈现2倍关系的值（如 0， 0.01， 0.01， 0.04， 0.08， 0.015， 0.032， 0.064， 1.28, 2.56, 5.12, 10共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。 选择正则化系数的方法为： 使用训练集训练出12个不同程度正则化的模型 用12个模型分别对交叉验证集计算的出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上： 当 λ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大随着 λ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 通过学习曲线评估偏差和方差学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。 例如： 如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。 如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观 也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。 如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。 也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。 学习曲线的总结: 当Jtrain和Jcv都偏高的时候，处于高偏差(欠拟合)的情况，此时增加训练数据不会有更好的结果 当Jtrain偏低而Jcv偏高的时候，处于高方差(过拟合)的情况，此时增加训练数据往往会获得更好的训练结果 神经网络的方差和偏差 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。 通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。 解决高偏差和高方差的总结解决高方差问题: 获得更多的训练实例——解决高方差 尝试减少特征的数量——解决高方差 尝试增加正则化程度λ——解决高方差 解决高偏差的问题 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 类偏斜的误差度量类偏斜和查准率查全率的介绍类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例， 这时候很难用一般的误差度量方法来度量其真实的误差。 例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 因为，下面将引入查准率和查全率的概念。 我们将算法预测的结果分成四种情况： 正确肯定（True Positive,TP）：预测为真，实际为真 正确否定（True Negative,TN）：预测为假，实际为假 错误肯定（False Positive,FP）：预测为真，实际为假 错误否定（False Negative,FN）：预测为假，实际为真 查准率（Precision）和查全率（Recall）的公式为 ： 查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。 因此，可以看出用于度量偏斜类问题，查准率和查全率更能体现模型表现的好坏。 查准率和查全率之间的选择在很多的应用中，我们希望能够保证查准率和召回率的相对平衡。 继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。 查准率(Precision)=TP/(TP+FP) 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率(Recall)=TP/(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。 我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同： 我们希望有一个帮助我们选择这个阀值的方法。 一种方法是计算F1 值（F1 Score），其计算公式为： $F_1Score：2{PR\over{P+R}}$ 我们选择使得F1值最高的阀值, F1的想法就是尽量让查准率和查全率都不会太小。 2.机器学习系统的设计机器学习系统设计思路以一个垃圾邮件分类器算法为例 为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。 为了构建这个分类器算法，我们可以做很多事，例如： 收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本 基于邮件的路由信息开发一系列复杂的特征 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理 为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法 误差分析设计机器学习系统的首要任务 当设计一个模型的时候， 最好的方法是快速的将最简单版本的算法实现，一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。 这么做的原因是：因为通常你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。 除了画出学习曲线之外，一件非常有用的事是误差分析。比如在构造垃圾邮件分类器时，可以看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。 构建一个学习算法的推荐方法为： 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择 进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势 举个误差分析的栗子: 以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看： 是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。 发现是否缺少某些特征，记下这些特征出现的次数。例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。 误差分析需要交叉验证来验证 误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。 不要用测试集来做交叉验证 因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。 总结: 当你在研究一个新的机器学习问题时，推荐你实现一个较为简单快速、即便不是那么完美的算法。目前大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。 另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>设计过程</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras构建简单神经网络]]></title>
    <url>%2F2018%2F11%2F11%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fkeras%E6%9E%84%E5%BB%BA%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[Keras 构建神经网络该示例的一般流程是首先加载数据，然后定义网络，最后训练网络。 要使用 Keras，你需要知道以下几个核心概念。 创建神经网络序列模型123from keras.models import Sequential # Create the Sequential model model = Sequential() keras.models.Sequential 类是神经网络模型的封装容器。它会提供常见的函数，例如 fit()、evaluate() 和 compile()。我们将介绍这些函数（在碰到这些函数的时候）。我们开始研究模型的层级吧。 层级Keras 层级就像神经网络层级。有完全连接的层级、最大池化层级和激活层级。你可以使用模型的 add() 函数添加层级。例如，简单的模型可以如下所示： 1234567891011121314from keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten #创建序列模型 model = Sequential() #第一层级 - 添加有 32 个节点的输入层 model.add(Dense, input_dim=32) #第二层级 - 添加有 128 个节点的完全连接层级 model.add(Dense(128)) #第三层级 - 添加 softmax 激活层级 model.add(Activation('softmax')) #第四层级 - 添加完全连接的层级 model.add(Dense(10)) #第五层级 - 添加 Sigmoid 激活层级 model.add(Activation('sigmoid')) Keras 将根据第一层级自动推断后续所有层级的形状。这意味着，你只需为第一层级设置输入维度。 上面的第一层级 model.add(Flatten(input_dim=32)) 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。 模型编译和训练构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 categorical_crossentropy。我们还可以指定优化程序，暂时将使用 adam。最后，我们可以指定评估模型用到的指标。我们将使用准确率。 1model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics = [&apos;accuracy&apos;]) 我们可以使用以下命令来查看模型架构： model.summary() 然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。 model.fit(X, y, nb_epoch=1000, verbose=0) 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。 最后，我们可以使用以下命令来评估模型： model.evaluate() 很简单，对吧？我们实践操作下。 练习我们从最简单的示例开始。在此测验中，你将构建一个简单的多层前向反馈神经网络以解决 XOR 问题。 将第一层级设为 Flatten() 层级，并将 input_dim 设为 2。 将第二层级设为 Dense() 层级，并将输出宽度设为 8。 在第二层级之后使用 softmax 激活函数。 将输出层级宽度设为 2，因为输出只有 2 个类别。 在输出层级之后使用 softmax 激活函数。 对模型运行 10 个 epoch。 准确度应该为 50%。可以接受，当然肯定不是太理想！在 4 个点中，只有 2 个点分类正确？我们试着修改某些参数，以改变这一状况。例如，你可以增加 epoch 次数。如果准确率达到 75%，你将通过这道测验。能尝试达到 100% 吗？ 首先，查看关于模型和层级的 Keras 文档。 Keras 多层感知器网络示例和你要构建的类似。请将该示例当做指南，但是注意有很多不同之处。 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom keras.utils import np_utilsimport tensorflow as tftf.python.control_flow_ops = tf# Set random seednp.random.seed(42)# Our dataX = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')y = np.array([[0],[1],[1],[0]]).astype('float32')# Initial Setup for Kerasfrom keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten# One-hot encoding the outputy = np_utils.to_categorical(y)# Building the modelxor = Sequential()xor.add(Dense(32, input_dim=2))xor.add(Activation("sigmoid"))xor.add(Dense(2))xor.add(Activation("sigmoid"))xor.compile(loss="categorical_crossentropy", optimizer="adam", metrics = ['accuracy'])# Uncomment this line to print the model architecture# xor.summary()# Fitting the modelhistory = xor.fit(X, y, nb_epoch=100, verbose=0)# Scoring the modelscore = xor.evaluate(X, y)print("\nAccuracy: ", score[-1])# Checking the predictionsprint("\nPredictions:")print(xor.predict_proba(X)) 结果： 123456789Using TensorFlow backend.4/4 [==============================] - 0sAccuracy: 0.75Predictions:4/4 [==============================] - 0s[[0.6914389 0.6965836 ] [0.7073754 0.7086655 ] [0.6919555 0.68419015] [0.70766294 0.6967294 ]]]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络]]></title>
    <url>%2F2018%2F11%2F07%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1.非线性假设当我们使用线型回归或者逻辑回归的时候，有这样一个缺点，当特征太多的时候，计算的负荷会非常大。 下面是一个例子: 当我们使用 x1, x2的多次项式进行预测时，我们可以应用的很好。 使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合（x1x2 + x1x3 + x1x4 + …. + x99x100），，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。 假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB值），作为假设，我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车， 如下图所示： 假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约3百万个特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。 2.神经元和大脑介绍2 神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。 神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过让我和你分享，一些这方面的证据。 大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此，你才能明白我的话。神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。 下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。举几个例子： 这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA (美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。 第二个例子： 关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。 3.模型表示神经网络模型表示为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？ 每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多输入/树突，并且有一个输出/轴突。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。 下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。 轴突是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。 神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（weight）。 逻辑回归学习模型的神经元： 单个神经元的效果和逻辑回归的效果没有区别，但是神经网络会组成有神经元组成的网络。 一个简单的神经网络 其中 x1, x2, x3是输入单元（input units），我们将原始数据输入给它们。 a1, a2, a3是中间单元，代表三个神经元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算 hθ(x)。神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）： 下面引入一些标记法来帮助描述模型： $a_i^{(j)}$代表第 j 层的第 i 个激活单元(神经元)。 $\theta^{(j)}$代表从第 j 层映射到第 j+1 层时的权重的矩阵，例如 $\theta^{(1)}$ 代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 j + 1 层的激活单元数量为行数，以第 j 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中 $\theta^{(1)}$ 的尺寸为 3*4。 注: 每层权重矩阵的列数为 ( 神经元数 + 1 ), 行数为 ( 上一层神经元数 + 1 ) 对于上图所示的模型，激活单元和输出分别表达为： 上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型 我们可以知道：每一个 a 的输出都是由上一层所有的 x 和每一个 x 所对应的 θ 决定的（我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )）把 x , θ, a分别用矩阵表示： 神经网络模型向量化( FORWARD PROPAGATION ) 相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值： 为了更好了了解Neuron Networks的工作原理，我们先把左半部分遮住： 右半部分其实就是以a0, a1, a2, a3 , 按照Logistic Regression的方式输出 hθ(x)： 其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量 [x1 - x3] 变成了中间层的 [ $a_1^{(2)}$ - $a_3^{(2)}$] ,我们可以把a0, a1, a2, a3看成更为高级的特征值，也就是x0, x1, x2, x3 的进化体，并且它们是由 x 与 θ 决定的，因为是梯度下降的，所以 a 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 x 次方厉害，也能更好的预测新数据。 这就是神经网络相比于逻辑回归和线性回归的优势。 4. 特征的直观理解从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。举例说明： 逻辑与(AND)：下图中左半部分是神经网络的设计与output层表达式，右边上部分是sigmod函数，下半部分是真值表。 其中 θ0 = -30, θ1 = 20, θ2 = 20 我们的输出函数hθ(x) 即为：hθ(x) = g(-30 + 20x1 + 20x2), g(x) 的图像是： 所以我们有：hθ(x) ≈ x1 AND x2 逻辑或(OR): 下图是神经网络的设计与output层表达式和真值表。 逻辑非(NOT)： 异或(XNOR): 5.多分类神经网络当我们有不止两种分类时（也就是 y = 1, 2, 3, … ），比如以下这种情况，该怎么办？ 如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。假如输入向量 x 有三个维度，两个中间层，输出层4个神经元分别用来表示4类，也就是每一个数据在输出层都会出现 [a b c d] ^T, 且a, b, c, d 中仅有一个为1，表示当前类。下面是该神经网络的可能结构示例： 6. 神经网络常用的激活函数什么是激活函数在神经网络中，我们经常可以看到对于某一个隐藏层的节点，该节点的激活值计算一般分为两步： （1）输入该节点的值为 x1,x2时，在进入这个隐藏节点后，会先进行一个线性变换，计算出值 $z^{[1]}=w_1x_1+w_2x_2+b^{[1]}=W^{[1]}x+b^{[1]}$，上标 1表示第 1 层隐藏层。 （2）再进行一个非线性变换，也就是经过非线性激活函数，计算出该节点的输出值(激活值) $a^{(1)}=g(z^{(1)})$ ，其中 g(z)为非线性函数。 常用的激活函数在深度学习中，常用的激活函数主要有：sigmoid函数，tanh函数，ReLU函数。 1.sigmoid函数 在逻辑回归中我们介绍过sigmoid函数，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下： sigmoid激活函数缺点： （1）当 z 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 g′(z) 将接近 0 。这会导致权重 W 的梯度将接近 0 ，使得梯度更新十分缓慢，即梯度消失。 （2）函数的输出不是以0为均值，将不便于下层的计算。sigmoid函数可用在网络最后一层，作为输出层进行二分类，尽量不要使用在隐藏层。 (3) 指数计算消耗资源 2.tanh函数 tanh函数相较于sigmoid函数要常见一些，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1)之间，其公式与图形为： tanh函数的优点 (1) tanh解决了sigmoid的输出非“零为中心”的问题。 tanh函数的缺点 （1）同sigmoid函数的第一个缺点一样，当 z 很大或很小时，g′(z) 接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。 (2) 依旧是指数计算 3.ReLU函数 ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下： ReLU函数的优点：（1）在输入为正数的时候（对于大多数输入 zz 空间来说），不存在梯度消失问题。（2） 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）ReLU函数的缺点： （1）当输入为负时，梯度为0，会产生梯度消失问题。 4.Leaky ReLU 函数 这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下： Leaky ReLU函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题。 优点: 1.神经元不会出现死亡的情况。 2.对于所有的输入，不管是大于等于0还是小于0，神经元不会饱和。 3.由于Leaky ReLU线性、非饱和的形式，在SGD中能够快速收敛。 4.计算速度要快很多。Leaky ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。 7.神经网络代价函数假设神经网络的训练样本有 m 个，每个包含一组输入x 和一组输出信号 y ，L 表示神经网络层数，Si 表示每层的neuron个数( Sl表示输出层神经元个数)，SL 代表最后一层中处理单元的个数。 将神经网络的分类定义为两种情况：二类分类和多类分类， 二类分类: SL = 1, y = 0 or 1 表示哪一类； K类分类: SL = k, yi = 1 表示分到第i类； (k &gt; 2) 这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。正则化的那一项只是排除了每一层 θ0 后，每一层的 θ 矩阵的和。最里层的循环 j 循环所有的行（由 sl +1 层的激活单元数决定），循环 i 则循环所有的列，由该层（ sl 层）的激活单元数所决定。即：hθ(x) 与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。 8.反向传播算法反向传播算法介绍在计算神经网络预测结果的时候采用了正向传播的算法，即从第一层开始向正向一层的神经元一层一层进行计算，知道最后一层的hθ(x). 为了计算代价函数的偏导数${\delta\over\delta\theta_{ij}^{(i)}} J(\theta)$ , 现在需要用到反向传播算法，以极具是首先计算最后一层的误差，然后再一层一层的反向求出各层的误差，直到倒数第二层。下面举例说明反向传播算法： 假设我们的训练集只有一个实例(x^(1), y^(1)), 我们的神经网络是一个四层的神经网络，其中: K = 4, SL = 4, L = 4 其前向传播算法为: 反向传播算法推导过程 即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。 9. 模型构建常用技巧1.梯度检验 2. 随机初始化任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的， 会导致梯度消失的情况。我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：Theta1 = rand(10, 11) * (2*eps) – ep 10. 梯度小时和梯度爆照(1)简介梯度消失与梯度爆炸 层数比较多的神经网络模型在训练的时候会出现梯度消失(gradient vanishing problem)和梯度爆炸(gradient exploding problem)问题。梯度消失问题和梯度爆炸问题一般会随着网络层数的增加变得越来越明显。 例如，对于图1所示的含有3个隐藏层的神经网络，梯度消失问题发生时，靠近输出层的hidden layer 3的权值更新相对正常，但是靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。梯度爆炸的情况是：当初始的权值过大，靠近输入层的hidden layer 1的权值变化比靠近输出层的hidden layer 3的权值变化更快，就会引起梯度爆炸的问题。 (2)梯度不稳定问题 在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题。 梯度不稳定的原因：前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸。 (3)产生梯度消失的根本原因 我们以图2的反向传播为例，假设每一层只有一个神经元且对于每一层都可以用公式1表示，其中σ为sigmoid函数，C表示的是代价函数，前一层的输出和后一层的输入关系如公式1所示。我们可以推导出公式2。 图2：简单的深度神经网络 而sigmoid函数的导数σ’(x)如图3所示。 图3：sigmoid函数导数图像 可见，σ’(x)的最大值为 1/4，而我们一般会使用标准方法来初始化网络权重，即使用一个均值为0标准差为1的高斯分布。因此，初始化的网络权值通常都小于1，从而有 |σ’(z)w &lt;= 1/4|。对于2式的链式求导，层数越多，求导结果越小，最终导致梯度消失的情况出现。 (4)产生梯度爆炸的根本原因 当，也就是w比较大的情况。则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题。 (5)如何解决梯度消失和梯度爆炸 梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑以下三种方案解决： 用ReLU、Leaky ReLU、PReLU、RReLU、Maxout等替代sigmoid函数。 用Batch Normalization。 LSTM的结构设计也可以改善RNN中的梯度消失问题。 11.构建神经网络的综合步骤网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。第一层的单元数即我们训练集的特征数量。最后一层的单元数是我们训练集的结果的类的数量。如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。我们真正要决定的是隐藏层的层数和每个中间层的单元数。训练神经网络： 参数的随机初始化 利用正向传播方法计算所有的hθ(x) 编写计算代价函数 J 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数 一句话总结神经网络我认为神经网络就是通过各层神经元将变量的重新计算， 导致特征的维度被大大放大， 总之经过神经网络瞎搞之后，特征已不是简单的特征，而原理从根本还是逻辑回归，当特征数量较多，而且模型不能满足线型要求， 可以考虑使用神经网络]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 逻辑回归]]></title>
    <url>%2F2018%2F11%2F03%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归作用可以做概率预测，也可用于分类，仅能用于线性问题。通过计算真实值与预测值的概率，然后变换成损失函数，求损失函数最小值来计算模型参数，从而得出模型。 sklearn调用接口1class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, *random_state=None*, solver=’liblinear‘, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1) 参数** =&gt; penalty : str, ‘l1’ or ‘l2’ LogisticRegression和LogisticRegressionCV默认就带了正则化项。penalty参数可选择的值为”l1”和”l2”，分别对应L1的正则化和L2的正则化，默认是L2的正则化。 在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。 另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。 penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。 但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。 =&gt; dual : bool 对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False =&gt; tol : float, optional 迭代终止判据的误差范围。 =&gt; C : float, default: 1.0 C为正则化系数λ的倒数，通常默认为1。设置越小则对应越强的正则化。 =&gt; fit_intercept : bool, default: True 是否存在截距，默认存在 =&gt; intercept_scaling : float, default 1. 仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。 =&gt; class_weight : dict or ‘balanced’, default: None class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重， 或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。 如果class_weight选择**balanced**，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低；样本量越少，则权重越高。 当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y)) n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3] 0,1分别出现2次和三次 那么**class_weight**有什么作用呢？ ​ 在分类模型中，我们经常会遇到两类问题： ​ 第一种是误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。 ​ 第二种是样本是高度失衡的，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。 这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。 =&gt; random_state : int, RandomState instance or None, optional, default: None 随机数种子，默认为无，仅在正则化优化算法为sag,liblinear时有用。 =&gt; solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} solver参数决定了我们对逻辑回归损失函数的优化方法，有4种算法可以选择，分别是： a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候，SAG是一种线性收敛算法，这个速度远比SGD快。 从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear则既可以用L1正则化也可以用L2正则化。 同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了 =&gt; max_iter : int, optional 仅在正则化优化算法为newton-cg, sag and lbfgs 才有用，算法收敛的最大迭代次数。 =&gt; multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’ OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。 其他类的分类模型获得以此类推。 ​ 而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归， 得到模型参数。我们一共需要T(T-1)/2次分类。 可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。 但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。 =&gt; verbose : int, default: 0 =&gt; warm_start : bool, default: False =&gt; n_jobs : int, default: 1 如果multi_class =’ovr’“，并行数等于CPU内核数量。当“solver”设置为“liblinear”时，无论是否指定“multi_class”，该参数将被忽略。如果给定值-1，则使用所有内核。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[过拟合、正则化(Regularization)]]></title>
    <url>%2F2018%2F11%2F02%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.过拟合问题过拟合问题描述常用的机器学习优化算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(over-fitting)的问题，可能会导致它们效果很差。使用正则化技术可以减少过拟合问题。如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。下图是一个回归问题的例子： 第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。分类问题中也存在这样的问题： 就以多项式理解， 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 如果我们发现了过拟合问题，应该如何处理？ 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA） 正则化。 保留所有的特征，但是减少参数的大小（magnitude）。 2.代价函数上面的回归问题中如果我们的模型是：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3 + θ_4x_4^4$ 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数 θ 的值，这就是正则化的基本方法。 我们决定要减少θ3和 θ4的大小，我们要做的便是修改代价函数，在其中 θ3和 θ4 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 θ3和θ4。 修改后的代价函数如下： $min_θ {1\over 2m}[h_θ(x^{(i)} - y^{(i)})^2 + 1000θ_3^2 + 10000θ_4^2]$ 通过这样的代价函数选择出的 θ3和 θ4 对预测结果的影响就比之前要小许多， 因为为了θ3和θ4的系数很大，为了优化代价函数， 其值一定会特别的小。 假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设： $J_θ = {1\over 2m}[\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)}) + λ\sum_{j=1}^nθ_j^2]$其中又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对 θ0 进行惩罚。 经过正则化处理的模型与原模型的可能对比如下图所示： 如果选择的正则化参数λ过大，则会把所有的参数都最小化了，导致模型变成 $h_θ(x) = θ_0$，也就是上图中红色直线所示的情况，造成欠拟合。 那为什么增加的一项 $λ\sum_{j=1}^nθ_j^2$ 可以使的值减小呢？ 因为如果我们令 λ的值很大的话，为了使Cost Function 尽可能的小，所有的 θ 的值（不包括θ0） 都会在一定程度上减小。 但若λ的值太大了，那么不包括θ0）都会趋近于0，这样我们所得到的只能是一条平行于轴的直线。 所以对于正则化，我们要取一个合理的 λ 的值，这样才能更好的应用正则化。 回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。 3.正则化线性回归对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。正则化线性回归的代价函数为： $J_{(θ)} = {1\over 2m} \sum_{i=1}^m[(h_θ(x^{(i)})-y^{(i)})^2 + λ\sum_{j=1}^nθ_j^2]$ 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对θ进行正则化，所以梯度下降算法将分两种情形： 4.正规方程逻辑回归针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数J(θ)，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数J(θ)。 用Python实现 123456789import numpy as npdef costReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X*theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X*theta.T))) reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2)) return np.sum(first ‐ second) / (len(X)) + reg 要最小化该代价函数，通过求导，得出梯度下降算法为： 注：看上去同线性回归一样，但是知道 $h_θ(x) = g(θ^TX)$ ，所以与线性回归不同。 Octave 中，我们依旧可以用fminuc 函数来求解代价函数最小化的参数，值得注意的是θ0参数的更新规则与其他情况不同。 注意： 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者hθ(x)的不同所以还是有很大差别。 θ不参与其中的任何一个正则化。 5.其他防止过拟合的方法DropoutDropout是在深度学习中降低过拟合风险的常见方法，它是由大牛Hinton老爷子提出的。老爷子认为在神经网络产生过拟合主要是因为神经元之间的协同作用产生的。因此老爷子在神经网络进行训练的时候，让部分神经元失活，这样就阻断了部分神经元之间的协同作用，从而强制要求一个神经元和随机挑选出的神经元共同进行工作，减轻了部分神经元之间的联合适应性。Dropout的具体流程如下： 1.对 l 层第 j 个神经元产生一个随机数 $r^{(l)}_j∼Bernouli(p)$ 2.将 l 层第 j 个神经元的输入乘上产生的随机数作为这个神经元新的输入：$x^{(l)∗}_j=x^{(l)}_j∗r^{(l)}_j$ 3.此时第 l 层第 j 个神经元的输出为：$y{(l+1)}j=f(∑^k{j=1}(w^{(l+1)}_j ∗ x^{(l)∗}_j + b^{(l+1)}))$其中，k为第l层神经元的个数，f为该神经元的激活函数，b为偏置，w为权重向量。注意：当我们采用了Dropout之后再训练结束之后，应当将网络的权重乘上概率p得到测试网络的权重，或者可以在训练时，将样本乘上 1/P。 提前终止在对模型进行训练时，我们可以将我们的数据集分为三个部分，训练集、验证集、测试集。我们在训练的过程中，可以每隔一定量的step，使用验证集对训练的模型进行预测，一般来说，模型在训练集和验证集的损失变化如下图所示： 可以看出，模型在验证集上的误差在一开始是随着训练集的误差的下降而下降的。当超过一定训练步数后，模型在训练集上的误差虽然还在下降，但是在验证集上的误差却不在下降了。此时我们的模型就过拟合了。因此我们可以观察我们训练模型在验证集上的误差，一旦当验证集的误差不再下降时，我们就可以提前终止我们训练的模型。 增加样本量在实际的项目中，你会发现，上面讲述的那些技巧虽然都可以减轻过拟合的风险，但是却都比不上增加样本量来的更实在。 为什么增加样本可以减轻过拟合的风险呢？这个要从过拟合是啥来说。过拟合可以理解为我们的模型对样本量学习的太好了，把一些样本的特殊的特征当做是所有样本都具有的特征。举个简单的例子，当我们模型去训练如何判断一个东西是不是叶子时，我们样本中叶子如果都是锯齿状的话，如果模型产生过拟合了，会认为叶子都是锯齿状的，而不是锯齿状的就不是叶子了。如果此时我们把不是锯齿状的叶子数据增加进来，此时我们的模型就不会再过拟合了。因此其实上述的那些技巧虽然有用，但是在实际项目中，你会发现，其实大多数情况都比不上增加样本数据来的实在。]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 线型回归]]></title>
    <url>%2F2018%2F10%2F31%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[简单线性回归线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。 使用sklearn.linear_model.LinearRegression进行线性回归sklearn对Data Mining的各类算法已经有了较好的封装，基本可以使用fit、predict、score来训练、评价模型，并使用模型进行预测，一个简单的例子如下： 123456789from sklearn import linear_modelclf = linear_model.LinearRegression()X = [[0,0],[1,1],[2,2]]y = [0,1,2]clf.fit(X,y)print(clf.coef_)[ 0.5 0.5]print(clf.intercept_)1.11022302463e-16 LinearRegression已经实现了多元线性回归模型，当然，也可以用来计算一元线性模型，通过使用list[list]传递数据就行。下面是LinearRegression的具体说明。 使用方法实例化sklearn一直秉承着简洁为美得思想设计着估计器，实例化的方式很简单，使用clf = LinearRegression()就可以完成，但是仍然推荐看一下几个可能会用到的参数： fit_intercept：是否存在截距，默认存在 normalize：标准化开关，默认关闭 还有一些参数感觉不是太有用，就不再说明了，可以去官网文档中查看。 回归其实在上面的例子中已经使用了fit进行回归计算了，使用的方法也是相当的简单。 fit(X,y,sample_weight=None)：X,y以矩阵的方式传入，而sample_weight则是每条测试数据的权重，同样以array格式传入。 predict(X)：预测方法，将返回预测值y_pred score(X,y,sample_weight=None)：评分函数，将返回一个小于1的得分，可能会小于0 方程LinearRegression将方程分为两个部分存放，coef_存放回归系数，intercept_则存放截距，因此要查看方程，就是查看这两个变量的取值。 多项式回归其实，多项式就是多元回归的一个变种，只不过是原来需要传入的是X向量，而多项式则只要一个x值就行。通过将x扩展为指定阶数的向量，就可以使用LinearRegression进行回归了。sklearn已经提供了扩展的方法——sklearn.preprocessing.PolynomialFeatures。利用这个类可以轻松的将x扩展为X向量，下面是它的使用方法： 123456789&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; X_train = [[1],[2],[3],[4]]&gt;&gt;&gt; quadratic_featurizer = PolynomialFeatures(degree=2)&gt;&gt;&gt; X_train_quadratic = quadratic_featurizer.fit_transform(X_train)&gt;&gt;&gt; print(X_train_quadratic)[[ 1 1 1] [ 1 2 4] [ 1 3 9] [ 1 4 16]] 经过以上处理，就可以使用LinearRegression进行回归计算了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
        <tag>线型回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归 & 分类问题]]></title>
    <url>%2F2018%2F10%2F31%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1. 分类问题在分类问题中，如果想要预测的变量y是离散的值，那么可以只用一种叫做逻辑回归(logistic regression)的算法,这是在目前最流行的一种学习算法之一。 在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。 我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈0,1 ，其中 0 表示负向类，1 表示正向类。 线型回归不适合分类问题如果我们要用线性回归算法来解决一个分类问题，对于分类， y 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 y 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。 逻辑回归算法逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y 取值离散的情况，如：1 0 0 1。 2. 逻辑回归表达式在分类问题中，要用什么样的函数来表示我们的假设。在解决分类问题的时候，我们希望分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。 为什么线型回归不适合分类问题?回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线： 根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测： 当 $h_θ(x) &gt;= 0.5$ 时, 预测y= 1; 当$h_θ(x) &lt; 0.5$ 时，预测y = 0 对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。但是， 假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。 这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了， 所以使用线型回归模型，很难确定断定一个问题分类的边界。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决分类这样的问题。 逻辑回归模型我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是： $h_θ(x) = g(θ^TX)$ 函数g的表达式为: $g(z) = {1\over1+e^{-z}}$ 其中: X 代表特征向量, g代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function） g(z) 的函数图像为: 对逻辑回归模型理解 $h_θ(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算出输出的变量=1 的可能性, 即$h_θ(x) =P(y=1|x; θ)$ 例如, 如果对于给定的x， 通过已经确定的参数计算出$h_θ(x) = 0.7$ ， 则表示有70%的几率y为正向类，响应地y为负向类的几率为1-0.7=0.3 g(z) （Sigmoid function）函数的作用就是将$θ^TX$的结果映射到0-1之间， 3. 决策边界对决策边界的理解 决策边界(decision boundary)，能够更好地帮助我们理解逻辑回归的假设函数中计算什么 在逻辑回归中， 我们预测到: 当$h_θ(x) &gt;= 0.5$ 时， 预测 y= 1; 当 $h_θ(x) &gt;= 0.5$ 时，预测 y = 0； 根据上面绘制的S形函数图像，我们知道当 z = 0 时, g(z) = 0.5 z &gt; 时, g(z) &gt; 0.5 z &lt; 0 时, g(z) &lt; 0.5 又 $z = θ^Tx$, 即: $θ^TX &gt;= 0$ 时, 预测 y = 1 , $θ^TX &lt; 0$ 时， 预测 y = 0 现在假设我们有一个模型: 并且参数θ是向量[-3 1 1]. 则当 -3 + x1 + x2 &gt;= 0, 即x1 + x2 &gt;= 3时， 模型将预测 y = 1. 我们可以绘制直线x1 + x2 = 3, 这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。 复杂形状的决策边界假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？ 因为需要用曲线才能分隔 y = 0 的区域和 y = 1 的区域，我们需要二次方特征： 所以按照假设函数应该趋近， $h_θ(x) = g(θ_0 + θ_1x_1 + θ_2x_2 + θ_1x_1^2 + θ_2x_2^2 )$ θ最终结果趋近 [-1 0 0 1 1], 我们最终得到的判定边界恰好是圆点在原点且半径为1的圆形 , 综上我们可以用复杂的模型来适应飞象复杂形状的判定边界 4. 逻辑回归代价函数和梯度下降逻辑回归代价函数及简化对于线性回归模型，我们定义的代价函数是所有模型误差的平方和和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_θ(x) = g(θ^TX)$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）如下图，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 线型回归代价函数为: $J_{(θ)} = {1\over m} \sum_{i=1}^m{1\over2}(h_θ(x^{(i)})-y^{(i)})^2​$ 重新定义逻辑回归代价函数，使代价函数在逻辑回归算法中为凸函数, 过程如下: 根据推倒可以证明, 我们推倒的的代价值函数会给我们一个凸优化问题。代价函数 J(θ) 会是一个凸函数，并且没有局部最优值。 python代码实现 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X* theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X* theta.T))) return np.sum(first ‐ second) / (len(X)) 梯度下降算法推倒及简化在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为： Repeat { $θ_j : =θ_j − α{∂\over∂θ_j }J(θ)$ (simultaneously update all ) } 求导后得到: Repeat { $θ_j : =θj − α{1\over m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update all ) } 注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的 $h_θ(x) = g(θ^TX)$ 与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。 5. 高级优化算法一些高级算法的介绍现在我们换个角度来看，什么是梯度下降，我们有个代价函数 J(θ)，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数θ时，它们会计算出两样东西：J(θ) 以及 J 等于 0、1直到 n 时的偏导数项。 假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。 另一种考虑梯度下降的思路是：我们需要写出代码来计算 J(θ) 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。 对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数J(θ)。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些 J(θ)的收敛性，那么我们就需要自己编写代码来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$。所以，在写完能够计算这两者的代码之后，我们就可以使用梯度下降。 然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轭梯度法、 BFGS (变尺度法)、 L-BFGS (限制变尺度法) 就是其中一些更高级的优化算法，它们需要有一种方法来计算 J(θ)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。 这三种算法的优点： 一个是使用这其中任何一个算法，你通常不需要手动选择学习率 α ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 α ，并自动选择一个好的学习速率 α，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。 Octave 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，因此，如果你正在你的机器学习程序中使用一种不同的语言，比如如果你正在使用C、C++、Java等等，你可能会想尝试一些不同的库，以确保你找到一个能很好实现这些算法的库。 如何使用这些算法比方说，你有一个含两个参数的问题，这两个参数是θ1和θ2，因此，通过这个代价函数，你可以得到 θ1 和 θ2 的值，如果你将 J(θ) 最小化的话，那么它的最小值将是 θ1 = 5 ，θ2 = 5 。代价函数 J(θ) 的导数推出来就是这两个表达式： $α{∂\over∂θ_1}J(θ) = 2(θ_1 - 5)$ $α{∂\over∂θ_2}J(θ) = 2(θ_2 - 5)$ 如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的Octave 函数： 123456function [jVal, gradient]=costFunction(theta) jVal=(theta(1)‐5)^2+(theta(2)‐5)^2; gradient=zeros(2,1); gradient(1)=2*(theta(1)‐5); gradient(2)=2*(theta(2)‐5);end 这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个costFunction 函数后，你就可以调用高级的优化函数，这个函数叫fminunc，它表示Octave 里无约束最小化函数。调用它的方式如下： 123options=optimset('GradObj','on','MaxIter',100);initialTheta=zeros(2,1);[optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options); 你要设置几个options，这个 options 变量作为一个数据结构可以存储你想要的options，所以 GradObj 和On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说100，我们给出一个 的猜测初始值，它是一个2×1的向量，那么这个命令就调用fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的值。 实际运行过程示例 6. 多分类问题多分类的介绍一些多分类的例子: 例一: 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用 y=1, y=2, y=3, y=4 来表示 例二: 如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用 y=1 这个类别来代表；或者患了感冒，用 y=2 来代表；或者得了流感用 y=3 来代表. 然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样： 对于一个多类分类问题，我们的数据集或许看起来像这样： 一对于多分类思路我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为“一对余”方法。 现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 y = 1 ，方框表示 y = 2，叉叉表示 y = 3。下面要做的就是使用一个训练集，将其分成3个二元分类问题。我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。 这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。 为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作$h_θ^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作$h_θ^{(2)}(x)$ ,依此类推。 最后我们得到一系列的模型简记为：$h_θ^{(i)}(x) = P(y=i|x; θ)$ 其中：i = (1, 2, 3, 4, 5, …, k) k为总分类数量最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_θ^{(i)}(x)$ ， 其中 i 对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 x ，然后我们选择一个让 $h_θ^{(i)}(x)$ 最大的 i 作为最终的分类结果。现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论值是多少，我们都有最高的概率值，我们预测就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Octave基础操作]]></title>
    <url>%2F2018%2F10%2F26%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2Foctave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[基础功能命令修改命令行的提示1PS1('&gt;&gt; ') % &gt;&gt; 就是修改后的提示符号 变量赋值语句1A = 5 + 6 ; % 后面加上一个可以不在终端打印变量的值 显示工作空间的所有变量12who % 显示工作空间的所有变量whos % 显示工作空间的所有变量和详细信息 删除变量12clear A % 删除变量Aclear % 删除所有变量 打印变量12A % 直接再终端输入变量名称就可以将变量的值打印出来disp(A) % 通过disp函数将变量打印出来 修改全局的输出内容的长短12format long % 将输出数值的长度定义为long类型format short % 将输出数值的长度定义为short类型 查看命令的帮助信息123helo randhelp eyehelp help 添加搜索路径1addpath path % 添加路径到函数和数据等的某人搜索路径 基础运算数值运算13-2； 5*8； 1/2； % 基础运算 逻辑运算12341 &amp;&amp; 0 % 逻辑与1 || 0 % 逻辑或~ 1 % 逻辑费XOR(1, 0) % 异或运算 判断语句121 == 2 % 相等判断1 ~= 2 % 不等于判断 矩阵运算创建矩阵1234567891011A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵v = [ 1 2 3 ] % 创建一个行向量v = 1:6 % 创建一个从1到6的行向量v = 1:0.1:2 % 创建一个从1开始，以0.1为步长，直到2的行向量v = ones(2, 3) % 创建一个2行3列的元素都是1的矩阵v = 2 * ones(2, 3) % 创建一个2行3列的元素都是2的矩阵v = zeros(2, 3) % 创建一个2行3列的元素都是0的矩阵v = rand(2, 3) % 创建一个2行3列的元素都是0-1之间的随机数的矩阵v = randn(2, 3) % 创建一个2行3列的元素都符合平局值为0,1为标准差的正态分布随机数的矩阵I = eye(6) % 创建一个大小为6的单位矩阵v = type(3) % 返回一个3*3的随机矩阵 矩阵运算12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵B = [ 11, 12; 13 14; 15 16 ]2 * A % A矩阵中的每个元素都乘以2A .* B % 将A矩阵和B矩阵中的每一个对应位置的元素相乘,同样适用于+ - / ^等A .* % A矩阵的每个元素取二次方log(v) % 矩阵的每个院对对数运算exp(v) % 矩阵的每个元素进行以为底，以这些元素为幂的运算abs(v) % 对v矩阵的每个元素取绝对值A + 1 % 将A矩阵的每个元素加上1A&apos; % 取A矩阵的转置矩阵 获取矩阵尺寸12345A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵size(A) % 返回A矩阵的尺寸,返回的内容同样是行向量size(A，1) % 返回矩阵的行数size(A，2) % 返回矩阵的列数lengh(A) % 返回A矩阵的维度大小，即矩阵的长宽中比较大的维度，矩阵A的length即为3 矩阵的索引12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵A(3, 2) % 取A矩阵的第三行第二列的元素A(2, :) % 返回第二行的所有元素A(:, 2) % 返回第二列的所有元素A([1 3], :) % 取第1行和第3行的所有元素A(:, 2) = [10; 11; 12] % 将矩阵的第二列重新赋值A = [A, [100, 101, 102]] % 给矩阵附加了一列矩阵，给矩阵增加一列。带，和不带，相同[A B] % 两个矩阵直接连在一起，矩阵A在左边，矩阵B在右边[A;B] % 两个矩阵直接连在一起，矩阵A在上边, 矩阵B在下边A(:) % 将矩阵的所有元素导向一个单独的列向量排列起来 矩阵的计算123456789101112131415A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵val = max(a) % 返回a矩阵中的最大值, 当a为多维矩阵的时候返回的是每一列的最大值val = max(A, a) % 两个矩阵所有元素逐个比较返回较大的值max(A,[],1) % 得到矩阵每一列元素的最大值max(A,[],2) % 得到矩阵每一行元素的最大值[val, ind] = max(a) % 返回a矩阵中的最大值和对应的索引sum（a) % 把a矩阵的所有元素相加起来, 如果是多维矩阵默认为每一列的总和sum（a,1) % 求多维矩阵每一列的总和sum(a, 3) % 求多维矩阵每一行的总和a&lt;3 % 对矩阵每一个元素进行&lt;3判断，判断为真返回1, 否则返回0find(a&lt;3) % 返回哪些元素小于3(是索引值)prod（a) % 将a矩阵的所有元素相乘floor(a) % 将a矩阵的所有元素进行向下取舍ceil(a) % 将a矩阵的所有元素进行向上取整pinv(v) % 求v矩阵的逆矩阵 绘制图像绘制直方图123w = -6 + sqrt(10) * (randn(1, 10000))hist(w) % 绘制w矩阵的直方图hist(w, 50) % 绘制w矩阵的直方图，并指定50个长方形 绘制曲线图1234567891011121314t = [0:0.1:1];y1 = sin(2*pi*4t);plot(t, y1);hold on % 新绘制的图不会覆盖之前的图片，会在之前的图片上生成新的曲线y2 = cos(2*pi*4*t)plot(t, y2, &apos;r&apos;) % 绘制新的直线图，r表示线的颜色为红色xlable(&apos;time&apos;) % 添加x轴名称ylable(&apos;value&apos;) % 给y轴添加名称legend(&apos;sin, &apos;cos&apos;) % 给线命名title(&apos;myplot&apos;) % 给图片一个标题名称print -dpng &apos;myplot.png&apos; % 输出图片plot clos % 关掉图片axis([0.5 1 ‐1 1]) % 改变坐标返回，前两个参数控制横坐标，后两参数控制纵坐标Clf % 清除一个图像 在一张画纸上绘制两张直线图123456789t = [0:0.1:1];y1 = sin(2*pi*4t);y2 = cos(2*pi*4*t)；figure(1); plot(t, y1); % 绘制第一张图片figure(2); plot(t, y2); % 绘制第二张图片subplot(1,2,1) %将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。plot(t,y1) % 将图片绘制到第一个格子suplot(1,2,2) % 使用第二个格子plot(t,y2) % 将图片绘制到第二个格子 彩色格图绘制12imagesc(A ) % 绘制彩色格子图imagesc(A)，colorbar，colormap gray % 生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。 移动数据导入数据1load('featureD.dat') % 加载featureD.dat中的所有数据，并将其复制给变量featureD 导出数据12save hello.mat v % 将变量A导出为一个叫hello.mat文件 二进制形式save hello.mat v -ascii % 将变量A导出为一个叫hello.mat文件 ascii形式 控制语句for循环1234v = zeros(10,1)for i=1:10, v(i) = 2^1;end while 循环123456v = zeros(10,1)i = 1while i &lt;= 5, v(i) = 100; i = i+1;end if - else - elif 语句12345678v = zeros(10,1)if v(1) == 1, disp('1');elseif v(1) == 2, disp('2');else disp('3');end 自定义函数定义函数1.先创建一个文件​ squarethisnumber.m # .m前定义的就是函数名2.编写函数文件 12function y = squareThisNumber(x)y = x^2; 第一行写着 function y = squareThisNumber(x) ，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 里。另外，它告诉了Octave这个函数有一个参数，就是参数 ，还有定义的函数体，y = x^2 使用自定义函数1.切换到函数文件所在目录2.直接通过函数名squareThisNumber() 调用函数]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线型回归 & 梯度下降]]></title>
    <url>%2F2018%2F10%2F20%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.模型表示问题的概述 在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。 模型引入假使我们回归问题的训练集（Training Set）如下表所示： 我们将要用来描述这个回归问题的标记如下:m 代表训练集中实例的数量x 代表特征/输入变量y 代表目标变量/输出变量(x, y) 代表训练集中的实例(x^i, y^i)代 表第 个观察实例h 代表学习算法的解决方案或函数也称为假设（hypothesis） 这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。 h代表hypothesis(假设)， 表h示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h根据输入的 值来得出 y 值， y 值对应房子的价格因此， h 是一个从 x 到 y 的函数映射。 我将选择最初的使用规则代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 h？一种可能的表达方式为：hθ(x)=θ0+θ1∗x ，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。 2.代价函数什么是代价函数 在线性回归中我们有一个像这样的训练集， m代表了训练样本的数量 。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：hθ(x)=θ0+θ1∗x 。我们通过训练获得的预测函数在预测时必然会和实际值有一些偏差，下面就给出了这部分预测偏差的定义 代价函数的定义: 我们选择的参数决定了我们得到的模型相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）,下图蓝色线段变为预测和实际的误差。 平方误差代价函数: 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。 怎么优化线型回归模型 优化回归模型的目标: 便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数最小。 代价函数公式: 代价函数坐标图 可以看出在三维空间中存在一个使得J(θ_0,θ_1)代价函数最小的点， 当代价函数取得最小值的时候，我们会获得最符合训练集的预测模型。 3.梯度下降算法算法介绍 梯度下降是一个用来求函数最小值的算法。其思想是利用迭代的思想来逼近最低点，沿着梯度的负方向更新模型权重。我们将使用梯度下降算法来求出代价函数 J(θ_0,θ_1) 的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0, θ1, …., θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（globalminimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 梯度下降算法公式:公式介绍 repeat until convergence{$$θ_j=θ_j−α∂/∂θ_j J(θ0,θ1) (for j=0 and j=1)$$} 参数α: 学习率, 它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。当到达损失函数的最低点的时候，其导数为0，已经到达了局部最优点，即θ不会再发生改变 注意: 在梯度下降算法中，一般都会采用同步更新的方式来优化模型, 即上面公式的θj同步更新。同步更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。 学习率的选择对算法的影响 学习率过小的影响: 则达到收敛所需的迭代次数会非常高 学习率过大的影响: 每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛 怎么确定模型是否收敛 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛, 如上图在300步之后基本接近收敛。 也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较 梯度下降算法分类批量梯度下降 在上面的线性回归场景中的梯度下降算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。 批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点： （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 随机梯度下降公式: 随机梯度下降指在梯度下降的每一步中只用到了训练集的一个样本或者一部分样本，最后得到的可能是全局最优解，也可能是局部最优解。 优点： （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。缺点： （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。 （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。 （3）不易于并行实现。 4.梯度下降线型回归模型单变量线型回归梯度下降梯度下降、线型回归算法比较 单变量梯度下降公式代价函数计算 参数θ的计算 多变量线型回归梯度下降多变量特征现在对回归模型增加更多的特征，例如房价预测问题，增加房间数楼层等，构成一个含有多个变量的模型，模型中的特征如下图。 增添更多特征后，引入一系列新的注释： n 代表特征的数量 x^(i)^ 代表第i个训练实例，是特征矩阵的第i行，是一个向量(vector). x~j~^(i)^ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征 如: $x_2^{(2)} = 3, x_3^{(2)} = 2$ 支持多个变量的h表示为: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 这个公式中有个n+1参数和n个变量，为了使得公式能够简化一些，引入$x_0=1$，则公式转化为:$h_θ(x) = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m* (n+1), 因此变量的假设公式可以表示为: $h_θ(x) = θ^TX$ 多变量梯度下降公式与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即： $J_{(θ_0, .., θn)} = {1\over2m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 , 其中: h_θ(x) = θ^TX = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度梯度下降下降公式： 求导数后得到: 5.特征和多项式回归特征选择 有的时候的们的特征并不是很好的可以直接用来训练模型，比如房子价格预测模型的两个特征 特征: x1 房子的临街宽度， x2 房子的纵向深度 此时很明显房子的价格用 特征一和特征二的乘积，即房子的占地面积来预测更合适 $x=x_1 * x_2 = area (面积)$ 多项式回归 很多时候，我们并没有那么幸运，正好所有的数据之间都是线性关系，能用一条直线，一个平面就描述地出来，大多数时候，我们可能需要画成曲线、曲面或者曲啥更高维度的东西 比如下图数据集，线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，用二次方程（蓝色的线）和三次方程（绿色的线）来拟合这些点，但是二次方程有个缺点，到了后面，反而会下降，这和我们的认知不符合，而三次方程不会遇到这个问题，和二次方程相加又会和上升的部分相互抵消一部分，所以最后选择了三次方程加上二次方程。 二次方模型公式：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2$ 三次方模型公式: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3$ 通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以把多项式回归看成是另类的多变量线性回归，我们可以令：$x_2 = x_2^2$ , $x_3 = x_3^3$ , 从而将模型转化为线型回归模型 注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要, 因为比如x的范围是1-100的话，$x^2$的范围就是1-10000，$x^3$的范围就是1-1000000。 6.特征缩放为什么要特征缩放 在使用梯度下降的时候会遇到因为特征大小不统一，导致收敛速度过慢的问题，在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。 以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。 解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。 特征缩放的两种方法线型归一化 原理： 通过对原始数据进行变换把数据映射到(默认为[0,1])之间 公式 归一化的弊端 使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景 特征标准化 原理： 通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内 公式： 标准化的有点 如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。 7.正规方程线型回归正规方程算法介绍 对于线型回归问题经常都是在使用梯度下降算法，但是对于某些线型回归问题，正规方程是更好的解决办法。如下面的代价函数: 梯度下降和正规方程比较 梯度下降 正规方程 需要选择学习率 不需要 需要多次迭代 一次运算得出 当特征数量n特别大时能比较适用 需要计算$(X^TX)^{-1}$如果特征数量n较大则运算代价大因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型等其他模型 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习介绍]]></title>
    <url>%2F2018%2F10%2F19%2F10.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%9F%BA%E7%A1%80%E5%86%85%E5%AE%B9%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[机器学习的发展 机器学习被应用到越来越多的领域这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。 再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。 手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。 一个比较好的机器学习定义 一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升 类比于下棋，经验E 就是程序上万次的自我练习的经验而任务T 就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。 机器学习基本算法最常用的两个算法监督学习算法 我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，即我们可以根据正确答案来不断的优化我们的模型。 监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。 无监督学习算法 根据未知的训练样本(未被标记分类), 从数据中找出不同的结构，将数据识别为不同的聚集簇。 无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？ 还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。 监督学习算法常见问题回归问题 通过像预测房子的价格一样，通过回归模型通过输入值来推出一个连续的输出值 分类问题 通过数学模型来判断输入数据所属的类别，可以是二类别问题（是/不是），也可以是多类别问题（在多个类别中判断输入数据具体属于哪一个类别） 无监督学习常年问题聚类问题 依据研究对象（样品或指标）的特征，将其分为不同的分类。 ###]]></content>
      <categories>
        <category>机器学习</category>
        <category>基础内容</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Registry 私有库]]></title>
    <url>%2F2018%2F10%2F17%2F11.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2F%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.关于Registry仓库官方的Docker hub是一个用于管理公共镜像的好地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景需要我们拥有一个私有的镜像仓库用于管理我们自己的镜像。这个可以通过开源软件Registry来达成目的。 Registry在github上有两份代码：老代码库和新代码库。老代码是采用python编写的，存在pull和push的性能问题，出到0.9.1版本之后就标志为deprecated，不再继续开发。从2.0版本开始就到在新代码库进行开发，新代码库是采用go语言编写，修改了镜像id的生成算法、registry上镜像的保存结构，大大优化了pull和push镜像的效率。 官方在Docker hub上提供了registry的镜像（详情），我们可以直接使用该registry镜像来构建一个容器，搭建我们自己的私有仓库服务。Tag为latest的registry镜像是0.9.1版本的，我们直接采用2.1.1版本。 2.Registry的部署运行下面命令获取registry镜像 1$ sudo docker pull registry:2.1.1 然后启动一个容器 1$ sudo docker run -d -v /opt/data/registry/:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1 验证服务是否启动成功 说明我们已经启动了registry服务，打开浏览器输入http://127.0.0.1:5000/v2 3.验证向仓库中push镜像 现在我们通过将镜像push到registry来验证一下。我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库， 1$ sudo docker tag hello-world 127.0.0.1:5000/hello-world 接下来，我们运行docker push将hello-world镜像push到我们的私有仓库中， 1$ sudo docker push 127.0.0.1:5000/hello-world 1234567The push refers to a repository [127.0.0.1:5000/hello-world] (len: 1)975b84d108f1: Image successfully pushed3f12c794407e: Image successfully pushedlatest: digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b size: 2744 现在我们可以查看我们本地/opt/registry目录下已经有了刚推送上来的hello-world。我们也在浏览器中输入http://127.0.0.1:5000/v2/_catalog，如下图所示， 从镜像库中拉取镜像 现在我们可以先将我们本地的127.0.0.1:5000/hello-world和hello-world先删除掉， 123$ sudo docker rmi hello-world$ sudo docker rmi 127.0.0.1:5000/hello-world 然后使用docker pull从我们的私有仓库中获取hello-world镜像， 1$ sudo docker pull 127.0.0.1:5000/hello-world 123456789101112131415161718192021Using default tag: latestlatest: Pulling from hello-worldb901d36b6f2f: Pull complete0a6ba66e537a: Pull completeDigest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4bStatus: Downloaded newer image for 127.0.0.1:5000/hello-world:latestlienhua34@lienhua34-Compaq-Presario-CQ35-Notebook-PC ~ $ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEregistry 2.1.1 b91f745cd233 5 days ago 220.1 MBubuntu 14.04 a5a467fddcb8 6 days ago 187.9 MB127.0.0.1:5000/hello-world latest 0a6ba66e537a 2 weeks ago 960 B 4.查询镜像库查询镜像库中的镜像 1http://10.0.110.218:5000/v2/_catalog 5.错误排查错误描述 在push 到docker registry时，可能会报错： 123The push refers to a repository [192.168.1.100:5000/registry]Get https://192.168.1.100:5000/v1/_ping: http: server gave HTTP response to HTTPS client 这个问题可能是由于客户端采用https，docker registry未采用https服务所致。一种处理方式是把客户对地址“192.168.1.100:5000”请求改为http。 目前很多文章都是通过修改docker的配置文件“etc/systemconfig/docker”，重启docker来解决这个问题。但发现docker1.12.3版本并无此文件，根据网上创建此文件，并填入相应内容，重启docker无效果，仍然报此错误。 解决办法 在”/etc/docker/“目录下，创建”daemon.json“文件。在文件中写入： { “insecure-registries”:[“192.168.1.100:5000”] } 保存退出后，重启docker。]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[githubpage + hexo + yilia 搭建个人博客]]></title>
    <url>%2F2018%2F10%2F17%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2F%E5%8D%9A%E5%AE%A2%2Fgithub%2Bhexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[0.本博客的由来本来感觉写博客很费时间，但是最近感觉这两年攒手里的笔记太多了，不方便整理和分享 所以打算以后就干脆直接将笔记整理到github，这样也比自己维护一个博客省心 下面就将搭建本博客的步骤也同步上来，当做一个hello-world吧。 1.环境准备电脑环境是Windows，安装好git后，所有搭建操作均在git bash内完成 1）安装hexo(首先要安装git, node.js, npm) 注意：首次安装git 要配置user信息 123$git config --global user.name "yourname" #（yourname是git的用户名）$git config --global user.email email） 2）使用npm安装hexo 1$npm install -g hexo 3）创建hexo文件夹 12$mkdir hexo_blog$cd hexo_lobg 4）初始化框架 1234567$hexo init #hexo #会自动创建网站所需要的文件$npm install #安装依赖包$hexo generate $hexo server #现在可以用127.0.0.1:4000访问hexo默认的hello world界面 ,hexo s = hexo server 2.部署到github1）首次使用github需要配置密钥 1ssh-keygen -t rsa -C "email" 生成ssh密钥，按三次回车键，密码为空,这边会生成id_rsa和_rsa.pub文件 打开id_rsa.pub，复制全文添加到GitHub 的Add SSH key中。 2）创建Respository， 并开启githubPage 首先注册登录github,然后创建页面仓库，Repository name 命名应该是 youname.github.io 在setting界面， 配置 3）安装hexo-deployer-git 1$npm install hexo-deployer-git --save 用来推送项目到github 4）生成博客，并push到github 123$hexo generate$hexo deploy 5）验证结果 通过https://youname.github.io 进行访问 3.更换博客模板目前访问的博客模板比较简略，下面介绍使用：yilia模板 1）拉取模板文件 1$git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 2）更改配置文件修改模板为yilia 打开项目目录下的_config.yml文件，更改主题theme; theme: yilia然后配置yilia文件下的_config.yml（目录：hexo/themes/yilia/_config.yml） 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127# Headermenu: 主页: / 归档: /archives #分类: /categories #标签: /tags# SubNavsubnav: github: &quot;https://github.com/KyleAdultHub&quot; #weibo: &quot;#&quot; #rss: &quot;#&quot; #zhihu: &quot;#&quot; qq: &quot;/information&quot; #weixin: &quot;#&quot; #jianshu: &quot;#&quot; #douban: &quot;#&quot; #segmentfault: &quot;#&quot; #bilibili: &quot;#&quot; #acfun: &quot;#&quot; mail: &quot;/information&quot; #facebook: &quot;#&quot; #google: &quot;#&quot; #twitter: &quot;#&quot; #linkedin: &quot;#&quot; rss: /atom.xml# 是否需要修改 root 路径# 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，# 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。root: /# Content# 文章太长，截断按钮文字excerpt_link: more# 文章卡片右下角常驻链接，不需要请设置为falseshow_all_link: &apos;展开全文&apos;# 数学公式mathjax: false# 是否在新窗口打开链接open_in_new: false# 打赏# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏reward_type: 0# 打赏wordingreward_wording: &apos;谢谢你&apos;# 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpgalipay: # 微信二维码图片地址weixin: # 目录# 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录toc: 1# 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为falsetoc_hide_index: true# 目录为空时的提示toc_empty_wording: &apos;目录，不存在的…&apos;# 是否有快速回到顶部的按钮top: true# Miscellaneousbaidu_analytics: &apos;&apos;google_analytics: &apos;&apos;favicon: /favicon.png#你的头像urlavatar: /img/header.jpg#是否开启分享share_jia: true#评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment#不需要使用某项，直接设置值为false，或注释掉#具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/#1、多说duoshuo: false#2、网易云跟帖wangyiyun: false#3、畅言changyan_appid: *** #这个畅言id和conf写自己的changyan_conf: ***#4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的disqus: false#5、Gitmentgitment_owner: false #你的 GitHub IDgitment_repo: &apos;&apos; #存储评论的 repogitment_oauth: client_id: &apos;&apos; #client ID client_secret: &apos;&apos; #client secret# 样式定制 - 一般不需要修改，除非有很强的定制欲望…style: # 头像上面的背景颜色 header: &apos;#4d4d4d&apos; # 右滑板块背景 slider: &apos;linear-gradient(200deg,#a0cfe4,#e8c37e)&apos;# slider的设置slider: # 是否默认展开tags板块 showTags: false# 智能菜单# 如不需要，将该对应项置为false# 比如#smart_menu:# friends: falsesmart_menu: innerArchive: &apos;所有文章&apos; friends: &apos;友链&apos; aboutme: &apos;关于我&apos;friends: #友情链接1: http://localhost:4000/ aboutme: 程序猿一枚&lt;br&gt;]]></content>
      <categories>
        <category>开发工具</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2016%2F10%2F12%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式介绍什么是正则表达式 Regular Expression, 又称规则表达式。正则表达式就是用事先定义好的一些特定字符（组合），组成一个“规则字符串”，这个“规则字符串”用来描述一种字符串的匹配模式（pattern）； 正则的作用 可以用来检查一个字串是否包含某种子串、将匹配的子串替换或者取出 正则的特点 灵活性、逻辑性和功能性非常强大 正则表达式的使用方法re模块常用函数和方法 import re result_obj = re.search(正则表达式， 数据，flag=0） —-查找数据中第一个符合匹配规则的字符串 search()函数从数据中只能查找到第一个符合正则数据放到result_obj中， 如果没有匹配到想要匹配的结果会返回None result_obj.group() —-查看正则匹配的结果内容 result_obj.group(1， 2/ 组名) 返回需要组的匹配的结果，返回一个包含多个组匹配结果的元组； result_obj.group() == result_obj.group(0) == 正个正则表达式所有匹配的字符 re模块其他常用方法 compile 编译 作用 对正则表达式匹配规则进行预编译，在大量使用到正则的时候，可以提高匹配的速度 使用方法 p = re.compile(‘匹配规则’, re.DATALL) p.search(‘字符串’) 按照编译的规则对字符串进行匹配正则表达式中的特殊字符 匹配单个字符 正则表达式常用匹配方式匹配单个字符 空白字符\s == [ \f\n\r\t\v] 非空白字符 \S == [^\f\t\v\n\r] 在正则表达式中若只是想要匹配一个像特殊字符的普通字符需要在特殊字符前面加转义字符“\” 例如“.” 特殊字符在[ ]中例如：[. | * + ？等 ]没有特殊功能只代表普通字符 在[ ]中若是想使用“-”普通字符要加上转义字符\ 匹配多个字符 常用定位符 正则表达式的分组 注意: 在使用（|）的时候尽量使特殊的或者通用的变量放在前边 在引用分组的时候注意：\num表示八进制数num所表示的普通ASCII码字符，所以在引用的时候会默认表示ascii码字符，所以要注意转义或者使用原生字符串 匹配所有的汉字的方法 re.compole(r’[\u4e00-\u9fa5]’) 备注：匹配所有unicode编码的中文 正则表达式的贪婪与懒惰概念： 贪婪-尽可能多的匹配 懒惰-尽可能少的匹配默认为贪婪模式的匹配模式 在python中 +/*/{m,n}默认情况下总是贪婪的如何让贪婪模式变为懒惰模式 在量词后加上一个? 例子： 原生字符串的应用特殊字符的转义在表达式中如果包含“\”表示转义\后面的字符为八进制数字代表的ascii对应的特殊字符，在python中会对ascii包含的数字或者字符进行转义，这种情况会导致会将匹配规则的字符进行转义，结果不能匹配到想要匹配的字符串内容 解决办法： 取消转义 在每一个’\’字符前加上’\’，对”\’进行转义，这样会取消\的转义功能，将\只代表一个\字符，不会对后边的字符进行转义 ascii不包括的字符，如果前边有转义字符\，不需要加以转义，python会自动转义 原生字符串 如果在表达式或字符串前边加上r“”对字符串中的\字符自动转义 在使用的时候，匹配规则可以和想要匹配的内容写法相同，r会自动帮我们转义 示例： re.search(r’abc\nabc’, ‘abc\nabc’) 正则表达式的常见问题 如何让 . 特殊符号可以匹配所有内容（包括\n） 解决办法： 使用re.DOTALL 参数 示例： re.findall(r’abc.’, ‘abc\n\nsfgs’, re.DOTALL) 备注：也可以使用re.S 代替re.DOTALL 效果上是一样的Ascii码对应关系 在字符串，或者正则表达式中，\n\t等控制字符 或者 \数字（表示八进制的num所表示的普通ascii码）等显示字符，在应用的时候会默认为在调用ascii码的控制字符或者是显示字符，所以如果只是想表达单纯的字符 需要用\n或者r””这种形式进行转意，可以使用chr（八进制数）来查询对应的ascii字符 ascii码表]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫基础介绍]]></title>
    <url>%2F2016%2F10%2F10%2F4.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%E7%9B%B8%E5%85%B3-%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Evernote Export body, td { font-family: 微软雅黑; font-size: 10pt; } 爬虫的介绍爬虫的定义 模拟浏览器其发送请求，获取到和浏览器一模一样的数据 浏览器能够看到的，我们爬虫才能够获取到，否则是没有办法获取到的，所以，只要浏览器能做的事情，原则上，爬虫都能做爬虫获取的数据的用途呈现数据，呈现在app或者在网站上伪造网站请求，进行自动的访问网站进行数据分析，获得结论爬虫的分类通用爬虫：搜索引擎的爬虫，通常指搜索引擎的爬虫，通用网络爬虫利用种子url从互联网中搜集网页，采集信息，这些网页信息用于为搜索引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。聚焦爬虫：针对特定网站的爬虫，指定url进行爬取，有明确的爬取目标通用爬虫工作原理第一步：数据抓取首先选取一部分的种子URL，将这些URL放入待抓取URL队列；取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环....第二步：数据存储搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。第三步：预处理搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。提取文字中文分词消除噪音（比如版权声明文字、导航条、广告等……）索引处理链接关系计算特殊文件处理第四步：提供检索服务，网站排名搜索引擎在对信息进行组织和处理后，为用户提供关键字检索服务，将用户检索相关的信息展示给用户。同时会根据页面的PageRank值（链接的访问量排名）来进行网站排名，这样Rank值高的网站在搜索结果中会排名较前，当然也可以直接使用 Money 购买搜索引擎网站排名，简单粗暴。通用爬虫的局限性通用搜索引擎所返回的网页里90%的内容无用。图片、音频、视频多媒体的内容通用搜索引擎无能为力不同用户搜索的目的不全相同，但是返回内容相同聚焦爬虫的工作流程爬虫爬取哪些数据教育机构：其他教育机构的开班，招生，就业，口碑资讯公司：特定领域的新闻数据的爬虫金融公司：关于各个公司的动态的信息，酒店/旅游：携程，去哪儿的酒店价格信息/机票，景点价格，其他旅游公司价格信息房地产、高铁：10大房地产楼盘门户网站，政府动态等强生保健医药：医疗数据，价格，目前的市场的行情Robots协议：网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。这个协议之是道德层面的协议，这个协议并不能从技术上阻止去对其网站进行爬取。例如：https://www.taobao.com/robots.txtHTTP/HTTPS协议url的形式url表现形式：scheme://host[:port#]/path/…/[?query-string][#anchor]scheme：协议(例如：http, https, ftp)host：服务器的IP地址或者域名port：服务器的端口（如果是走协议默认端口，80 or 443）path：访问资源的路径query-string：参数，发送给http服务器的数据anchor：锚（跳转到网页的指定锚点位置）示例：http://localhost:4000/file/part01/1.2.htmlhttp://item.jd.com/11936238.html#product-detailHTTP/HTTPS的区别HTTP超文本传输协议 默认端口号:80HTTPSHTTP + SSL(安全套接字层)默认端口号：443HTTP和HTTPS区别浏览器默认请求服务器是以HTTP协议进行请求，如果服务器支持HTTPS协议，会返回给浏览器端一个协议相关的响应，浏览器会重新发起HTTPS协议的请求； HTTP请求报文的形式http的请求过程域名---&gt;dns（拿ip）---&gt;浏览器请求ip---&gt;服务器---&gt;返回资源HTTP常见的请求头1. Host (主机和端口号)对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的Host部分2. Connection (链接类型)keep-alive 客户端携带表示支持长连接keep-alive 服务器端如果回复keep-alive， 代表允许双方建立长连接close： 服务器端回复close，代表不允许建立长连接，浏览器接收到响应后会主动断开连接3. Upgrade-Insecure-Requests (浏览器支持升级为HTTPS请求)升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。4. User-Agent (浏览器名称)对方的服务器通过User-Agent判断出来我们是一个手机版的浏览器还是电脑版的，同时能够判断出来浏览器的平台，型号，版本，内核版本5. Accept (传输文件类型)Accept: */*：表示什么都可以接收。Accept：image/gif：表明客户端希望接受GIF图像格式的资源；Accept：text/html：表明客户端希望接受html文本。Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。text：用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；application用于传输应用程序数据或者二进制数据。6. Referer (页面跳转处)表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。防盗链：有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载。7. Accept-Encoding（文件编解码格式）指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=08. Cookie （Cookie）cookies 能够记录用户信息，会在请求的时候一起传递给对方的服务器，对方的服务器能够根据cookie判断出来是否登陆过（用户的状态）9. x-requested-with :XMLHttpRequest (是Ajax 异步请求)10.Accept-Language (接受语言)指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。11.Accept-Charset（字符编码）指出浏览器可以接受的字符编码。举例：Accept-Charset:iso-8859-1,gb2312,utf-812.Content-Type (POST数据类型)Content-Type：POST请求里用来表示的内容类型。举例：Content-Type = Text/XML; charset=gb2312：0.请求体常见的请求方法GET特点：请求所带参数包含在url中，显示在地址栏，请求数据可以被缓存，请求参数有长度限制，请求速度快POST特点：请求所带参数在请求体中，请求数据不可以被缓存，请求数据没有长度限制，请求速度慢常用的响应报头1. Cache-Control：must-revalidate, no-cache, private。告诉客户端，对资源的缓存建议；2. Connection：keep-alive作为回应客户端的Connection：keep-alive，告诉客户端是否同意建立长连接；3. Content-Encoding:gzip告诉客户端，服务端发送的资源是采用gzip编码的；4. Content-Type：text/html;charset=UTF-8告诉客户端，资源文件的类型，还有字符编码；5. Date：Sun, 21 Sep 2016 06:18:21 GMT这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。6. Expires:Sun, 1 Jan 2000 01:00:00 GMT告诉客户端在这个时间前，缓存的到期时间7. Pragma:no-cache这个含义与Cache-Control等同。8.Server：Tengine/1.4.6这个是服务器和相对应的版本，只是告诉客户端服务器的信息。9. Transfer-Encoding：chunked这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。HTTP常见响应状态码100~199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。200：表示服务器成功接收请求并已完成整个处理过程200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。302：临时转移至新的url300~399表示请求转307：临时转移至新的url404：not found400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。500：服务器内部错误为什么浏览器渲染出来的页面和爬虫请求的页面不一样爬虫请求结果爬虫只会请求当前url地址，不会主动请求js，所以往往当前URL对应的响应和element的内容不一样浏览器浏览器请求服务器渲染出来的界面，以及最终的在终端element显示的内容，和爬虫爬取到的不一样，因为浏览器会对页面中的静态文件的url进行请求，将请求结果渲染到浏览器中，导致最终的网页代码和显示的结果，已经是被js文件进行修改过；在哪里查看当前url地址对应的响应（不包括对静态文件请求的响应）：抓包（network），network下的第一个url地址，当前url地址的response右键显示网页源码字符串和字节（str/ bytes）python3 字符串应用的字符集str ：unicode的呈现形式（python3应用的unicode的子集utf-8的形式对字符串进行呈现）bytes：二进制互联网上数据的都是以二进制的方式传输的，bytes是二进制格式的字符串对服务器的请求，要将请求数据先转化为bytes格式；获取到服务器的响应要将获取到的bytes类型的数据，进行字符串的转化Unicode/UTF8/ASCII字符集的介绍字符(Character)各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等字符集(Character set)多个字符的集合字符集包括：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集ASCII编码是1个字节，而Unicode编码通常是2个字节，UTF-8是Unicode的实现方式之一，它是一种变长的编码方式，可以是1，2，3个字节（一句字符的内容而定）str到bytes之间的转化（str、bytes）python3中字节和字符之间转化的方法从str---&gt; bytes：str使用encode方法转化为 bytes从bytes---&gt; str：bytes通过decode转化为str注意：编码方式解码方式必须一样，否则就会出现乱码  ]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫基础</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
</search>
