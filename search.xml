<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据调度平台后台]]></title>
    <url>%2F2019%2F11%2F30%2F11.Golang%2FGolang%E9%A1%B9%E7%9B%AE%2F%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E5%90%8E%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[欢迎来到 data-platform-admindata-platform-admin 是什么data-platform-admin 是data-platorm的一个后台项目，用来配置数据源的对接，编写数据处理方法等； 安装方法（单独安装/和data-platform一起安装） 单独安装 前提条件：用有python3的运行环境 12pip install -r requirements.txt # 安装依赖python manage.py runserver --insecure 8800 # 启动后台程序， 访问8800端口就可以访问后台系统 和data-platform一起安装 在安装data-platform的时候，就已经安装好的data-platform-admin ， 可以直接使用 如果想在data-platform项目中单独启动data-platform-admin, 可以通过如下方法 12cd data-platform/docker-deploydocker-compose -f docker-compose-local.yml data-platform-admin up -d 使用方法 平台首页 数据源配置相关界面 数据api配置页面 变量配置界面 快速体验下面体验一下快速的对接数据源并提供http接口的步骤 先按照之前的步骤搭建好数据调度平台，并运行data-platform-admin项目 然后我们有一个数据库里面有这样一条数据，想要通过接口访问的方式拿到 在数据地址管理界面先配置好数据源的地址信息(以mysql数据为例) 增加数据源，在数据源管理界面增加一条mysql数据源信息（mysql为例） 接下来就可以直接通过http请求，请求数据 1curl localhost:3000/api/basic -X POST -d 'hello=world' 这里只演示接入数据的最基本方式，想要对数据进行处理或者做参数加工，返回结果加工和封装，日志日定义，等可以多看一下页面有哪些相关配置，或者读取源码来更深入了解 贡献代码 拉取代码 1git clone git@git.100credit.cn:blockchain/data-platform-admin.git 创建非maser分支，更改代码后提交请求]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang项目</category>
      </categories>
      <tags>
        <tag>数据平台</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据调度平台]]></title>
    <url>%2F2019%2F11%2F30%2F11.Golang%2FGolang%E9%A1%B9%E7%9B%AE%2F%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[欢迎来到 data-platformdata-platform 是什么data-platform 是一个快速对接数据源, 可以实现自由的加工(js代码)和整合数据， 并自动提供封装api的一个平台，并具有结构化的log输出和rest的接口风格； 其分为五个服务: 数据源对接服务(data-source)，数据源http代理服务(data-source-api), 数据api封装服务(data-api), 数据变量加工服务(data-derive), 数据平台总代理(data-proxy)； 在使用平台的时候可以根据需求选择需要的服务, 不需要所有服务都使用; 数据接入和编写加工逻辑等, 可以通过 data-platform-admin 项目进行页面可视化编辑; 数据源对接服务数据源对接服务, 可以快速的实现数据对接, 并提供rpc接口供访问 支持数据种类: mysql， oracle，redis，mongo，file，http 数据源http代理服务对数据源对接服务得rpc接口进行封装，进行反向代理，提供http服务供调用，用户可以根据需求选择是否使用 数据api封装服务对数据源接入的数据返回进行加工，返回加工后的结果，并自动提供http接口供访问，支持对访问参数做加工，对返回结果做加工， 对log输出结果做加工； 数据变量加工服务对批量的数据api封装的结果进行整合，并加工和封装，并可以以变量和变量集合的方式作为接口调用的返回，支持对访问参数做加工，对返回结果做加工， 对log输出结果做加工，对变量整合成集合等； 数据平台总代理对调用进行代理，规整化请求方法，清洗接口调用方式 安装方法(提前安装docker，docker-compose) 拉取代码 1git clone git@git.100credit.cn:blockchain/platform-dis.git 制作二进制程序(如果已经存在可以跳过) 123456cd docker-deploggo build -o data-source ../services/main-control/main_data_source_plus.go # 编译data-sourcego build -o data-source-api ../apis/data-source/api.go # 编译data-source-apigo build -o data-api ../webs/main-control/main_data_api.go # 编译data-apigo build -o data-derive ../webs/main-control/main_data_derive.go # 编译data-derivego build -o data-proxy ../data-proxy/main.go # 编译data-proxy 修改配置文件 配置文件位置： docker-compose/config 目录 ， 可以根据需求自己更改配置文件的内容 启动数据平台依赖 12bash import-images.shdocker-compose -f docker-compose-db.yml up -d 启动数据平台服务 1docker-compose -f docker-compose-local.yml up -d 使用方法 数据源接入，加工数据等方式 数据源接入，加工数据，参数，log日志等都是通过页面配置的方式进行，改部分参考 data-platform 项目 数据调用 数据调用都是通过http的方式进行，暴露的接口集合如下 数据源相关接口123456789数据源所需参数获取方式url: /ds/query/paramsmethod: POSTparams: code 数据源代码 数据调用方式url: /ds/query/datamethod: POSTparams: code 数据源代码 | data 数据源配置所需要的参数 | cache 0/1 是否需要缓存数据 数据api封装相关接口1234567891011121314api所需参数获取方式url: /data-api/data/paramsmethod: POSTparams: code 数据api代码api数据调用方式url: /data-api/data/querymethod: POSTparams: code api代码 | data api所需要的参数 | cache 0/1 是否需要缓存数据api接口验证方式url: /data-api/data/verifymethod: POSTparams: code api代码 数据变量相关接口 123456789101112131415161718192021222324变量所需参数获取方式url: /data-derive/data/paramsmethod: POSTparams: code 数据变量代码变量调用方式url: /data-derive/data/queryDerivemethod: POSTparams: code 变量代码 | data 变量配置所需要的参数 | cache 0/1 是否需要缓存数据变量数据接口验证方式url: /data-api/data/verifyDerivemethod: POSTparams: code 变量代码 变量集调用方式url: /data-derive/data/queryDeriveSetmethod: POSTparams: code 变量集代码 | data 变量集配置所需要的参数 | cache 0/1 是否需要缓存数据变量集数据接口验证方式url: /data-api/data/verifyDeriveSetmethod: POSTparams: code 变量集代码 consul 服务取消1curl --request PUT http://127.0.0.1:8500/v1/agent/service/deregister/&#123;&#125; 贡献代码 拉取代码 1git clone git@git.100credit.cn:blockchain/platform-dis.git 创建非maser分支，更改代码后提交请求]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang项目</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>数据平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 共识机制]]></title>
    <url>%2F2019%2F09%2F11%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F7.Fabric%20%E5%85%B1%E8%AF%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[fabric 共识机制由于fabric是分布式的系统，因此需要共识机制来保障各个节点以相同的顺序状态保存账本，达成一致性。 在当前fabric1.4版本中，存在三种共识机制，分别是solo，kafka，etcdraft。交易的共识包括3个阶段的处理：提议阶段、打包阶段和验证阶段。 1.Solo 共识模式Solo共识模式指网络环境中只有一个排序节点，从Peer节点发送来的消息由一个排序节点进行排序和产生区块；由于排序服务只有一个排序节点为所有Peer节点服务，没有高可用性和可扩展性，不适合用于生产环境，通常用于开发和测试环境。 Solo共识模式调用过程说明： Peer节点通过gPRC连接排序服务，连接成功后，发送交易信息。 排序服务通过Recv接口，监听Peer节点发送过来的信息，收到信息后进行数据区块处理。 排序服务根据收到的消息生成数据区块，并将数据区块写入账本（Ledger）中，返回处理信息。 Peer节点通过deliver接口，获取排序服务生成的区块数据。 2.Kafka 共识模式Hyperledger Fabric的核心共识算法通过Kafka集群实现，简单来说，就是通过Kafka对所有交易信息进行排序（如果系统存在多个channel，则对每个channel分别排序）。Kafka是一个分布式的流式信息处理平台，目标是为实时数据提供统一的、高吞吐、低延迟的性能。Kafka由以下几类角色构成： Broker：消息处理节点，主要任务是接收producers发送的消息，然后写入对应的topic的partition中，并将排序后的消息发送给订阅该topic的consumers。 大量的Broker节点提高了数据吞吐量，并互相对partition数据做冗余备份（类似RAID技术）。 Zookeeper：为Brokers提供集群管理服务和共识算法服务（paxos算法），例如，选举leader节点处理消息并将结果同步给其它followers节点，移除故障节点以及加入新节点并将最新的网络拓扑图同步发送给所有Brokers。 Producer：消息生产者，应用程序通过调用Producer API将消息发送给Brokers。 Consumer：消息消费者，应用程序通过Consumer API订阅topic并接收处理后的消息。 Kafka将消息分类保存为多个topic，每个topic中包含多个partition，消息被连续追加写入partition中，形成目录式的结构。一个topic可以被多个consumers订阅。简单来说，partition就是一个FIFO的消息管道，一端由producer写入消息，另一端由consumer取走消息（注意，这里的取走并不会移除消息，而是移动consumer的位置指针）。 在Hyperledger Fabric中的Kafka实际运行逻辑如下： 对于每一条链，都有一个对应的分区 每个链对应一个单一的分区主题 排序节点负责将来自特定链的交易（通过广播RPC接收）中继到对应的分区 排序节点可以读取分区并获得在所有排序节点间达成一致的排序交易列表 一个链中的交易是定时分批处理的，也就是说当一个新的批次的第一个交易进来时，开始计时 当交易达到最大数量时或超时后进行批次切分，生成新的区块 定时交易是另一个交易，由上面描述的定时器生成 每个排序节点为每个链维护一个本地日志，生成的区块保存在本地账本中 交易区块通过分发RPC返回客户端 当发生崩溃时，可以利用不同的排序节点分发区块，因为所有的排序节点都维护有本地日志 3. Etcdraft 共识模式Raft 是 v1.4.1 中引入的，它是一种基于 etcd 的崩溃容错（CFT）排序服务。Raft 遵循 “领导者和追随者” 模型，其中领导者在通道中的orderer节点之间动态选出（这个节点集合称为“consenter set”），该领导者将消息复制到跟随者节点。由于系统可以承受节点（包括领导节点）的丢失，只要剩下大多数排序节点（即所谓的“仲裁”），Raft就被称为“崩溃容错”（CFT）。换句话说，如果一个通道中有三个节点，它可以承受一个节点的丢失（剩下两个节点）。 3.1 raft相关概念 日志条目：Raft排序服务中的主要工作单元是“日志条目”，这些条目的完整序列称为“日志”。如果成员的多数（法定人数，换言之）成员到条目及其顺序达成一致，我们认为日志是一致的。 Consenter设置：排序节点主动参与给定信道的共识机制并接收信道的复制日志。这可以是所有可用节点（在单个群集中或在对系统通道有贡献的多个群集中），或者是这些节点的子集。 有限状态机（FSM）：Raft中的每个排序节点都有一个FSM，它们共同用于确保各个排序节点中的日志序列是确定性的（以相同的顺序编写）。 法定人数：描述需要确认提案的最少数量的同意者，以便可以提交交易。对于每个consenter集，这是 大多数节点。在具有五个节点的群集中，必须有三个节点才能存在仲裁。如果由于任何原因导致法定数量的节点不可用，则orderer将无法用于通道上的读取和写入操作，并且不能提交新日志。 Leader：Leader负责提取新的日志条目，将它们复制到跟随者订购节点，以及管理何时认为条目已提交。这不是特殊类型orderer人。在情况决定的情况下，这只是orderer在某些时候可能拥有的角色，而不是其他角色。 Follower：Follower从Leader那里接收日志并确定性地复制它们，确保日志保持一致。Follower也会收到来自Leader的“心跳”信息。如果Leader停止在可配置的时间内发送这些消息，则追将发起选举，其中一个Follower将被选为新Leader。 3.2 raft在交易中的流程每个通道都在Raft协议的单独实例上运行，这允许每个实例选择不同的leader。还允许集群由不同组织控制的排序节点组成的用例中进一步分散服务。虽然所有Raft节点必须是系统通道的一部分，但它们不一定必须是所有应用程序通道的一部分。通道创建者（和通道管理员）可以选择可用orderer的子集，并根据需要添加或删除orderer（一次只能添加或删除一个节点）。 在Raft中，交易（提案或配置更新的形式）由接收交易的orderer节点自动路由到该信道的当前leader。这意味着peer和应用程序不需要知道leader是谁。只有orderer节点需要知道。 3.3 raft节点选举日志传输raft节点始终处于以下三种状态之一：follower，candidate或leader。所有节点最初都是follower。在这种状态下，他们可以接受来自leader的日志条目（如果已经当选），或者为leader投票。如果在设定的时间内没有收到日志条目或心跳（例如，五秒），则节点会自我提升到candidate状态。在候选状态中，节点请求来自其他节点的投票。如果候选人获得法定数量的选票，则将其提升为leader。leader接受新的日志条目并将其复制给follower。 虽然可以无限期地保留所有日志，但为了节省磁盘空间，Raft使用一个名为“snapshotting”的进程，用户可以在其中定义将在日志中保留多少字节的数据。每个快照将包含一定数量的块）。]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电子合同和电子签名]]></title>
    <url>%2F2019%2F07%2F30%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2FCA%2F%E7%94%B5%E5%AD%90%E5%90%88%E5%90%8C%E5%92%8C%E7%94%B5%E5%AD%90%E7%AD%BE%E5%90%8D%2F</url>
    <content type="text"><![CDATA[电子合同的简介什么是电子合同电子合同实现了合同的签约方可以通过互联网进行签订，广泛应用于互联网金融、电子商务、O2O等行业，是互联网中一项相当重要的基础性服务，也是从事互联网行业的产品经理必须着重研究学习的领域。电子合同的有效性来自于电子签名，本篇文章将详细解析电子合同及电子签名的原理以及电子合同的产品设计思路。 依据2013年商务部公布的《电子合同在线订立流程规范》, 电子合同是指”平等主体的自然人、法人、其他组织之间以数据电文为载体, 并利用电子通信手段设立、变更、终止民事权利义务关系的协议”。 而根据《电子签名法》的规定, 数据电文是指”以电子、光学、磁或者类似手段生成、发送、接收或者储存的信息”。据此, 我们理解, 电子合同是以数据电文为载体, 以电子通信为手段的协议, 适用关于数据电文的相关规定。 电子合同的属性根据《合同法》的规定, 当事人订立合同, 有书面形式、口头形式和其他形式。法律、行政法规规定采用书面形式应当采用书面形式。当事人约定采用书面形式的, 应当采用书面形式。同时, 《合同法》也规定, 书面形式是指合同书、信件和数据电文(包括电报、电传、传真、电子数据交换和电子邮件)等可以有形地表现所载内容的形式。据此, 结合上述论述, 电子合同既然以数据电文作为载体, 即能够作为书面形式合同的一种, 得到《合同法》的认可与适用。 电子合同的电子签名考虑到一份成立并生效的电子合同需要合意的达成作为先决条件, 又结合电子合同与传统纸质合同相比采用了新的信息承载方式, 所以电子合同作为一种特殊的数据电文, 除了要满足传统合同所要求的法律要件外, 还需要满足《电子签名法》中关于数据电文的意思表示达成方式的法律要件。 电子合同的组成结合法律法规, 一份完整的数据电文主要包括: 数据电文和电子签名； 其中电子签名又包括 数字证书、可信时间戳 和 其他信息。如下图所列示。 CA 证书法律效益(有效性)电子签名法条例第十一条： 书面形式是指合同书、信件和数据电文（包括电报、电传、传真、电子数据交换和电子邮件）等可以有形地表现所载内容的形式。 该条例确定了电子合同属于书面合同的一种，得到了合同法的任何和适用； 第十三条: 电子签名制作数据用于电子签名时，属于电子签名人专有； 签署时电子签名制作数据仅由电子签名人控制； 签署后对电子签名的任何改动能够被发现； 签署后对数据电文内容和形式的任何改动能够被发现。 该条例规不仅认可了可靠电子签名（电子签章）的法律效力，同时也对可靠电子签名（电子签章）做出了要求：锁定签约主体真实身份、有效防止文件篡改、精确记录签约时间。 第十四条: 可靠的电子签名与手写签名或者盖章具有同等的法律效力 “可靠的电子签名”: 就是采用合法CA签发的数字证书签署产生的数字签名 第十六条: 电子签名法第十六条: 电子签名需要有第三方认证的，由依法设立的电子认证服务提供者提供认证服务。 该条例规定了电子签名的认证来源 总结: 电子签名是由依法设立的电子认证服务提供者提供的法律认可的一种书面形式的合同； 并且有效的电子合同需要满足电子签名法第十三条以下四个属性, 简称为: “真实身份、真实意愿、签名未改、原文未改“ 合法的第三发CA机构具有法律效益的国家CA机构有大约37家左右，任何一家都可以作为电子合同的第三方认证机构； 比较常用的CA机构有: 沃通CA、CFCA 等。。。 电子签名技术原理(确保有效性)电子签名满足电子签名法第十四条的规定，需要满足四个属性，简称为“身份真实、真实意愿、签名未改、原文未改”，是进行产品设计时要考虑的首要因素。 这些属性，电子签名认证提供商需要充分考虑， 只有满足了这些属性，才能确保电子签名具有一定的有效性； 1. 真实身份对签约方真实身份的确定，是通过各种实名认证方式来进行保证，所以实名认证是电子合同产品设计中必不可少的一环。实名认证功能可以由电子合同服务提供商提供，也可以由其他第三方符合要求的服务商提供。 实名认证分为多种方式，可靠性和用户体验各有不同，要根据产品需求灵活选用。 2. 真实意愿意愿认证是指在每一次签署前对用户的身份进行确认，保证是用户本人操作的认证方式。我们常见的用户密码登录其实就是一种意愿认证，但由于目前登录密码丢失、被盗取、被破解的风险很高，所以对于涉及金额较大和较重要的电子合同，产品设计中要考虑加入其他意愿认证方式进行可靠性强化。 常见的意愿认证方式有：指纹登入、短信验证码登入、语音验证码登入、人脸识别登入、UKEY登入等。各种意愿认证方式同样可靠性和用户体验各有不同，根据不同产品要求可以进行选用。 比如在比特币网络中，用户如果想要进行交易，则必须用保存在本地的私钥进行签名才能进行交易，因此也确保了真是意愿性； 3. 签名未改及原文未改签名未改及原文未改是通过数字签名技术+时间戳+CA机构颁发的数字证书一起保证和实现的，数字签名技术是其中的核心部分，时间戳和数字证书的实现方式都使用到数字签名。 对于数字签名和非对称算法等内容，可以参考区块链部分的密码学文章； 下面介绍一下数字签名在电子签名中的应用: 假设一个用户A执行了电子合同签约，则服务端先对电子合同原文进行哈希算法，得到电子合同原文的哈希值。然后使用用户A的私钥对哈希值进行加密，把加密得到的数字签名连同电子合同一起发送出去。 用户B收到带数字签名的电子合同后，先对原文用哈希算法得到哈希值，然后使用用户A的公钥对数字签名进行解密（对于用户A的公钥可信度是由CA机构的数字证书来保证，只要是国家认证的CA机构颁发的数字证书都认为是可信的，在此不展开讲解），解密得到的哈希值与原文用哈希算法得到哈希值进行比对，如果哈希值一致，则证明电子合同确实是A所签署的，并且没有被纂改过，也就达到验证“签名未改、原文未改”的目的。 时间戳在电子合同中的应用： 签订时间同样是电子签名成立和有效性关键环节，电子签名一般使用时间戳技术来对电子合同的签订时间进行有效确认。原理是对电子合同进行一次哈希运算，将哈希值发送给时间戳签发中心，时间戳签发中心使用数字签名技术对哈希值和当前时间进行一次数字签名。 由于对于电子合同哈希值具有唯一性，所以时间戳的数字签名可以确认电子合同签约的时间点，这样就保证了“签名未改”的实现。 如何接入第三方CA机构(CFCA安心签 + 供应链金融 为例)CFCA的证据保全与司法服务CFCA为证据采集、证据准备及证据受理提供全流程服务。 在证据采集阶段，电子证据保全系统通过采集客户业务活动过程中产生的电子数据，进行保全，形成电子证据，并出具证据保全报告。 在证据准备阶段，通过数字签名验证平台为客户业务活动中使用的数字签名进行验证，并出具数字签名验证报告。 在证据受理阶段，司法机关通过法律服务平台对数字签名验证报告和证据保全报告等电子证据进行专业化在线验证。 CFCA 对供应链金融的解决方法业务概述 随着社会化生产方式的不断深入，市场竞争已经从单一客户之间的竞争转变为供应链与供应链之间的竞争，同一供应链内部各方相互依存，同时赊销已成为交易的主流方式，处于供应链中上游的供应商，很难通过传统的信贷方式获得银行的资金支持，而资金短缺又会直接导致后续环节的停滞。 2011年以来，各家商业银行受到信贷规模的限制，可以发放的贷款额度十分有限，但是通过承兑、票据、信用证等延期支付工具，既能够增强企业之间的互相信任，也稳定了一批客户，银行业空前重视供应链金融业务。面对国内互联网金融的飞速发展，传统线下的供应链金融模式已经很难满足企业客户对于高效、便捷、不受时空限制的需求，利用线上供应链金融平台实现线上业务办理已是趋势。 传统供应链金融采用现场面签的方式，而采用互联网模式的供应链金融面临的最大的挑战还是法律风险、交易风险、技术风险，引入CFCA数字证书体系可以有效的解决上述风险。 平台价值​ 解决方案​ 总体架构 ·参与各方（融资企业、核心企业、商业银行）获得由供应链金融平台发放的数字证书，登录平台、签署电子合同时使用证书做身份验证和电子签名； ·供应链金融平台调用RA系统实现证书管理功能，调用签名验签服务器实现签名验签的功能，RA系统连接CFCA的CA系统完成证书签发和管理。 业务流程​ 方案特点用户真实身份 线下面对面并使用纸质证件代表参与各方的身份，线上使用数字证书实现高强度的身份认证，并解决用户身份真实性验证问题，防止假冒用户恶意操作； 电子合同法律效力 在平台签署电子合同时使用电子签名技术，符合电子签名法的要求，从而保证平台签署的电子合同具有法律效力； 电子合同信息做到防篡改和保密 平台各参与方签署电子合同包含敏感业务信息，使用签名和加密技术实现敏感业务信息的防篡改和保密； 平台用户的易操作性 数字证书和签名技术已经是广泛应用的成熟技术，目前在网上银行广泛使用，平台用户已经成熟掌握该技术的操作和使用。 安心签平台接入流程安心签平台介绍安心签是中国金融认证中心（以下简称“CFCA”）旗下的第三方电子缔约服务平台，为持有有效数字证书的用户提供数据电文或电子缔约文件的在线签署、存储和管理服务。 名词介绍客户平台: 与安心签对接的电子合同建设方。 平台用户: 客户平台的用户，通过客户平台方使用安心签电子合同服务。 用户三要素: 用户名、证件类型、证件号码，用以判断用户身份及唯一性，相同的三要素视为同一个用户。 业务流程分类客户平台与安心签对接，实现电子合同签署，根据业务场景，可以分为以下四种方式。 1. 证书托管 + 合同模板方式 平台用户通过客户平台在安心签开户，安心签自动为用户生成数字证书并保存在安心签中。客户平台预先生成合同模板并保存在安心签中。合同签署时，客户平台将关键信息及用户信息传给安心签，安心签自动将关键信息填入相应的合同模板中，并调用用户证书进行签署，生成电子合同。 2.证书托管 + 自定义合同方式 平台用户通过客户平台在安心签开户，安心签自动为用户生成数字证书并保存在安心签中。合同签署时，客户平台将合同文件和用户信息传给安心签，安心签调用用户证书在合同的指定位置进行签署，生成电子合同。 上述两种方式的区别在于，如果每份合同差别不大，则适用合同模板的方式；如果每份合同差别较大，则适用自定义合同的方式。 3.自有证书签署 + 合同模板方式 如果客户平台或平台用户不希望安心签托管证书，平台用户可以通过客户平台在安心签开户，并绑定自有的CFCA证书，即在安心签中使用用户自己保管的CFCA证书（该证书信息必须已经收录在网络身份认证平台数据库中，具体业务中客户若需要绑定自有证书，请技术支持与业务确认该信息）。客户平台预先生成合同模板并保存在安心签中。合同签署时，客户平台将关键信息及用户信息传给安心签，安心签自动将关键信息填入相应的合同模板中，生成合同全文，获取合同摘要（哈希值），客户平台在用户本地调用用户证书对合同摘要进行签署（签名值），并将签名值发送到安心签，生成电子合同。 4.证书托管 + 合同模板方式 如果客户平台或平台用户不希望安心签托管证书，平台用户可以通过客户平台在安心签开户，并绑定自有的CFCA证书，即在安心签中使用用户自己保管的CFCA证书（该证书信息必须已经收录在网络身份认证平台数据库中，具体业务中客户若需要绑定自有证书，请技术支持与业务确认该信息）。合同签署时，客户平台将合同文件和用户信息传给安心签。客户平台在获取合同摘要（哈希值）后，在用户本地调用用户证书对合同摘要进行签署（签名值），并将签名值发送到安心签，生成电子合同。]]></content>
      <categories>
        <category>开发工具</category>
        <category>CA</category>
      </categories>
      <tags>
        <tag>true</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperleder explorer 部署]]></title>
    <url>%2F2019%2F07%2F26%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F6.Hyperledger%20explorer%2F</url>
    <content type="text"><![CDATA[该篇文档的前提是已经存在部署好的fabric网络, 并且在peer节点有运行的自定义链码 Explorer 区块链浏览器部署区块链网络部署(前提) 在部署 Explorer项目前提前要有存在的区块链网络，这里说一下区块链网络部署时应该注意点的点 在区块链网络中启动操作服务器 如果要使用区块链监控，在启动网络的时候应该加上如下环境变量 1234- ORDERER_OPERATIONS_LISTENADDRESS=0.0.0.0:8443 # operation RESTful API- ORDERER_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API- CORE_OPERATIONS_LISTENADDRESS=0.0.0.0:9443 # operation RESTful API- CORE_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API 务必开启gossip 开启gossip， 使区块链网络开启发现服务 12- CORE_PEER_GOSSIP_BOOTSTRAP=X.X.X.X:XXXX- CORE_PEER_GOSSIP_EXTERNALENDPOINT=X.X.X.X:XXXX 项目拉取12git clone https://github.com/hyperledger/blockchain-explorer.gitcd blockchain-explorer 安装数据库配置prostgreSQL数据库 第一种配置方法 cd blockchain-explorer/app 修改 explorerconfig.json 1234567"postgreSQL": &#123; "host": "127.0.0.1", "port": "5432", "database": "fabricexplorer", "username": "hppoc", "passwd": "password"&#125; 第二种配置方法 12345export DATABASE_HOST=127.0.0.1export DATABASE_PORT=5432export DATABASE_DATABASE=fabricexplorerexport DATABASE_USERNAME=hppocexport DATABASE_PASSWD=pass12345 给执行sql脚本的目录带执行权限1chomd -R 775 /blockchain-explorer/app/persistence/fabric/postgreSQL/db 执行数据库脚本12cd blockchain-explorer/app/persistence/fabric/postgreSQL/dbsudo -u postgres ./createdb.sh 配置身份认证配置jwt​ cd blockchain-explorer/app 修改explorerconfig.json 文件 1234"jwt": &#123; "secret" : "a secret phrase!!", # 用来签名payload的密钥 "expiresIn": "2 days" # token 过期时间 Eg: 60, "2 days", "10h", "7d".&#125; 配置explorer 链接到区块链网络配置 explorer 链接区块链网络 配置网络文件位置: /blockchain-explorer/app/platform/fabric/config.json 配置链接文件位置: /blockchain-explorer/app/platform/fabric/connection-profile 配置 prometheus 链接到操作服务器​ 1. 配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml 安装并配置 grafana 配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json 配置文件位置: app/platform/fabric/artifacts/operations/grafana_conf/provisioning/dashboards/dashboard.yaml 配置文件位置: app/platform/fabric/artifacts/operations/grafana_conf/provisioning/datasources/datasource.yaml Build Explorer 项目12./main.sh clean./main.sh install 运行Explorer项目1234567891011121314cd blockchain-explorer/app 修改explorerconfig.json 同步频率 主机类型 区块同步时间的详细配置# 普通方式运行和停止cd blockchain-explorer/./start.sh (it will have the backend up)../start.sh debug (it will have the backend in debug mode)../start.sh print (it will print help).Launch the URL http(s)://localhost:8080 on a browser../stop.sh (it will stop the node server).# Sync方式运行和停止cd blockchain-explorer/./syncstart.sh (it will have the sync node up)../syncstop.sh (it will stop the sync node). Explorer Docker 部署 前提: bash Docker Docker Compose 仓库地址: Hyperledger Explorer docker repository https://hub.docker.com/r/hyperledger/explorer/ Hyperledger Explorer PostgreSQL docker repository https://hub.docker.com/r/hyperledger/explorer-db 区块链网络部署(前提) 在部署 Explorer项目前提前要有存在的区块链网络，这里说一下区块链网络部署时应该注意点的点 在区块链网络中启动操作服务器 如果要使用区块链监控，在启动网络的时候应该加上如下环境变量 1234- ORDERER_OPERATIONS_LISTENADDRESS=0.0.0.0:8443 # operation RESTful API- ORDERER_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API- CORE_OPERATIONS_LISTENADDRESS=0.0.0.0:9443 # operation RESTful API- CORE_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API 务必开启gossip 开启gossip， 使区块链网络开启发现服务 12- CORE_PEER_GOSSIP_BOOTSTRAP=X.X.X.X:XXXX- CORE_PEER_GOSSIP_EXTERNALENDPOINT=X.X.X.X:XXXX Explorer项目拉取12git clone https://github.com/hyperledger/blockchain-explorer.gitcd blockchain-explorer 配置Explorer配置explorer和fabric网络的连接参考配置文件位置: examples/net1/config.json 连接的主配置文件 examples/net1/connection-profile 和 fabric 的连接配置 examples/net1/crypto 网络传输加密文件目录(如果有的话) 配置 prometheus参考配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml 配置 grafana参考配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json 一个dashboard的详细配置 app/platform/fabric/artifacts/operations/grafana_conf/provisioning/dashboards/dashboard.yaml dashboard的存储目录等基本配置 app/platform/fabric/artifacts/operations/grafana_conf/provisioning/datasources/datasource.yaml 数据源的配置 配置docker-compose 文件配置网络配置网络，可以指定已经存在的网络，如果和区块链网络在同一台机器上可以使用区块链网络 如果和区块链网络是分开部署的，可以指定一个其他的任意网络 1234networks: mynetwork.com: external: name: net_byfn 配置数据库1234567environment: # 配置数据库名称，用户密码等 - DATABASE_DATABASE=fabricexplorer - DATABASE_USERNAME=hppoc - DATABASE_PASSWORD=password volumes: # 配置数据库创建监本位置； 配置数据库持久化的位置 - ./app/persistence/fabric/postgreSQL/db/createdb.sh:/docker-entrypoint-initdb.d/createdb.sh - pgdata:/var/lib/postgresql/data 配置 explorer12345678910environment: - DATABASE_HOST=explorerdb.mynetwork.com # 连接的数据库配置 - DATABASE_USERNAME=hppoc - DATABASE_PASSWD=password - DISCOVERY_AS_LOCALHOST=false # 如果是通过桥接的方式连接到区块链浏览器，务必设置为false volumes: # 引用的配置文件位置配置 - ./examples/net1/config.json:/opt/explorer/app/platform/fabric/config.json - ./examples/net1/connection-profile:/opt/explorer/app/platform/fabric/connection-profile - ./examples/net1/crypto:/tmp/crypto - walletstore:/opt/wallet 配置 prometheus123volumes: - ./app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml:/etc/prometheus/prometheus.yml # 引用的配置文件位置 - prometheus-storage:/prometheus # prometheus数据持久化的位置 配置 grafana1234volumes: # 配置引用配置文件的位置，以及数据持久化的文职 - ./app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json:/var/lib/grafana/dashboards/mydashboard.json - ./app/platform/fabric/artifacts/operations/grafana_conf/provisioning:/etc/grafana/provisioning - grafana-storage:/var/lib/grafana docker-compose.yaml 文件的示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970version: '2.1'volumes: pgdata: walletstore: grafana-storage: prometheus-storage:networks: mynetwork.com: external: name: net_byfnservices: explorerdb.mynetwork.com: image: hyperledger/explorer-db:latest container_name: explorerdb.mynetwork.com hostname: explorerdb.mynetwork.com environment: - DATABASE_DATABASE=fabricexplorer - DATABASE_USERNAME=hppoc - DATABASE_PASSWORD=password volumes: - ./app/persistence/fabric/postgreSQL/db/createdb.sh:/docker-entrypoint-initdb.d/createdb.sh - pgdata:/var/lib/postgresql/data networks: - mynetwork.com explorer.mynetwork.com: image: hyperledger/explorer:latest container_name: explorer.mynetwork.com hostname: explorer.mynetwork.com environment: - DATABASE_HOST=explorerdb.mynetwork.com - DATABASE_USERNAME=hppoc - DATABASE_PASSWD=password - DISCOVERY_AS_LOCALHOST=false volumes: - ./examples/net1/config.json:/opt/explorer/app/platform/fabric/config.json - ./examples/net1/connection-profile:/opt/explorer/app/platform/fabric/connection-profile - ./examples/net1/crypto:/tmp/crypto - walletstore:/opt/wallet command: sh -c "sleep 16&amp;&amp; node /opt/explorer/main.js &amp;&amp; tail -f /dev/null" ports: - 8090:8080 networks: - mynetwork.com proms: container_name: proms image: prom/prometheus:latest volumes: - ./app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml:/etc/prometheus/prometheus.yml - prometheus-storage:/prometheus ports: - '9090:9090' networks: - mynetwork.com grafana: container_name: grafana image: grafana/grafana:latest volumes: - ./app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json:/var/lib/grafana/dashboards/mydashboard.json - ./app/platform/fabric/artifacts/operations/grafana_conf/provisioning:/etc/grafana/provisioning - grafana-storage:/var/lib/grafana ports: - '3000:3000' networks: - mynetwork.com 启动网络1docker-compose up -d]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webpack 使用]]></title>
    <url>%2F2019%2F07%2F16%2F03.web%E5%89%8D%E7%AB%AF%2FVUE%2Fwebpack%20%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、webpack介绍1、由来由于前端之前js、css、图片文件需要单独进行压缩和打包，这样团队人员处理很繁琐，然后 Instagram 团队就想让这些工作自动化，然后webpack应运而生。 2、介绍webpack是一个模块打包器（module bundler），webpack视HTML，JS，CSS，图片等文件都是一种 资源 ，每个资源文件都是一个模块（module）文件，webpack就是根据每个模块文件之间的依赖关系将所有的模块打包（bundle）起来。 3、作用 对 CommonJS 、 AMD 、ES6的语法做了兼容 对js、css、图片等资源文件都支持打包（适合团队化开发） ，比方你写一个js文件，另外一个人也写一个js文件，需要合并很麻烦，现在交给webpack合并很简单 有独立的配置文件webpack.config.js 可以将代码切割成不同的chunk，实现按需加载，降低了初始化时间 具有强大的Plugin（插件）接口，大多是内部插件，使用起来比较灵活 4、拓展说明 CommonJS、AMD、CMD是用于JavaScript模块管理的三大规范，CommonJS定义的是模块的同步加载，是一个更偏向于服务器端的规范（也可以在浏览器中使用），主要用于Nodejs，根据CommonJS规范，一个单独的文件就是一个模块，加载模块使用require()方法，该方法读取一个文件并执行，最后返回文件内部的exports对象。 AMD和CMD则是定义模块异步加载适用于浏览器端，都是为了 JavaScript 的模块化开发，（这里说一下为什要有异步加载，因为浏览器如果使用common.js同步加载模块的话，就会导致性能等问题，所以针对这个问题，又出了一个规范，这个规范可以实现异步加载依赖模块） AMD规范会提前加载依赖模块，AMD规范是通过requireJs 在推广过程中对模块定义的规范化产出。 CMD规范会延迟加载依赖模块， CMD 规范是 SeaJs 在推广过程中对模块定义的规范化产出。 AMD规范和CMD规范的区别 对于依赖的模块，AMD 是提前执行，CMD 是延迟执行。不过 RequireJS 从 2.0 开始，也改成可以延迟执行（根据写法不同，处理方式不同）CMD 推崇依赖就近，AMD 推崇依赖前置AMD 的 API 默认是一个当多个用，CMD 的 API 严格区分，推崇职责单一。比如 AMD 里，require 分全局 require 和局部 require，都叫 require。CMD 里，没有全局 require，而是根据模块系统的完备性，提供 seajs.use 来实现模块系统的加载启动。CMD 里，每个 API 都简单纯粹webpack和gulp的区别 gulp是前端自动化构建工具，强调的是前端开发的工作流程，我们可以通过配置一系列的task，定义task处理的事情（代码压缩、合并、编译、浏览器实时更新等），然后定义执行顺序，来让gulp执行这些task，从而构建项目的整个前端开发流程，自动化构建工具并不能把所有模块打包到一起，也不能构建不同模块之间的依赖关系。webpack是 JavaScript 应用程序的模块打包器，强调的是一个前端模块化方案，更侧重模块打包，我们可以把开发中的所有资源（图片、js文件、css文件等）都看成模块，通过loader（加载器）和plugins（插件）对资源进行处理，打包成符合生产环境部署的前端资源。 5、webpack整体认知 (1)、webpack的核心概念分为 入口(Entry)、加载器(Loader)、插件(Plugins)、出口(Output); 入口(Entry)：入口起点告诉 webpack 从哪里开始，并根据依赖关系图确定需要打包的文件内容 加载器(Loader)：webpack 将所有的资源（css, js, image 等）都看做模块，但是 webpack 能处理的只是 JavaScript，因此，需要存在一个能将其他资源转换为模块，让 webpack 能将其加入依赖树中的东西，它就是 loader。loader用于对模块的源代码进行转换。loader 可以使你在 import 或”加载”模块时预处理文件。因此，loader 类似于其他构建工具中“任务(task)”，并提供了处理前端构建步骤的强大方法。 1234567js rules: [ &#123; test: /\.(js|jsx)$/, use: 'babel-loader' &#125; ] 插件(Plugins)：loader 只能针对某种特定类型的文件进行处理，而 plugin 的功能则更为强大。在 plugin 中能够介入到整个 webpack 编译的生命周期，Plugins用于解决 loader 无法实现的其他事情，也就是说loader是预处理文件，那plugin 就是后处理文件。比如对loader打包后的模块文件（bundle.js）进行二次优化处理，例如：代码压缩从而减小文件体积；提供辅助开发的作用：例如：热更新（浏览器实时显示） 1234plugins: [ new webpack.optimize.UglifyJsPlugin(), new HtmlWebpackPlugin(&#123;template: './src/index.html'&#125;)] 二、webpack安装1、安装node使用 node -v 命令检查版本 2、安装cnpm1npm install -g cnpm --registry=https://registry.npm.taobao.org 使用 cnpm -v 命令检查版本 3、安装nrm的两种方法nrm可以帮助我们切换不同的NPM源的快捷开关，可以切换的NPM源包括：npm，cnpm，taobao， rednpm， npmMirror ， edunpm 第一种方法（由于是外网访问进行安装，可能会被墙） 1npm install -g nrm 第二种方法（国内的淘宝镜像，访问稳定，推荐） 1cnpm install -g nrm 使用 nrm - V 命令检查版本(注意这里的 V 是大写的)使用nrm ls 命令可以查看当前可以可以切换的 NPM源使用 npm use cnpm 命令 指定要使用的哪种NPM源 4、安装webpack全局安装 1npm install --global webpack 在项目中安装最新版本或特定版本，分别执行以下命令： 12npm install --save-dev webpack npm install --save-dev webpack@&lt; version &gt; 三、webpack配置0、搭建项目结构12345678Project- dist- src- - js- - - moudle1.js- - - main.js- index.html- webpack.config.js ## 手动创建 1、初始化一个项目（会创建一个package.json文件）1npm init 2、在当前的项目中安装Webpack作为依赖包1npm install --save-dev webpack –save ：将配置信息保存到package.json中，也是项目生产环境，项目发布之后还依赖的东西，保存在dependencies –save-dev ：是项目开发环境依赖的东西，保存在devDependencies中, 例如：写 ES6 代码，如果你想编译成 ES5 发布那么 babel 就是devDependencies 3、当前项目结构 4、实现CSS打包1cnpm install css-loader style-loader --save-dev 在src—&gt;css目录中新建main.css 1234css #first&#123; border: 1px solid red; &#125; 在webpack.config.js中配置相关的loader 123456789101112131415161718192021222324const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径// const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错// 常量存储的是一个不可以变化的变量。// module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, './dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125; ] &#125;&#125; 在main.js中获取css目录中的main.css文件 12345678910111213141516// 1、获取index.html中的dom对象var first = document.getElementById('first');var btn = document.getElementById('btn');var two = document.getElementById('two');var res = document.getElementById('res'); btn.onclick = function()&#123; var firstValue = parseFloat(first.value); var twoValue = parseFloat(two.value); //2、获取 module1.js中的 sum函数 //http://www.ruanyifeng.com/blog/2015/05/require.html var sum = require('./module1.js'); res.value = sum(firstValue,twoValue); &#125;// 3、获取css目录中的main.css文件require('../css/main.css'); 在终端中输入 webpack命令进行css文件打包 5、实现SCSS打包在src目录中新建 sass目录–&gt; scss1.scss 12345// scss1.scss文件 $color:purple; #two&#123; border:1px solid $color; &#125; 安装对应的loader 1cnpm install sass-loader css-loader style-loader node-sass webpack --save-dev 在webpack.config.js中配置相关的loader 1234567891011121314151617181920212223242526272829const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径// const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错// 常量存储的是一个不可以变化的变量。// module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, './dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css\$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125; ] &#125; 在js目录中 main.js里面引入 scss1.scss 1234567891011121314151617// 1、获取index.html中的dom对象var first = document.getElementById('first');var btn = document.getElementById('btn');var two = document.getElementById('two');var res = document.getElementById('res');btn.onclick = function()&#123; var firstValue = parseFloat(first.value); var twoValue = parseFloat(two.value); //2、获取 module1.js中的 sum函数 //http://www.ruanyifeng.com/blog/2015/05/require.html var sum = require('./module1.js'); res.value = sum(firstValue,twoValue);&#125;// 3、获取css目录中的main.css文件require('../css/main.css');// 4、获取sass目录中的scss1.scss文件require('../sass/scss1.scss'); 在终端中输入 webpack命令进行scss文件打包 6、实现打包url资源（图片、gif、图标等）功能在src 目录中 新建imgs目录，放入两张不同大小的图片 在index.html中新增 &lt; div id=”bg1” &gt;&lt; /div &gt; &lt; div id=”bg2” &gt;&lt; /div &gt; 在mian.css中新增 // mian.css文件 12345#bg1&#123; width: 200px; height: 200px; background: url('../imgs/bxg.jpg');&#125; 12345#bg2&#123; width: 200px; height: 200px; background: url('../imgs/web.jpg') no-repeat;&#125; 如果我们希望在页面引入图片（包括img的src和background的url）。当我们基于webpack进行开发时，引入图片会遇到一些问题。 其中一个就是引用路径的问题。拿background样式用url引入背景图来说，我们都知道，webpack最终会将各个模块打包成一个文件，因此我们样式中的url路径是相对入口html页面的，而不是相对于原始css文件所在的路径的。这就会导致图片引入失败。这个问题是用file-loader解决的，file-loader可以解析项目中的url引入（不仅限于css），根据我们的配置，将图片拷贝到相应的路径，再根据我们的配置，修改打包后文件引用路径，使之指向正确的文件。 另外，如果图片较多，会发很多http请求，会降低页面性能。这个问题可以通过url-loader解决。url-loader会将引入的图片编码，生成dataURl。相当于把图片数据翻译成一串字符。再把这串字符打包到文件中，最终只需要引入这个文件就能访问图片了。当然，如果图片较大，编码会消耗性能。因此url-loader提供了一个limit参数，小于limit字节的文件会被转为DataURl，大于limit的还会使用file-loader进行copy。 url-loader和file-loader是什么关系呢？简答地说，url-loader封装了file-loader。url-loader不依赖于file-loader，即使用url-loader时，只需要安装url-loader即可，不需要安装file-loader，因为url-loader内置了file-loader。通过上面的介绍，我们可以看到，url-loader工作分两种情况：1.文件大小小于limit参数，url-loader将会把文件转为DataURL；2.文件大小大于limit，url-loader会调用file-loader进行处理，参数也会直接传给file-loader。因此我们只需要安装url-loader即可。 安装 1cnpm install url-loader file-loader --save-dev 在webpack.config.js中配置相关的loader 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径// const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错// 常量存储的是一个不可以变化的变量。// module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, './dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 publicPath:'dist/' // path:所有输出文件的目标路径; // publicPath:输出解析文件的目录，url 相对于 HTML 页面 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css\$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss\$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less\$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片和字体文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf|eot|woff|woff2|svg)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式（为什么呢？因为一个很小的图片，不值当的去发送一个请求，减少请求次数; 其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为dataurl形式） &#125; &#125; ] //保证输出的图片名称与之前命名的图片名称保持一致(目前只是支持这样的写法，webpack3 没有响应的options进行配置) // use:'url-loader?limit=8192&amp;name=imgs/[name].[ext]' &#125; ] &#125;&#125; 在main.js中引入mui目录中icons-extra.css的文件 12// 6、获取src目录中的mui目录中icons-extra.css的文件require('../mui/css/icons-extra.css'); 7、Webpack-dev-server结合后端服务器的热替换配置webpack-dev-server提供了一个简单的 web 服务器，并且能够实时重新加载(live reloading)，同时把生成好的js和html构建到我们的电脑内存中，这样的话，即使我们的目录中没有了相关js等文件，还能够加载出来，这样能够提高我们页面运行速度。 安装 webpack-dev-server 插件 1cnpm install webpack-dev-server --save-dev 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// webpack.config.js const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径 // const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错 // 常量存储的是一个不可以变化的变量。 module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125; ] &#125; ] &#125;, devServer: &#123; // contentBase: './dist', // 在 localhost:8080(默认) 下建立服务，将 dist 目录下的文件，作为可访问文件contentBase：告诉服务器从哪里提供内容 // 或者通过以下方式配置 contentBase: path.join(__dirname, "dist"), compress: true, // 当它被设置为true的时候对所有的服务器资源采用gzip压缩 // 对JS，CSS资源的压缩率很高，可以极大得提高文件传输的速率，从而提升web性能 port: 9000, // 如果想要改端口，可以通过 port更改 hot: true, // 启用 webpack 的模块热替换特性() inline: true, // 实现实时重载（实现自动刷新功能）默认情况下是 true。 host: "localhost" // 如果你希望服务器外部可访问，指定使用一个 host。默认是 localhost(也就是你可以不写这个host这个配置属性)。 &#125;&#125; 123456789101112131415161718192021222324252627// package.json&#123; "name": "mywebpack", "version": "1.0.0", "description": "", "main": "webpack.config.js", "scripts": &#123; "test": "echo \"Error: no test specified\" &amp;&amp; exit 1", "start": "webpack-dev-server --open" // "start": "webpack-dev-server --open --port 8080 --hot --inline" // 如果在这里配置了，就不用在webpack.config.js中配置devServer属性了。 &#125;, "author": "", "license": "ISC", "devDependencies": &#123; "css-loader": "^0.28.7", "file-loader": "^1.1.5", "html-webpack-plugin": "^2.30.1", "less": "^3.0.0-alpha.3", "less-loader": "^4.0.5", "node-sass": "^4.5.3", "sass-loader": "^6.0.6", "style-loader": "^0.19.0", "url-loader": "^0.6.2", "webpack": "^3.8.1", "webpack-dev-server": "^2.9.3" &#125;&#125; 在命令行中运行 npm start，就会看到浏览器自动加载页面。如果现在修改和保存任意源文件，web 服务器就会自动重新加载编译后的代码，但是打开后发现，打开的是 dist目录，我们想要的是 index.html显示我们的页面，所以这是我们还要借助里另一个 html-webpack-plugin 插件。 8、html-webpack-plugin 插件安装html-webpack-plugin 简单创建 HTML 文件，用于服务器访问，其中包括使用script标签的body中的所有webpack包。 安装 html-webpack-plugin 插件 1cnpm install --save-dev html-webpack-plugin webpack.config.js配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// webpack.config.js const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径 // const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错 // 常量存储的是一个不可以变化的变量。 module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125; ] &#125; ] &#125;, devServer: &#123; // contentBase: './dist', // 在 localhost:8080(默认) 下建立服务，将 dist 目录下的文件，作为可访问文件contentBase：告诉服务器从哪里提供内容 // 或者通过以下方式配置 contentBase: path.join(__dirname, "dist"), compress: true, // 当它被设置为true的时候对所有的服务器资源采用gzip压缩 // 对JS，CSS资源的压缩率很高，可以极大得提高文件传输的速率，从而提升web性能 port: 9000, // 如果想要改端口，可以通过 port更改 hot: true, // 启用 webpack 的模块热替换特性() inline: true, // 实现实时重载（实现自动刷新功能）默认情况下是 true。 host: "localhost" // 如果你希望服务器外部可访问，指定使用一个 host。默认是 localhost(也就是你可以不写这个host这个配置属性)。 &#125; plugins: [ new HtmlWebpackPlugin(&#123; title: '首页', // 用于生成的HTML文档的标题 filename: 'index.html', //写入HTML的文件。默认为index.html。也可以指定一个子目录（例如：）assets/admin.html template: 'index.html' // Webpack需要模板的路径 &#125;), new webpack.HotModuleReplacementPlugin() // 需要结合 启用热替换模块(Hot Module Replacement)，也被称为 HMR ]&#125; 再次使用npm start命令就可以实现浏览器自动更新。 问题来了，HtmlWebpackPlugin中的 title并没有显示出来，原因是需要在定义的template模板中使用ejs语法， 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt; &lt;!-- EJS 语法 /* EJS是一个简单高效的模板语言，通过数据和模板，可以生成HTML标记文本。可以说EJS是一个JavaScript库，EJS可以同时运行在客户端和服务器端，客户端安装直接引入文件即可 */ --&gt; &lt;title&gt;&lt;%= htmlWebpackPlugin.options.title%&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;input type="text" id="first"&gt; &lt;input type="button" id="btn" value="+"&gt; &lt;input type="text" id="two"&gt; &lt;input type="button" id="btn" value="="&gt; &lt;input type="text" id="res"&gt; &lt;div id="bg1"&gt;&lt;/div&gt; &lt;div id="bg2"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 9、ES6转换为ES5语法安装 1cnpm install --save-dev babel-loader babel-core babel-preset-env babel-core 如果某些代码需要调用Babel的API进行转码，就要使用babel-core模块babel-preset-env 通过根据您的目标浏览器或运行时环境自动确定您需要的Babel插件babel 对一些公共方法使用了非常小的辅助代码，比如 _extend。 默认情况下会被添加到每一个需要它的文件中,你可以引入 babel runtime 作为一个独立模块，来避免重复引入。 你必须执行 npm install babel-plugin-transform-runtime –save-dev 来把它包含到你的项目中，也要使用 npm install babel-runtime –save 把 babel-runtime 安装为一个依赖 配置 1234567891011121314151617181920212223242526// 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [&#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125;] &#125;, // 实现 ES6转 ES5 &#123; test: /\.js$/, exclude: /(node_modules)/, // exclude 排除的意思，把 node_modules文件夹排除编译之外 use: &#123; loader: 'babel-loader', options: &#123; // presets 预设列表（一组插件）加载和使用 presets: ['env'], plugins: ['transform-runtime'] // 加载和使用的插件列表 &#125; &#125; &#125; 把一些代码改成ES6 语法的写法 1234567891011// moudule1.jsfunction sum(x,y)&#123; return x + y;&#125;// 导出 sum 函数// module.exports = sum;// 改成ES6 的写法语法export default&#123; sum&#125; 12345678910111213141516171819202122232425262728293031323334353637// main.js// 1、获取index.html中的dom对象var first = document.getElementById('first');var btn1 = document.getElementById('btn1');var two = document.getElementById('two');var res = document.getElementById('res');console.log(1);btn1.onclick = function() &#123; var firstValue = parseFloat(first.value); var twoValue = parseFloat(two.value); //2、获取 module1.js中的 sum函数 //http://www.ruanyifeng.com/blog/2015/05/require.html console.log(2); /* var sum = require('./module1.js'); res.value = sum(firstValue,twoValue);*/ res.value = sumObj.sum(firstValue, twoValue);&#125;// 3、获取css目录中的main.css文件// require('../css/main.css');// 把步骤3 改为 ES6写法,引入css目录中的main.css文件import '../css/main.css';// 4、获取sass目录中的scss1.scss文件require('../sass/scss1.scss');// 5、获取less目录中的less1.less文件require('../less/less1.less');// 6、获取src目录中的mui目录中icons-extra.css的文件require('../mui/css/icons-extra.css');// 把 var sum = require('./module1.js'); 写成 ES6语法import sumObj from './module1.js' 10、防止文件缓存（生成带有20位的hash值的唯一文件）1234567// webpack.config.jsoutput: &#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 // filename: 'bulid.js' // 制定出口文件的名称 filename: '[name].[hash].js' // 将入口文件重命名为带有20位的hash值的唯一文件 &#125; 11、抽取CSS为单独文件安装插件从 build.js文件中提取文本（CSS）到单独的文件 1npm install --save-dev extract-text-webpack-plugin 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// webpack.config.js const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径 // const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错 // 常量存储的是一个不可以变化的变量。 module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125;, &#123; test: /\.css$/, use: ExtractTextPlugin.extract(&#123; fallback: "style-loader", use: "css-loader" &#125;) &#125; ] &#125; ] &#125;, devServer: &#123; // contentBase: './dist', // 在 localhost:8080(默认) 下建立服务，将 dist 目录下的文件，作为可访问文件contentBase：告诉服务器从哪里提供内容 // 或者通过以下方式配置 contentBase: path.join(__dirname, "dist"), compress: true, // 当它被设置为true的时候对所有的服务器资源采用gzip压缩 // 对JS，CSS资源的压缩率很高，可以极大得提高文件传输的速率，从而提升web性能 port: 9000, // 如果想要改端口，可以通过 port更改 hot: true, // 启用 webpack 的模块热替换特性() inline: true, // 实现实时重载（实现自动刷新功能）默认情况下是 true。 host: "localhost" // 如果你希望服务器外部可访问，指定使用一个 host。默认是 localhost(也就是你可以不写这个host这个配置属性)。 &#125; plugins: [ new HtmlWebpackPlugin(&#123; title: '首页', // 用于生成的HTML文档的标题 filename: 'index.html', //写入HTML的文件。默认为index.html。也可以指定一个子目录（例如：）assets/admin.html template: 'index.html' // Webpack需要模板的路径 &#125;), new webpack.HotModuleReplacementPlugin() // 需要结合 启用热替换模块(Hot Module Replacement)，也被称为 HMR ]&#125; 12、开发环境和生产环境的分离（1）开发环境与生产环境分离的原因如下：在开发环境中，我们使用热更新插件帮助我们实现浏览器的自动更新功能，我们的代码没有进行压缩，如果压缩了不方便我们调试代码等等，所以以上这些代码不应出现在生产环境中。生产环境中，我们把项目部署到服务器时，我们会对代码进行各种各样的优化，比如压缩代码等等，这时候我们不应该把这些代码放到开发环境中，不利于代码开发和调试。 总结：针对以上这些说明，我们很有必要把区分开发环境与生产环境分离。 （2）开发环境的配置和生产换环境配置的区别。开发环境有的配置，生产环境不一定有，比如说热更新时使用到的HotModuleReplacementPlugin。 生产环境有的配置，开发环境不一定有，比如说用来压缩js用的UglifyJsPlugin。 （3）如何是开开发和生产分离？ 1&gt; 因为webpack 默认找的是 webpack.config.js配置文件，所以要把开发环境的webpack.config.js配置文件改为webpack.dev.config.js代表开发环境的配置文件。 2&gt; 新建一个webpack.prod.config.js，再把开发环境中的webpack.config.js复制进去（没用的配置文件该删除的删除） 3&gt; 修改package.json文件（在scripts 标签中添加”dev”和”prod” 属性配置） 123456js "scripts": &#123; "test": "echo \"Error: no test specified\" &amp;&amp; exit 1", "dev": "webpack --config webpack.dev.config.js", "prod": "webpack --config webpack.prod.config.js" &#125;, 执行构建命令 1234# 执行开发环境的中配置 npm run dev# 执行生产环境的中配置 npm run prod 13、在生产环境中配置代码压缩功能配置webpack.prod.config.js 文件 12345678910111213// webpack.prod.config.jsvar UglifyJsPlugin = webpack.optimize.UglifyJsPlugin; // …… plugins: [ // …… // js代码 压缩 new UglifyJsPlugin(&#123; compress: &#123; warnings: false &#125; &#125;)] 执行 npm run prod 命令]]></content>
      <categories>
        <category>web前端</category>
        <category>VUE</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>VUE</tag>
        <tag>webpack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue-cli 使用]]></title>
    <url>%2F2019%2F07%2F15%2F03.web%E5%89%8D%E7%AB%AF%2FVUE%2Fvue-cli%20%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载自: https://www.cnblogs.com/m18320364494/p/9560522.html 一、安装vue-cli安装vue-cli的前提是你已经安装了npm，安装npm你可以直接下载node的安装包进行安装。你可以在命令行工具里输入npm -v 检测你是否安装了npm和版本情况。出现版本号说明你已经安装了npm和node，我这里的npm版本为3.10.10。如果该命令不可以使用，需要安装node软件包，根据你的系统版本选择下载安装就可以了。 npm没有问题，接下来我们可以用npm 命令安装vue-cli了，在命令行输入下面的命令： 1npm install vue-cli -g -g :代表全局安装。如果你安装时报错，一般是网络问题，你可以尝试用cnpm来进行安装。安装完成后，可以用vue -V来进行查看 vue-cli的版本号。注意这里的V是大写的。我这里版本号是2.8.1. 如果vue -V的命令管用了，说明已经顺利的把vue-cli安装到我们的计算机里了。 二、初始化项目我们用vue init命令来初始化项目，具体看一下这条命令的使用方法。 1vue init &lt;template-name&gt; &lt;project-name&gt; init：表示我要用vue-cli来初始化项目 template-name：表示模板名称，vue-cli官方为我们提供了5种模板， webpack: 一个全面的webpack+vue-loader的模板，功能包括热加载，linting,检测和CSS扩展。 webpack-simple: 一个简单webpack+vue-loader的模板，不包含其他功能，让你快速的搭建vue的开发环境。 browserify: 一个全面的Browserify+vueify 的模板，功能包括热加载，linting,单元检测。 browserify-simple: 一个简单Browserify+vueify的模板，不包含其他功能，让你快速的搭建vue的开发环境。 simple: 一个最简单的单页应用模板。 project-name: 标识项目名称，这个你可以根据自己的项目来起名字。 在实际开发中，一般我们都会使用webpack这个模板，那我们这里也安装这个模板，在命令行输入以下命令： 1vue init webpack vuecliTest 输入命令后，会询问我们几个简单的选项，我们根据自己的需要进行填写就可以了。 Project name :项目名称 ，如果不需要更改直接回车就可以了。注意：这里不能使用大写，所以我把名称改成了vueclitest Project description:项目描述，默认为A Vue.js project,直接回车，不用编写。 Author：作者，如果你有配置git的作者，他会读取。 Install vue-router? 是否安装vue的路由插件，我们这里需要安装，所以选择Y Use ESLint to lint your code? 是否用ESLint来限制你的代码错误和风格。我们这里不需要输入n，如果你是大型团队开发，最好是进行配置。 setup unit tests with Karma + Mocha? 是否需要安装单元测试工具Karma+Mocha，我们这里不需要，所以输入n。 Setup e2e tests with Nightwatch?是否安装e2e来进行用户行为模拟测试，我们这里不需要，所以输入n。 命令行出现上面的文字，说明我们已经初始化好了第一步。命令行提示我们现在可以作的三件事情。 1、cd vuecliTest 进入我们的vue项目目录。 2、npm install 安装我们的项目依赖包，也就是安装package.json里的包，如果你网速不好，你也可以使用cnpm来安装。 3、npm run dev 开发模式下运行我们的程序（实际上是执行了package.json 中的script脚本）。给我们自动构建了开发用的服务器环境和在浏览器中打开，并实时监视我们的代码更改，即时呈现给我们。 三、Vue-cli项目结构讲解vue-cli脚手架工具就是为我们搭建了开发所需要的环境，为我们省去了很多精力。有必要对这个环境进行熟悉，我们就从项目的结构讲起。 Ps：由于版本实时更新和你选择安装的不同（这里列出的是模板为webpack的目录结构），所以你看到的有可能和下边的有所差别。 123456789101112131415161718192021222324252627282930.|-- build // 项目构建(webpack)相关代码| |-- build.js // 生产环境构建代码| |-- check-version.js // 检查node、npm等版本| |-- dev-client.js // 热重载相关| |-- dev-server.js // 构建本地服务器| |-- utils.js // 构建工具相关| |-- webpack.base.conf.js // webpack基础配置| |-- webpack.dev.conf.js // webpack开发环境配置| |-- webpack.prod.conf.js // webpack生产环境配置|-- config // 项目开发环境配置| |-- dev.env.js // 开发环境变量| |-- index.js // 项目一些配置变量| |-- prod.env.js // 生产环境变量| |-- test.env.js // 测试环境变量|-- src // 源码目录| |-- components // vue公共组件| |-- store // vuex的状态管理| |-- App.vue // 页面入口文件| |-- main.js // 程序入口文件，加载各种公共组件|-- static // 静态文件，比如一些图片，json数据等| |-- data // 群聊分析得到的数据用于数据可视化|-- .babelrc // ES6语法编译配置|-- .editorconfig // 定义代码格式|-- .gitignore // git上传需要忽略的文件格式|-- README.md // 项目说明|-- favicon.ico |-- index.html // 入口页面|-- package.json // 项目基本信息. 重要文件讲解： package.jsonpackage.json文件是项目根目录下的一个文件，定义该项目开发所需要的各种模块以及一些项目配置信息（如项目名称、版本、描述、作者等）。 package.json 里的scripts字段，这个字段定义了你可以用npm运行的命令。在开发环境下，在命令行工具中运行npm run dev 就相当于执行 node build/dev-server.js .也就是开启了一个node写的开发行建议服务器。由此可以看出script字段是用来指定npm相关命令的缩写。 1234"scripts": &#123; "dev": "node build/dev-server.js", "build": "node build/build.js"&#125;, dependencies字段和devDependencies字段 dependencies字段指项目运行时所依赖的模块； devDependencies字段指定了项目开发时所依赖的模块； 在命令行中运行npm install命令，会自动安装dependencies和devDempendencies字段中的模块。package.json还有很多相关配置，如果你想全面了解，可以专门去百度学习一下。 webpack配置相关我们在上面说了运行npm run dev 就相当于执行了node build/dev-server.js,说明这个文件相当重要，先来熟悉一下它。 我贴出代码并给出重要的解释。 dev-server.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 检查 Node 和 npm 版本require('./check-versions')()// 获取 config/index.js 的默认配置var config = require('../config')// 如果 Node 的环境无法判断当前是 dev / product 环境// 使用 config.dev.env.NODE_ENV 作为当前的环境if (!process.env.NODE_ENV) process.env.NODE_ENV = JSON.parse(config.dev.env.NODE_ENV)// 使用 NodeJS 自带的文件路径工具var path = require('path')// 使用 expressvar express = require('express')// 使用 webpackvar webpack = require('webpack')// 一个可以强制打开浏览器并跳转到指定 url 的插件var opn = require('opn')// 使用 proxyTablevar proxyMiddleware = require('http-proxy-middleware')// 使用 dev 环境的 webpack 配置var webpackConfig = require('./webpack.dev.conf')// default port where dev server listens for incoming traffic// 如果没有指定运行端口，使用 config.dev.port 作为运行端口var port = process.env.PORT || config.dev.port// Define HTTP proxies to your custom API backend// https://github.com/chimurai/http-proxy-middleware// 使用 config.dev.proxyTable 的配置作为 proxyTable 的代理配置var proxyTable = config.dev.proxyTable// 使用 express 启动一个服务var app = express()// 启动 webpack 进行编译var compiler = webpack(webpackConfig)// 启动 webpack-dev-middleware，将 编译后的文件暂存到内存中var devMiddleware = require('webpack-dev-middleware')(compiler, &#123; publicPath: webpackConfig.output.publicPath, stats: &#123; colors: true, chunks: false &#125;&#125;)// 启动 webpack-hot-middleware，也就是我们常说的 Hot-reloadvar hotMiddleware = require('webpack-hot-middleware')(compiler)// force page reload when html-webpack-plugin template changescompiler.plugin('compilation', function (compilation) &#123; compilation.plugin('html-webpack-plugin-after-emit', function (data, cb) &#123; hotMiddleware.publish(&#123; action: 'reload' &#125;) cb() &#125;)&#125;)// proxy api requests// 将 proxyTable 中的请求配置挂在到启动的 express 服务上Object.keys(proxyTable).forEach(function (context) &#123; var options = proxyTable[context] if (typeof options === 'string') &#123; options = &#123; target: options &#125; &#125; app.use(proxyMiddleware(context, options))&#125;)// handle fallback for HTML5 history API// 使用 connect-history-api-fallback 匹配资源，如果不匹配就可以重定向到指定地址app.use(require('connect-history-api-fallback')())// serve webpack bundle output// 将暂存到内存中的 webpack 编译后的文件挂在到 express 服务上app.use(devMiddleware)// enable hot-reload and state-preserving// compilation error display// 将 Hot-reload 挂在到 express 服务上app.use(hotMiddleware)// serve pure static assets// 拼接 static 文件夹的静态资源路径var staticPath = path.posix.join(config.dev.assetsPublicPath, config.dev.assetsSubDirectory)// 为静态资源提供响应服务app.use(staticPath, express.static('./static'))// 让我们这个 express 服务监听 port 的请求，并且将此服务作为 dev-server.js 的接口暴露module.exports = app.listen(port, function (err) &#123; if (err) &#123; console.log(err) return &#125; var uri = 'http://localhost:' + port console.log('Listening at ' + uri + '\n') // when env is testing, don't need open it // 如果不是测试环境，自动打开浏览器并跳到我们的开发地址 if (process.env.NODE_ENV !== 'testing') &#123; opn(uri) &#125;&#125;) webpack.base.confg.js webpack的基础配置文件123456789101112131415161718192021222324252627282930......module.export = &#123; // 编译入口文件 entry: &#123;&#125;, // 编译输出路径 output: &#123;&#125;, // 一些解决方案配置 resolve: &#123;&#125;, resolveLoader: &#123;&#125;, module: &#123; // 各种不同类型文件加载器配置 loaders: &#123; ... ... // js文件用babel转码 &#123; test: /\.js$/, loader: 'babel', include: projectRoot, // 哪些文件不需要转码 exclude: /node_modules/ &#125;, ... ... &#125; &#125;, // vue文件一些相关配置 vue: &#123;&#125;&#125; .babelrcBabel解释器的配置文件，存放在根目录下。Babel是一个转码器，项目里需要用它将ES6代码转为ES5代码。如果你想了解更多，可以查看babel的知识。 1234567891011121314151617&#123; //设定转码规则 "presets": [ ["env", &#123; "modules": false &#125;], "stage-2" ], //转码用的插件 "plugins": ["transform-runtime"], "comments": false, //对BABEL_ENV或者NODE_ENV指定的不同的环境变量，进行不同的编译操作 "env": &#123; "test": &#123; "presets": ["env", "stage-2"], "plugins": [ "istanbul" ] &#125; &#125;&#125; .editorconfig该文件定义项目的编码规范，编译器的行为会与.editorconfig文件中定义的一致，并且其优先级比编译器自身的设置要高，这在多人合作开发项目时十分有用而且必要。 123456789root = true[*] // 对所有文件应用下面的规则charset = utf-8 // 编码规则用utf-8indent_style = space // 缩进用空格indent_size = 2 // 缩进数量为2个空格end_of_line = lf // 换行符格式insert_final_newline = true // 是否在文件的最后插入一个空行trim_trailing_whitespace = true // 是否删除行尾的空格 这是比较重要的关于vue-cli的配置文件，当然还有很多文件，我们会在以后的文章中讲解。 四、Vue-cli的模板1、npm run build 命令有小伙伴问我，如何把写好的Vue网页放到服务器上，那我就在这里讲解一下，主要的命令就是要用到npm run build 命令。我们在命令行中输入npm run build命令后，vue-cli会自动进行项目发布打包。你在package.json文件的scripts字段中可以看出，你执行的npm run build命令就相对执行的 node build/build.js 。 package.json的scripts 字段：1234"scripts": &#123; "dev": "node build/dev-server.js", "build": "node build/build.js"&#125;, 在执行完npm run build命令后，在你的项目根目录生成了dist文件夹，这个文件夹里边就是我们要传到服务器上的文件。 dist文件夹下目录包括： index.html 主页文件:因为我们开发的是单页web应用，所以说一般只有一个html文件。 static 静态资源文件夹：里边js、CSS和一些图片。 2、main.js文件解读main.js是整个项目的入口文件,在src文件夹下： 12345678910111213import Vue from 'vue' import App from './App'import router from './router'Vue.config.productionTip = false //生产环境提示，这里设置成了false/* eslint-disable no-new */new Vue(&#123; el: '#app', router, template: '&lt;App/&gt;', components: &#123; App &#125;&#125;) 通过代码可以看出这里引进了App的组件和’&lt; APP /&gt;’的模板，它是通过 import App from ‘./App’这句代码引入的。 我们找到App.vue文件，打开查看。 3、App.vue文件:1234567891011121314151617181920212223&lt;template&gt; &lt;div id=&quot;app&quot;&gt; &lt;img src=&quot;./assets/logo.png&quot;&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &apos;app&apos;&#125;&lt;/script&gt;&lt;style&gt;#app &#123; font-family: &apos;Avenir&apos;, Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; color: #2c3e50; margin-top: 60px;&#125;&lt;/style&gt; app.vue文件我们可以分成三部分解读， &lt; template &gt;&lt; /template &gt;标签包裹的内容：这是模板的HTMLDom结构，里边引入了一张图片和&lt; router-view &gt;&lt; /router-view &gt;标签，&lt; router-view &gt;标签说明使用了路由机制。我们会在以后专门拿出一篇文章讲Vue-router。 &lt; script &gt;&lt; /script &gt;标签包括的js内容：你可以在这里些一些页面的动态效果和Vue的逻辑代码。 &lt; style&gt;&lt; /style &gt;标签包裹的css内容：这里就是你平时写的CSS样式，对页面样子进行装饰用的，需要特别说明的是你可以用&lt; style scoped &gt;&lt; /style &gt;来声明这些css样式只在本模板中起作用。 4、router/index.js 路由文件引文在app.vue中我们看到了路由文件，虽然router的内容比较多，但是我们先简单的看一下。 123456789101112131415import Vue from 'vue'import Router from 'vue-router'import Hello from '@/components/Hello'Vue.use(Router)export default new Router(&#123; routes: [ &#123; path: '/', name: 'Hello', component: Hello &#125; ]&#125;) 我们可以看到 import Hello from ‘@/components/Hello’这句话， 文件引入了/components/Hello.vue文件。这个文件里就配置了一个路由，就是当我们访问网站时给我们显示Hello.vue的内容。 5、Hello.vue文件解读：这个文件就是我们在第一节课看到的页面文件了。也是分为&lt; template &gt;&lt; script &gt;&lt; style &gt;三个部分，以后我们大部分的工作都是写这些.vue结尾的文件。现在我们可以试着改一些内容，然后预览一下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;template&gt; &lt;div class=&quot;hello&quot;&gt; &lt;h1&gt;&#123;&#123; msg &#125;&#125;&lt;/h1&gt; &lt;h2&gt;Essential Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://vuejs.org&quot; target=&quot;_blank&quot;&gt;Core Docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://forum.vuejs.org&quot; target=&quot;_blank&quot;&gt;Forum&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://gitter.im/vuejs/vue&quot; target=&quot;_blank&quot;&gt;Gitter Chat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://twitter.com/vuejs&quot; target=&quot;_blank&quot;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &lt;br&gt; &lt;li&gt;&lt;a href=&quot;http://vuejs-templates.github.io/webpack/&quot; target=&quot;_blank&quot;&gt;Docs for This Template&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Ecosystem&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://router.vuejs.org/&quot; target=&quot;_blank&quot;&gt;vue-router&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vuex.vuejs.org/&quot; target=&quot;_blank&quot;&gt;vuex&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vue-loader.vuejs.org/&quot; target=&quot;_blank&quot;&gt;vue-loader&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/vuejs/awesome-vue&quot; target=&quot;_blank&quot;&gt;awesome-vue&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &apos;hello&apos;, data () &#123; return &#123; msg: &apos;Welcome to Your Vue.js App&apos; &#125; &#125;&#125;&lt;/script&gt;&lt;!-- Add &quot;scoped&quot; attribute to limit CSS to this component only --&gt;&lt;style scoped&gt;h1, h2 &#123; font-weight: normal;&#125;ul &#123; list-style-type: none; padding: 0;&#125;li &#123; display: inline-block; margin: 0 10px;&#125;a &#123; color: #42b983;&#125;&lt;/style&gt;]]></content>
      <categories>
        <category>web前端</category>
        <category>VUE</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>VUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue 基础语法]]></title>
    <url>%2F2019%2F07%2F10%2F03.web%E5%89%8D%E7%AB%AF%2FVUE%2Fvue%20%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. vue的基础语法介绍1-1基本数据绑定12345678910&lt;div id="app"&gt; &#123;&#123; msg &#125;&#125;&lt;/div&gt;//scriptnew Vue(&#123; el:"#app",//代表vue的范围 data:&#123; msg:'hello Vue' //数据 &#125;&#125;) 在这个例子中我们可以进行赋值 123var app = new Vue(...);app.msg = '初探vue';//那么页面的值就被改变了 1-2 v-html命令12345678910&lt;div id="app" v-html="msg"&gt; &lt;/div&gt;//scriptnew Vue(&#123; el:"#app",//代表vue的范围 data:&#123; msg:'&lt;h1&gt;你好，世界&lt;/h1&gt;' //这里就不会是文本了 而是会被当成是html标签了 &#125;&#125;) 1-3 v-on:click||@click指令12345678910111213141516171819202122232425262728293031&lt;div id="app"&gt;&lt;button v-on:clikc="clickHead"&gt;事件&lt;/button&gt;&lt;button @click="clickHead"&gt;事件&lt;/button&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", methods:&#123; clickHead:functoin()&#123; alert('vue的事件绑定') &#125; &#125;&#125;)//在es6语法中对象中的函数可以const json=&#123; clickHead()&#123; //do something &#125;&#125;json.clickHead()调用方法//和一样的const json=&#123; clickHead:function()&#123; //do something &#125;&#125; 1-4 v-bind 属性绑定指令 例如绑定class 和id 已经已经存在的属性 和自定义属性 绑定类名 12345678910111213141516171819202122&lt;div id="app"&gt; &lt;p v-bind:class="className"&gt;&#123;&#123;msg&#125;&#125;&lt;/p&gt; &lt;/div&gt;/*v-bind 自定义名字v-bind:id="..." 绑定id名字v-bind:title="..."绑定title属性v-bind:style="..." 绑定样式属性 v-bind:...="..."绑定自定义属性、、、*///jsnew Vue(&#123; el:"#app", data:&#123; msg:'这是v-bind绑定数据', className:'contatiner' &#125;,&#125;)const Name = document.querySelector('.contatiner');console.log(Name) //能正常的获取这个元素 1-5 v-show 根据值的真假 控制元素的display的属性 12345678910111213&lt;div id="app"&gt; &lt;p v-show="msg"&gt; 可以看到啊 &lt;/p&gt; &lt;p v-show="msg1"&gt; 不可以看到啊 &lt;/p&gt; &lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:1+1==2, msg1:1+1!=2 &#125;&#125;) 1-6 v-text 赋予文本值 1234567891011&lt;div id="app"&gt; &lt;p v-text="msg"&gt; 可以看到啊 &lt;/p&gt; &lt;!-- 最终会被替换掉 1+1==2 --&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:'你好哈 v-text' &#125;&#125;) 1-7 v-for 循环 123456789101112131415161718 &lt;div id="app"&gt; &lt;ol&gt; &lt;li v-for="module in modules"&gt;&#123;&#123;module.msg&#125;&#125;&lt;/li&gt; &lt;/ol&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; modules:[ &#123;msg:'第一个'&#125;, &#123;msg:'第二个'&#125;, &#123;msg:'第三个'&#125;, &#123;msg:'第四个'&#125; ] &#125;&#125;) 1-8 v-model 双向数据绑定123456789101112&lt;div id="app"&gt; &lt;input type="text" v-model="msg"&gt; &lt;p&gt;&#123;&#123;msg&#125;&#125;&lt;/p&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:'' &#125;&#125;) 1-9 template属性123456789101112&lt;div id="app"&gt; &lt;span&gt;你好啊&lt;/span&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:'这是数据' &#125;, template:"&lt;div&gt;模板div&lt;/div&gt;"&#125;) 最终的效果是把id为app的div直接替换成template里面的元素 注意在template的值中不能含有兄弟根节点 1234new Vue(&#123; el:"#app", template:"&lt;div&gt;1&lt;div&gt;&lt;div&gt;2&lt;div&gt;"&#125;) 这样是错误的 , 在template 可以把团苏片段放在script标签内 12345678910111213141516171819&lt;div id="app"&gt; &lt;span&gt;你好啊&lt;/span&gt; &lt;/div&gt; &lt;script type="x-template" id="temp"&gt; //这个地方就是模板片段 &lt;div id="tpl"&gt; &lt;p&gt; 这是模板啊,&#123;&#123;msg&#125;&#125; &lt;/p&gt; &lt;input type="text" v-model="msg" /&gt; &lt;/div&gt; &lt;/script&gt; &lt;script src="js/vue.min.js"&gt;&lt;/script&gt; &lt;script&gt; new Vue(&#123; el:"#app", data:&#123; msg:'' &#125;, template:"#temp" &#125;) &lt;/script&gt; 2. vue实例中的属性2-1 el el表示在vue的实例中的作用范围 123new Vue(&#123; el:"#app" //作用在id名为app的div上&#125;) 2-2 data data的属性就是数据绑定 12345678new Vue(&#123; data:&#123; msg:'数据的值' arrayData:[ &#123;title:'这是1'&#125; ] &#125;&#125;) 2-3 methods methods绑定事件 12345678new Vue(&#123; el:"#app", methods:&#123; mouseClick()&#123; alert('绑定事件') &#125; &#125;&#125;) 2-3 template template模板的绑定 1234new Vue(&#123; el:"#app", template:'&lt;div&gt;这是模板属性&lt;/div&gt;'&#125;) 2-4 render render模板的绑定 1234567891011121314151617181920212223242526new Vue(&#123; el:"#app", render(createElement)&#123; return createElement( "ol", &#123; //新创建的元素身上绑定属性 style:&#123; fontSize:'30px', border:'1px solid red', fontWeight:'bold' &#125;, attrs:&#123; title:'你好啊', coundNum:'01', id:'ls', class:'bg' &#125; &#125;, [ createElement("li",'这是第一个文本'), createElement("li",'这是第二个文本'), createElement("li",'这是第三个文本'), ] ) &#125;&#125;) 3. vue的自定义指令 在vue中除了内置的指令如v-for、v-if…,用户可以自定义指令官网 123456789//这里定义v-focusdirectives: &#123; focus: &#123; // 指令的定义 inserted: function (el) &#123; el.focus() &#125; &#125;&#125; 一个指令定义对象可以提供如下几个钩子函数 (均为可选)： bind：只调用一次，指令第一次绑定到元素时调用。在这里可以进行一次性的初始化设置。 inserted：被绑定元素插入父节点时调用 (仅保证父节点存在，但不一定已被插入文档中)。 update：所在组件的 VNode 更新时调用，但是可能发生在其子 VNode 更新之前。指令的值可能发生了改变，也可能没有。但是你可以通过比较更新前后的值来忽略不必要的模板更新 (详细的钩子函数参数见下)。 componentUpdated：指令所在组件的 VNode 及其子 VNode 全部更新后调用。 unbind：只调用一次，指令与元素解绑时调用。 4. vue的扩展4-1 v-bind根据条件绑定类名 案例 比如现在在true的情况下绑定red类名 1234567891011121314&lt;div id="app"&gt; &lt;span :class="&#123;red:addClass&#125;"&gt;&#123;&#123;msg&#125;&#125;&lt;/span&gt; &lt;!--可以利用简单的表达式--&gt;&lt;!--这是v-bind指令可以省略--&gt;&lt;/div&gt;&lt;script src="js/vue.min.js"&gt;&lt;/script&gt;&lt;script&gt; new Vue(&#123; el:"#app", data:&#123; msg:'条件绑定类名', addClass:true &#125; &#125;)&lt;/script&gt; 4-2 v-on || @eventName 事件绑定 有一个事件修饰符12345678910//阻止事件冒泡&lt;div v-on:click.stop="eventHadles"&gt;&lt;/div&gt;//阻止默认事件&lt;div v-on:click.prevent="eventHadles"&gt;&lt;/div&gt;//事件只能触发一次&lt;div v-on:click.once="eventHadles"&gt;&lt;/div&gt;//只能回车触发事件&lt;div @keyup.enter="eventHadles"&gt;&lt;/div&gt;//只能指定keyCode的值触发事件&lt;div @keyup.13="eventHadles"&gt;&lt;/div&gt; 5. vue的计算属性 -computed 例如一个案例需要过滤一些列表 而我们需要利用v-for进行循环出来列表 需要用到我们的实例的属性 computed 说透点就是过滤你的数据根据你的条件进行过滤 1234567891011121314151617181920212223242526272829303132333435363738394041//jsnew Vue(&#123; el:"#app", data:&#123; list:[ &#123; title:'你好啊', isChecked:true &#125;, &#123; title:'你好啊2', isChecked:false &#125; ]， hash:'all' //过滤条件 &#125;, computed:&#123; //重头戏 filterData()&#123; //这个根据你的条件进行过滤 记住这个函数最终返回的是数据需要return 数据出来 不需要调用此函数 let filterDatas = &#123; all(list)&#123; return list &#125;, unfinshed(list)&#123; return list.filter(function(item)&#123; return !item.isChecked &#125;) &#125;, finshed(list)&#123; return list.filter(function(item)&#123; return item.isChecked &#125;) &#125; &#125; return filterDatas[你的条件]?filterDatas[你的条件](this.list):this.list //这里的你的条件可以使hash值 然后很久hash值的不同进行过滤 不需要调用这个函数 &#125; &#125;&#125;)//然后在v-for的指令中&lt;div v-for="item in filterData"&gt;&lt;/div&gt; &lt;!--注意不是list了 而是刚刚的computed中的计算属性的函数名字--&gt; 6. vue的组件6-1 底层学习组件 组件 (Component) 是 Vue.js 最强大的功能之一。组件可以扩展 HTML 元素，封装可重用的代码。在较高层面上，组件是自定义元素，Vue.js 的编译器为它添加特殊功能。在有些情况下，组件也可以表现为用 is 特性进行了扩展的原生 HTML 元素。 12345678910111213//html&lt;div id="app"&gt; &lt;my-test&gt;&lt;/my-test&gt; &lt;!--自定义标签--&gt;&lt;/div&gt;//jsVue.component('my-test',&#123; //注册组件 template:'&lt;div&gt;初学组件&lt;/div&gt;' &#125;);new Vue(&#123; el:"#app"&#125;) 6-2 父子组件通信 利用props进行传值 1234567891011121314151617181920212223242526//局部组件&lt;div id="app"&gt; &lt;my-test msg="你好"&gt;&lt;/my-test&gt; &lt;my-test msg="传值2"&gt;&lt;/my-test&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", components:&#123; 'my-test':&#123; props:['msg'], template:'&lt;div&gt;&#123;&#123;msg&#125;&#125;&lt;/div&gt;' &#125; &#125;&#125;)//全局组件&lt;div id="app"&gt; &lt;my-test msg="你好"&gt;&lt;/my-test&gt; &lt;my-test msg="传值2"&gt;&lt;/my-test&gt;&lt;/div&gt;//jsVue.component('my-test',&#123; props:['msg'], template:'&lt;div&gt;&#123;&#123;msg&#125;&#125;&lt;/div&gt;'&#125;) 如果需要传的值在vue的实例中 12345678910111213141516171819202122232425//html&lt;div id="app"&gt; &lt;my-test v-bind:listData="list"&gt;&lt;/my-test&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; list:[ &#123;title:'这是数据'&#125;, &#123;title:'这是数据22'&#125; ] &#125;, components:&#123; 'my-test':&#123; props:['listData'], template:` &lt;select name="" id=""&gt; &lt;option v-for="item in listData"&gt;&#123;&#123;item.title&#125;&#125;&lt;/option&gt; &lt;/select&gt; ` &#125; &#125;&#125;) 7. vue获取dom元素1234567891011121314在想获取的元素身上&lt;div class="container" rel="getDom"&gt;//jsnew Vue(&#123; el:"#app", methods:&#123; _someDo()&#123; this.dom = this.$refs.getDom; &#125; &#125;, mounted()&#123; this._someDo(); //vue完成挂载 调用函数 &#125;&#125;) 8. vue渲染dom是异步$.nextTick()函数 因为vue渲染dom是异步的所以直接操作dom是会出错的 案例 例如 创建vue实例的时候请求接口数据，然后要进行dom元素操作 1234567891011121314151617new Vue(&#123; data:&#123; result:'' &#125;, created()&#123; axios.get('/data') .then(data=&gt;&#123; this.result = data.data //如果在dom中用到了v-for这些元素 而我们乡操作这些元素 this.$nextTick(()=&gt;&#123; //这个函数的意义就是等待dom渲染结束后执行 &#125;) &#125;) .catch(err=&gt;&#123; //错误处理 &#125;) &#125;&#125;)]]></content>
      <categories>
        <category>web前端</category>
        <category>VUE</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>VUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6 新语法 (二)]]></title>
    <url>%2F2019%2F07%2F09%2F03.web%E5%89%8D%E7%AB%AF%2FJavaScript%2FES6%20%E6%96%B0%E8%AF%AD%E6%B3%95(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[ES6 对象对象字面量属性的简洁表示法 ES6允许对象的属性直接写变量，这时候属性名是变量名，属性值是变量值。 1234567891011const age = 12; const name = "Amy";const person = &#123;age, name&#125;; person //&#123;age: 12, name: "Amy"&#125; //等同于 const person = &#123;age: age, name: name&#125; 方法名也可以简写 12345678910111213const person = &#123; sayHi()&#123; console.log("Hi"); &#125;&#125;person.sayHi(); //"Hi"//等同于const person = &#123; sayHi:function()&#123; console.log("Hi"); &#125;&#125;person.sayHi();//"Hi" 如果是Generator 函数，则要在前面加一个星号: 1234567891011const obj = &#123; * myGenerator() &#123; yield 'hello world'; &#125;&#125;;//等同于const obj = &#123; myGenerator: function* () &#123; yield 'hello world'; &#125;&#125;; 属性名表达式 ES6允许用表达式作为属性名，但是一定要将表达式放在方括号内。 123456const obj = &#123; ["he"+"llo"]()&#123; return "Hi"; &#125;&#125;obj.hello(); //"Hi" 注意点：属性的简洁表示法和属性名表达式不能同时使用，否则会报错。 1234567891011const hello = "Hello";const obj = &#123; [hello]&#125;;obj //SyntaxError: Unexpected token &#125; const hello = "Hello";const obj = &#123; [hello+"2"]:"world"&#125;;obj //&#123;Hello2: "world"&#125; 对象拓展运算符拓展运算符（…）用于取出参数对象所有可遍历属性然后拷贝到当前对象。 基本用法 123let person = &#123;name: "Amy", age: 15&#125;;let someone = &#123; ...person &#125;;someone; //&#123;name: "Amy", age: 15&#125; 可用于合并两个对象 1234let age = &#123;age: 15&#125;;let name = &#123;name: "Amy"&#125;;let person = &#123;...age, ...name&#125;;person; //&#123;age: 15, name: "Amy"&#125; 对象的新方法一、Object.assign 1Object.assign(target, source_1, ···) 用于将源对象的所有可枚举属性复制到目标对象中。 基本用法 123456let target = &#123;a: 1&#125;;let object2 = &#123;b: 2&#125;;let object3 = &#123;c: 3&#125;;Object.assign(target,object2,object3); // 第一个参数是目标对象，后面的参数是源对象target; // &#123;a: 1, b: 2, c: 3 如果目标对象和源对象有同名属性，或者多个源对象有同名属性，则后面的属性会覆盖前面的属性。 如果该函数只有一个参数，当参数为对象时，直接返回该对象；当参数不是对象时，会先将参数转为对象然后返回。 12Object.assign(3); // Number &#123;3&#125;typeof Object.assign(3); // "object" 因为 null 和 undefined 不能转化为对象，所以会报错: 1234567Object.assign(null); // TypeError: Cannot convert undefined or null to objectObject.assign(undefined); // TypeError: Cannot convert undefined or null to object当参数不止一个时，null 和 undefined 不放第一个，即不为目标对象时，会跳过 null 和 undefined ，不报错Object.assign(1,undefined); // Number &#123;1&#125;Object.assign(&#123;a: 1&#125;,null); // &#123;a: 1&#125; Object.assign(undefined,&#123;a: 1&#125;); // TypeError: Cannot convert undefined or null to object 注意点 assign 的属性拷贝是浅拷贝: 12345let sourceObj = &#123; a: &#123; b: 1&#125;&#125;;let targetObj = &#123;c: 3&#125;;Object.assign(targetObj, sourceObj);targetObj.a.b = 2;sourceObj.a.b; // 2 同名属性替换 1234targetObj = &#123; a: &#123; b: 1, c:2&#125;&#125;;sourceObj = &#123; a: &#123; b: "hh"&#125;&#125;;Object.assign(targetObj, sourceObj);targetObj; // &#123;a: &#123;b: "hh"&#125;&#125; 数组的处理 1Object.assign([2,3], [5]); // [5,3] 会将数组处理成对象，所以先将 [2,3] 转为 {0:2,1:3} ，然后再进行属性复制，所以源对象的 0 号属性覆盖了目标对象的 0。 二、Object.is 1Object.is(value1, value2) 用来比较两个值是否严格相等，与（===）基本类似。 基本用法 1234Object.is("q","q"); // trueObject.is(1,1); // trueObject.is([1],[1]); // falseObject.is(&#123;q:1&#125;,&#123;q:1&#125;); // false 与（===）的区别 123456//一是+0不等于-0Object.is(+0,-0); //false+0 === -0 //true//二是NaN等于本身Object.is(NaN,NaN); //trueNaN === NaN //false ES6 数组数组创建Array.of 将参数中所有值作为元素形成数组。 1234567console.log(Array.of(1, 2, 3, 4)); // [1, 2, 3, 4] // 参数值可为不同类型console.log(Array.of(1, '2', true)); // [1, '2', true] // 参数为空时返回空数组console.log(Array.of()); // [] Array.from 将类数组对象或可迭代对象转化为数组。 12345// 参数为数组,返回与原数组一样的数组console.log(Array.from([1, 2])); // [1, 2] // 参数含空位console.log(Array.from([1, , 3])); // [1, undefined, 3] array.from 接收的参数: Array.from(arrayLike[, mapFn[, thisArg]]) ; 返回值为转换后的数组。 arrayLike: 想要转换的类数组对象或可迭代对象。 1console.log(Array.from([1, 2, 3])); // [1, 2, 3] mapFn: 可选，map 函数，用于对每个元素进行处理，放入数组的是处理后的元素。 1console.log(Array.from([1, 2, 3], (n) =&gt; n * 2)); // [2, 4, 6] thisArg: 可选，用于指定 map 函数执行时的 this 对象。 123456789let map = &#123; do: function(n) &#123; return n * 2; &#125;&#125;let arrayLike = [1, 2, 3];console.log(Array.from(arrayLike, function (n)&#123; return this.do(n);&#125;, map)); // [2, 4, 6] 类数组对象 一个类数组对象必须含有 length 属性，且元素属性名必须是数值或者可转换为数值的字符。 1234567891011121314151617181920212223let arr = Array.from(&#123; 0: '1', 1: '2', 2: 3, length: 3&#125;);console.log(); // ['1', '2', 3] // 没有 length 属性,则返回空数组let array = Array.from(&#123; 0: '1', 1: '2', 2: 3,&#125;);console.log(array); // [] // 元素属性名不为数值且无法转换为数值，返回长度为 length 元素值为 undefined 的数组 let array1 = Array.from(&#123; a: 1, b: 2, length: 2&#125;);console.log(array1); // [undefined, undefined] 转换可迭代对象 转换map 12345let map = new Map();map.set('key0', 'value0');map.set('key1', 'value1');console.log(Array.from(map)); // [['key0', 'value0'],['key1',// 'value1']] 转换set 123let arr = [1, 2, 3];let set = new Set(arr);console.log(Array.from(set)); // [1, 2, 3] 转换字符串 12let str = 'abc';console.log(Array.from(str)); // ["a", "b", "c"] 数组拓展方法查找 find() 查找数组中符合条件的元素,若有多个符合条件的元素，则返回第一个元素。 12345let arr = Array.of(1, 2, 3, 4);console.log(arr.find(item =&gt; item &gt; 2)); // 3 // 数组空位处理为 undefinedconsole.log([, 1].find(n =&gt; true)); // undefined findIndex() 查找数组中符合条件的元素索引，若有多个符合条件的元素，则返回第一个元素索引。 1234567let arr = Array.of(1, 2, 1, 3);// 参数1：回调函数// 参数2(可选)：指定回调函数中的 this 值console.log(arr.findIndex(item =&gt; item = 1)); // 0 // 数组空位处理为 undefinedconsole.log([, 1].findIndex(n =&gt; true)); //0 填充 fill() 将一定范围索引的数组元素内容填充为单个指定的值。 12345let arr = Array.of(1, 2, 3, 4);// 参数1：用来填充的值// 参数2：被填充的起始索引// 参数3(可选)：被填充的结束索引，默认为数组末尾console.log(arr.fill(0,1,2)); // [1, 0, 3, 4] copyWithin() 将一定范围索引的数组元素修改为此数组另一指定范围索引的元素。 123456789// 参数1：被修改的起始索引// 参数2：被用来覆盖的数据的起始索引// 参数3(可选)：被用来覆盖的数据的结束索引，默认为数组末尾console.log([1, 2, 3, 4].copyWithin(0,2,4)); // [3, 4, 3, 4] // 参数1为负数表示倒数console.log([1, 2, 3, 4].copyWithin(-2, 0)); // [1, 2, 1, 2] console.log([1, 2, ,4].copyWithin(0, 2, 4)); // [, 4, , 4] 遍历 entries()： 遍历键值对。 keys()： 遍历键名； values(): 遍历键值； 12345678910111213for(let [key, value] of ['a', 'b'].entries())&#123; console.log(key, value);&#125;// 0 "a"// 1 "b" // 不使用 for... of 循环let entries = ['a', 'b'].entries();console.log(entries.next().value); // [0, "a"]console.log(entries.next().value); // [1, "b"] // 数组含空位console.log([...[,'a'].entries()]); // [[0, undefined], [1, "a"]] 包含 includes() 数组是否包含指定值。 注意：与 Set 和 Map 的 has 方法区分；Set 的 has 方法用于查找值；Map 的 has 方法用于查找键名。 12345678// 参数1：包含的指定值[1, 2, 3].includes(1); // true // 参数2：可选，搜索的起始索引，默认为0[1, 2, 3].includes(1, 2); // false // NaN 的包含判断[1, NaN, 3].includes(NaN); // true 嵌套数组转一维数组 flat() 12345678910console.log([1 ,[2, 3]].flat()); // [1, 2, 3] // 指定转换的嵌套层数console.log([1, [2, [3, [4, 5]]]].flat(2)); // [1, 2, 3, [4, 5]] // 不管嵌套多少层console.log([1, [2, [3, [4, 5]]]].flat(Infinity)); // [1, 2, 3, 4, 5] // 自动跳过空位console.log([1, [2, , 3]].flat()); // [1, 2, 3] flatMap() 先对数组中每个元素进行了的处理，再对数组执行 flat() 方法。 123// 参数1：遍历函数，该遍历函数可接受3个参数：当前元素、当前元素索引、原数组// 参数2：指定遍历函数中 this 的指向console.log([1, 2, 3].flatMap(n =&gt; [n * 2])); // [2, 4, 6] 数组缓冲区数组缓冲区是内存中的一段地址。 定型数组的基础。 实际字节数在创建时确定，之后只可修改其中的数据，不可修改大小。 创建数组缓冲区 通过构造函数创建: 123456let buffer = new ArrayBuffer(10);console.log(buffer.byteLength); // 10分割已有数组缓冲区let buffer = new ArrayBuffer(10);let buffer1 = buffer.slice(1, 3);console.log(buffer1.byteLength); // 2 视图 视图是用来操作内存的接口。 视图可以操作数组缓冲区或缓冲区字节的子集,并按照其中一种数值数据类型来读取和写入数据。 DataView 类型是一种通用的数组缓冲区视图,其支持所有8种数值型数据类型。 12345678910// 默认 DataView 可操作数组缓冲区全部内容let buffer = new ArrayBuffer(10); dataView = new DataView(buffer); dataView.setInt8(0,1);console.log(dataView.getInt8(0)); // 1 // 通过设定偏移量(参数2)与长度(参数3)指定 DataView 可操作的字节范围let buffer1 = new ArrayBuffer(10); dataView1 = new DataView(buffer1, 0, 3);dataView1.setInt8(5,1); // RangeError 定型数组数组缓冲区的特定类型的视图。 可以强制使用特定的数据类型，而不是使用通用的 DataView 对象来操作数组缓冲区。 创建 通过数组缓冲区生成 123let buffer = new ArrayBuffer(10), view = new Int8Array(buffer);console.log(view.byteLength); // 10 通过构造函数 1234567891011121314151617181920212223242526let view = new Int32Array(10);console.log(view.byteLength); // 40console.log(view.length); // 10 // 不传参则默认长度为0// 在这种情况下数组缓冲区分配不到空间，创建的定型数组不能用来保存数据let view1 = new Int32Array();console.log(view1.byteLength); // 0console.log(view1.length); // 0 // 可接受参数包括定型数组、可迭代对象、数组、类数组对象let arr = Array.from(&#123; 0: '1', 1: '2', 2: 3, length: 3&#125;);let view2 = new Int16Array([1, 2]), view3 = new Int32Array(view2), view4 = new Int16Array(new Set([1, 2, 3])), view5 = new Int16Array([1, 2, 3]), view6 = new Int16Array(arr);console.log(view2 .buffer === view3.buffer); // falseconsole.log(view4.byteLength); // 6console.log(view5.byteLength); // 6console.log(view6.byteLength); // 6 注意要点 length 属性不可写，如果尝试修改这个值，在非严格模式下会直接忽略该操作，在严格模式下会抛出错误。 123let view = new Int16Array([1, 2]);view.length = 3;console.log(view.length); // 2 定型数组可使用 entries()、keys()、values()进行迭代。 123456let view = new Int16Array([1, 2]);for(let [k, v] of view.entries())&#123; console.log(k, v);&#125;// 0 1// 1 2 find() 等方法也可用于定型数组，但是定型数组中的方法会额外检查数值类型是否安全,也会通过 Symbol.species 确认方法的返回值是定型数组而非普通数组。concat() 方法由于两个定型数组合并结果不确定，故不能用于定型数组；另外，由于定型数组的尺寸不可更改,可以改变数组的尺寸的方法，例如 splice() ，不适用于定型数组。 12let view = new Int16Array([1, 2]);view.find((n) &gt; 1); // 2 所有定型数组都含有静态 of() 方法和 from() 方法,运行效果分别与 Array.of() 方法和 Array.from() 方法相似,区别是定型数组的方法返回定型数组,而普通数组的方法返回普通数组。 12let view = Int16Array.of(1, 2);console.log(view instanceof Int16Array); // true 定型数组不是普通数组，不继承自 Array 。 12let view = new Int16Array([1, 2]);console.log(Array.isArray(view)); // false 定型数组中增加了 set() 与 subarray() 方法。 set() 方法用于将其他数组复制到已有定型数组, subarray() 用于提取已有定型数组的一部分形成新的定型数组。 123456789101112131415161718// set 方法// 参数1：一个定型数组或普通数组// 参数2：可选，偏移量，开始插入数据的位置，默认为0let view= new Int16Array(4);view.set([1, 2]);view.set([3, 4], 2);console.log(view); // [1, 2, 3, 4] // subarray 方法// 参数1：可选，开始位置// 参数2：可选，结束位置(不包含结束位置)let view= new Int16Array([1, 2, 3, 4]), subview1 = view.subarray(), subview2 = view.subarray(1), subview3 = view.subarray(1, 3);console.log(subview1); // [1, 2, 3, 4]console.log(subview2); // [2, 3, 4]console.log(subview3); // [2, 3] 拓展运算符复制数组 12345678let arr = [1, 2], arr1 = [...arr];console.log(arr1); // [1, 2] // 数组含空位let arr2 = [1, , 3], arr3 = [...arr2];console.log(arr3); [1, undefined, 3] ES6 迭代器IteratorIterator 是 ES6 引入的一种新的遍历机制，迭代器有两个核心概念： 迭代器是一个统一的接口，它的作用是使各种数据结构可被便捷的访问，它是通过一个键为Symbol.iterator 的方法来实现。 迭代器是用于遍历数据结构元素的指针（如数据库中的游标）。 迭代过程迭代原理 通过 Symbol.iterator 创建一个迭代器，指向当前数据结构的起始位置 随后通过 next 方法进行向下迭代指向下一个位置， next 方法会返回当前位置的对象，对象包含了 value 和 done 两个属性， value 是当前属性的值， done 用于判断是否遍历结束 当 done 为 true 时则遍历结束 下面通过一个简单的例子进行说明： 1234567891011const items = ["zero", "one", "two"];const it = items[Symbol.iterator](); it.next();&gt;&#123;value: "zero", done: false&#125;it.next();&gt;&#123;value: "one", done: false&#125;it.next();&gt;&#123;value: "two", done: false&#125;it.next();&gt;&#123;value: undefined, done: true&#125; 上面的例子，首先创建一个数组，然后通过 Symbol.iterator 方法创建一个迭代器，之后不断的调用 next 方法对数组内部项进行访问，当属性 done 为 true 时访问结束。 迭代器是协议（使用它们的规则）的一部分，用于迭代。该协议的一个关键特性就是它是顺序的：迭代器一次返回一个值。这意味着如果可迭代数据结构是非线性的（例如树），迭代将会使其线性化。 可迭代的数据结构可迭代结构 Array String Map Set Dom元素（正在进行中） for … of 循环for…of 是 ES6 新引入的循环，用于替代 for..in 和 forEach() ，并且支持新的迭代协议。它可用于迭代常规的数据类型，如 Array 、 String 、 Map 和 Set 等等。 循环方法 12345678910111213141516171819202122let myMap = new Map();myMap.set(0, "zero");myMap.set(1, "one");myMap.set(2, "two"); // 遍历 key 和 valuefor (let [key, value] of myMap) &#123; console.log(key + " = " + value);&#125;for (let [key, value] of myMap.entries()) &#123; console.log(key + " = " + value);&#125; // 只遍历 keyfor (let key of myMap.keys()) &#123; console.log(key);&#125; // 只遍历 valuefor (let value of myMap.values()) &#123; console.log(value);&#125; 可迭代数据结构of 操作数必须是可迭代，这意味着如果是普通对象则无法进行迭代。如果数据结构类似于数组的形式，则可以借助 Array.from() 方法进行转换迭代。 1234567891011121314const arrayLink = &#123;length: 2, 0: "zero", 1: "one"&#125;// 报 TypeError 异常for (let item of arrayLink) &#123; console.log(item);&#125; // 正常运行// output:// zero// onefor (let item of Array.from(arrayLink)) &#123; console.log(item);&#125; let 、const 和 var 用于 for..of 如果使用 let 和 const ，每次迭代将会创建一个新的存储空间，这可以保证作用域在迭代的内部。 1234567const nums = ["zero", "one", "two"]; for (const num of nums) &#123; console.log(num);&#125;// 报 ReferenceErrorconsole.log(num); 从上面的例子我们看到，最后一句会报异常，原因 num 的作用域只在循环体内部，外部无效，具体可查阅 let 与 const 章节。使用 var 则不会出现上述情况，因为 var 会作用于全局，迭代将不会每次都创建一个新的存储空间。 1234567const nums = ["zero", "one", "two"]; forv (var num of nums) &#123; console.log(num);&#125;// output: twoconsole.log(num); ES6 Class 类概述在ES6中，class (类)作为对象的模板被引入，可以通过 class 关键字定义类。 class 的本质是 function。 它可以看作一个语法糖，让对象原型的写法更加清晰、更像面向对象编程的语法。 基础用法类定义类表达式可以为匿名或命名。 123456789101112// 匿名类let Example = class &#123; constructor(a) &#123; this.a = a; &#125;&#125;// 命名类let Example = class Example &#123; constructor(a) &#123; this.a = a; &#125;&#125; 类声明12345class Example &#123; constructor(a) &#123; this.a = a; &#125;&#125; 注意要点：不可重复声明。 123456789class Example&#123;&#125;class Example&#123;&#125;// Uncaught SyntaxError: Identifier 'Example' has already been // declared let Example1 = class&#123;&#125;class Example&#123;&#125;// Uncaught SyntaxError: Identifier 'Example' has already been // declared 注意要点类定义不会被提升，这意味着，必须在访问前对类进行定义，否则就会报错。 类中方法不需要 function 关键字。 方法间不能加分号。 类的主体属性 prototype ES6 中，prototype 仍旧存在，虽然可以直接自类中定义方法，但是其实方法还是定义在 prototype 上的。 覆盖方法 / 初始化时添加方法 123Example.prototype=&#123; //methods&#125; 添加方法 123Object.assign(Example.prototype,&#123; //methods&#125;) 静态属性 静态属性：class 本身的属性，即直接定义在类内部的属性（ Class.propname ），不需要实例化。 ES6 中规定，Class 内部只有静态方法，没有静态属性。 123456class Example &#123;// 新提案 static a = 2;&#125;// 目前可行写法Example.b = 2; 公共属性 12class Example&#123;&#125;Example.prototype.a = 2; 实例属性 实例属性：定义在实例对象（ this ）上的属性。 123456class Example &#123; a = 2; constructor () &#123; console.log(this.a); &#125;&#125; name属性 返回跟在 class 后的类名(存在时)。 12345678910111213let Example=class Exam &#123; constructor(a) &#123; this.a = a; &#125;&#125;console.log(Example.name); // Exam let Example=class &#123; constructor(a) &#123; this.a = a; &#125;&#125;console.log(Example.name); // Example 方法 constructor 方法 constructor 方法是类的默认方法，创建类的实例化对象时被调用。 123456class Example&#123; constructor()&#123; console.log('我是constructor'); &#125;&#125;new Example(); // 我是constructor 返回对象 1234567891011121314class Test &#123; constructor()&#123; // 默认返回实例对象 this &#125;&#125;console.log(new Test() instanceof Test); // true class Example &#123; constructor()&#123; // 指定返回对象 return new Test(); &#125;&#125;console.log(new Example() instanceof Example); // false 静态方法 123456class Example&#123; static sum(a, b) &#123; console.log(a+b); &#125;&#125;Example.sum(1, 2); // 3 原型方法 1234567class Example &#123; sum(a, b) &#123; console.log(a + b); &#125;&#125;let exam = new Example();exam.sum(1, 2); // 3 实例方法 1234567class Example &#123; constructor() &#123; this.sum = (a, b) =&gt; &#123; console.log(a + b); &#125; &#125;&#125; 类的实例化new class 的实例化必须通过 new 关键字。 1234class Example &#123;&#125; let exam1 = Example(); // Class constructor Example cannot be invoked without 'new' 实例化对象 共享原型对象 12345678910111213141516171819class Example &#123; constructor(a, b) &#123; this.a = a; this.b = b; console.log('Example'); &#125; sum() &#123; return this.a + this.b; &#125;&#125;let exam1 = new Example(2, 1);let exam2 = new Example(3, 1);console.log(exam1._proto_ == exam2._proto_); // true exam1._proto_.sub = function() &#123; return this.a - this.b;&#125;console.log(exam1.sub()); // 1console.log(exam2.sub()); // 2 decoratordecorator 是一个函数，用来修改类的行为，在代码编译时产生作用。 类修饰 一个参数 第一个参数 target，指向类本身。 123456function testable(target) &#123; target.isTestable = true;&#125;@testableclass Example &#123;&#125;Example.isTestable; // true 多个参数——嵌套实现 12345678function testable(isTestable) &#123; return function(target) &#123; target.isTestable=isTestable; &#125;&#125;@testable(true)class Example &#123;&#125;Example.isTestable; // true 实例属性 上面两个例子添加的是静态属性，若要添加实例属性，在类的 prototype 上操作即可。 方法修饰 3个参数：target（类的原型对象）、name（修饰的属性名）、descriptor（该属性的描述对象） 12345678910class Example &#123; @writable sum(a, b) &#123; return a + b; &#125;&#125;function writable(target, name, descriptor) &#123; descriptor.writable = false; return descriptor; // 必须返回&#125; 修饰器执行顺序 由外向内进入，由内向外执行。 123456789101112131415class Example &#123; @logMethod(1) @logMthod(2) sum(a, b)&#123; return a + b; &#125;&#125;function logMethod(id) &#123; console.log('evaluated logMethod'+id); return (target, name, desctiptor) =&gt; console.log('excuted logMethod '+id);&#125;// evaluated logMethod 1// evaluated logMethod 2// excuted logMethod 2// excuted logMethod 1 封装与继承getter / setter 定义 12345678910111213141516171819202122232425262728293031class Example&#123; constructor(a, b) &#123; this.a = a; // 实例化时调用 set 方法 this.b = b; &#125; get a()&#123; console.log('getter'); return this.a; &#125; set a(a)&#123; console.log('setter'); this.a = a; // 自身递归调用 &#125;&#125;let exam = new Example(1,2); // 不断输出 setter ，最终导致 RangeErrorclass Example1&#123; constructor(a, b) &#123; this.a = a; this.b = b; &#125; get a()&#123; console.log('getter'); return this._a; &#125; set a(a)&#123; console.log('setter'); this._a = a; &#125;&#125;let exam1 = new Example1(1,2); // 只输出 setter , 不会调用 getter 方法console.log(exam._a); // 1, 可以直接访问 getter 不可单独出现 123456789class Example &#123; constructor(a) &#123; this.a = a; &#125; get a() &#123; return this.a; &#125;&#125;let exam = new Example(1); // Uncaught TypeError: Cannot set property // a of #&lt;Example&gt; which has only a getter getter 与 setter 必须同级出现 123456789101112131415161718192021222324252627282930313233343536class Father &#123; constructor()&#123;&#125; get a() &#123; return this._a; &#125;&#125;class Child extends Father &#123; constructor()&#123; super(); &#125; set a(a) &#123; this._a = a; &#125;&#125;let test = new Child();test.a = 2;console.log(test.a); // undefined class Father1 &#123; constructor()&#123;&#125; // 或者都放在子类中 get a() &#123; return this._a; &#125; set a(a) &#123; this._a = a; &#125;&#125;class Child1 extends Father1 &#123; constructor()&#123; super(); &#125;&#125;let test1 = new Child1();test1.a = 2;console.log(test1.a); // 2 extends 通过 extends 实现类的继承。 1class Child extends Father &#123; ... &#125; super 子类 constructor 方法中必须有 super ，且必须出现在 this 之前。 1234567891011121314class Father &#123; constructor() &#123;&#125;&#125;class Child extends Father &#123; constructor() &#123;&#125; // or // constructor(a) &#123; // this.a = a; // super(); // &#125;&#125;let test = new Child(); // Uncaught ReferenceError: Must call super // constructor in derived class before accessing 'this' or returning // from derived constructor 调用父类构造函数,只能出现在子类的构造函数。 12345678910111213141516171819class Father &#123; test()&#123; return 0; &#125; static test1()&#123; return 1; &#125;&#125;class Child extends Father &#123; constructor()&#123; super(); &#125;&#125;class Child1 extends Father &#123; test2() &#123; super(); // Uncaught SyntaxError: 'super' keyword unexpected // here &#125;&#125; 调用父类方法, super 作为对象，在普通方法中，指向父类的原型对象，在静态方法中，指向父类 123456789101112class Child2 extends Father &#123; constructor()&#123; super(); // 调用父类普通方法 console.log(super.test()); // 0 &#125; static test3()&#123; // 调用父类静态方法 return super.test1+2; &#125;&#125;Child2.test3(); // 3 注意要点 不可继承常规对象 12345678910var Father = &#123; // ...&#125;class Child extends Father &#123; // ...&#125;// Uncaught TypeError: Class extends value #&lt;Object&gt; is not a constructor or null // 解决方案Object.setPrototypeOf(Child.prototype, Father); ES6 模块概述在 ES6 前， 实现模块化使用的是 RequireJS 或者 seaJS（分别是基于 AMD 规范的模块化库， 和基于 CMD 规范的模块化库）。 ES6 引入了模块化，其设计思想是在编译时就能确定模块的依赖关系，以及输入和输出的变量。 ES6 的模块化分为导出（export） @与导入（import）两个模块。 ES6 模块的特点ES6 的模块自动开启严格模式，不管你有没有在模块头部加上 use strict;。 模块中可以导入和导出各种类型的变量，如函数，对象，字符串，数字，布尔值，类等。 每个模块都有自己的上下文，每一个模块内声明的变量都是局部变量，不会污染全局作用域。 每一个模块只加载一次（是单例的）， 若再去加载同目录下同文件，直接从内存中读取。 export 与 import基本 用法 模块导入导出各种类型的变量，如字符串，数值，函数，类。 导出的函数声明与类声明必须要有名称（export default 命令另外考虑）。 不仅能导出声明还能导出引用（例如函数）。 export 命令可以出现在模块的任何位置，但必需处于模块顶层。 import 命令会提升到整个模块的头部，首先执行。 1234567891011121314151617/*-----export [test.js]-----*/let myName = "Tom";let myAge = 20;let myfn = function()&#123; return "My name is" + myName + "! I'm '" + myAge + "years old."&#125;let myClass = class myClass &#123; static a = "yeah!";&#125;export &#123; myName, myAge, myfn, myClass &#125; /*-----import [xxx.js]-----*/import &#123; myName, myAge, myfn, myClass &#125; from "./test.js";console.log(myfn());// My name is Tom! I'm 20 years old.console.log(myAge);// 20console.log(myName);// Tomconsole.log(myClass.a );// yeah! 建议使用大括号指定所要输出的一组变量写在文档尾部，明确导出的接口。 函数与类都需要有对应的名称，导出文档尾部也避免了无对应名称。 as 的用法 export 命令导出的接口名称，须和模块内部的变量有一一对应关系。 导入的变量名，须和导出的接口名称相同，即顺序可以不一致。 12345678910111213141516171819/*-----export [test.js]-----*/let myName = "Tom";export &#123; myName as exportName &#125; /*-----import [xxx.js]-----*/import &#123; exportName &#125; from "./test.js";console.log(exportName);// Tom使用 as 重新定义导出的接口名称，隐藏模块内部的变量/*-----export [test1.js]-----*/let myName = "Tom";export &#123; myName &#125;/*-----export [test2.js]-----*/let myName = "Jerry";export &#123; myName &#125;/*-----import [xxx.js]-----*/import &#123; myName as name1 &#125; from "./test1.js";import &#123; myName as name2 &#125; from "./test2.js";console.log(name1);// Tomconsole.log(name2);// Jerry 不同模块导出接口名称命名重复， 使用 as 重新定义变量名。 import 命令特点 只读属性：不允许在加载模块的脚本里面，改写接口的引用指向，即可以改写 import 变量类型为对象的属性值，不能改写 import 变量类型为基本类型的值。 12345import &#123;a&#125; from "./xxx.js"a = &#123;&#125;; // error import &#123;a&#125; from "./xxx.js"a.foo = "hello"; // a = &#123; foo : 'hello' &#125; 单例模式：多次重复执行同一句 import 语句，那么只会执行一次，而不会执行多次。import 同一模块，声明不同接口引用，会声明对应变量，但只执行一次 import 。 1234567import &#123; a &#125; "./xxx.js";import &#123; a &#125; "./xxx.js";// 相当于 import &#123; a &#125; "./xxx.js"; import &#123; a &#125; from "./xxx.js";import &#123; b &#125; from "./xxx.js";// 相当于 import &#123; a, b &#125; from "./xxx.js"; 静态执行特性：import 是静态执行，所以不能使用表达式和变量。 1234567891011import &#123; "f" + "oo" &#125; from "methods";// errorlet module = "methods";import &#123; foo &#125; from module;// errorif (true) &#123; import &#123; foo &#125; from "method1";&#125; else &#123; import &#123; foo &#125; from "method2";&#125;// error export default 命令基本属性 在一个文件或模块中，export、import 可以有多个，export default 仅有一个。 export default 中的 default 是对应的导出接口变量。 通过 export 方式导出，在导入时要加{ }，export default 则不需要。 export default 向外暴露的成员，可以使用任意变量来接收。 123456var a = "My name is Tom!";export default a; // 仅有一个export default var c = "error"; // error，default 已经是对应的导出变量，不能跟着变量声明语句 import b from "./xxx.js"; // 不需要加&#123;&#125;， 使用任意变量接收 复合使用export 和 import 复合使用 export 与 import 可以在同一模块使用，使用特点： 可以将导出接口改名，包括 default。 复合使用 export 与 import ，也可以导出全部，当前模块导出的接口会覆盖继承导出的。 ES6 Promise对象概述是异步编程的一种解决方案。 从语法上说，Promise 是一个对象，从它可以获取异步操作的消息。 Promise 状态状态的特点 Promise 异步操作有三种状态：pending（进行中）、fulfilled（已成功）和 rejected（已失败）。除了异步操作的结果，任何其他操作都无法改变这个状态。 Promise 对象只有：从 pending 变为 fulfilled 和从 pending 变为 rejected 的状态改变。只要处于 fulfilled 和 rejected ，状态就不会再变了即 resolved（已定型）。 12345678910111213141516const p1 = new Promise(function(resolve,reject)&#123; resolve('success1'); resolve('success2');&#125;); const p2 = new Promise(function()&#123; resolve('success3'); reject('reject');&#125;); p1.then(function(value)&#123; console.log(value); // success1&#125;);p2.then(function(value)&#123; console.log(value); // success3&#125;); 状态的缺点 无法取消 Promise ，一旦新建它就会立即执行，无法中途取消。 如果不设置回调函数，Promise 内部抛出的错误，不会反应到外部。 当处于 pending 状态时，无法得知目前进展到哪一个阶段（刚刚开始还是即将完成）。 then方法在 JavaScript 事件队列的当前运行完成之前，回调函数永远不会被调用。 1234567891011const p = new Promise(function(resolve,reject)&#123; resolve('success');&#125;); p.then(function(value)&#123; console.log(value);&#125;); console.log('first');// first// success 通过 .then 形式添加的回调函数，不论什么时候，都会被调用。 通过多次调用 .then 可以添加多个回调函数， 他们会按照插入顺序并且独立运行 123456789101112131415161718const p = new Promise(function(resolve,reject)&#123; resolve(1);&#125;).then(function(value)&#123; // 第一个then // 1 console.log(value); return value * 2;&#125;).then(function(value)&#123; // 第二个then // 2 console.log(value);&#125;).then(function(value)&#123; // 第三个then // undefined console.log(value); return Promise.resolve('resolve'); &#125;).then(function(value)&#123; // 第四个then // resolve console.log(value); return Promise.reject('reject'); &#125;).then(function(value)&#123; // 第五个then //reject:reject console.log('resolve:' + value);&#125;, function(err) &#123; console.log('reject:' + err);&#125;); then 方法将返回一个 resolved 或 rejected 状态的 Promise 对象用于链式调用，且 Promise 对象的值就是这个返回值。 then 方法注意点 简便的 Promise 链式编程最好保持扁平化，不要嵌套 Promise。 注意总是返回或终止 Promise 链。 12345const p1 = new Promise(function(resolve,reject)&#123; resolve(1);&#125;).then(function(result) &#123; p2(result).then(newResult =&gt; p3(newResult));&#125;).then(() =&gt; p4()); 创建新 Promise 但忘记返回它时，对应链条被打破，导致 p4 会与 p2 和 p3 同时进行。 大多数浏览器中不能终止的 Promise 链里的 rejection，建议后面都跟上 .catch(error =&gt; console.log(error)); ES6 Generator 函数ES6 新引入了 Generator 函数，可以通过 yield 关键字，把函数的执行流挂起，为改变执行流程提供了可能，从而为异步编程提供解决方案。 基本用法 Generator 函数组成Generator 有两个区分于普通函数的部分： 一是在 function 后面，函数名之前有个 * ； 函数内部有 yield 表达式。 其中 * 用来表示函数为 Generator 函数，yield 用来定义函数内部的状态。 12345678function* func()&#123; console.log("one"); yield '1'; console.log("two"); yield '2'; console.log("three"); return '3';&#125; 执行机制调用 Generator 函数和调用普通函数一样，在函数名后面加上()即可，但是 Generator 函数不会像普通函数一样立即执行，而是返回一个指向内部状态对象的指针，所以要调用遍历器对象Iterator 的 next 方法，指针就会从函数头部或者上一次停下来的地方开始执行。 1234567891011121314f.next();// one// &#123;value: "1", done: false&#125; f.next();// two// &#123;value: "2", done: false&#125; f.next();// three// &#123;value: "3", done: true&#125; f.next();// &#123;value: undefined, done: true&#125; 第一次调用 next 方法时，从 Generator 函数的头部开始执行，先是打印了 one ,执行到 yield 就停下来，并将yield 后边表达式的值 ‘1’，作为返回对象的 value 属性值，此时函数还没有执行完， 返回对象的 done 属性值是 false。 第二次调用 next 方法时，同上步 。 第三次调用 next 方法时，先是打印了 three ，然后执行了函数的返回操作，并将 return 后面的表达式的值，作为返回对象的 value 属性值，此时函数已经结束，多以 done 属性值为true 。 第四次调用 next 方法时， 此时函数已经执行完了，所以返回 value 属性值是 undefined ，done 属性值是 true 。如果执行第三步时，没有 return 语句的话，就直接返回 {value: undefined, done: true}。 函数返回的遍历器对象的方法next 方法 一般情况下，next 方法不传入参数的时候，yield 表达式的返回值是 undefined 。当 next 传入参数的时候，该参数会作为上一步yield的返回值。 12345678function* sendParameter()&#123; console.log("strat"); var x = yield '2'; console.log("one:" + x); var y = yield '3'; console.log("two:" + y); console.log("total:" + (x + y));&#125; next不传参 1234567891011121314151617181920212223var sendp1 = sendParameter();sendp1.next();// strat// &#123;value: "2", done: false&#125;sendp1.next();// one:undefined// &#123;value: "3", done: false&#125;sendp1.next();// two:undefined// total:NaN// &#123;value: undefined, done: true&#125;next传参var sendp2 = sendParameter();sendp2.next(10);// strat// &#123;value: "2", done: false&#125;sendp2.next(20);// one:20// &#123;value: "3", done: false&#125;sendp2.next(30);// two:30// total:50// &#123;value: undefined, done: true&#125; 除了使用 next ，还可以使用 for… of 循环遍历 Generator 函数生产的 Iterator 对象。 return 方法 return 方法返回给定值，并结束遍历 Generator 函数。 return 方法提供参数时，返回该参数；不提供参数时，返回 undefined 。 123456789101112131415161718192021222324252627282930313233function* foo()&#123; yield 1; yield 2; yield 3;&#125;var f = foo();f.next();// &#123;value: 1, done: false&#125;f.return("foo");// &#123;value: "foo", done: true&#125;f.next();// &#123;value: undefined, done: true&#125;throw 方法throw 方法可以再 Generator 函数体外面抛出异常，再函数体内部捕获。var g = function* () &#123; try &#123; yield; &#125; catch (e) &#123; console.log('catch inner', e); &#125;&#125;; var i = g();i.next(); try &#123; i.throw('a'); i.throw('b');&#125; catch (e) &#123; console.log('catch outside', e);&#125;// catch inner a// catch outside b 遍历器对象抛出了两个错误，第一个被 Generator 函数内部捕获，第二个因为函数体内部的catch 函数已经执行过了，不会再捕获这个错误，所以这个错误就抛出 Generator 函数体，被函数体外的 catch 捕获。 yield*表达式 yield* 表达式表示 yield 返回一个遍历器对象，用于在 Generator 函数内部，调用另一个 Generator 函数。 1234567891011121314151617181920212223242526function* callee() &#123; console.log('callee: ' + (yield));&#125;function* caller() &#123; while (true) &#123; yield* callee(); &#125;&#125;const callerObj = caller();callerObj.next();// &#123;value: undefined, done: false&#125;callerObj.next("a");// callee: a// &#123;value: undefined, done: false&#125;callerObj.next("b");// callee: b// &#123;value: undefined, done: false&#125; // 等同于function* caller() &#123; while (true) &#123; for (var value of callee) &#123; yield value; &#125; &#125;&#125; 使用场景实现Iterator 为不具备 Iterator 接口的对象提供遍历方法。 12345678910111213function* objectEntries(obj) &#123; const propKeys = Reflect.ownKeys(obj); for (const propKey of propKeys) &#123; yield [propKey, obj[propKey]]; &#125;&#125; const jane = &#123; first: 'Jane', last: 'Doe' &#125;;for (const [key,value] of objectEntries(jane)) &#123; console.log(`$&#123;key&#125;: $&#123;value&#125;`);&#125;// first: Jane// last: Doe Reflect.ownKeys() 返回对象所有的属性，不管属性是否可枚举，包括 Symbol。 jane 原生是不具备 Iterator 接口无法通过 for… of遍历。这边用了 Generator 函数加上了 Iterator 接口，所以就可以遍历 jane 对象了。 ES6 async 函数asyncasync 是 ES7 才有的与异步操作有关的关键字，和 Promise ， Generator 有很大关联的。 语法1async function name([param[, param[, ... param]]]) &#123; statements &#125; name: 函数名称。 param: 要传递给函数的参数的名称。 statements: 函数体语句。 返回值async 函数返回一个 Promise 对象，可以使用 then 方法添加回调函数。 123456789async function helloAsync()&#123; return "helloAsync"; &#125; console.log(helloAsync()) // Promise &#123;&lt;resolved&gt;: "helloAsync"&#125; helloAsync().then(v=&gt;&#123; console.log(v); // helloAsync&#125;) async 函数中可能会有 await 表达式，async 函数执行时，如果遇到 await 就会先暂停执行 ，等到触发的异步操作完成后，恢复 async 函数的执行并返回解析值。 await 关键字仅在 async function 中有效。如果在 async function 函数体外使用 await ，你只会得到一个语法错误。 12345678910111213141516function testAwait()&#123; return new Promise((resolve) =&gt; &#123; setTimeout(function()&#123; console.log("testAwait"); resolve(); &#125;, 1000); &#125;);&#125; async function helloAsync()&#123; await testAwait(); console.log("helloAsync"); &#125;helloAsync();// testAwait// helloAsync awaitawait 操作符用于等待一个 Promise 对象, 它只能在异步函数 async function 内部使用。 语法1[return_value] = await expression; expression: 一个 Promise 对象或者任何要等待的值。 返回值返回 Promise 对象的处理结果。如果等待的不是 Promise 对象，则返回该值本身。 如果一个 Promise 被传递给一个 await 操作符，await 将等待 Promise 正常处理完成并返回其处理结果。 1234567891011121314function testAwait (x) &#123; return new Promise(resolve =&gt; &#123; setTimeout(() =&gt; &#123; resolve(x); &#125;, 2000); &#125;);&#125; async function helloAsync() &#123; var x = await testAwait ("hello world"); console.log(x); &#125;helloAsync ();// hello world 正常情况下，await 命令后面是一个 Promise 对象，它也可以跟其他值，如字符串，布尔值，数值以及普通函数。 12345678910function testAwait()&#123; console.log("testAwait");&#125;async function helloAsync()&#123; await testAwait(); console.log("helloAsync");&#125;helloAsync();// testAwait// helloAsync await针对所跟不同表达式的处理方式： Promise 对象：await 会暂停执行，等待 Promise 对象 resolve，然后恢复 async 函数的执行并返回解析值。 非 Promise 对象：直接返回对应的值。]]></content>
      <categories>
        <category>web前端</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6 新语法 (一)]]></title>
    <url>%2F2019%2F07%2F09%2F03.web%E5%89%8D%E7%AB%AF%2FJavaScript%2FES6%20%E6%96%B0%E8%AF%AD%E6%B3%95(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[ECMAScript简介介绍ES6， 全称 ECMAScript 6.0 ，是 JavaScript 的下一个版本标准，2015.06 发版。 ES6 主要是为了解决 ES5 的先天不足，比如 JavaScript 里并没有类的概念，但是目前浏览器的 JavaScript 是 ES5 版本，大多数高版本的浏览器也支持 ES6，不过只实现了 ES6 的部分特性和功能。 ECMAScript 的背景JavaScript 是大家所了解的语言名称，但是这个语言名称是商标（ Oracle 公司注册的商标）。因此，JavaScript 的正式名称是 ECMAScript 。1996年11月，JavaScript 的创造者网景公司将 JS 提交给国际化标准组织 ECMA（European computer manufactures association，欧洲计算机制造联合会），希望这种语言能够成为国际标准，随后 ECMA 发布了规定浏览器脚本语言的标准，即 ECMAScript。这也有利于这门语言的开放和中立。 ECMAScript的历史ES6 是 ECMAScript 标准十余年来变动最大的一个版本，为其添加了许多新的语法特性。 1997 年 ECMAScript 1.0 诞生。 1998 年 6 月 ECMAScript 2.0 诞生，包含一些小的更改，用于同步独立的 ISO 国际标准。 1999 年 12 月 ECMAScript 3.0诞生，它是一个巨大的成功，在业界得到了广泛的支持，它奠定了 JS 的基本语法，被其后版本完全继承。直到今天，我们一开始学习 JS ，其实就是在学 3.0 版的语法。 2000 年的 ECMAScript 4.0 是当下 ES6 的前身，但由于这个版本太过激烈，对 ES 3 做了彻底升级，所以暂时被”和谐”了。 2009 年 12 月，ECMAScript 5.0 版正式发布。ECMA 专家组预计 ECMAScript 的第五个版本会在 2013 年中期到 2018 年作为主流的开发标准。2011年6月，ES 5.1 版发布，并且成为 ISO 国际标准。 2013 年，ES6 草案冻结，不再添加新的功能，新的功能将被放到 ES7 中；2015年6月， ES6 正式通过，成为国际标准。 webpackwebpack介绍webpack 是一个现代 JavaScript 应用程序的静态模块打包器 (module bundler) 。当 webpack 处理应用程序时，它会递归地构建一个依赖关系图 (dependency graph) ，其中包含应用程序需要的每个模块，然后将所有这些模块打包成一个或多个 bundle 。 web的是个核心概念 入口 (entry) 输出 (output) loader 插件 (plugins) 入口(entry)入口会指示 webpack 应该使用哪个模块，来作为构建其内部依赖图的开始。进入入口起点后，webpack 会找出有哪些模块和库是入口起点（直接和间接）依赖的。在 webpack 中入口有多种方式来定义，如下面例子： 12345678910// 单个入口（简写）语法:const config = &#123; entry: "./src/main.js"&#125;// 对象语法:const config = &#123; app: "./src/main.js", vendors: "./src/vendors.js"&#125; 输出(output)output 属性会告诉 webpack 在哪里输出它创建的 bundles ，以及如何命名这些文件，默认值为 ./dist: 1234567const config = &#123; entry: "./src/main.js", output: &#123; filename: "bundle.js", path: path.resolve(__dirname, 'dist') &#125;&#125; loaderloader 让 webpack 可以去处理那些非 JavaScript 文件（ webpack 自身只理解 JavaScript ）。loader 可以将所有类型的文件转换为 webpack 能够有效处理的模块，例如，开发的时候使用 ES6 ，通过 loader 将 ES6 的语法转为 ES5 ，如下配置： 12345678910111213141516171819const config = &#123; entry: "./src/main.js", output: &#123; filename: "bundle.js", path: path.resolve(__dirname, 'dist') &#125;, module: &#123; rules: [ &#123; test: /\.js$/, exclude: /node_modules/, loader: "babel-loader", options: [ presets: ["env"] ] &#125; ] &#125;&#125; 插件(plugins)loader 被用于转换某些类型的模块，而插件则可以做更多的事情。包括打包优化、压缩、定义环境变量等等。插件的功能强大，是 webpack 扩展非常重要的利器，可以用来处理各种各样的任务。使用一个插件也非常容易，只需要 require() ，然后添加到 plugins 数组中。 12345678910111213141516171819// 通过 npm 安装const HtmlWebpackPlugin = require('html-webpack-plugin');// 用于访问内置插件 const webpack = require('webpack'); const config = &#123; module: &#123; rules: [ &#123; test: /\.js$/, exclude: /node_modules/, loader: "babel-loader" &#125; ] &#125;, plugins: [ new HtmlWebpackPlugin(&#123;template: './src/index.html'&#125;) ]&#125;; 利用 webpack 搭建应用1234567891011121314151617181920212223242526const path = require('path'); module.exports = &#123; mode: "development", // "production" | "development" // 选择 development 为开发模式， production 为生产模式 entry: "./src/main.js", output: &#123; filename: "bundle.js", path: path.resolve(__dirname, 'dist') &#125;, module: &#123; rules: [ &#123; test: /\.js$/, exclude: /node_modules/, loader: "babel-loader", options: [ presets: ["env"] ] &#125; ] &#125;, plugins: [ ... ]&#125; ES6 新语法特性let 与 constlet 的使用方法 let 生命的是块级的变量，声明的变量只在 let 命令所在的代码块内有效。 1234567891011121314151617181920212223242526&#123; let a = 0; a // 0&#125;a // 报错 ReferenceError: a is not defined&#123; let a = 0; var b = 1;&#125;a // ReferenceError: a is not definedb // 1// for 循环计数器很适合用 letfor (var i = 0; i &lt; 10; i++) &#123; setTimeout(function()&#123; console.log(i); &#125;)&#125;// 输出十个 10for (let j = 0; j &lt; 10; j++) &#123; setTimeout(function()&#123; console.log(j); &#125;)&#125;// 输出 0123456789 变量 i 是用 var 声明的，在全局范围内有效，所以全局中只有一个变量 i, 每次循环时，setTimeout 定时器里面的 i 指的是全局变量 i ，而循环里的十个 setTimeout 是在循环结束后才执行，所以此时的 i 都是 10。 变量 j 是用 let 声明的，当前的 i 只在本轮循环中有效，每次循环的 j 其实都是一个新的变量，所以 setTimeout 定时器里面的 j 其实是不同的变量，即最后输出12345。（若每次循环的变量 j 都是重新声明的，如何知道前一个循环的值？这是因为 JavaScript 引擎内部会记住前一个循环的值）。 let的特点 let 声明的变量代码块内有效； 不能重复声明； 不存在变量提升，在声明之前饮用会报错； const 命令const 声明一个只读变量，声明之后不允许改变。意味着，一旦声明必须初始化，否则会报错。 基本用法 1234const PI = "3.1415926";PI // 3.1415926const MY_AGE; // SyntaxError: Missing initializer in const declaration 暂时性死区 12345var PI = "a";if(true)&#123; console.log(PI); // ReferenceError: PI is not defined const PI = "3.1415926";&#125; ES6 明确规定，代码块内如果存在 let 或者 const，代码块会对这些命令声明的变量从块的开始就形成一个封闭作用域。代码块内，在声明变量 PI 之前使用它会报错。 解构赋值概念解构赋值是对赋值运算符的扩展。 他是一种针对数组或者对象进行模式匹配，然后对其中的变量进行赋值。 在代码书写上简洁且易读，语义更加清晰明了；也方便了复杂对象中数据字段获取。 解构模型在解构中，有下面两部分参与： 解构的源，解构赋值表达式的右边部分。解构的目标，解构赋值表达式的左边部分。 数组模型的解构(Array)基本 1let [a, b, c] = [1, 2, 3]; // a = 1 // b = 2 // c = 3 可嵌套 1let [a, [[b], c]] = [1, [[2], 3]]; // a = 1 // b = 2 // c = 3 可忽略 1let [a, , b] = [1, 2, 3]; // a = 1 // b = 3 不完全解构 1let [a = 1, b] = []; // a = 1, b = undefined 剩余运算符 1let [a, ...b] = [1, 2, 3]; //a = 1 //b = [2, 3] 字符串等 在数组的解构中，解构的目标若为可遍历对象，皆可进行解构赋值。可遍历对象即实现 Iterator 接口的数据。 1let [a, b, c, d, e] = 'hello'; // a = 'h' // b = 'e' // c = 'l' // d = 'l' // e = 'o' 解构默认值 123let [a = 2] = [undefined]; // a = 2// 当解构模式有匹配结果，且匹配结果是 undefined 时，会触发默认值作为返回结果。 12345678let [a = 3, b = a] = []; // a = 3, b = 3 let [a = 3, b = a] = [1]; // a = 1, b = 1 let [a = 3, b = a] = [1, 2]; // a = 1, b = 2// a 与 b 匹配结果为 undefined ，触发默认值：a = 3; b = a =3// a 正常解构赋值，匹配结果：a = 1，b 匹配结果 undefined ，触发默认值：b = a =1// a 与 b 正常解构赋值，匹配结果：a = 1，b = 2 对象模型的解构（Object）基本 123456789let &#123; foo, bar &#125; = &#123; foo: 'aaa', bar: 'bbb' &#125;; // foo = 'aaa' // bar = 'bbb' let &#123; baz : foo &#125; = &#123; baz : 'ddd' &#125;; // foo = 'ddd' 可嵌套可忽略 12345678910111213let obj = &#123;p: ['hello', &#123;y: 'world'&#125;] &#125;; let &#123;p: [x, &#123; y &#125;] &#125; = obj;// x = 'hello' // y = 'world' let obj = &#123;p: ['hello', &#123;y: 'world'&#125;] &#125;; let &#123;p: [x, &#123; &#125;] &#125; = obj; // x = 'hello' 不完全解构 1234567let obj = &#123;p: [&#123;y: 'world'&#125;] &#125;; let &#123;p: [&#123; y &#125;, x ] &#125; = obj; // x = undefined // y = 'world' 剩余运算符 1234567let &#123;a, b, ...rest&#125; = &#123;a: 10, b: 20, c: 30, d: 40&#125;; // a = 10 // b = 20 // rest = &#123;c: 30, d: 40&#125; 解构默认值 1234567let &#123;a = 10, b = 5&#125; = &#123;a: 3&#125;; // a = 3; b = 5; let &#123;a: aa = 10, b: bb = 5&#125; = &#123;a: 3&#125;; // aa = 3; bb = 5; ES6 Symbol概述ES6 引入了一种新的原始数据类型 Symbol ，表示独一无二的值，最大的用法是用来定义对象的唯一属性名。 ES6 数据类型除了 Number 、 String 、 Boolean 、 Objec t、 null 和 undefined ，还新增了 Symbol 。 基本用法Symbol 函数栈不能用 new 命令，因为 Symbol 是原始数据类型，不是对象。可以接受一个字符串作为参数，为新创建的 Symbol 提供描述，用来显示在控制台或者作为字符串的时候使用，便于区分。 1234567let sy = Symbol("KK");console.log(sy); // Symbol(KK)typeof(sy); // "symbol" // 相同参数 Symbol() 返回的值不相等let sy1 = Symbol("kk"); sy === sy1; // false 使用场景作为属性名 由于每一个 Symbol 的值都是不相等的，所以 Symbol 作为对象的属性名，可以保证属性不重名。 1234567891011121314151617let sy = Symbol("key1"); // 写法1let syObject = &#123;&#125;;syObject[sy] = "kk";console.log(syObject); // &#123;Symbol(key1): "kk"&#125; // 写法2let syObject = &#123; [sy]: "kk"&#125;;console.log(syObject); // &#123;Symbol(key1): "kk"&#125; // 写法3let syObject = &#123;&#125;;Object.defineProperty(syObject, sy, &#123;value: "kk"&#125;);console.log(syObject); // &#123;Symbol(key1): "kk"&#125; Symbol 作为对象属性名时不能用.运算符，要用方括号。因为.运算符后面是字符串，所以取到的是字符串 sy 属性，而不是 Symbol 值 sy 属性。 12345let syObject = &#123;&#125;;syObject[sy] = "kk"; syObject[sy]; // "kk"syObject.sy; // undefined 注意点 Symbol 值作为属性名时，该属性是公有属性不是私有属性，可以在类的外部访问。但是不会出现在 for…in 、 for…of 的循环中，也不会被 Object.keys() 、 Object.getOwnPropertyNames() 返回。如果要读取到一个对象的 Symbol 属性，可以通过 Object.getOwnPropertySymbols() 和 Reflect.ownKeys() 取到。 1234567891011let syObject = &#123;&#125;;syObject[sy] = "kk";console.log(syObject); for (let i in syObject) &#123; console.log(i);&#125; // 无输出 Object.keys(syObject); // []Object.getOwnPropertySymbols(syObject); // [Symbol(key1)]Reflect.ownKeys(syObject); // [Symbol(key1)] 定义常量 在 ES5 使用字符串表示常量。例如： 123const COLOR_RED = "red";const COLOR_YELLOW = "yellow";const COLOR_BLUE = "blue"; 但是用字符串不能保证常量是独特的，这样会引起一些问题： 12345678910111213141516171819const COLOR_RED = "red";const COLOR_YELLOW = "yellow";const COLOR_BLUE = "blue";const MY_BLUE = "blue"； function getConstantName(color) &#123; switch (color) &#123; case COLOR_RED : return "COLOR_RED"; case COLOR_YELLOW : return "COLOR_YELLOW "; case COLOR_BLUE: return "COLOR_BLUE"; case MY_BLUE: return "MY_BLUE"; default: throw new Exception('Can't find this color'); &#125;&#125; 但是使用 Symbol 定义常量，这样就可以保证这一组常量的值都不相等。用 Symbol 来修改上面的例子。 12345678910111213141516const COLOR_RED = Symbol("red");const COLOR_YELLOW = Symbol("yellow");const COLOR_BLUE = Symbol("blue"); function getConstantName(color) &#123; switch (color) &#123; case COLOR_RED : return "COLOR_RED"; case COLOR_YELLOW : return "COLOR_YELLOW "; case COLOR_BLUE: return "COLOR_BLUE"; default: throw new Exception('Can't find this color'); &#125;&#125; Symbol 的值是唯一的，所以不会出现相同值得常量，即可以保证 switch 按照代码预想的方式执行。 *Symbol.for() * Symbol.for() 类似单例模式，首先会在全局搜索被登记的 Symbol 中是否有该字符串参数作为名称的 Symbol 值，如果有即返回该 Symbol 值，若没有则新建并返回一个以该字符串参数为名称的 Symbol 值，并登记在全局环境中供搜索。 123456let yellow = Symbol("Yellow");let yellow1 = Symbol.for("Yellow");yellow === yellow1; // false let yellow2 = Symbol.for("Yellow");yellow1 === yellow2; // true Symbol.keyFor() 12let yellow1 = Symbol.for("Yellow");Symbol.keyFor(yellow1); // "Yellow" ES6 Map 与 SetMap 对象Map 对象保存键值对。任何值(对象或者原始值) 都可以作为一个键或一个值。 Maps 与Objects 的区别 一个 Object 的键只能是字符串或者 Symbols，但一个 Map 的键可以是任意值。 Map 中的键值是有序的（FIFO 原则），而添加到对象中的键则不是。 Map 的键值对个数可以从 size 属性获取，而 Object 的键值对个数只能手动计算。 Object 都有自己的原型，原型链上的键名有可能和你自己在对象上的设置的键名产生冲突。 Map 中的keykey 是字符串 123456789var myMap = new Map(); var keyString = "a string"; myMap.set(keyString, "和键'a string'关联的值"); myMap.get(keyString); // "和键'a string'关联的值" myMap.get("a string"); // "和键'a string'关联的值" ，因为 keyString === 'a string' key 是对象 123456789var myMap = new Map(); var keyObj = &#123;&#125; myMap.set(keyObj, "和键 keyObj 关联的值"); myMap.get(keyObj); // "和键 keyObj 关联的值" myMap.get(&#123;&#125;); // undefined, 因为 keyObj !== &#123;&#125; key 是函数 123456789var myMap = new Map(); var keyFunc = function () &#123;&#125;, // 函数 myMap.set(keyFunc, "和键 keyFunc 关联的值"); myMap.get(keyFunc); // "和键 keyFunc 关联的值" myMap.get(function() &#123;&#125;) // undefined, 因为 keyFunc !== function () &#123;&#125; key 是 NaN 12345678910var myMap = new Map(); myMap.set(NaN, "not a number"); myMap.get(NaN); // "not a number" var otherNaN = Number("foo"); myMap.get(otherNaN); // "not a number"// 虽然 NaN 和任何值甚至和自己都不相等(NaN !== NaN 返回true)，NaN作为Map的键来说是没有区别的。 Map 的迭代for…of 123456789101112131415161718192021222324var myMap = new Map();myMap.set(0, "zero");myMap.set(1, "one"); // 将会显示两个 log。 一个是 "0 = zero" 另一个是 "1 = one"for (var [key, value] of myMap) &#123; console.log(key + " = " + value);&#125;for (var [key, value] of myMap.entries()) &#123; console.log(key + " = " + value);&#125;/* 这个 entries 方法返回一个新的 Iterator 对象，它按插入顺序包含了 Map 对象中每个元素的 [key, value] 数组。 */ // 将会显示两个log。 一个是 "0" 另一个是 "1"for (var key of myMap.keys()) &#123; console.log(key);&#125;/* 这个 keys 方法返回一个新的 Iterator 对象， 它按插入顺序包含了 Map 对象中每个元素的键。 */ // 将会显示两个log。 一个是 "zero" 另一个是 "one"for (var value of myMap.values()) &#123; console.log(value);&#125;/* 这个 values 方法返回一个新的 Iterator 对象，它按插入顺序包含了 Map 对象中每个元素的值。 */ forEach 12345678var myMap = new Map();myMap.set(0, "zero");myMap.set(1, "one"); // 将会显示两个 logs。 一个是 "0 = zero" 另一个是 "1 = one"myMap.forEach(function(value, key) &#123; console.log(key + " = " + value);&#125;, myMap) Map对象的操作Map 与 Array的转换 1234567var kvArray = [["key1", "value1"], ["key2", "value2"]]; // Map 构造函数可以将一个 二维 键值对数组转换成一个 Map 对象var myMap = new Map(kvArray); // 使用 Array.from 函数可以将一个 Map 对象转换成一个二维键值对数组var outArray = Array.from(myMap); Map 的克隆 12345var myMap1 = new Map([["key1", "value1"], ["key2", "value2"]]);var myMap2 = new Map(myMap1); console.log(original === clone); // 打印 false。 Map 对象构造函数生成实例，迭代出新的对象。 Map 的合并 12345var first = new Map([[1, 'one'], [2, 'two'], [3, 'three'],]);var second = new Map([[1, 'uno'], [2, 'dos']]); // 合并两个 Map 对象时，如果有重复的键值，则后面的会覆盖前面的，对应值即 uno，dos， threevar merged = new Map([...first, ...second]); Set 对象Set 对象允许你存储任何类型的唯一值，无论是原始值或者是对象引用。 Set 中的特殊值 Set 对象存储的值总是唯一的，所以需要判断两个值是否恒等。有几个特殊值需要特殊对待： +0 与 -0 在存储判断唯一性的时候是恒等的，所以不重复； undefined 与 undefined 是恒等的，所以不重复； NaN 与 NaN 是不恒等的，但是在 Set 中只能存一个，不重复。 基本使用 123456789101112let mySet = new Set(); mySet.add(1); // Set(1) &#123;1&#125;mySet.add(5); // Set(2) &#123;1, 5&#125;mySet.add(5); // Set(2) &#123;1, 5&#125; 这里体现了值的唯一性mySet.add("some text"); // Set(3) &#123;1, 5, "some text"&#125; 这里体现了类型的多样性var o = &#123;a: 1, b: 2&#125;; mySet.add(o);mySet.add(&#123;a: 1, b: 2&#125;); // Set(5) &#123;1, 5, "some text", &#123;…&#125;, &#123;…&#125;&#125; // 这里体现了对象之间引用不同不恒等，即使值相同，Set 也能存储 Set 类型转换 12345678// Array 转 Setvar mySet = new Set(["value1", "value2", "value3"]);// 用...操作符，将 Set 转 Arrayvar myArray = [...mySet];String// String 转 Setvar mySet = new Set('hello'); // Set(4) &#123;"h", "e", "l", "o"&#125;// 注：Set 中 toString 方法是不能将 Set 转换成 String Set 对象的作用数组去重 123var mySet = new Set([1, 2, 3, 4, 4]); [...mySet]; // [1, 2, 3, 4] 并集 12345var a = new Set([1, 2, 3]); var b = new Set([4, 3, 2]); var union = new Set([...a, ...b]); // &#123;1, 2, 3, 4&#125; 交集 12345var a = new Set([1, 2, 3]); var b = new Set([4, 3, 2]); var intersect = new Set([...a].filter(x =&gt; b.has(x))); // &#123;2, 3&#125; 差集 12345var a = new Set([1, 2, 3]); var b = new Set([4, 3, 2]); var difference = new Set([...a].filter(x =&gt; !b.has(x))); // &#123;1&#125; ES6 Reflect 与 Proxy概述Proxy 与 Reflect 是 ES6 为了操作对象引入的 API 。 Proxy 可以对目标对象的读取、函数调用等操作进行拦截，然后进行操作处理。它不直接操作对象，而是像代理模式，通过对象的代理对象进行操作，在进行这些操作时，可以添加一些需要的额外操作。 Reflect 可以用于获取目标对象的行为，它与 Object 类似，但是更易读，为操作对象提供了一种更优雅的方式。它的方法与 Proxy 是对应的。 Proxy基本用法一个 Proxy 对象由两个部分组成： target 、 handler 。 在通过 Proxy 构造函数生成实例对象时，需要提供这两个参数。 target 即目标对象， handler 是一个对象，声明了代理 target 的指定行为。 1234567891011121314151617181920212223242526272829303132333435363738394041424344let target = &#123; name: 'Tom', age: 24&#125;let handler = &#123; get: function(target, key) &#123; console.log('getting '+key); return target[key]; // 不是target.key &#125;, set: function(target, key, value) &#123; console.log('setting '+key); target[key] = value; &#125;&#125;let proxy = new Proxy(target, handler)proxy.name // 实际执行 handler.getproxy.age = 25 // 实际执行 handler.set// getting name// setting age// 25 // target 可以为空对象let targetEpt = &#123;&#125;let proxyEpt = new Proxy(targetEpt, handler)// 调用 get 方法，此时目标对象为空，没有 name 属性proxyEpt.name // getting name// 调用 set 方法，向目标对象中添加了 name 属性proxyEpt.name = 'Tom'// setting name// "Tom"// 再次调用 get ，此时已经存在 name 属性proxyEpt.name// getting name// "Tom" // 通过构造函数新建实例时其实是对目标对象进行了浅拷贝，因此目标对象与代理对象会互相影响targetEpt)// &#123;name: "Tom"&#125; // handler 对象也可以为空，相当于不设置拦截操作，直接访问目标对象let targetEmpty = &#123;&#125;let proxyEmpty = new Proxy(targetEmpty,&#123;&#125;)proxyEmpty.name = "Tom"targetEmpty) // &#123;name: "Tom"&#125; Proxy实例方法get 方法 1get(target, propKey, receiver) 用于 target 对象上 propKey 的读取操作。 12345678910111213let exam =&#123; name: "Tom", age: 24&#125;let proxy = new Proxy(exam, &#123; get(target, propKey, receiver) &#123; console.log('Getting ' + propKey); return target[propKey]; &#125;&#125;)proxy.name // Getting name// "Tom" get() 方法可以继承。 1234567891011121314let proxy = new Proxy(&#123;&#125;, &#123; get(target, propKey, receiver) &#123; // 实现私有属性读取保护 if(propKey[0] === '_')&#123; throw new Erro(`Invalid attempt to get private "$&#123;propKey&#125;"`); &#125; console.log('Getting ' + propKey); return target[propKey]; &#125;&#125;); let obj = Object.create(proxy);obj.name// Getting name set 方法 1set(target, propKey, value, receiver) 用于拦截 target 对象上的 propKey 的赋值操作。如果目标对象自身的某个属性，不可写且不可配置，那么set方法将不起作用。 12345678910111213141516171819let validator = &#123; set: function(obj, prop, value) &#123; if (prop === 'age') &#123; if (!Number.isInteger(value)) &#123; throw new TypeError('The age is not an integer'); &#125; if (value &gt; 200) &#123; throw new RangeError('The age seems invalid'); &#125; &#125; // 对于满足条件的 age 属性以及其他属性，直接保存 obj[prop] = value; &#125;&#125;;let proxy= new Proxy(&#123;&#125;, validator)proxy.age = 100;proxy.age // 100proxy.age = 'oppps' // 报错proxy.age = 300 // 报错 第四个参数 receiver 表示原始操作行为所在对象，一般是 Proxy 实例本身。 12345678910111213const handler = &#123; set: function(obj, prop, value, receiver) &#123; obj[prop] = receiver; &#125;&#125;;const proxy = new Proxy(&#123;&#125;, handler);proxy.name= 'Tom';proxy.name=== proxy // true const exam = &#123;&#125;Object.setPrototypeOf(exam, proxy)exam.name = "Tom"exam.name === exam // true 注意，严格模式下，set代理如果没有返回true，就会报错。 apply 方法 1apply(target, ctx, args) 用于拦截函数的调用、call 和 reply 操作。target 表示目标对象，ctx 表示目标对象上下文，args 表示目标对象的参数数组。 12345678910111213function sub(a, b)&#123; return a - b;&#125;let handler = &#123; apply: function(target, ctx, args)&#123; console.log('handle apply'); return Reflect.apply(...arguments); &#125;&#125;let proxy = new Proxy(sub, handler)proxy(2, 1) // handle apply// 1 has 方法 1has(target, propKey) 用于拦截 HasProperty 操作，即在判断 target 对象是否存在 propKey 属性时，会被这个方法拦截。此方法不判断一个属性是对象自身的属性，还是继承的属性。 1234567891011let handler = &#123; has: function(target, propKey)&#123; console.log("handle has"); return propKey in target; &#125;&#125;let exam = &#123;name: "Tom"&#125;let proxy = new Proxy(exam, handler)'name' in proxy// handle has// true 注意：此方法不拦截 for … in 循环。 construct 方法 1construct(target, args) 用于拦截 new 命令。返回值必须为对象。 1234567891011121314let handler = &#123; construct: function(target, args, newTarget)&#123; console.log("handle construct"); return Reflect.construct(target, args, newTarget); &#125; &#125;class exam = &#123; constructor(name)&#123; this.name = name; &#125;&#125;let proxy = new Proxy(exam,handler)new proxy("Tom")// handle construct// exam &#123;name: "Tom"&#125; deleteProperty 方法 1deleteProperty(target, propKey) 用于拦截 delete 操作，如果这个方法抛出错误或者返回 false ，propKey 属性就无法被 delete 命令删除。 defineProperty 方法 1defineProperty(target, propKey, propDesc) 用于拦截 Object.definePro若目标对象不可扩展，增加目标对象上不存在的属性会报错； 若属性不可写或不可配置，则不能改变这些属性。 123456789101112131415161718192021222324let handler = &#123; defineProperty: function(target, propKey, propDesc)&#123; console.log("handle defineProperty"); return true; &#125;&#125;plet target = &#123;&#125;let proxy = new Proxy(target, handler)proxy.name = "Tom"// handle definePropertytarget// &#123;name: "Tom"&#125; // defineProperty 返回值为false，添加属性操作无效let handler1 = &#123; defineProperty: function(target, propKey, propDesc)&#123; console.log("handle defineProperty"); return false; &#125;&#125;let target1 = &#123;&#125;let proxy1 = new Proxy(target1, handler1)proxy1.name = "Jerry"target1// &#123;&#125; getOwnPropertyDescriptor方法 1getOwnPropertyDescriptor(target, propKey) 用于拦截 Object.getOwnPropertyD() 返回值为属性描述对象或者 undefined 。 123456789let handler = &#123; getOwnPropertyDescriptor: function(target, propKey)&#123; return Object.getOwnPropertyDescriptor(target, propKey); &#125;&#125;ilet target = &#123;name: "Tom"&#125;let proxy = new Proxy(target, handler)Object.getOwnPropertyDescriptor(proxy, 'name')// &#123;value: "Tom", writable: true, enumerable: true, configurable: // true&#125; getPrototypeOf 方法 1getPrototypeOf(target) 主要用于拦截获取对象原型的操作。包括以下操作： Object.prototype.proto Object.prototype.isPrototypeOf() Object.getPrototypeOf() Reflect.getPrototypeOf() instanceof 1234567let exam = &#123;&#125;let proxy = new Proxy(&#123;&#125;,&#123; getPrototypeOf: function(target)&#123; return exam; &#125;&#125;)Object.getPrototypeOf(proxy) // &#123;&#125; 注意，返回值必须是对象或者 null ，否则报错。另外，如果目标对象不可扩展（non-extensible），getPrototypeOf 方法必须返回目标对象的原型对象。 1234567let proxy = new Proxy(&#123;&#125;,&#123; getPrototypeOf: function(target)&#123; return true; &#125;&#125;)Object.getPrototypeOf(proxy)// TypeError: 'getPrototypeOf' on proxy: trap returned neither object // nor null isExtensible 方法 1isExtensible(target) 用于拦截 Object.isExtensible 操作。 该方法只能返回布尔值，否则返回值会被自动转为布尔值。 123456let proxy = new Proxy(&#123;&#125;,&#123; isExtensible:function(target)&#123; return true; &#125;&#125;)Object.isExtensible(proxy) // true 注意：它的返回值必须与目标对象的isExtensible属性保持一致，否则会抛出错误。 12345678let proxy = new Proxy(&#123;&#125;,&#123; isExtensible:function(target)&#123; return false; &#125;&#125;)Object.isExtensible(proxy)// TypeError: 'isExtensible' on proxy: trap result does not reflect // extensibility of proxy target (which is 'true') ownKeys 方法 1ownKeys(target) 用于拦截对象自身属性的读取操作。主要包括以下操作： Object.getOwnPropertyNames() Object.getOwnPropertySymbols() Object.keys() or…in 方法返回的数组成员，只能是字符串或 Symbol 值，否则会报错。 若目标对象中含有不可配置的属性，则必须将这些属性在结果中返回，否则就会报错。 若目标对象不可扩展，则必须全部返回且只能返回目标对象包含的所有属性，不能包含不存在的属性，否则也会报错。 123456789101112131415161718192021222324252627282930313233let proxy = new Proxy( &#123; name: "Tom", age: 24&#125;, &#123; ownKeys(target) &#123; return ['name']; &#125;&#125;);Object.keys(proxy)// [ 'name' ]f返回结果中，三类属性会被过滤：// - 目标对象上没有的属性// - 属性名为 Symbol 值的属性// - 不可遍历的属性 let target = &#123; name: "Tom", [Symbol.for('age')]: 24,&#125;;// 添加不可遍历属性 'gender'Object.defineProperty(target, 'gender', &#123; enumerable: false, configurable: true, writable: true, value: 'male'&#125;);let handler = &#123; ownKeys(target) &#123; return ['name', 'parent', Symbol.for('age'), 'gender']; &#125;&#125;;let proxy = new Proxy(target, handler);Object.keys(proxy)// ['name'] preventExtensions 方法 1preventExtensions(target) 拦截 Object.preventExtensions 操作。 该方法必须返回一个布尔值，否则会自动转为布尔值。 1234567891011121314151617181920// 只有目标对象不可扩展时（即 Object.isExtensible(proxy) 为 false ），// proxy.preventExtensions 才能返回 true ，否则会报错var proxy = new Proxy(&#123;&#125;, &#123; preventExtensions: function(target) &#123; return true; &#125;&#125;);// 由于 proxy.preventExtensions 返回 true，此处也会返回 true，因此会报错Object.preventExtensions(proxy) 被// TypeError: 'preventExtensions' on proxy: trap returned truish but // the proxy target is extensible // 解决方案 var proxy = new Proxy(&#123;&#125;, &#123; preventExtensions: function(target) &#123; // 返回前先调用 Object.preventExtensions Object.preventExtensions(target); return true; &#125;&#125;);Object.preventExtensions(proxy)// Proxy &#123;&#125; setPrototypeOf 方法 1setPrototypeOf 主要用来拦截 Object.setPrototypeOf 方法。 返回值必须为布尔值，否则会被自动转为布尔值。 若目标对象不可扩展，setPrototypeOf 方法不得改变目标对象的原型。 12345678910let proto = &#123;&#125;let proxy = new Proxy(function () &#123;&#125;, &#123; setPrototypeOf: function(target, proto) &#123; console.log("setPrototypeOf"); return true; &#125;&#125;);Object.setPrototypeOf(proxy, proto);// setPrototypeOf revocable 方法 用于返回一个可取消的 Proxy 实例。 12345let &#123;proxy, revoke&#125; = Proxy.revocable(&#123;&#125;, &#123;&#125;);proxy.name = "Tom";revoke();proxy.name // TypeError: Cannot perform 'get' on a proxy that has been revoked Reflect介绍ES6 中将 Object 的一些明显属于语言内部的方法移植到了 Reflect 对象上（当前某些方法会同时存在于 Object 和 Reflect 对象上），未来的新方法会只部署在 Reflect 对象上。 Reflect 对象对某些方法的返回结果进行了修改，使其更合理。 Reflect 对象使用函数的方式实现了 Object 的命令式操作。 Reflect静态方法Reflect.get 1Reflect.get(target, name, receiver) 查找并返回 target 对象的 name 属性。 123456789101112131415161718192021let exam = &#123; name: "Tom", age: 24, get info()&#123; return this.name + this.age; &#125;&#125;Reflect.get(exam, 'name'); // "Tom" // 当 target 对象中存在 name 属性的 getter 方法， getter 方法的 this 会绑定 // receiverlet receiver = &#123; name: "Jerry", age: 20&#125;Reflect.get(exam, 'info', receiver); // Jerry20 // 当 name 为不存在于 target 对象的属性时，返回 undefinedReflect.get(exam, 'birth'); // undefined // 当 target 不是对象时，会报错Reflect.get(1, 'name'); // TypeError Reflect.set 1Reflect.set(target, name, value, receiver) 将 target 的 name 属性设置为 value。返回值为 boolean ，true 表示修改成功，false 表示失败。当 target 为不存在的对象时，会报错。 123456789101112131415161718192021222324252627let exam = &#123; name: "Tom", age: 24, set info(value)&#123; return this.age = value; &#125;&#125;exam.age; // 24Reflect.set(exam, 'age', 25); // trueexam.age; // 25 // value 为空时会将 name 属性清除Reflect.set(exam, 'age', ); // trueexam.age; // undefined // 当 target 对象中存在 name 属性 setter 方法时，setter 方法中的 this 会绑定 // receiver , 所以修改的实际上是 receiver 的属性,let receiver = &#123; age: 18&#125;Reflect.set(exam, 'info', 1, receiver); // truereceiver.age; // 1 let receiver1 = &#123; name: 'oppps'&#125;Reflect.set(exam, 'info', 1, receiver1);receiver1.age; // 1 Reflect.has 1Reflect.has(obj, name) 是 name in obj 指令的函数化，用于查找 name 属性在 obj 对象中是否存在。返回值为 boolean。如果 obj 不是对象则会报错 TypeError。 12345let exam = &#123; name: "Tom", age: 24&#125;Reflect.has(exam, 'name'); // true Reflect.deleteProperty 1Reflect.deleteProperty(obj, property) 是 delete obj[property] 的函数化，用于删除 obj 对象的 property 属性，返回值为 boolean。如果 obj 不是对象则会报错 TypeError。 12345678let exam = &#123; name: "Tom", age: 24&#125;Reflect.deleteProperty(exam , 'name'); // trueexam // &#123;age: 24&#125; // property 不存在时，也会返回 trueReflect.deleteProperty(exam , 'name'); // true Reflect.construct 1Reflect.construct(obj, args) 等同于 new target(…args)。 1234function exam(name)&#123; this.name = name;&#125;Reflect.construct(exam, ['Tom']); // exam &#123;name: "Tom"&#125; Reflect.getPrototypeOf 1Reflect.getPrototypeOf(obj) 用于读取 obj 的 _proto_ 属性。在 obj 不是对象时不会像 Object 一样把 obj 转为对象，而是会报错。 123class Exam&#123;&#125;let obj = new Exam()Reflect.getPrototypeOf(obj) === Exam.prototype // true Reflect.setPrototypeOf 1Reflect.setPrototypeOf(obj, newProto) 用于设置目标对象的 prototype。 12let obj =&#123;&#125;Reflect.setPrototypeOf(obj, Array.prototype); // true Reflect.apply 1Reflect.apply(func, thisArg, args) 等同于 Function.prototype.apply.call(func, thisArg, args) 。func 表示目标函数；thisArg 表示目标函数绑定的 this 对象；args 表示目标函数调用时传入的参数列表，可以是数组或类似数组的对象。若目标函数无法调用，会抛出 TypeError 。 1Reflect.apply(Math.max, Math, [1, 3, 5, 3, 1]); // 5 Reflect.defineProperty 1Reflect.defineProperty(target, propertyKey, attributes) 用于为目标对象定义属性。如果 target 不是对象，会抛出错误。 1234let myDate= &#123;&#125;Reflect.defineProperty(MyDate, 'now', &#123; value: () =&gt; Date.now()&#125;); // true Reflect.getOwnPropertyDescriptor 1Reflect.getOwnPropertyDescriptor(target, propertyKey) 用于得到 target 对象的 propertyKey 属性的描述对象。在 target 不是对象时，会抛出错误表示参数非法，不会将非对象转换为对象。 123456789101112var exam = &#123;&#125;Reflect.defineProperty(exam, 'name', &#123; value: true, enumerable: false,&#125;)Reflect.getOwnPropertyDescriptor(exam, 'name')// &#123; configurable: false, enumerable: false, value: true, writable:// false&#125; // propertyKey 属性在 target 对象中不存在时，返回 undefinedReflect.getOwnPropertyDescriptor(exam, 'age') // undefined Reflect.isExtensible 1Reflect.isExtensible(target) 用于判断 target 对象是否可扩展。返回值为 boolean 。如果 target 参数不是对象，会抛出错误。 12let exam = &#123;&#125;Reflect.isExtensible(exam) // true Reflect.preventExtensions 1Reflect.preventExtensions(target) 用于让 target 对象变为不可扩展。如果 target 参数不是对象，会抛出错误。 12let exam = &#123;&#125;Reflect.preventExtensions(exam) // true Reflect.ownKeys 1Reflect.ownKeys(target) 用于返回 target 对象的所有属性，等同于 Object.getOwnPropertyNames 与Object.getOwnPropertySymbols 之和。 12345var exam = &#123; name: 1, [Symbol.for('age')]: 4&#125;Reflect.ownKeys(exam) // ["name", Symbol(age)] Proxy 和 Reflect组合使用Reflect 对象的方法与 Proxy 对象的方法是一一对应的。所以 Proxy 对象的方法可以通过调用 Reflect 对象的方法获取默认行为，然后进行额外操作。 1234567891011121314151617181920let exam = &#123; name: "Tom", age: 24&#125;let handler = &#123; get: function(target, key)&#123; console.log("getting "+key); return Reflect.get(target, key); &#125;, set: function(target, key, value)&#123; console.log("setting "+key+" to "+value) Reflect.set(target, key, value); &#125;&#125;let proxy = new Proxy(exam, handler)proxy.name = "Jerry"proxy.name// setting name to Jerry// getting name// "Jerry" 使用场景拓展 1234567891011121314// 定义 Set 集合const queuedObservers = new Set();// 把观察者函数都放入 Set 集合中const observe = fn =&gt; queuedObservers.add(fn);// observable 返回原始对象的代理，拦截赋值操作const observable = obj =&gt; new Proxy(obj, &#123;set&#125;);function set(target, key, value, receiver) &#123; // 获取对象的赋值操作 const result = Reflect.set(target, key, value, receiver); // 执行所有观察者 queuedObservers.forEach(observer =&gt; observer()); // 执行赋值操作 return result;&#125;]]></content>
      <categories>
        <category>web前端</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GlusterFS 原理及使用]]></title>
    <url>%2F2019%2F07%2F01%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FGlusterFS%2FGlusterFS%2F</url>
    <content type="text"><![CDATA[Gluster 简介Gluster 概述 Glusterfs是一个开源的分布式文件系统； 是Scale存储的核心,能够处理千数量级的客户端.在传统的解决方案中Glusterfs能够灵活的结合物理的,虚拟的和云资源去体现高可用和企业级的性能存储. Glusterfs通过TCP/IP或InfiniBand RDMA网络链接将客户端的存储资块源聚集在一起； 使用单一的全局命名空间来管理数据,磁盘和内存资源. Glusterfs基于堆叠的用户空间设计,可以为不同的工作负载提供高优的性能. Glusterfs支持运行在任何标准IP网络上标准应用程序的标准客户端，用户可以在全局统一的命名空间中使用NFS/CIFS等标准协议来访问应用数据. Gluster 主要特征扩展性和高性能GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。 高可用性GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT3、ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问。 全局统一命名空间全局统一命名空间将磁盘和内存资源聚集成一个单一的虚拟存储池，对上层用户和应用屏蔽了底层的物理硬件。存储资源可以根据需要在虚拟存储池中进行弹性扩展，比如扩容或收缩。当存储虚拟机映像时，存储的虚拟映像文件没有数量限制，成千虚拟机均通过单一挂载点进行数据共享。虚拟机I/O可在命名空间内的所有服务器上自动进行负载均衡，消除了SAN环境中经常发生的访问热点和性能瓶颈问题。 弹性哈希算法GlusterFS采用弹性哈希算法在存储池中定位数据，而不是采用集中式或分布式元数据服务器索引。在其他的Scale-Out存储系统中，元数据服务器通常会导致I/O性能瓶颈和单点故障问题。GlusterFS中，所有在Scale-Out存储配置中的存储系统都可以智能地定位任意数据分片，不需要查看索引或者向其他服务器查询。这种设计机制完全并行化了数据访问，实现了真正的线性性能扩展。 基于标准协议Gluster存储服务支持NFS, CIFS, HTTP, FTP以及Gluster原生协议，完全与POSIX标准兼容。现有应用程序不需要作任何修改或使用专用API，就可以对Gluster中的数据进行访问。这在公有云环境中部署Gluster时非常有用，Gluster对云服务提供商专用API进行抽象，然后提供标准POSIX接口。 显著优点 开源，且网上的文档基本上足够你维护一套简单的存储系统。 支持多客户端处理，当前已知的支持千级别及以上规模客户。 支持POSIX&lt;简单来说，软件跨平台，也可以看找的其他博客说明，见‘参考文件’&gt;。 支持各种低端硬件，当然并不推崇，但是可以实现。我在线上用过8核，32G，依然跑的很溜。 支持NFS/SMB/CIFS/GLUSTERFS等行业标准协议访问。 提供各种优秀的功能，如磁盘配额、复制式、分布式、快照、性能检测命令等。 支持大容量存储，当前已知的支持PB及以上规模存储。 可以使用任何支持扩展属性的ondisk文件系统，比如xattr. 强大且简单的扩展能力，你可以在任何时候快速扩容。 自动配置故障转移，在故障发生时，无需任何操作即可恢复数据，且最新副本依然从仍在运行的节点获取。 术语介绍Brick: GFS中的存储单元，通常是一个受信存储池中的服务器的一个导出目录。可以通过主机名和目录名来标识，如’SERVER:EXPORT’ Client: 挂载了GFS卷的设备 Extended Attributes: xattr是一个文件系统的特性，其支持用户或程序关联文件/目录和元数据。 FUSE: Filesystem Userspace是一个可加载的内核模块，其支持非特权用户创建自己的文件系统而不需要修改内核代码。通过在用户空间运行文件系统的代码通过FUSE代码与内核进行桥接。 GFID: GFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode Namespace: 每个Gluster卷都导出单个ns作为POSIX的挂载点 Node: 一个拥有若干brick的设备 RDMA: 远程直接内存访问，支持不通过双方的OS进行直接内存访问。 RRDNS: round robin DNS是一种通过DNS轮转返回不同的设备以进行负载均衡的方法 Self-heal: 用于后台运行检测复本卷中文件和目录的不一致性并解决这些不一致。 Split-brain: 脑裂 Volfile: glusterfs进程的配置文件，通常位于/var/lib/glusterd/vols/volname Volume: 一组bricks的逻辑集合 工作原理 首先是在客户端， 用户通过glusterfs的mount point 来读写数据， 对于用户来说，集群系统的存在对用户是完全透明的，用户感觉不到是操作本地系统还是远端的集群系统。 用户的这个操作被递交给 本地linux系统的VFS来处理。 VFS 将数据递交给FUSE 内核文件系统:在启动 glusterfs 客户端以前，需要想系统注册一个实际的文件系统FUSE,如上图所示，该文件系统与ext3在同一个层次上面， ext3 是对实际的磁盘进行处理， 而fuse 文件系统则是将数据通过/dev/fuse 这个设备文件递交给了glusterfs client端。所以， 我们可以将 fuse文件系统理解为一个代理。 数据被fuse 递交给Glusterfs client 后， client 对数据进行一些指定的处理（所谓的指定，是按照client 配置文件据来进行的一系列处理， 我们在启动glusterfs client 时需要指定这个文件。 在glusterfs client的处理末端，通过网络将数据递交给 Glusterfs Server，并且将数据写入到服务器所控制的存储设备上。 GlusterFS 整体工作流 1，只要安装了glusterFS，会创建一个gluster管理守护进程(glusterd)二进制文件，该守护进程应该在集群中的所有设备上运行。 2，启动glusterd后，可以创建一个由所有存储服务器节点组成的受信任的服务器池(TSP可以包含单个节点) 3，作为基本存储单元的brick可以再这些服务器中作为导出目录创建。 4，从这个TSP的任何数量的brick都可以被联合起来形成一个整体。 5，一旦创建了volume，glusterfsd进程就会开始在每个参与的brick中运行，除此之外，将在/var/lib/glusterd/vols/生成为vol文件的配置文件。 6，volume中将会有与每个brick对应的配置文件，文件中包含关于特定brick的所有内容。 7，客户端进程所需的配置文件也将被创建。 8，文件系统完成，挂载使用。 9，挂载的IP/主机名可以是受信任服务器池中创建所需volume的任何节点的IP/主机名。 10，当我们在客户端安装volume时，客户端glusterfs进程与服务器的glusterd进程进行通信。 11，服务器glusterd进程发送一个配置文件(vol)文件，其中包括客户端转换器列表，另一个包含volume中每个brick的信息。 12，借助于该文件，客户端glusterfs进程可以与每个brick的glusterfsd进行通信。 13，当挂载的文件系统中，客户端发出系统调用&lt;文件操作或Fop或打开文件&gt;时， 14，VFS识别文件系统类型为glusterfs，会将请求发送到FUSE内核模块。 15，FUSE内核模块将通过/dev/fuse将其发送到客户机节点的用户空间中的glusterFS。 16，客户端上的glusterFS进程由一堆成为客户端翻译器的翻译器组成。这些翻译器在存储服务器glusterd进程发送的配置文件(vol文件)中定义。 17，这些翻译器中的第一个由FUSE库(libfuse)组成的FUSE翻译器。 18，每个翻译器都具有与每个文件操作对应的功能或glusterfs支持的fop。 19，该请求将在每个翻译器中发挥相应的功能。 常用volume 卷类型分布（distributed）默认模式，既DHT, 也叫 分布卷: 将文件已hash算法随机分布到 一台服务器节点中存储。 复制（replicate）复制模式，既AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。 条带（striped）条带模式，既Striped, 创建volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。 分布式条带卷(distribute stripe volume)分布式条带模式（组合型），最少需要4台服务器才能创建。 创建volume 时 stripe 2 server = 4 个节点： 是DHT 与 Striped 的组合型。 分布式复制卷(distribute replica volume)分布式复制模式（组合型）, 最少需要4台服务器才能创建。 创建volume 时 replica 2 server = 4 个节点：是DHT 与 AFR 的组合型。 条带复制卷(stripe replica volume)条带复制卷模式（组合型）, 最少需要4台服务器才能创建。 创建volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。 分布式条带复制卷(distribute stripe replicavolume) 三种模式混合, 至少需要8台 服务器才能创建。 stripe 2 replica 2 , 每4个节点 组成一个 组。 环境准备前置条件 至少需要两台设备/节点。 具备正常的网络连接 至少需要两个磁盘，一个用于OS安装，一个用于服务gluster存储； 环境配置关闭防火墙 12systemctl stop firewalld.service #停止firewalldsystemctl disable firewalld.service #禁止firewalld开机自启 关闭SELinux 123sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config #关闭SELinuxsetenforce 0getenforce 配置host文件 123410.0.0.101 node0110.0.0.102 node0210.0.0.103 node0310.0.0.104 node04 同步时间 1ntpdate time.windows.com #同步时间 *安装 gluster 源 * 1yum -y install centos-release-gluster312.noarch 安装glusterfs 1yum -y --enablerepo=centos-gluster*-test install glusterfs-server glusterfs-cli glusterfs-geo-replication 启动gluster， 并设置开机自启动 12345glusterfs -vsystemctl start glusterd.servicesystemctl enable glusterd.servicesystemctl status glusterd.servicenetstat -lntup | grep gluster 格式化磁盘(全部gluster ) 在每台主机上创建几块硬盘，做接下来的分布式存储使用 注：创建的硬盘要用xfs格式来格式化硬盘，如果用ext4来格式化硬盘的话，对于大于16TB空间格式化就无法实现了。所以这里要用xfs格式化磁盘(centos7默认的文件格式就是xfs)，并且xfs的文件格式支持PB级的数据量 123456789fdisk -l # 查看所有磁盘设备mkfs.xfs -i size=512 /dev/sdb # 格式化磁盘, 一块磁盘相当于一个brickmkfs.xfs -i size=512 /dev/sdcmkfs.xfs -i size=512 /dev/sddmkdir -p /data/brick&#123;1..3&#125; # 创建挂载块设备的目录echo '/dev/sdb /data/brick1 xfs defaults 0 0' &gt;&gt; /etc/fstabecho '/dev/sdc /data/brick2 xfs defaults 0 0' &gt;&gt; /etc/fstabecho '/dev/sdd /data/brick3 xfs defaults 0 0' &gt;&gt; /etc/fstabmount -a 将server 主机加入到信任池 随便在一个开启glusterfs服务的主机上将其他主机加入到一个信任的主机池里，这里选择node01 1234567gluster peer probe node01gluster peer probe node02gluster peer probe node03gluster peer probe node04gluster peer detach xxx # 用来将节点从信任池中删除gluster peer status # 查看信任主机池状态# 当客户端需要挂载卷的时候，也需要加入到信任主机池中 gluster 卷 操作GlusterFS 五种卷的创建注意 Distributed：分布式卷，文件通过 hash 算法随机分布到由 bricks 组成的卷上。 Replicated: 复制式卷，类似 RAID 1，replica 数必须等于 volume 中 brick 所包含的存储服务器数，可用性高。 Striped: 条带式卷，类似 RAID 0，stripe 数必须等于 volume 中 brick 所包含的存储服务器数，文件被分成数据块，以 Round Robin 的方式存储在 bricks 中，并发粒度是数据块，大文件性能好。 Distributed Striped: 分布式的条带卷，volume中 brick 所包含的存储服务器数必须是 stripe 的倍数（&gt;=2倍），兼顾分布式和条带式的功能。 Distributed Replicated: 分布式的复制卷，volume 中 brick 所包含的存储服务器数必须是 replica 的倍数（&gt;=2倍），兼顾分布式和复制式的功能。 分布式复制卷的brick顺序决定了文件分布的位置，一般来说，先是两个brick形成一个复制关系，然后两个复制关系形成分布。 企业一般用后两种，大部分会用分布式复制（可用容量为 总容量/复制份数），通过网络传输的话最好用万兆交换机，万兆网卡来做。这样就会优化一部分性能。它们的数据都是通过网络来传输的。 配置分布式卷123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#在信任的主机池中任意一台设备上创建卷都可以，而且创建好后可在任意设备挂载后都可以查看[root@node01 ~] gluster volume create gv1 node01:/data/brick1 node02:/data/brick1 force #创建分布式卷volume create: gv1: success: please start the volume to access data[root@node01 ~] gluster volume start gv1 #启动卷gv1volume start: gv1: success[root@node01 ~] gluster volume info gv1 #查看gv1的配置信息 Volume Name: gv1Type: Distribute #分布式卷Volume ID: 85622964-4b48-47d5-b767-d6c6f1e684ccStatus: StartedSnapshot Count: 0Number of Bricks: 2Transport-type: tcpBricks:Brick1: node01:/data/brick1Brick2: node02:/data/brick1Options Reconfigured:transport.address-family: inetnfs.disable: on[root@node01 ~] mount -t glusterfs 127.0.0.1:/gv1 /opt #挂载gv1卷[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sdc 5.0G 33M 5.0G 1% /data/brick2/dev/sdd 5.0G 33M 5.0G 1% /data/brick3/dev/sdb 5.0G 33M 5.0G 1% /data/brick1127.0.0.1:/gv1 10G 65M 10G 1% /opt # 两个设备容量之和[root@node01 ~] cd /opt/[root@node01 opt] touch &#123;a..f&#125; #创建测试文件[root@node01 opt] ll总用量 0-rw-r--r--. 1 root root 0 2月 2 23:59 a-rw-r--r--. 1 root root 0 2月 2 23:59 b-rw-r--r--. 1 root root 0 2月 2 23:59 c-rw-r--r--. 1 root root 0 2月 2 23:59 d-rw-r--r--. 1 root root 0 2月 2 23:59 e-rw-r--r--. 1 root root 0 2月 2 23:59 f# 在node04也可看到新创建的文件，信任存储池中的每一台主机挂载这个卷后都可以看到[root@node04 ~] mount -t glusterfs 127.0.0.1:/gv1 /opt[root@node04 ~] ll /opt/总用量 0-rw-r--r--. 1 root root 0 2月 2 2018 a-rw-r--r--. 1 root root 0 2月 2 2018 b-rw-r--r--. 1 root root 0 2月 2 2018 c-rw-r--r--. 1 root root 0 2月 2 2018 d-rw-r--r--. 1 root root 0 2月 2 2018 e-rw-r--r--. 1 root root 0 2月 2 2018 f[root@node01 opt] ll /data/brick1/总用量 0-rw-r--r--. 2 root root 0 2月 2 23:59 a-rw-r--r--. 2 root root 0 2月 2 23:59 b-rw-r--r--. 2 root root 0 2月 2 23:59 c-rw-r--r--. 2 root root 0 2月 2 23:59 e[root@node02 ~] ll /data/brick1总用量 0-rw-r--r--. 2 root root 0 2月 2 23:59 d-rw-r--r--. 2 root root 0 2月 2 23:59 f#文件实际存在位置node01和node02上的/data/brick1目录下,通过hash分别存到node01和node02上的分布式磁盘上 配置复制卷复制模式，既AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 这条命令的意思是使用Replicated的方式，建立一个名为gv2的卷(Volume)，存储块(Brick)为2个，分别为node01:/data/brick2 和 node02:/data/brick2；# fore为强制创建：因为复制卷在双方主机通信有故障再恢复通信时容易发生脑裂。本次为实验环境，生产环境不建议使用。[root@node01 ~] gluster volume create gv2 replica 2 node01:/data/brick2 node02:/data/brick2 force volume create: gv2: success: please start the volume to access data[root@node01 ~] gluster volume start gv2 #启动gv2卷volume start: gv2: success[root@node01 ~] gluster volume info gv2 #查看gv2信息Volume Name: gv2Type: Replicate #复制卷Volume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off[root@node01 ~] mount -t glusterfs 127.0.0.1:/gv2 /mnt[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sdc 5.0G 33M 5.0G 1% /data/brick2/dev/sdd 5.0G 33M 5.0G 1% /data/brick3/dev/sdb 5.0G 33M 5.0G 1% /data/brick1127.0.0.1:/gv1 10G 65M 10G 1% /opt127.0.0.1:/gv2 5.0G 33M 5.0G 1% /mnt #容量是总容量的一半[root@node01 ~] cd /mnt/[root@node01 mnt] touch &#123;1..6&#125;[root@node01 mnt] ll /data/brick2总用量 0-rw-r--r--. 2 root root 0 2月 3 01:06 1-rw-r--r--. 2 root root 0 2月 3 01:06 2-rw-r--r--. 2 root root 0 2月 3 01:06 3-rw-r--r--. 2 root root 0 2月 3 01:06 4-rw-r--r--. 2 root root 0 2月 3 01:06 5-rw-r--r--. 2 root root 0 2月 3 01:06 6[root@node02 ~] ll /data/brick2总用量 0-rw-r--r--. 2 root root 0 2月 3 01:06 1-rw-r--r--. 2 root root 0 2月 3 01:06 2-rw-r--r--. 2 root root 0 2月 3 01:06 3-rw-r--r--. 2 root root 0 2月 3 01:06 4-rw-r--r--. 2 root root 0 2月 3 01:06 5-rw-r--r--. 2 root root 0 2月 3 01:06 6# 创建文件的实际存在位置为node01和node02上的/data/brick2目录下，因为是复制卷，这两个目录下的内容是完全一致的。 配置条带卷12345678910111213141516171819202122232425262728293031323334353637383940414243[root@node01 ~] gluster volume create gv3 stripe 2 node01:/data/brick3 node02:/data/brick3 forcevolume create: gv3: success: please start the volume to access data[root@node01 ~] gluster volume start gv3volume start: gv3: success[root@node01 ~] gluster volume info gv3 Volume Name: gv3Type: StripeVolume ID: 54c16832-6bdf-42e2-81a9-6b8d7b547c1aStatus: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick3Brick2: node02:/data/brick3Options Reconfigured:transport.address-family: inetnfs.disable: on[root@node01 ~] mkdir /data01[root@node01 ~] mount -t glusterfs 127.0.0.1:/gv3 /data01[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点127.0.0.1:/gv3 10G 65M 10G 1% /data01[root@node01 ~] dd if=/dev/zero bs=1024 count=10000 of=/data01/10M.file[root@node01 ~] dd if=/dev/zero bs=1024 count=20000 of=/data01/20M.file[root@node01 ~] ll /data01/ -h总用量 30M-rw-r--r--. 1 root root 9.8M 2月 3 02:03 10M.file-rw-r--r--. 1 root root 20M 2月 3 02:04 20M.file*************************************************************************************#文件的实际存放位置：[root@node01 ~] ll -h /data/brick3总用量 15M-rw-r--r--. 2 root root 4.9M 2月 3 02:03 10M.file-rw-r--r--. 2 root root 9.8M 2月 3 02:03 20M.file[root@node02 ~] ll -h /data/brick3总用量 15M-rw-r--r--. 2 root root 4.9M 2月 3 02:03 10M.file-rw-r--r--. 2 root root 9.8M 2月 3 02:04 20M.file# 上面可以看到 10M 20M 的文件分别分成了 2 块（这是条带的特点），写入的时候是循环地一点一点在node01和node02的磁盘上.#上面配置的条带卷在生产环境是很少使用的，因为它会将文件破坏，比如一个图片，它会将图片一份一份地分别存到条带卷中的brick上。 配置分布式复制卷(拓展卷)注意: 块服务器的数量必须是复制的倍数 将按块服务器的排列顺序指定相邻的块服务器成为彼此的复制； 例如，8台服务器： 当复制副本为2时，按照服务器列表的顺序，服务器1和2作为一个复制,3和4作为一个复制,5和6作为一个复制,7和8作为一个复制 当复制副本为4时，按照服务器列表的顺序，服务器1/2/3/4作为一个复制,5/6/7/8作为一个复制 1234567891011121314151617181920212223242526272829#将原有的复制卷gv2进行扩容，使其成为分布式复制卷；#要扩容前需停掉gv2[root@node01 ~] gluster volume stop gv2[root@node01 ~] gluster volume add-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 force #添加brick到gv2中volume add-brick: success[root@node01 ~] gluster volume start gv2volume start: gv2: success[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: Distributed-Replicate # 这里显示是分布式复制卷，是在 gv2 复制卷的基础上增加 2 块 brick 形成的Volume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StartedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Brick3: node03:/data/brick1Brick4: node04:/data/brick1Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off注意：当你给分布式复制卷和分布式条带卷增加 bricks 时，你增加的 bricks 数目必须是复制或条带数目的倍数，例如：你给一个分布式复制卷的 replica 为 2，你在增加 bricks 的时候数量必须为2、4、6、8等。扩容后进行测试，发现文件都分布在扩容前的卷中。 配置分布式条带卷(拓展卷)1234567891011121314151617181920212223#将原有的复制卷gv3进行扩容，使其成为分布式条带卷#要扩容前需停掉gv3[root@node01 ~] gluster volume stop gv3[root@node01 ~] gluster volume add-brick gv3 stripe 2 node03:/data/brick2 node04:/data/brick2 force #添加brick到gv3中[root@node01 ~] gluster volume start gv3volume start: gv3: success[root@node01 ~] gluster volume info gv3 Volume Name: gv3Type: Distributed-Stripe # 这里显示是分布式条带卷，是在 gv3 条带卷的基础上增加 2 块 brick 形成的Volume ID: 54c16832-6bdf-42e2-81a9-6b8d7b547c1aStatus: StartedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: node01:/data/brick3Brick2: node02:/data/brick3Brick3: node03:/data/brick2Brick4: node04:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: on 磁盘存储平衡(拓展卷后操作)平衡布局是很有必要的，因为布局结构是静态的，当新的 bricks 加入现有卷，新创建的文件会分布到旧的 bricks 中，所以需要平衡布局结构，使新加入的 bricks 生效。布局平衡只是使新布局生效，并不会在新的布局中移动老的数据，如果你想在新布局生效后，重新平衡卷中的数据，还需要对卷中的数据进行平衡。 12345678910111213141516171819202122232425262728293031#在gv2的分布式复制卷的挂载目录中创建测试文件如下[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点127.0.0.1:/gv2 10G 65M 10G 1% /mnt[root@node01 ~] cd /mnt/[root@node01 mnt] touch &#123;x..z&#125;#新创建的文件只在老的brick中有，在新加入的brick中是没有的[root@node01 mnt]# ls /data/brick21 2 3 4 5 6 x y z[root@node02 ~] ls /data/brick21 2 3 4 5 6 x y z[root@node03 ~] ll -h /data/brick1总用量 0[root@node04 ~] ll -h /data/brick1总用量 0# 从上面可以看到，新创建的文件还是在之前的 bricks 中，并没有分布中新加的 bricks 中# 下面进行磁盘存储平衡[root@node01 ~] gluster volume rebalance gv2 start[root@node01 ~] gluster volume rebalance gv2 status #查看平衡存储状态# 查看磁盘存储平衡后文件在 bricks 中的分布情况[root@node01 ~] ls /data/brick21 5 y[root@node02 ~] ls /data/brick21 5 y[root@node03 ~] ls /data/brick12 3 4 6 x z[root@node04 ~] ls /data/brick12 3 4 6 x z# 从上面可以看出部分文件已经平衡到新加入的brick中了# 每做一次扩容后都需要做一次磁盘平衡。 磁盘平衡是在万不得已的情况下再做的，一般再创建一个卷就可以了。 移除brick(收缩卷)你可能想在线缩小卷的大小，例如：当硬件损坏或网络故障的时候，你可能想在卷中移除相关的 bricks。 注意：当你移除 bricks 的时候，你在 gluster 的挂载点将不能继续访问数据，只有配置文件中的信息移除后你才能继续访问 bricks 中的数据。当移除分布式复制卷或者分布式条带卷的时候，移除的 bricks 数目必须是 replica 或者 stripe 的倍数。 但是移除brick在生产环境中基本上不做的，如果是硬盘坏掉的话，直接换个好的硬盘即可，然后再对新的硬盘设置卷标识就可以使用了，后面会演示硬件故障或系统故障的解决办法。 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@node01 ~] gluster volume stop gv2# 先将数据迁移到其它可用的Brick，迁移结束后才将该Brick移除：[root@node01 ~] gluster volume remove-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 start# 在执行了start之后，可以使用status命令查看移除进度[root@node01 ~] gluster volume remove-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 status# 不进行数据迁移，直接删除该Brick[root@node01 ~] gluster volume remove-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 commit[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: ReplicateVolume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StoppedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off# 如果误操作删除了后，其实文件还在 /storage/brick1 里面的，加回来就可以了[root@node01 ~] gluster volume add-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 force volume add-brick: success[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: Distributed-ReplicateVolume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StoppedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Brick3: node03:/data/brick1Brick4: node04:/data/brick1Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 迁移卷(迁移卷)12345678910111213141516171819202122[root@node01 ~] gluster volume stop gv2# 先将数据迁移到其它可用的Brick：[root@node01 ~] gluster volume replace-brick gv2 node03:/data/brick1 node06:/data/brick1 start# 在执行了start之后，可以使用status命令查看迁移进度[root@node01 ~] gluster volume replace-brick gv2 node03:/data/brick1 node06:/data/brick1 status# 在数据迁移结束后，执行commit命令来进行Brick替换，如果不进行迁移直接commit会造成数据丢失[root@node01 ~] gluster volume replace-brick gv2 node03:/data/brick1 node06:/data/brick1 commit[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: ReplicateVolume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StoppedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 删除卷一般会用在命名不规范的时候才会删除 12[root@node01 ~] gluster volume stop gv1[root@node01 ~] gluster volume delete gv 卷误删解决办法12345678910[root@node01 ~] ls /var/lib/glusterd/vols/gv2 gv3[root@node01 ~] rm -rf /var/lib/glusterd/vols/gv3 #删除卷gv3的卷信息[root@node01 ~] ls /var/lib/glusterd/vols/ #再查看卷信息情况如下：gv3卷信息被删除了gv2[root@node01 ~] gluster volume sync node02 #因为其他节点服务器上的卷信息是完整的，比如从node02上同步所有卷信息如下：Sync volume may make data inaccessible while the sync is in progress. Do you want to continue? (y/n) yvolume sync: success[root@node01 ~] ls /var/lib/glusterd/vols/ #验证卷信息是否同步过来gv2 gv3 卷数据不一致解决办法1234567891011[root@node01 ~] ls /data/brick2 #复制卷的存储位置的数据1 5 y[root@node01 ~] rm -f /data/brick2/y [root@node01 ~] ls /data/brick21 5[root@node02 ~] ls /data/brick21 5 y[root@node01 ~] gluster start gv2 #因为之前关闭了，如果未关闭可以忽略此步。[root@node01 ~] cat /mnt/y #通过访问这个复制卷的挂载点的数据来同步数据[root@node01 ~] ls /data/brick2/ #这时候再看复制卷的数据是否同步成功1 5 y glusterfs 分布式存储优化优化参数 1234567891011121314Auth_allow #IP访问授权；缺省值（*.allow all）；合法值：Ip地址Cluster.min-free-disk #剩余磁盘空间阀值；缺省值（10%）；合法值：百分比Cluster.stripe-block-size #条带大小；缺省值（128KB）；合法值：字节Network.frame-timeout #请求等待时间；缺省值（1800s）；合法值：1-1800Network.ping-timeout #客户端等待时间；缺省值（42s）；合法值：0-42Nfs.disabled #关闭NFS服务；缺省值（Off）；合法值：Off|onPerformance.io-thread-count #IO线程数；缺省值（16）；合法值：0-65Performance.cache-refresh-timeout #缓存校验时间；缺省值（1s）；合法值：0-61Performance.cache-size #读缓存大小；缺省值（32MB）；合法值：字节Performance.quick-read: #优化读取小文件的性能Performance.read-ahead: #用预读的方式提高读取的性能，有利于应用频繁持续性的访问文件，当应用完成当前数据块读取的时候，下一个数据块就已经准备好了。Performance.write-behind:先写入缓存内，在写入硬盘，以提高写入的性能。Performance.io-cache:缓存已经被读过的、 优化方式 123456789命令格式：gluster.volume set &lt;卷&gt;&lt;参数&gt;例如：#打开预读方式访问存储[root@node01 ~]# gluster volume set gv2 performance.read-ahead on#调整读取缓存的大小[root@mystorage gv2]# gluster volume set gv2 performance.cache-size 256M glusterfs 监控及维护1234567891011121314151617181920212223242526272829303132333435363738394041424344使用zabbix自带的模板即可，CPU、内存、磁盘空间、主机运行时间、系统load。日常情况要查看服务器监控值，遇到报警要及时处理。#看下节点有没有在线gluster volume status nfsp#启动完全修复gluster volume heal gv2 full#查看需要修复的文件gluster volume heal gv2 info#查看修复成功的文件gluster volume heal gv2 info healed#查看修复失败的文件gluster volume heal gv2 heal-failed#查看主机的状态gluster peer status#查看脑裂的文件gluster volume heal gv2 info split-brain#激活quota功能gluster volume quota gv2 enable#关闭quota功能gulster volume quota gv2 disable#目录限制（卷中文件夹的大小）gluster volume quota limit-usage /data/30MB --/gv2/data#quota信息列表gluster volume quota gv2 list#限制目录的quota信息gluster volume quota gv2 list /data#设置信息的超时时间gluster volume set gv2 features.quota-timeout 5#删除某个目录的quota设置gluster volume quota gv2 remove /data备注：quota功能，主要是对挂载点下的某个目录进行空间限额。如：/mnt/gulster/data目录，而不是对组成卷组的空间进行限制。 故障处理一台主机故障一台节点故障的情况包含以下情况： 物理故障 同时有多块硬盘故障，造成数据丢失 系统损坏不可修复 解决方法： ​ 找一台完全一样的机器，至少要保证硬盘数量和大小一致，安装系统，配置和故障机同样的 IP，安装 gluster 软件，保证配置一样，在其他健康节点上执行命令 gluster peer status，查看故障服务器的 uuid 1234567891011121314151617181920212223242526[root@mystorage2 ~] gluster peer statusNumber of Peers: 3Hostname: mystorage3Uuid: 36e4c45c-466f-47b0-b829-dcd4a69ca2e7State: Peer in Cluster (Connected)Hostname: mystorage4Uuid: c607f6c2-bdcb-4768-bc82-4bc2243b1b7aState: Peer in Cluster (Connected)Hostname: mystorage1Uuid: 6e6a84af-ac7a-44eb-85c9-50f1f46acef1State: Peer in Cluster (Disconnected)复制代码修改新加机器的 /var/lib/glusterd/glusterd.info 和 故障机器一样[root@mystorage1 ~] cat /var/lib/glusterd/glusterd.infoUUID=6e6a84af-ac7a-44eb-85c9-50f1f46acef1operating-version=30712在信任存储池中任意节点执行gluster volume heal gv2 full就会自动开始同步，但在同步的时候会影响整个系统的性能。可以查看状态gluster volume heal gv2 info]]></content>
      <categories>
        <category>数据存储</category>
        <category>GlusterFS</category>
      </categories>
      <tags>
        <tag>glusterfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric架构]]></title>
    <url>%2F2019%2F06%2F21%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F5.Fabric%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Fabric的交易流程Fabric 节点的主要分类记账节点、背书节点、排序节点 交易七步曲 Propose 客户端提交交易提案，及调用了链码的请求 Execute endorse节点运行仿真结果，产生读写集，并签名返回给客户端 Proposal 客户端接收到了endorse节点的运行结果，根据背书策略，如果背书通过，产生proposal给order节点打包区块 Order Trasantion Order节点根据一定规则将交易打包进区块，此处会应用一定的共识策略 Deliver 排序节点将打包好的区块分发给commiter节点 Validate Commiter节点收到区块后，对每一个交易进行验证，如果验证通过(会进行背书策略，当前key的状态等验证)，就运行读写集，并将区块上链 Notify 客户端收到区块上链的Event，确认了交易成功 Fabric 怎么解决双花的在第一步双花的情况会运行成功，但是当第一次花费成功的情况下，后面节点再验证交易的读写集的时候，会因为key的状态变化而验证失败，所以双花只会在第一次的交易中成功； 交易流程图 Fabric 的channelchannel的分类系统channel: 在网络启动的时候系统自动创建的channel 自定义channel: 根据业务逻辑在后面开发的业务channel 创建channel的过程在创建channel的时候， 会想系统channel 发送一个创建channel的请求， 产生创建channel的交易 不同channel的数据是隔离的，即一个channel会维护同一套账本 在channel上部署智能合约同一个channel中的智能合约可以互相调用 不同channel的智能合约指定查询但是不能更改账本的内容 Fabric peerPeer 节点介绍peer 节点的角色 代表联盟链中每个组织的节点 是区块链网络的基础，是账本和智能合约的载体 红色框为peer节点在网络中的地位 peer节点的功能 连接了一个或者多个channel 维护了一个或者多个账本 提供智能合约的运行环境 提供身份认证，加密签名，背书等功能 背书策略的示例 Ledger 的介绍Ledger包含的内容leder包含: blockchain(由区块组成的链，代表区块链账本) 、Word state(记录账本的全局状态) 区块存放的信息 区块头存放信息 交易存放的信息 全局数据库维护账本的当前信息， 代表区块链的全局状态 智能合约的介绍智能合约的作用 定义了不同组织记账的规则 生成交易并更改账本的状态 fabric通过chaincode来实现智能合约 智能合约是怎么更改账本的 chaincode 生命周期 系统chaincode LSCC: 处理链码打包 install instance等 CSCC: 组件channel 对channel 进行join 配置等操作 QSCC: 提供账本query 等api ESCC: 对背书节点对提案运行的结果进行签名 VSCC: 验证交易的背书策略， ESCC + VSCC 主要解决了fabric 的共识问题 Gossip协议 用来管理peer节点的发现 和channel的成员管理， 不断发现新的peer peer节点的账本数据传播 新加入的peer，可以通过gossip 进行点对点快速更新数据 peer节点的分类Leader peer 连接 order 节点，并接受新区块 将区块信息分发给组织中其他committing peers 一个组织中可以有一个或者多个leader peer leader peer 的选举规则可以是: static(静态指定), Dynamic（动态选举） Anchor Peer 通过 gossip 协议， 让不同组织之间的 peer 节点互相认识， 帮助不同组织之间的peer节点建立连接 leader peer 节点的配置方法 Orderer 节点Order节点的性质 参与排序的工作，对所有节点提交的交易进行排序成块 每个order节点产生块的hash值必须一致 要有容错机制，即一个order节点异常后，其他的节点要可以继续执行排序工作(一般会有容错的机制) 强一致性，和pow不同，fabric提交的区块即是确定的区块，交易是不能被复写的，所以一致性是必须要求的 不允许有背叛的order节点， order是CFT的排序算法； Order 出块规则BatchSize: MaxMessageCount: AbsoluteMaxBytes PreferredMaxBytes BatchTimeout: Timeout: Orer节点怎么创建Channel的system Channel: 管理其他用户的链， 创建链的时候通过systemchannel创建一个新的channel的 genesis block Order 节点类型SOLO只有一个节点， 只进行简单的打包 hash 和出块，只可以用于测试环境 Kafka通过Kafka 和 zookeeper进行排序 通过 Kafka的 TTC message 来保证所有的order节点，每次打包的区块包含的交易相同，出块的hash值一样； Raft 基于 Etcd/raft 的library 进行的共识 不需要zookeeper等外部依赖，运维简单 实现了order之间的通讯层 一个channel可以在所有order节点的子集中运行 所有节点之间必须使用TLS通讯 与kafka区别是，kafka是基于交易做的共识， raft是对块做的共识 MSP与CAFabric ca功能 注册创建用户实例 将用户证书签名并下载 证书的更新和撤销 fabric ca 是典型的CS结构 fabric server 的参数 fabric ca init 后产生的文件 fabric ca client端的命令参数 fabric ca 支持Identity类型 peer、order、client、 user 生成证书的示例 Identity lifecycle PKI-X.509PKI介绍公钥基础设施(PKI)是一个利用非对称加密算法原理和技术实现并提供安全服务的具有通用性的技术规范和标准。是管理非对称加密算法的密钥和确认信息。整合数字证书、公钥加密技术和CA的系统。其结合了软件、加密技术和组织需要进行非对称加密算法的服务。 公钥基础设施(PKI)是一种遵循既定标准的密钥管理平台，它通过“信息加密”和“数字签名”等密码服务及所必需的密钥和证书管理体系，为实现网络通信保密性、完整性和不可否认性的一套完整、成熟可靠的解决方案。简单来说，PKI就是利用公钥理论和技术建立的提供安全服务的基础设施。PKI技术是信息安全技术的核心。也是电子商务的关键和基础技术。 MSP介绍 MSP结构 Fabric 应用开发开发流程 部署Fabric网络 开发并部署智能合约 通过fabric-ca 和fabric-sdk 进行application层的业务逻辑开发 Fabric 网络部署流程 安装ordering service 安装peer节点 并配置 记账和背书节点 创建channel 将组织加入到channel 安装并实例化chaincode 调用链码 开发人员主要关心的步骤通过fabric进行开发，我们主要需要开发的有两部分： 即上图橘黄色的部分: 1. chaincode开发； 2. Application开发]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 智能合约编写]]></title>
    <url>%2F2019%2F06%2F11%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F4.Fabric%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E7%BC%96%E5%86%99%2F</url>
    <content type="text"><![CDATA[Chaincode 介绍chaincode作用Fabric的Chaincode是一段运行在容器中的程序。Chaincode是客户端程序和Fabric之间的桥梁。 通过Chaincode客户端程序可以发起交易，查询交易。 chaincode 运行环境Chaincode是运行在Dokcer容器中，因此相对来说安全。 支持语言目前支持 java,node，go,go是最稳定的。其他还在完善。 chaincode 运行和使用步骤 创建代码目录 mkdir -p {path} 存放编写好的chaincode源代码 部署chaincode 1peer chaincode install -n &#123;chaincodeId&#125; -v &#123;version&#125; -p &#123;path&#125; 实例化chaincode 1peer chaincode instantiate -o &#123;order_address&#125; -C &#123;channel&#125; -n &#123;chaincodeId&#125; -v &#123;version&#125; -c '&#123;"Args": ["init", "a", "100", "b", "200"]&#125;' -P "OR('Org1MSP.member', 'Org2MSP.member')" 调用chaincode 1peer chaincode invoke -o &#123;order_address&#125; -C &#123;chainnel&#125; -n &#123;chaincodeId&#125; -v &#123;version&#125; -c '&#123;"Args":["invoke", "1", "a", "b"]&#125;' Golang 版Chaincode 编写规范Chaincode 代码结构包名一个chaincode通常是一个 Goalng源文件，包名必须是main —— package main shim包“github.com/hyperledger/fabric/core/shaincode/shim” pb “github.com/hyperledger/fabric/protos/peer” shim提供了Fabric系统的上下文环境，包含了Chaincode和Fabic交互的接口。 在Chaincode中，执行赋值，查询，等功能都是需要通过shim. 定义结构体1type chainCodeExample struct &#123;&#125; chaincode必须定义一个结构体，结构体的名称可以是任意符合Golang命名规范的字符串，并且必须实现Init 和 Invoke 接口方法； 实现Init方法123func (t *chainCodeExample) Init(stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters()&#125; Init方法是系统初始化方法，当执行命令 peer chaincode instantiate 实例化chaincode的时候调用该方法； 实现Invoke方法123func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters()&#125; Invoke方法主要是写入数据等对链进行查询和插入修改等操作的主要方法； shim包核心方法Success1234func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters() return shim.Success([]byte("success invoke"))&#125; Success 方法负责将正确的消息返回给调用chaincode 的客户端 Error1234func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters() return shim.Success([]byte("success invoke"))&#125; Error方法将错误的信息返回给调用Chaincode的客户端。 LogLevel12345func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; logLevel, _ := shim.LogLevel("DEBUG") shim.SetLoggingLevel(logLevel) return shim.Success([]byte("success invoke and not opter!"))&#125; LogLevel 方法负责修改Chaincode 中运行日志级别 Chaincode StubInterface 接口方法shim包提供了一个接口 ChaincodeStubInte， 在Invoke方法和Init 方法中该接口作为参数传入方法中； ChaincodeStubInte 主要方法 GetFunctionAndParameters 获取调用链码的方法名和参数列表 PutState 存储数据到账本中 DelState 删除账本中的数据 GetState 从账本中获取指定数据 CreateCompositeKey 创建符合键 GetStateByPartialCompositeKey 通过符合键取值 SplitCompositeKey 拆分复合键 GetStateByRange 查询指定key 指定范围的历史记录 GetHistoryForKey 获取指定key的历史记录 GetTxID 获取交易编号 GetTxTimestamp 获取交易的时间 GetCreator 获取交易的创建者 InvokeChaincode 调用其他的链码 1、 PutState方法将调用者的数据存储到Fabric链上 1234func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; stub.PutState("a", []byte("test")) return shim.Success([]byte("success invoke"))&#125; 2、 GetState方法将调用者的数据存储到Fabric链上 123456789func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; var keyValue []byte var err error keyValue, err = stub.GetState("getKey") if err != nil &#123; retutn shim.Error("find error") &#125; reutrn shim.Success(keyValue)&#125; 3、GetStateByRange方法根据Key的范围来查询相关数据 12345678910111213141516171819202122232425func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; startKey := "startKey" enKey := "endKey" keysIter, err := stub.GetStateByRange(startKey, endKey) if err != nil &#123; return shim.Error(fmt.Sprintf("GetStateByRange find err: %s", err)) &#125; defer keysIter.Close() var keyValues map[string]string for keysIter.HasNext() &#123; response, iterError := keysIter.Next() if iterError != nil &#123; return shim.Error(fmt.Sprintf("find and error %s", iterErr)) &#125; keyValues[response.Key] = response.Value &#125; for key, value := range keyValues &#123; fmt.Printf("key %d contains %s\n", key, value) &#125; jsonKeys, err := json.Marshal(keys) if err != nil &#123; return shim.Error(fmt.Sprintf("find error on marshaling json: %s", err)) &#125; return shim.Success(jsonKeys)&#125; 4、GetHistoryForKey方法查询某个键的历史记录 12345678910111213141516171819202122232425262728func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; keyName := "key" keysIter, err := stub.GetHistoryForKey(keyName) if err != nil &#123; return shim.Error(fmt.Sprintf("GetHistoryForKey failed err: %s", err)) &#125; defer keysIter.Close() var keys []string for keysIter.HasNext() &#123; response, iterError := keysIter.Next() if iterError != nil &#123; return shim.Error(fmt.Sprintf("iter occur error %s", iterErr)) &#125; txid := response.TxId txvalue := response.Value txstatus := response.IsDelete txtimesamp := response.Timestamp tm := time.Unix(txtimesamp.Seconds, 0) datestr := tm.Format("2006-01-02 03:04:05 PM") fmt.Printf("Tx info - txid: %s value: %s if delete: %t datetime: %s \n", txid, string(txvalue), txstatus, datestr) keys = append(keys, txid) &#125; jsonKeys, err := json.Marshal(keys) if err != nil &#123; return shim.Error(fmt.Sprintf("find error on marshaling json: %s", err)) &#125; return shim.Success(jsonKeys)&#125; 5、DelState 方法12345678func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; delKey := "testKey" err := stub.DelState(delKey) if err != nil &#123; return shim.Error("delete error !") &#125; reutrn shim.Success([]byte("delete success !"))&#125; 6、CreateCompositeKey 方法负责创建组合键 1234567891011func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; params := []string("a", "b", "c") cparams := "test_params" keyName := "testKey" cKey, _ := stub.CreateCompositeKey(keyName, params) err := stub.PutState(ckey, []byte(c_params)) if err != nil &#123; fmt.Println("find errors %s", err) &#125; return shim.Success([]byte(cKey))&#125; 7、GetStateByPartialCompositeKey 和 SplitCompositeKey 方法用来查询复合键的值 12345678910111213141516171819202122232425func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; params := []string("a", "b") keyName := "testKey" response, err := stub.GetStateByPartialCompositeKey(keyName, params) if err != nil &#123; error_str := fmt.Sprintf("find error: %s", err) return shim.Error(error_str) &#125; defer response.Close() var i int var tlist []string for i = 0; response.HasNext; i++ &#123; responseRange, err := response.Next() if err != nil &#123; err_str := fmt.Springf("find error: %s", err) fmt.Println(err_str) return shim.Error(error_str) &#125; value1, compositeKeyParts, _ := stub.SplitCompositeKey(responseRange.Key) value2 := compositeKeyParts[0] value3 := compositeKeyParts[1] fmt.Printf("find value v1:%s v2:%s v3:%s\n", value1, value2, value3) &#125; return shim.Success("success")&#125; 8、GetTxTimestamp方法负责好偶去当前客户端发送交易的时间戳 12345678910func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; txTime, err := stub.GetTxTimestamp() if err != nil &#123; fmt.Printf("Error getting transaction timestamp: %s", err) return shim.Error(fmt.Springf("Error getting transaction timestamp: %s', err)) &#125; tm := time.Unix(txTime.Seconds, 0) fmt.Printf("Transaction Time: %v\n", time.Format("2006-01-02 03:04:05 PM")) return shim.Success([]byte(fmt.Springf("time is : %s", tm.Format("2006-01-02 03:04:05 PM"))))&#125; 9、InvokeChaincode方法用来在chaincode中调用其他的chaincode链码 123456789101112131415161718func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; chaincodeId := "chaincode2" channelId := "channel2" params1 := []string&#123;"query", "a"&#125; queryArgs := make([][]byte, len(params1)) for i, arg := range params1 &#123; queryArgs[i] = []byte(arg) &#125; response := stub.InvokeChaincode(chaincodeId, params1, channelId) if response.Status != shim.OD &#123; errStr := fmt.Sprintf("Failed to query chaincode, got error: %s", response.Payload) fmt.Printf(errStr) return shim.Error(errStr) &#125; result := string(response.Payload) fmt.Printf("invoke chaincode %s", result) return shim.Success([]byte("success invoke chaincode and not opter!"))&#125; Chaincode 操作命令123456789101112131415161718Available Commands: install instantiate invoke list package query signpackage upgradeFlags: --cafile -o, --orderer --tls --transientGlobal Flags: --logging-level --test.coverprofile -v, --version Chaincode背书规则指定背书介绍Fabaric 中对数据参与方对数据的确认是真实通过Chaincode来进行的。 什么是背书呢？ 背书就是仪表交易被确认的过程。大概意思就是交易你必须背会一本书才能操作。 背书策略被用来指示对相关的参与方如何对交易进行确认。当一个节点接收到一个交易请求的时候，会调用vscc系统（系统Chaincode，专门负责处理背书相关的操作）与交易的Chaincode共同来验证交易的合法性。在vscc和交易的 Chaincode共同对交易的确认中，通常会做一下的校验。 所有的背书是否有效 参与背书的数量是否满足要求 所有背书参与方是否满足要求 指定背书规则的方法背书策略的设置是通过Chaincode部署时instantiate命令中的-p参数来设置的。 1peer chaincode instantiate -o &#123;order_address&#125; -C &#123;channel&#125; -n &#123;chaincodeId&#125; -v &#123;version&#125; -c '&#123;"Args": ["init", "a", "100", "b", "200"]&#125;' -P "OR('Org1MSP.member', 'Org2MSP.member')" 这个参数包说明是当前Chaincode发起的交易，需要组织编号为 Org1MSP的组织编号为Org2MSP的组织中的任何一个用户共同参与交易的确认并且同意。这样交易才能生效并且记录到 区块链中。 通过上述背书策略的示例我们可以知道背书策略是通过一定的关键字和系统的属性组成的。 背书编写示例 逻辑与关系 1AND('Org1MSP.member', 'Org2MSP.member', 'Org3MSP.member') 逻辑或关系 1OR('Org1MSP.member', 'Org2MSP.member', 'Org3MSP.member') 逻辑与或关系并存 1OR('Org1MSP.member', AND('Org2MSP.member', 'Org3MSP.member'))]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oauth2原理及应用]]></title>
    <url>%2F2019%2F05%2F27%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Foauth2%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[OAuth2 的简单解释OAuth2 是目前比较流行的授权机制，用来授权第三方应用来使用用户的一些数据 通过OAuth2可以比较简单的实现跨平台的资源共享，下面简单介绍一下OAuth2具体解决了哪方面的问题； 一个关于小区送快递的场景在一个大型的居民小区，小区入口处的门有密码锁； 进出门的时候，需要业主输入密码下能进入； 但是小区经常会有外卖和快递员出入，快递员需要在有快件派送的时候有权限进入小区，现在必须有一种办法可以让快递员在没有账号密码的情况下通过门禁系统，进入小区 如果业主将密码告诉了快递员， 那快递员就拥有了和业主一样的权限，快递员可以用密码反反复复出入小区, 好像并不能这样设计。 小区授权机制设计对于以上问题，可以设计一套如下的授权方案，授权的流程如下 门禁系统的密码输入下方，提供一个获取授权的按钮。快递员需要首先按这个按钮，去申请授权。 按下按钮以后， 业主的手机就会弹出对话框， 有人正在要求授权。系统还会显示出该快递员的姓名，工号和所属的快递公司，业主确认快递员的请求属实，就会点击按钮，告诉门禁系统，我同意给与他进入小区的权限； 门禁系统得到业主的确认以后，向快递员显示一个进入小区的令牌(一个临时密码, 一般会有一个期限)。 快递员输入临时密码后，便可以进入小区； 互联网授权机制OAuth关于快递的例子，解决了一个不属于本小区用户，通过业主(资源所有者)的授权，有了临时进入小区，执行一些业主权限的问题； 把该例子应用到互联网应用的场景，就是OAuth的作用了； 一、居民小区就相当于拥有用户数据资源的资源服务器。比如，支付宝存储了我们的头像和信用分等信息，想要获取这些数据，用户一定要先成功登陆支付宝； 二、快递员相当于第三方应用，想要穿过门禁系统，进入小区。比如，某贷款机构想要通过支付宝，获取用户的芝麻饮用分来判断贷款金额； 三、就是业主本人相当于互联网应用的资源所有者，同意快递员第三方进入小区，执行业主本人的一些权限。 总结: OAuth就是一种授权机制， 数据所有者告诉系统，同意并授权第三方进入系统，获取一些指定的数据的权限。系统从而会提供一个短期的令牌给第三方，在一定时间内，第三方都可以通过该令牌获取用户已经授权获取的数据；OAuth解决了第三方应用获取平台用户数据的问题； OAuth2的一些基本概念什么是OAuth2.0OAuth 2.0, 允许第三方应用程序来代表资源所有者获得对HTTP服务的有限访问权限以自己的名义获取访问权限。 在传统的客户端 - 服务器身份验证模型中，客户端通过使用资源所有者的凭据向服务器进行身份验证来请求服务器上的访问受限资源（受保护资源）。为了向第三方应用程序提供对受限资源的访问，资源所有者与第三方共享其凭据。这会产生一些问题和限制。 OAuth通过引入授权层并将客户端的角色与资源所有者的角色分开来解决这些问题。在OAuth中，客户端请求访问由资源所有者控制并由资源服务器托管的资源，并发出与资源所有者不同的凭据集。 客户端不是使用资源所有者的凭证来访问受保护资源，而是获取访问令牌 - 表示特定范围，生命周期和其他访问属性的字符串。授权服务器在资源所有者的批准下向第三方客户端颁发访问令牌。客户端使用访问令牌来访问资源服务器托管的受保护资源 什么是OpenID Connect 1.0OpenID Connect 1.0是OAuth 2.0协议之上的简单身份层。它使客户端能够根据授权服务器执行的身份验证来验证最终用户的身份，以及以可互操作和类似REST的方式获取有关最终用户的基本配置文件信息。 作为背景，OAuth 2.0授权框架和OAuth 2.0承载令牌使用规范为第三方应用程序提供了一个通用框架，以获取和使用对HTTP资源的有限访问。它们定义了获取和使用访问令牌来访问资源的机制，但没有定义标准方法来提供身份信息。值得注意的是，如果没有分析OAuth 2.0，它就无法提供有关最终用户身份验证的信息。 OpenID Connect实现身份验证，作为OAuth 2.0授权过程的扩展。 OpenID Connect允许所有类型的客户端（包括基于Web，移动和JavaScript客户端）请求和接收有关经过身份验证的会话和最终用户的信息。规范套件是可扩展的，允许参与者在对它们有意义时使用可选功能，例如身份数据加密，OpenID提供程序的发现和会话管理。 OAuth2 的角色定义 资源所有者 资源所有者是 OAuth 2 四大基本角色之一，在 OAuth 2 标准中，资源所有者即代表授权客户端访问本身资源信息的用户（User），也就是应用场景中的“开发者A”。客户端访问用户帐户的权限仅限于用户授权的“范围”（aka. scope，例如读取或写入权限）。 如果没有特别说明，下文中出现的”用户”将统一代表资源所有者。 用户认证中心 验证用户身份的地方，比如网站的登录系统（密码验证/ session验证）； 资源服务器(resource server) 资源服务器托管了所有的受保护的用户资源等, 存储服务资源的地方就是资源服务器； 我们使用授权的目的，就是获取在资源服务器上和用户相关资源的资格； 授权中心(oauth2 server) 资源服务器托管了受保护的用户账号信息，而授权服务器验证用户身份然后为客户端派发资源访问令牌。 在上述应用场景中，Github 既是授权服务器也是资源服务器，个人信息和仓库信息即为资源（Resource）。而在实际工程中，不同的服务器应用往往独立部署，协同保护用户账户信息资源。 客户端 ( oauth2 client ) 执行授权服务流程的后台程序, 该部分一般要由第三方独立开发； 有的资源提供商，会提供sdk等供用户进行客户端的开发； 用户终端(浏览器…) 调用客户端执行授权过程的地方，一般为浏览器。。。 令牌与密码令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。 （1）令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。 （2）令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。 （3）令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。 上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。 注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。这也是为什么令牌的有效期，一般都设置得很短的原因。 OAuth2.0 的授权模式与使用OAuth 2.0 规定了四种获得令牌的流程。下面就是这四种授权方式。 Auth2.0的四种授权模式 ​ 授权码模式（Authorization Code）(支持refresh token) ​ 隐藏模式（Implicit）(不支持refresh token) ​ 密码模式（Resource Owner Password Credentials） (支持refresh token) ​ 客户端模式（Client Credentials） (不支持refresh token) 注意，不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。 授权码模式授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。 这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。 第一步，A 网站提供一个链接，用户点击后就会跳转到 B 网站，授权用户数据给 A 网站使用。 第二步，用户跳转后，B 网站会要求用户登录，然后询问是否同意给予 A 网站授权。用户表示同意，这时 B 网站就会跳回redirect_uri参数指定的网址。跳转时，会传回一个授权码。 第三步，A 网站拿到授权码以后，就可以在后端，向 B 网站请求令牌。 第四步，B 网站收到请求以后，就会颁发令牌。具体做法是向redirect_uri指定的网址，发送一段 JSON 数据包含了令牌的详细内容。 隐藏模式有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。 第一步，A 网站提供一个链接，要求用户跳转到 B 网站，授权用户数据给 A 网站使用。 第二步，用户跳转到 B 网站，登录后同意给予 A 网站授权。这时，B 网站就会跳回redirect_uri参数指定的跳转网址，并且把令牌作为 URL 参数，传给 A 网站。 这种方式把令牌直接传给前端，是很不安全的。因此，只能用于一些安全要求不高的场景，并且令牌的有效期必须非常短，通常就是会话期间（session）有效，浏览器关掉，令牌就失效了。 密码模式如果你高度信任某个应用，也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为”密码式”（password）。 第一步，A 网站要求用户提供 B 网站的用户名和密码。拿到以后，A 就直接向 B 请求令牌。 第二步，B 网站验证身份通过后，直接给出令牌。注意，这时不需要跳转，而是把令牌放在 JSON 数据里面，作为 HTTP 回应，A 因此拿到令牌。 这种方式需要用户给出自己的用户名/密码，显然风险很大，因此只适用于其他授权方式都无法采用的情况，而且必须是用户高度信任的应用。 客户端模式最后一种方式是凭证式（client credentials），适用于没有前端的命令行应用，即在命令行下请求令牌。 第一步，A 应用在命令行向 B 发出请求。 第二步，B 网站验证通过以后，直接返回令牌。 这种方式给出的令牌，是针对第三方应用的，而不是针对用户的，即有可能多个用户共享同一个令牌。 令牌的使用A 网站拿到令牌以后，就可以向 B 网站的 API 请求数据了。 此时，每个发到 API 的请求，都必须带有令牌。具体做法是在请求的头信息，加上一个Authorization字段，令牌就放在这个字段里面。 123&gt; curl -H "Authorization: Bearer ACCESS_TOKEN" \&gt; "https://api.b.com"&gt; 上面命令中，ACCESS_TOKEN就是拿到的令牌。 更新令牌令牌的有效期到了，如果让用户重新走一遍上面的流程，再申请一个新的令牌，很可能体验不好，而且也没有必要。OAuth 2.0 允许用户自动更新令牌。 具体方法是，B 网站颁发令牌的时候，一次性颁发两个令牌，一个用于获取数据，另一个用于获取新的令牌（refresh token 字段）。令牌到期前，用户使用 refresh token 发一个请求，去更新令牌。 123456&gt; https://b.com/oauth/token?&gt; grant_type=refresh_token&amp;&gt; client_id=CLIENT_ID&amp;&gt; client_secret=CLIENT_SECRET&amp;&gt; refresh_token=REFRESH_TOKEN&gt; 上面 URL 中，grant_type参数为refresh_token表示要求更新令牌，client_id参数和client_secret参数用于确认身份，refresh_token参数就是用于更新令牌的令牌。 第三方授权例子一、场景举例举例来说，A 网站允许使用B网站账号登录，背后就是下面的流程。 A 网站让用户跳转到B网站。 B要求用户登录，然后询问”A 网站要求获得 xx 权限，你是否同意？” 用户同意，B就会重定向回 A 网站，同时发回一个授权码。 A 网站使用授权码，向 B 请求令牌。 B网站返回令牌. A 网站使用令牌，向B资源服务器请求用户数据。 二、授权流程 Authrization Request客户端向用户请求对资源服务器的authorization grant。 Authorization Grant（Get）如果用户授权该次请求，客户端将收到一个authorization grant。 Authorization Grant（Post）客户端向授权服务器发送它自己的客户端身份标识和上一步中的authorization grant，请求访问令牌。 Access Token（Get）如果客户端身份被认证，并且authorization grant也被验证通过，授权服务器将为客户端派发access token。授权阶段至此全部结束。 Access Token（Post &amp;&amp; Validate）客户端向资源服务器发送access token用于验证并请求资源信息。 Protected Resource（Get）如果access token验证通过，资源服务器将向客户端返回资源信息。 三、豆瓣授权流程 User Authorization Request 首先，客户端构造了一个用于请求authorization code的URL并引导User-agent跳转访问。 123456https://authorization-server.com/auth ?response_type=code &amp;client_id=29352915982374239857 &amp;redirect_uri=https%3A%2F%2Fexample-client.com%2Fcallback &amp;scope=create+delete &amp;state=xcoiv98y2kd22vusuye3kch response_type=code此参数和参数值用于提示授权服务器当前客户端正在进行Authorization Code授权流程。 client_id客户端身份标识。 redirect_uri标识授权服务器接收客户端请求后返回给User-agent的跳转访问地址。 scope指定客户端请求的访问级别。 state由客户端生成的随机字符串，步骤2中用户进行授权客户端的请求时也会携带此字符串用于比较，这是为了防止CSRF攻击。 User Authorizes Applcation 当用户点击上文中的示例链接时，用户必须已经在授权服务中进行登录（否则将会跳转到登录界面，不过 OAuth 2 并不关心认证过程），然后授权服务会提示用户授权或拒绝应用程序访问其帐户。以下是授权应用程序的示例： Authorization Code Grant 如果用户确认授权，授权服务器将重定向User-agent至之前客户端提供的指向客户端的redirect_uri地址，并附带code和state参数（由之前客户端提供），于是客户端便能直接读取到authorization code值。 123https://example-client.com/redirect ?code=g0ZGZmNjVmOWIjNTk2NTk4ZTYyZGI3 &amp;state=xcoiv98y2kd22vusuye3kch state值将与客户端在请求中最初设置的值相同。客户端将检查重定向中的状态值是否与最初设置的状态值相匹配。这可以防止CSRF和其他相关攻击。 code是授权服务器生成的authorization code值。code相对较短，通常持续1到10分钟，具体取决于授权服务器设置。 Access Token Request 现在客户端已经拥有了服务器派发的authorization code，接下来便可以使用authorization code和其他参数向服务器请求access token（POST方式）。其他相关参数如下： grant_type=authorization_code - 这告诉服务器当前客户端正在使用Authorization Code授权流程。 code - 应用程序包含它在重定向中给出的授权码。 redirect_uri - 与请求authorization code时使用的redirect_uri相同。某些资源（API）不需要此参数。 client_id - 客户端标识。 client_secret - 应用程序的客户端密钥。这确保了获取access token的请求只能从客户端发出，而不能从可能截获authorization code的攻击者发出。 Access Token Grant 服务器将会验证第4步中的请求参数，当验证通过后（校验authorization code是否过期，client id和client secret是否匹配等），服务器将向客户端返回access token。 1234567&#123; &quot;access_token&quot;:&quot;MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3&quot;, &quot;token_type&quot;:&quot;bearer&quot;, &quot;expires_in&quot;:3600, &quot;refresh_token&quot;:&quot;IwOGYzYTlmM2YxOTQ5MGE3YmNmMDFkNTVk&quot;, &quot;scope&quot;:&quot;create delete&quot;&#125; 至此，授权流程全部结束。直到access token 过期或失效之前，客户端可以通过资源服务器API访问用户的帐户，并具备scope中给定的操作权限。 Ory Hydra OAuth2.0 框架ORY Hydra解决了身份验证和授权问题，是OAuth 2.0和OpenID Connect提供商。 什么是ORY HydraORY Hydra是OAuth 2.0和OpenID Connect Provider。因此，它能够发出访问，刷新和ID令牌。与其他项目相反，ORY Hydra不提供用户管理（登录，注销，配置文件管理，注册），而是使用基于重定向的流和REST API将用户身份验证（登录）委派给您实现的服务，控制。这允许您构建适合您的用户管理，使用您喜欢的前端技术，以及您的用例所需的身份验证机制（例如基于令牌的2FA，SMS 2FA）。 因此，ORY Hydra是最灵活的OAuth 2.0和OpenID Connect提供商，为您提供了实现业务逻辑的极大自由，并且仍然可以从OAuth 2.0和OpenID Connect中获得所有好处。 除了OAuth 2.0功能之外，ORY Hydra还为加密密钥提供安全存储（例如，用于签署JSON Web令牌），并且能够管理OAuth 2.0客户端。 ORY Hydra是OpenID Connect认证（待定），并实现了OpenID Foundation规定的所有要求。因此，它正确地实现了IETF和OpenID Foundation所预期的不同OAuth 2.0和OpenID Connect流程。 ORY Hydra 介绍Hydra是OAuth 2.0授权框架和OpenID Connect Core 1.0的服务器实现。现有的OAuth2实现通常作为库或SDK。 在不了解整个规范的情况下实现和使用OAuth2具有挑战性，并且即使在使用SDK时也容易出错。Hydra的主要目标是使OAuth 2.0和OpenID Connect 1.0的设置更轻松，更易于使用。 Hydra实现OAuth2和OpenID Connect 1.0中描述的流程，而不强制您使用“Hydra用户管理”或某些模板引擎或预定义的前端。相反，它依赖于HTTP重定向和加密方法来验证用户同意，允许您将Hydra与任何身份验证端点一起使用。 ORY Hydra 不管理用户要理解的第一个重要概念是ORY Hydra是OAuth 2.0授权和OpenID Connect服务器。有些人将这些功能误认为存储用户数据并将您登录的系统。事实并非如此。相反，此类服务器负责将用户凭据（通常是用户名和密码）“转换”为OAuth 2.0访问和刷新令牌以及OpenID Connect ID令牌。它基本上就像您使用会话数据存储cookie，但更灵活，它也适用于第三方应用程序。 ORY Hydra不存储用户配置文件，用户名，密码。此功能取决于您。ORY Hydra使用我们称之为用户登录和同意流的东西。此流使用HTTP重定向将任何传入的授权请求（“请给我一个访问令牌。”）转发给登录提供者和同意提供者。这些应用程序是您实现的。它可以是新应用程序或您现有的登录系统。从较高的层面来看，这些提供商可归纳为： 登录提供者负责通过验证他或她的凭证（例如用户名+密码）来验证用户（“登录”）。 同意提供商负责允许OAuth 2.0应用程序代表用户获取令牌（“您是否希望允许foobar-app访问您的所有个人消息和图像？”。 Ory Hydra OAuth 2.0授权流程： 开发人员在授权服务器（ORY Hydra）上注册OAuth 2.0客户端，目的是代表用户获取信息。 应用程序UI要求用户授权应用程序代表他/她访问信息/数据。 用户被重定向到授权服务器。 授权服务器确认用户的身份，并要求用户授予OAuth 2.0客户端某些权限。 授权服务器发出OAuth 2.0客户端用于代表用户访问资源的令牌。 Ory Hydra OAuth 2.0 登录认证网络流程图​ 我们的OAuth2 实现 角色介绍 三方平台 (任何想要拿到其他平台授权的应用) 资源服务器 (我们数据提供端) 授权服务器 (OAuth2.0 + IDP), 统一授权中心和身份认证中心； Account Service (基于普通用户的注册，登录，和修改等接口； 另外提供一个公共的OAuth 客户端，三方平台可通过client_id 和 secret 来使用该公共客户端，用来获取授权) 资源所有者(终端用户) 三方平台接入方法 先申请 client_id, 和 secret ， 之后才可以使用授权服务； 开发跳转接口， 该接口用来接收终端用户收到的code码， 用来获取access_token等 三方平台授权流程(参考上图) 调用Account Service 获取授权平台地址并请求； 用户验证身份通过，并同意授权； 通过跳转(需提前开发好跳转接口)，拿到授权得到的code； 调用Account Service (使用授权code)， 获取access_token 等； 之后便可使用access_token, 调用资源服务器授权范围内的数据；]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Orm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-micro 框架使用]]></title>
    <url>%2F2019%2F05%2F22%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fgo-micro%20%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Micro介绍什么是MicroMicro是一个着眼于分布式系统开发的微服务生态系统。 Micro由开源的库与工具组成，旨在辅助微服务开发。 Micro开源库 go-micro - 基于Go语言的可插拔RPC微服务开发框架；包含服务发现、RPC客户/服务端、广播/订阅机制等等。 go-plugins - go-micro的插件, 其包含etcd、kubernetes、nats、rabbitmq、grpc等等。 micro - 微服务工具集包含传统的入口点（entry point）；API 网关、CLI、Slack Bot、代理及Web UI。 文档位置: https://micro.mu/docs/ 下面主要介绍使用go-micro 对service 的编写注册和启动流程，关于micro的工具箱的使用，可以参考上面提供的文档地址 使用GO-Micro编写服务端环境准备1234brew install protobuf # 下载protoc工具go get github.com/micro/go-micro # 安装go-microgo get github.com/golang/protobuf/&#123;proto,protoc-gen-go&#125; go get github.com/micro/&#123;protoc-gen-micro,micro&#125; go-micro Service接口先看一下go-micro的service interface，是构建micro服务所需的主要组件。它把所有Go-Micror的基础包打包成单一组件接口。 接下来对于服务的开发都将主要围绕service 接口来进行 12345678type Service interface &#123; Init(...Option) Options() Options Client() client.Client Server() server.Server Run() error String() string&#125; 编写.proto文件，定义Service 的Api我们使用protobuf文件来定义服务的API接口。使用protobuf可以非常方便去严格定义API，提供服务端与客户端双边具体一致的类型。 下面是定义的示例 greeter.proto 12345678910111213syntax = "proto3";service Greeter &#123; rpc Hello(HelloRequest) returns (HelloResponse) &#123;&#125;&#125;message HelloRequest &#123; string name = 1;&#125;message HelloResponse &#123; string greeting = 2;&#125; 我们定义了一个服务叫做Greeter的处理器，它有一个接收HelloRequest并返回HelloResponse的Hello方法。 .proto文件的详细定义方法可以参考: https://www.jianshu.com/p/ea656dc9b037 生成API接口我们需要下面这个工具来生成protobuf代码文件，它们负责生成定义的go代码实现。 1protoc --proto_path=$GOPATH/src:. --micro_out=. --go_out=. greeter.proto 生成的类现在可以引入handler中，在服务或客户端来创建请求了。 下面是部分生成的代码 handler的开发，将会直接饮用生成的Request 和Response 等对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051type HelloRequest struct &#123; Name string `protobuf:"bytes,1,opt,name=name" json:"name,omitempty"`&#125;type HelloResponse struct &#123; Greeting string `protobuf:"bytes,2,opt,name=greeting" json:"greeting,omitempty"`&#125;// Greeter service 客户端的APItype GreeterClient interface &#123; Hello(ctx context.Context, in *HelloRequest, opts ...client.CallOption) (*HelloResponse, error)&#125;type greeterClient struct &#123; c client.Client serviceName string&#125;func NewGreeterClient(serviceName string, c client.Client) GreeterClient &#123; if c == nil &#123; c = client.NewClient() &#125; if len(serviceName) == 0 &#123; serviceName = "greeter" &#125; return &amp;greeterClient&#123; c: c, serviceName: serviceName, &#125;&#125;func (c *greeterClient) Hello(ctx context.Context, in *HelloRequest, opts ...client.CallOption) (*HelloResponse, error) &#123; req := c.c.NewRequest(c.serviceName, "Greeter.Hello", in) out := new(HelloResponse) err := c.c.Call(ctx, req, out, opts...) if err != nil &#123; return nil, err &#125; return out, nil&#125;// Greeter service 服务端type GreeterHandler interface &#123; Hello(context.Context, *HelloRequest, *HelloResponse) error&#125;func RegisterGreeterHandler(s server.Server, hdlr GreeterHandler) &#123; s.Handle(s.NewHandler(&amp;Greeter&#123;hdlr&#125;))&#125; 实现handler处理器服务端需要注册handlers，这样才能提供服务并接收请求。处理器相当于是一个拥有公共方法的公共类，它需要符合签名func(ctx context.Context, req interface{}, rsp interface{}) error 通过上面的内容，我们看到，Greeter interface的签名的看上去就是这样： 123type GreeterHandler interface &#123; Hello(context.Context, *HelloRequest, *HelloResponse) error&#125; Greeter处理器实现： 12345678import proto "github.com/micro/examples/service/proto"type Greeter struct&#123;&#125;func (g *Greeter) Hello(ctx context.Context, req *proto.HelloRequest, rsp *proto.HelloResponse) error &#123; rsp.Greeting = "Hello " + req.Name return nil&#125; 服务启动 创建Service服务 可以使用micro.NewService创建服务 12import "github.com/micro/go-micro"service := micro.NewService() 初始化时，也可以传入相关选项 1234service := micro.NewService( micro.Name("greeter"), micro.Version("latest"),) 所有的可选参数参考：配置项 Go Micro也提供通过命令行参数micro.Flags传递配置参数： 12345678910111213import ( "github.com/micro/cli" "github.com/micro/go-micro")service := micro.NewService( micro.Flags( cli.StringFlag&#123; Name: "environment", Usage: "The environment", &#125;, )) 初始化服务 初始化服务时候，可以解析命令行标识参数，增加标识参数可以使用micro.Action选项： 12345678service.Init( micro.Action(func(c *cli.Context) &#123; env := c.StringFlag("environment") if len(env) &gt; 0 &#123; fmt.Println("Environment set to", env) &#125; &#125;),) Go Micro提供预置的标识，service.Init执行时就会设置并解析这些参数。所有的标识参考. 注册服务 处理器会与服务一起被注册，就像http处理器一样。 12345service := micro.NewService( micro.Name("greeter"),)proto.RegisterGreeterHandler(service.Server(), new(Greeter)) 运行服务 Service 服务可以调用server.Run运行起来。这一步会让服务绑到配置中的地址（默认遵循RFC1918，分配随机的端口）接收请求。 另外，这一步会在服务启动时向注册中心注册，并在服务接收到关闭信号时卸载。 123if err := service.Run(); err != nil &#123; log.Fatal(err)&#125; 之后可以通过 go run greeter.go 注: 如果需要更改register为 consul， 更改环境变量 export MICRO_REGISTRY=consul 完成的服务greeter.go 123456789101112131415161718192021222324252627282930313233package mainimport ( "log" "github.com/micro/go-micro" proto "github.com/micro/examples/service/proto" "golang.org/x/net/context")type Greeter struct&#123;&#125;func (g *Greeter) Hello(ctx context.Context, req *proto.HelloRequest, rsp *proto.HelloResponse) error &#123; rsp.Greeting = "Hello " + req.Name return nil&#125;func main() &#123; service := micro.NewService( micro.Name("greeter"), micro.Version("latest"), ) service.Init() proto.RegisterGreeterHandler(service.Server(), new(Greeter)) if err := service.Run(); err != nil &#123; log.Fatal(err) &#125;&#125; 需要注意的是，要保证服务发现机制运行起来，这样服务才能注册，其它服务或客户端才能发现它。 对服务端进行测试使用命令 1micro call SrvName funcName args 使用示例 1micro call go.micro.srv.demo Demo.Call "&#123;\"name\": \"John\"&#125;" 使用GO-Micro编写客户端Client包用于查询服务，当创建服务时，也包含了一个客户端，这个客户端匹配服务所使用的初始化包。 查询上面的服务很简单： 12345678910111213// 创建greate客户端，这需要传入服务名与服务的客户端方法构建的客户端对象greeter := proto.NewGreeterClient("greeter", service.Client())// 在Greeter handler上请求调用Hello方法rsp, err := greeter.Hello(context.TODO(), &amp;proto.HelloRequest&#123; Name: "John",&#125;)if err != nil &#123; fmt.Println(err) return&#125;fmt.Println(rsp.Greeter) proto.NewGreeterClient 需要服务名与客户端实例来请求服务。]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-xorm 使用]]></title>
    <url>%2F2019%2F05%2F20%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fxorm%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Xorm 使用创建引擎 创建引擎 driverName, dataSourceName和database/sql接口相同 第一种方法 1234567891011import ( _ "github.com/go-sql-driver/mysql" "github.com/go-xorm/xorm")var engine *xorm.Enginefunc main() &#123; var err error engine, err = xorm.NewEngine("mysql", "root:123@/test?charset=utf8")&#125; 第二种方法 1234567891011import ( _ "github.com/mattn/go-sqlite3" "github.com/go-xorm/xorm")var engine *xorm.Enginefunc main() &#123; var err error engine, err = xorm.NewEngine("sqlite3", "./test.db")&#125; 创建Engine组(操作集群) 123456dataSourceNameSlice := []string&#123;masterDataSourceName, slave1DataSourceName, slave2DataSourceName&#125;engineGroup, err := xorm.NewEngineGroup(driverName, dataSourceNameSlice)masterEngine, err := xorm.NewEngine(driverName, masterDataSourceName)slave1Engine, err := xorm.NewEngine(driverName, slave1DataSourceName)slave2Engine, err := xorm.NewEngine(driverName, slave2DataSourceName)engineGroup, err := xorm.NewEngineGroup(masterEngine, []*Engine&#123;slave1Engine, slave2Engine&#125;) 所有使用 engine 都可以简单的用 engineGroup 来替换。 同步结构体到数据库表 定义一个和表同步的结构体，并且自动同步结构体到数据库 1234567891011type User struct &#123; Id int64 Name string Salt string Age int Passwd string `xorm:"varchar(200)"` Created time.Time `xorm:"created"` Updated time.Time `xorm:"updated"`&#125;err := engine.Sync2(new(User)) CURD操作Insert 操作 Insert 插入一条或者多条记录 12345678910111213affected, err := engine.Insert(&amp;user)// INSERT INTO struct () values ()affected, err := engine.Insert(&amp;user1, &amp;user2)// INSERT INTO struct1 () values ()// INSERT INTO struct2 () values ()affected, err := engine.Insert(&amp;users)// INSERT INTO struct () values (),(),()affected, err := engine.Insert(&amp;user1, &amp;users)// INSERT INTO struct1 () values ()// INSERT INTO struct2 () values (),(),() Delete操作 Delete 删除记录，需要注意，删除必须至少有一个条件，否则会报错。要清空数据库可以用EmptyTable 12345affected, err := engine.Where(...).Delete(&amp;user)// DELETE FROM user Where ...affected, err := engine.ID(2).Delete(&amp;user)// DELETE FROM user Where id = ? update 更新操作 Update 更新数据，除非使用Cols,AllCols函数指明，默认只更新非空和非0的字段 1234567891011121314151617181920affected, err := engine.ID(1).Update(&amp;user)// UPDATE user SET ... Where id = ?affected, err := engine.Update(&amp;user, &amp;User&#123;Name:name&#125;)// UPDATE user SET ... Where name = ?var ids = []int64&#123;1, 2, 3&#125;affected, err := engine.In(ids).Update(&amp;user)// UPDATE user SET ... Where id IN (?, ?, ?)// force update indicated columns by Colsaffected, err := engine.ID(1).Cols("age").Update(&amp;User&#123;Name:name, Age: 12&#125;)// UPDATE user SET age = ?, updated=? Where id = ?// force NOT update indicated columns by Omitaffected, err := engine.ID(1).Omit("name").Update(&amp;User&#123;Name:name, Age: 12&#125;)// UPDATE user SET age = ?, updated=? Where id = ?affected, err := engine.ID(1).AllCols().Update(&amp;user)// UPDATE user SET name=?,age=?,salt=?,passwd=?,updated=? Where id = ? query操作 Query 最原始的也支持SQL语句查询，返回的结果类型为 []map[string]byte。QueryString 返回 []map[string]string, QueryInterface 返回 []map[string]interface{}. 12345678results, err := engine.Query("select * from user")results, err := engine.Where("a = 1").Query()results, err := engine.QueryString("select * from user")results, err := engine.Where("a = 1").QueryString()results, err := engine.QueryInterface("select * from user")results, err := engine.Where("a = 1").QueryInterface() Get 操作 Get 查询单条记录 12345678910111213141516171819202122has, err := engine.Get(&amp;user)// SELECT * FROM user LIMIT 1has, err := engine.Where(&quot;name = ?&quot;, name).Desc(&quot;id&quot;).Get(&amp;user)// SELECT * FROM user WHERE name = ? ORDER BY id DESC LIMIT 1var name stringhas, err := engine.Table(&amp;user).Where(&quot;id = ?&quot;, id).Cols(&quot;name&quot;).Get(&amp;name)// SELECT name FROM user WHERE id = ?var id int64has, err := engine.Table(&amp;user).Where(&quot;name = ?&quot;, name).Cols(&quot;id&quot;).Get(&amp;id)has, err := engine.SQL(&quot;select id from user&quot;).Get(&amp;id)// SELECT id FROM user WHERE name = ?var valuesMap = make(map[string]string)has, err := engine.Table(&amp;user).Where(&quot;id = ?&quot;, id).Get(&amp;valuesMap)// SELECT * FROM user WHERE id = ?var valuesSlice = make([]interface&#123;&#125;, len(cols))has, err := engine.Table(&amp;user).Where(&quot;id = ?&quot;, id).Cols(cols...).Get(&amp;valuesSlice)// SELECT col1, col2, col3 FROM user WHERE id = ? Find操作(查询多条记录) Find 查询多条记录，当然可以使用Join和extends来组合使用 1234567891011121314151617181920var users []Usererr := engine.Where("name = ?", name).And("age &gt; 10").Limit(10, 0).Find(&amp;users)// SELECT * FROM user WHERE name = ? AND age &gt; 10 limit 10 offset 0type Detail struct &#123; Id int64 UserId int64 `xorm:"index"`&#125;type UserDetail struct &#123; User `xorm:"extends"` Detail `xorm:"extends"`&#125;var users []UserDetailerr := engine.Table("user").Select("user.*, detail.*") Join("INNER", "detail", "detail.user_id = user.id"). Where("user.name = ?", name).Limit(10, 0). Find(&amp;users)// SELECT user.*, detail.* FROM user INNER JOIN detail WHERE user.name = ? limit 10 offset 0 记录Exist 判断 Exist 检测记录是否存在 12345678910111213141516171819has, err := testEngine.Exist(new(RecordExist))// SELECT * FROM record_exist LIMIT 1has, err = testEngine.Exist(&amp;RecordExist&#123; Name: &quot;test1&quot;, &#125;)// SELECT * FROM record_exist WHERE name = ? LIMIT 1has, err = testEngine.Where(&quot;name = ?&quot;, &quot;test1&quot;).Exist(&amp;RecordExist&#123;&#125;)// SELECT * FROM record_exist WHERE name = ? LIMIT 1has, err = testEngine.SQL(&quot;select * from record_exist where name = ?&quot;, &quot;test1&quot;).Exist()// select * from record_exist where name = ?has, err = testEngine.Table(&quot;record_exist&quot;).Exist()// SELECT * FROM record_exist LIMIT 1has, err = testEngine.Table(&quot;record_exist&quot;).Where(&quot;name = ?&quot;, &quot;test1&quot;).Exist()// SELECT * FROM record_exist WHERE name = ? LIMIT 1 执行原生sql语句 Exec 执行一个SQL语句 1affected, err := engine.Exec("update user set age = ? where name = ?", age, name) 数据库遍历 Iterate 和 Rows 根据条件遍历数据库，可以有两种方式: Iterate and Rows 1234567891011121314151617181920err := engine.Iterate(&amp;User&#123;Name:name&#125;, func(idx int, bean interface&#123;&#125;) error &#123; user := bean.(*User) return nil&#125;)// SELECT * FROM usererr := engine.BufferSize(100).Iterate(&amp;User&#123;Name:name&#125;, func(idx int, bean interface&#123;&#125;) error &#123; user := bean.(*User) return nil&#125;)// SELECT * FROM user Limit 0, 100// SELECT * FROM user Limit 101, 100rows, err := engine.Rows(&amp;User&#123;Name:name&#125;)// SELECT * FROM userdefer rows.Close()bean := new(Struct)for rows.Next() &#123; err = rows.Scan(bean)&#125; 统计操作 Count 获取记录条数 12counts, err := engine.Count(&amp;user)// SELECT count(*) AS total FROM user Sum 求和函数 1234567891011agesFloat64, err := engine.Sum(&amp;user, "age")// SELECT sum(age) AS total FROM useragesInt64, err := engine.SumInt(&amp;user, "age")// SELECT sum(age) AS total FROM usersumFloat64Slice, err := engine.Sums(&amp;user, "age", "score")// SELECT sum(age), sum(score) FROM usersumInt64Slice, err := engine.SumsInt(&amp;user, "age", "score")// SELECT sum(age), sum(score) FROM user 条件编辑器 条件编辑器 12err := engine.Where(builder.NotIn("a", 1, 2).And(builder.In("b", "c", "d", "e"))).Find(&amp;users)// SELECT id, name ... FROM user WHERE a NOT IN (?, ?) AND b IN (?, ?, ?) 事务操作 在一个Go程中多次操作数据库，但没有事务 123456789101112131415161718session := engine.NewSession()defer session.Close()user1 := Userinfo&#123;Username: "xiaoxiao", Departname: "dev", Alias: "lunny", Created: time.Now()&#125;if _, err := session.Insert(&amp;user1); err != nil &#123; return err&#125;user2 := Userinfo&#123;Username: "yyy"&#125;if _, err := session.Where("id = ?", 2).Update(&amp;user2); err != nil &#123; return err&#125;if _, err := session.Exec("delete from userinfo where username = ?", user2.Username); err != nil &#123; return err&#125;return nil 在一个Go程中有事务 12345678910111213141516171819202122232425session := engine.NewSession()defer session.Close()// add Begin() before any actionif err := session.Begin(); err != nil &#123; // if returned then will rollback automatically return err&#125;user1 := Userinfo&#123;Username: "xiaoxiao", Departname: "dev", Alias: "lunny", Created: time.Now()&#125;if _, err := session.Insert(&amp;user1); err != nil &#123; return err&#125;user2 := Userinfo&#123;Username: "yyy"&#125;if _, err := session.Where("id = ?", 2).Update(&amp;user2); err != nil &#123; return err&#125;if _, err := session.Exec("delete from userinfo where username = ?", user2.Username); err != nil &#123; return err&#125;// add Commit() after all actionsreturn session.Commit() 事物的简写方法 12345678910111213141516res, err := engine.Transaction(func(session *xorm.Session) (interface&#123;&#125;, error) &#123; user1 := Userinfo&#123;Username: "xiaoxiao", Departname: "dev", Alias: "lunny", Created: time.Now()&#125; if _, err := session.Insert(&amp;user1); err != nil &#123; return nil, err &#125; user2 := Userinfo&#123;Username: "yyy"&#125; if _, err := session.Where("id = ?", 2).Update(&amp;user2); err != nil &#123; return nil, err &#125; if _, err := session.Exec("delete from userinfo where username = ?", user2.Username); err != nil &#123; return nil, err &#125; return nil, nil&#125;) 上下文缓存 上下文缓存，如果启用，那么针对单个对象的查询将会被缓存到系统中，可以被下一个查询使用。 123456789101112131415161718192021222324sess := engine.NewSession()defer sess.Close()var context = xorm.NewMemoryContextCache()var c2 ContextGetStructhas, err := sess.ID(1).ContextCache(context).Get(&amp;c2)assert.NoError(t, err)assert.True(t, has)assert.EqualValues(t, 1, c2.Id)assert.EqualValues(t, "1", c2.Name)sql, args := sess.LastSQL()assert.True(t, len(sql) &gt; 0)assert.True(t, len(args) &gt; 0)var c3 ContextGetStructhas, err = sess.ID(1).ContextCache(context).Get(&amp;c3)assert.NoError(t, err)assert.True(t, has)assert.EqualValues(t, 1, c3.Id)assert.EqualValues(t, "1", c3.Name)sql, args = sess.LastSQL()assert.True(t, len(sql) == 0)assert.True(t, len(args) == 0)]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Orm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 godoc 生成文档]]></title>
    <url>%2F2019%2F04%2F29%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2F%E4%BD%BF%E7%94%A8%20godoc%20%E7%94%9F%E6%88%90%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[代码注释规范godoc 支持package、const、 var 和func 这些代码生成文档，而且只会对首字母大写自动生成，而小写的私有方法不会被生成到文档，下面介绍下各种注释的使用方式 Package 在package name 代码上面，紧挨着代码进行注释 注释内容中间不能有空行 如果一个包中有多个包注释，就会把多个包的注释放在一起，并按照文件名的首字母顺序排序 注释的格式为 package name [summery] （summery可以是多行） 注释最前面一句话会模块的summary会出现在package index中，第一句话以及之后的内容会出现在OverView中 注释代码 1234/*A web framework includes app server, logger, panicer, util and so on. */ package document 文档生成效果 常量、变量和函数 紧挨着定义代码，在常量、变量和函数上面进行注释 注释格式为: FunctionName Summary （Summary可以是多行） 想圈起来说明参数可以加缩进， 进行预格式化 注释最前面一句话会出现在package index中，第一句话以及之后的内容会出现在OverView中 常量的summary会放到[ Constants ] 里， 变量的summary会放到[ Variables ] 里， 函数的summary会放到 [ func ] 中 注释代码 1234567// Marshaler is the interface implemented by objects that/*can marshal themselves into valid JSON.*/ type Marshaler interface &#123; MarshalJSON() ([]byte, error)&#125; 文档生成效果 BUG godoc会先查找:[空格]BUG 然后显示在Package说明文档最下面 如果代码中有bug，可以用BUG注释, 它会被识别为一个 bug，可以在文档中的「Bugs」中看到。 注释代码 1BUG(who): xxx Deprecated 通过Deprecated注释的内容将不会体现在godoc中，但是还是挺有用的，Goland可以识别它并作出提示。 注释代码1// Deprecated: xxx 返回值Output标签 在函数体中如果定义了Output标签，会在文档页面上展示输出内容 如果没有定义将不会展示( 非必须 ) 一般为测试代码使用，用来展示方法的输出结果 注释代码 123456func ExamplePeel() &#123; fmt.Println("Hello Banana") // Output: // Hello Banana&#125; 使用doc.go书写注释 如果包注释超过3行，可以把注释都迁移到doc.go文件中（可以在当前目录新建一个doc.go文件）。 多行注释自然需要支持一些复杂的格式，如果单行中首字母是大写，并且结尾没有标点符号是标题(标题字体会加粗变蓝，中文加粗变蓝需要加上一个大写字母) 首字母是小写，或者结果又标点符号的是段落 有缩进是预格式化， 在有预格式化的注释段中，不会有标题特征 example_PackageName_test.go 的注释规则 包示例代码注释非常重要，项目的使用方法就是通过每个包例子搭建起来的, 文件必须放在当前包下 示例文件需要创建一个新文件，名称格式为 example_Packagename_test.go. 不加example前缀也是可以的，但是不加前缀通常是单元测试的文件命名规则， 包名的格式为 当前包名 + _test. 包中函数名称的格式为ExampleFuncName[_tag]。不加函数名的话是包级别的示例。加函数名的话是函数级别的示例。 函数的注释会展示在页面上 函数结果加上 // Output: 注释，可以说明函数的返回值，并展示在文档上 包级别的示例函数名称规则 Example 代码示例 123func Example() &#123; logger.Info(&quot;hello, world.&quot;)&#125; 文档展示结果 函数级别示例函数名称规则 ExampleFuncName 代码示例 123456func ExampleNewLogger() &#123; w := os.Stdout flag := log.Llongfile l := logger.NewWriterLogger(w, flag, 3) l.Info(&quot;hello, world&quot;)&#125; 文档展示结果 Output输出注释格式 OutPut: xxxxxxx 代码示例 1234567891011121314// 此函数将被展示在OverView区域, 并展示noOutput标签func Example_noOutput() &#123; fmt.Println("Hello OverView") // (Output: )非必须, 存在时将会展示输出结果&#125;// 此函数将被展示在Function区域// Peel必须是banana包实现的方法func ExamplePeel() &#123; fmt.Println("Hello Banana") // Output: // Hello Banana&#125; 本地运行godoc 文档1godoc -http=:6060]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swagger从源码生成spec]]></title>
    <url>%2F2019%2F04%2F29%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fswagger%E4%BB%8E%E6%BA%90%E7%A0%81%E7%94%9F%E6%88%90spec%2F</url>
    <content type="text"><![CDATA[从源码生成spec文档spec 生成命令12345678910111213141516171819Usage: swagger [OPTIONS] generate spec [spec-OPTIONS]generate a swagger spec document from a go applicationApplication Options: -q, --quiet silence logs -o, --output=LOG-FILE redirect logs to fileHelp Options: -h, --help Show this help message[spec command options] -b, --base-path= the base path to use (default: .) -t, --tags= build tags -m, --scan-models includes models that were annotated with &apos;swagger:model&apos; --compact when present, doesn&apos;t prettify the json -o, --output= the file to write to -i, --input= the file to use as input spec生成方法生成包的spec1swagger generate spec -o ./swagger.json 如果不提供一个mian文件给swagger， swagger 将遍历包中的左右文件以及文件的依赖并生成spec 如果想要提供给swagger一个main文件，可以在main文件中添加如下注释: 1//go:generate swagger generate spec 它使用go工具加载器加载应用程序，然后扫描代码库使用的所有软件包。这意味着对于可被发现的东西，它需要通过主包触发的代码路径来访问。 合并yml文件定义的spec1swagger generate spec -i ./swagger.yml -o ./swagger.json 生成yaml格式的spec文件1swagger generate spec -o ./swagger.yml Spec生成规则swagger:meta配置 包spec文件的一些原数据， 语法 1swagger:meta 可配置的属性如下 Annotation Format Terms Of Service allows for either a url or a free text definition describing the terms of services for the API Consumes a list of default (global) mime type values, one per line, for the content the API receives. List of supported mime types Produces a list of default (global) mime type values, one per line, for the content the API sends. List of supported mime types Schemes a list of default schemes the API accept (possible values: http, https, ws, wss) https is preferred as default when configured Version the current version of the API Host the host from where the spec is served Base path the default base path for this API Contact the name of for the person to contact concerning the API eg. John Doe &#x6a;&#x6f;&#x68;&#x6e;&#64;&#x62;&#x6c;&#111;&#103;&#115;&#46;&#x63;&#111;&#x6d; http://john.blogs.com License the name of the license followed by the URL of the license eg. MIT http://opensource.org/license/MIT Security a dictionary of key: []string{scopes} SecurityDefinitions list of supported authorization types https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#securityDefinitionsObject Extensions list of extensions to Swagger Schema. The field name MUST begin with x-, for example, x-internal-id. The value can be null, a primitive, an array or an object. 示例: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Package classification Petstore API.//// the purpose of this application is to provide an application// that is using plain go code to define an API//// This should demonstrate all the possible comment annotations// that are available to turn go code into a fully compliant swagger 2.0 spec//// Terms Of Service://// there are no TOS at this moment, use at your own risk we take no responsibility//// Schemes: http, https// Host: localhost// BasePath: /v2// Version: 0.0.1// License: MIT http://opensource.org/licenses/MIT// Contact: John Doe&lt;john.doe@example.com&gt; http://john.doe.com//// Consumes:// - application/json// - application/xml//// Produces:// - application/json// - application/xml//// Security:// - api_key://// SecurityDefinitions:// api_key:// type: apiKey// name: KEY// in: header// oauth2:// type: oauth2// authorizationUrl: /oauth2/auth// tokenUrl: /oauth2/token// in: header// scopes:// bar: foo// flow: accessCode//// Extensions:// x-meta-value: value// x-meta-array:// - value1// - value2// x-meta-array-obj:// - name: obj// value: field//// swagger:metapackage classification swagger:route路径配置的方法， 此操作获取一个唯一ID，用于后面该方法的名称 语法 1swagger:route [method] [path pattern] [?tag1 tag2 tag3] [operation id] 属性 Annotation Format Consumes a list of operation specific mime type values, one per line, for the content the API receives Produces a list of operation specific mime type values, one per line, for the content the API sends Schemes a list of operation specific schemes the API accept (possible values: http, https, ws, wss) https is preferred as default when configured Security a dictionary of key: []string{scopes} Responses a dictionary of status code to named response 示例 123456789101112131415161718192021222324252627282930// ServeAPI serves the API for this record storefunc ServeAPI(host, basePath string, schemes []string) error &#123; // swagger:route GET /pets pets users listPets // // Lists pets filtered by some parameters. // // This will show all available pets by default. // You can get the pets that are out of stock // // Consumes: // - application/json // - application/x-protobuf // // Produces: // - application/json // - application/x-protobuf // // Schemes: http, https, ws, wss // // Security: // api_key: // oauth: read, write // // Responses: // default: genericError // 200: someResponse // 422: validationError mountItem(&quot;GET&quot;, basePath+&quot;/pets&quot;, nil)&#125; swagger:parameters参数注释结构连接到一个或多个操作。 生成的swagger spec中的参数可以由多个结构组成。 语法 1swagger:parameters [operationid1 operationid2] 参数 Annotation Format In where to find the parameter Collection Format when a slice the formatter for the collection when serialized on the request Maximum specifies the maximum a number or integer value can have Minimum specifies the minimum a number or integer value can have Multiple of specifies a value a number or integer value must be a multiple of Minimum length the minimum length for a string value Maximum length the maximum length for a string value Pattern a regular expression a string value needs to match Minimum items the minimum number of items a slice needs to have Maximum items the maximum number of items a slice can have Unique when set to true the slice can only contain unique items Required when set to true this value needs to be present in the request Example an example value, parsed as the field’s type (objects and slices are parsed as JSON) 对于slice属性，还需要定义一些项。这可能是一个嵌套集合，用于指示嵌套级别，值是一个基于0的索引 Annotation Format Items.n.Maximum specifies the maximum a number or integer value can have at the level n Items.n.Minimum specifies the minimum a number or integer value can have at the level n Items.n.Multiple of specifies a value a number or integer value must be a multiple of Items.n.Minimum length the minimum length for a string value at the level n Items.n.Maximum length the maximum length for a string value at the level n Items.n.Pattern a regular expression a string value needs to match at the level n Items.n.Minimum items the minimum number of items a slice needs to have at the level n Items.n.Maximum items the maximum number of items a slice can have at the level n Items.n.Unique when set to true the slice can only contain unique items at the level n 示例： 12345678910111213141516171819// swagger:parameters listBars addBarstype BarSliceParam struct &#123; // a BarSlice has bars which are strings // // min items: 3 // max items: 10 // unique: true // items.minItems: 4 // items.maxItems: 9 // items.items.minItems: 5 // items.items.maxItems: 8 // items.items.items.minLength: 3 // items.items.items.maxLength: 10 // items.items.items.pattern: \w+ // collection format: pipe // in: query // example: [[[&quot;bar_000&quot;]]] BarSlice [][][]string `json:&quot;bar_slice&quot;`&#125; swagger:operation操作注释将路径链接到方法 语法 1swagger:operation [method] [path pattern] [?tag1 tag2 tag3] [operation id] 参数 Field Name Type Description tags [string] A list of tags for API documentation control. Tags can be used for logical grouping of operations by resources or any other qualifier. summary string A short summary of what the operation does. For maximum readability in the swagger-ui, this field SHOULD be less than 120 characters. description string A verbose explanation of the operation behavior. GFM syntax can be used for rich text representation. externalDocs External Documentation Object Additional external documentation for this operation. operationId string Unique string used to identify the operation. The id MUST be unique among all operations described in the API. Tools and libraries MAY use the operationId to uniquely identify an operation, therefore, it is recommended to follow common programming naming conventions. consumes [string] A list of MIME types the operation can consume. This overrides the consumesdefinition at the Swagger Object. An empty value MAY be used to clear the global definition. Value MUST be as described under Mime Types. produces [string] A list of MIME types the operation can produce. This overrides the producesdefinition at the Swagger Object. An empty value MAY be used to clear the global definition. Value MUST be as described under Mime Types. parameters [Parameter Object | Reference Object] A list of parameters that are applicable for this operation. If a parameter is already defined at the Path Item, the new definition will override it, but can never remove it. The list MUST NOT include duplicated parameters. A unique parameter is defined by a combination of a name and location. The list can use the Reference Object to link to parameters that are defined at the Swagger Object’s parameters. There can be one “body” parameter at most. responses Responses Object Required. The list of possible responses as they are returned from executing this operation. schemes [string] The transfer protocol for the operation. Values MUST be from the list: &quot;http&quot;, &quot;https&quot;, &quot;ws&quot;, &quot;wss&quot;. The value overrides the Swagger Object schemesdefinition. deprecated boolean Declares this operation to be deprecated. Usage of the declared operation should be refrained. Default value is false. security [Security Requirement Object] A declaration of which security schemes are applied for this operation. The list of values describes alternative security schemes that can be used (that is, there is a logical OR between the security requirements). This definition overrides any declared top-level security. To remove a top-level security declaration, an empty array can be used. 示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344// ServeAPI serves the API for this record storefunc ServeAPI(host, basePath string, schemes []string) (err error) &#123; // swagger:operation GET /pets getPet // // Returns all pets from the system that the user has access to // // Could be any pet // // --- // produces: // - application/json // - application/xml // - text/xml // - text/html // parameters: // - name: tags // in: query // description: tags to filter by // required: false // type: array // items: // type: string // collectionFormat: csv // - name: limit // in: query // description: maximum number of results to return // required: false // type: integer // format: int32 // responses: // '200': // description: pet response // schema: // type: array // items: // "$ref": "#/definitions/pet" // default: // description: unexpected error // schema: // "$ref": "#/definitions/errorModel" mountItem("GET", basePath+"/pets", nil) return&#125; swagger:response读取用swagger:response修饰的结构， 并使用该信息填充相应的标题和模式 swagger：route可以指定状态代码的响应名称，然后匹配的响应将用于swagger定义中的该操作。 语法 1swagger:response [?response name] 属性 Annotation Description In where to find the field Collection Format when a slice the formatter for the collection when serialized on the request Maximum specifies the maximum a number or integer value can have Minimum specifies the minimum a number or integer value can have Multiple of specifies a value a number or integer value must be a multiple of Minimum length the minimum length for a string value Maximum length the maximum length for a string value Pattern a regular expression a string value needs to match Minimum items the minimum number of items a slice needs to have Maximum items the maximum number of items a slice can have Unique when set to true the slice can only contain unique items Example an example value, parsed as the field’s type (objects and slices are parsed as JSON) 对于切片属性，还有要定义的项目。这可能是嵌套集合，用于指示嵌套级别，该值是基于0的索引，因此items.minLength与items.0.minLength相同 Annotation Format Items.n.Maximum specifies the maximum a number or integer value can have at the level n Items.n.Minimum specifies the minimum a number or integer value can have at the level n Items.n.Multiple of specifies a value a number or integer value must be a multiple of Items.n.Minimum length the minimum length for a string value at the level n Items.n.Maximum length the maximum length for a string value at the level n Items.n.Pattern a regular expression a string value needs to match at the level n Items.n.Minimum items the minimum number of items a slice needs to have at the level n Items.n.Maximum items the maximum number of items a slice can have at the level n Items.n.Unique when set to true the slice can only contain unique items at the level n 示例 123456789101112131415// A ValidationError is an error that is used when the required input fails validation.// swagger:response validationErrortype ValidationError struct &#123; // The error message // in: body Body struct &#123; // The validation message // // Required: true // Example: Expected type int Message string // An optional field name to which this validation applies FieldName string &#125;&#125; swagger:momdel定义结构数据 语法 1swagger:model [?model name] 属性 Annotation Description Maximum specifies the maximum a number or integer value can have Minimum specifies the minimum a number or integer value can have Multiple of specifies a value a number or integer value must be a multiple of Minimum length the minimum length for a string value Maximum length the maximum length for a string value Pattern a regular expression a string value needs to match Minimum items the minimum number of items a slice needs to have Maximum items the maximum number of items a slice can have Unique when set to true the slice can only contain unique items Required when set to true this value needs to be set on the schema Read Only when set to true this value will be marked as read-only and is not required in request bodies Example an example value, parsed as the field’s type (objects and slices are parsed as JSON) 对于切片属性，还有要定义的项目。这可能是嵌套集合，用于指示嵌套级别，该值是基于0的索引，因此items.minLength与items.0.minLength相同 Annotation Format Items.n.Maximum specifies the maximum a number or integer value can have at the level n Items.n.Minimum specifies the minimum a number or integer value can have at the level n Items.n.Multiple of specifies a value a number or integer value must be a multiple of Items.n.Minimum length the minimum length for a string value at the level n Items.n.Maximum length the maximum length for a string value at the level n Items.n.Pattern a regular expression a string value needs to match at the level n Items.n.Minimum items the minimum number of items a slice needs to have at the level n Items.n.Maximum items the maximum number of items a slice can have at the level n Items.n.Unique when set to true the slice can only contain unique items at the level n 示例 1234567891011121314151617181920212223242526272829// User represents the user for this application//// A user is the security principal for this application.// It&apos;s also used as one of main axes for reporting.//// A user can have friends with whom they can share what they like.//// swagger:modeltype User struct &#123; // the id for this user // // required: true // min: 1 ID int64 `json:&quot;id&quot;` // the name for this user // required: true // min length: 3 Name string `json:&quot;name&quot;` // the email address for this user // // required: true // example: user@provider.net Email strfmt.Email `json:&quot;login&quot;` // the friends for this user Friends []User `json:&quot;friends&quot;`&#125; swagger:allOf将嵌入类型标记为allOf的成员 语法 1swagger:allOf 示例 1234567891011121314151617181920212223242526272829303132333435// A SimpleOne is a model with a few simple fieldstype SimpleOne struct &#123; ID int64 `json:"id"` Name string `json:"name"` Age int32 `json:"age"`&#125;// A Something struct is used by other structstype Something struct &#123; DID int64 `json:"did"` Cat string `json:"cat"`&#125;// Notable is a model in a transitive package.// it's used for embedding in another model//// swagger:model withNotestype Notable struct &#123; Notes string `json:"notes"` Extra string `json:"extra"`&#125;// An AllOfModel is composed out of embedded structs but it should build// an allOf propertytype AllOfModel struct &#123; // swagger:allOf SimpleOne // swagger:allOf mods.Notable Something // not annotated with anything, so should be included CreatedAt strfmt.DateTime `json:"createdAt"`&#125; swagger:strfmtstrfmt标注名称的类型为字符串格式。该名称是必需的，将用作此特定字符串格式的格式名称。 语法 1swagger:strfmt [name] 字符串格式包含 uuid, uuid3, uuid4, uuid5 email uri (absolute) hostname ipv4 ipv6 credit card isbn, isbn10, isbn13 social security number hexcolor rgbcolor date date-time duration password custom string formats 示例 12345678910111213141516171819202122232425262728293031323334353637func init() &#123; eml := Email("") Default.Add("email", &amp;eml, govalidator.IsEmail)&#125;// Email represents the email string format as specified by the json schema spec//// swagger:strfmt emailtype Email string// MarshalText turns this instance into textfunc (e Email) MarshalText() ([]byte, error) &#123; return []byte(string(e)), nil&#125;// UnmarshalText hydrates this instance from textfunc (e *Email) UnmarshalText(data []byte) error &#123; // validation is performed later on *e = Email(string(data)) return nil&#125;func (b *Email) Scan(raw interface&#123;&#125;) error &#123; switch v := raw.(type) &#123; case []byte: *b = Email(string(v)) case string: *b = Email(v) default: return fmt.Errorf("cannot sql.Scan() strfmt.Email from: %#v", v) &#125; return nil&#125;func (b Email) Value() (driver.Value, error) &#123; return driver.Value(string(b)), nil&#125; swagger:discriminated将嵌入类型标记为allOf的成员并设置x-class值。在接口定义上，对允许swagger：name的方法有另一个注释 语法 1swagger:allOf org.example.something.TypeName 示例 12345678910111213141516171819202122232425262728293031323334// TeslaCar is a tesla car//// swagger:modeltype TeslaCar interface &#123; // The model of tesla car // // discriminator: true // swagger:name model Model() string // AutoPilot returns true when it supports autopilot // swagger:name autoPilot AutoPilot() bool&#125;// The ModelS version of the tesla car//// swagger:model modelStype ModelS struct &#123; // swagger:allOf com.tesla.models.ModelS TeslaCar // The edition of this Model S Edition string `json:"edition"`&#125;// The ModelX version of the tesla car//// swagger:model modelXtype ModelX struct &#123; // swagger:allOf com.tesla.models.ModelX TeslaCar // The number of doors on this Model X Doors int32 `json:"doors"`&#125; swagger:ignore将结构标记为从Swagger规范输出中显式忽略 语法 1swagger:ignore 在线swagger editor 文档编辑器启动方式 镜像拉取 docker pull swaggerapi/swagger-editor 镜像运行 docker run –rm -p 80:8080 swaggerapi/swagger-editor]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swagger spec 的编写规范]]></title>
    <url>%2F2019%2F04%2F29%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fswagger%20spec%20%E7%9A%84%E7%BC%96%E5%86%99%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[一、Swagger简介什么是swaggerSwagger是一个简单但功能强大的API表达工具。它具有地球上最大的API工具生态系统，数以千计的开发人员，使用几乎所有的现代编程语言，都在支持和使用Swagger。使用Swagger生成API，我们可以得到交互式文档，自动生成代码的SDK以及API的发现特性等。 Go swagger文档配置文件示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748swagger: "2.0"info: description: From the todo list tutorial on goswagger.io title: A Todo list application version: 1.0.0consumes: - application/io.goswagger.examples.todo-list.v1+jsonproduces:- application/io.goswagger.examples.todo-list.v1+jsonschemes:- httpdefinitions: item: type: object required: - description properties: id: type: integer format: int64 readOnly: true description: type: string minLength: 1 completed: type: booleanpaths: /: get: tags: - todos parameters: - name: since in: query type: integer format: int64 - name: limit in: query type: integer format: int32 default: 20 responses: 200: description: list the todo operations schema: type: array items: $ref: "#/definitions/item" Go-swagger 环境安装 安装依赖 1234567891011121314151617go get github.com/go-openapi/errorsgo get github.com/go-openapi/loadsgo get github.com/go-openapi/runtimego get github.com/go-openapi/specgo get github.com/go-openapi/strfmtgo get github.com/go-openapi/swaggo get github.com/go-openapi/validatego get github.com/jessevdk/go-flagsgo get golang.org/x/net/context 安装swagger 1go get -u github.com/go-swagger/go-swagger/cmd/swagger 语言选择: JSON vs YAML我们可以选择使用JSON或者YAML的语言格式来编写API文档。但是个人建议使用YAML来写，原因是它更简单。一图胜千言，先看用JSON写的文档： 123456789101112131415161718192021222324252627282930313233343536373839404142&#123; "swagger": "2.0", "info": &#123; "version": "1.0.0", "title": "Simple API", "description": "A simple API to learn how to write OpenAPI Specification" &#125;, "schemes": [ "https" ], "host": "simple.api", "basePath": "/openapi101", "paths": &#123; "/persons": &#123; "get": &#123; "summary": "Gets some persons", "description": "Returns a list containing all persons.", "responses": &#123; "200": &#123; "description": "A list of Person", "schema": &#123; "type": "array", "items": &#123; "properties": &#123; "firstName": &#123; "type": "string" &#125;, "lastName": &#123; "type": "string" &#125;, "username": &#123; "type": "string" &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 再来看看同一份API文档的YAML实现： 1234567891011121314151617181920212223242526272829303132swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 对于普通人来说，似乎用YAML更能够简化书写和阅读。这里我们并没有非此即彼的选择问题，因为： 几乎所用支持OpenAPI规范的工具都支持YAML 有很多的工具可以实现YAML-JSON之间的转换 所以，用自己喜欢的方式书写即可。（后面的示例文档也都是用YAML来写的。强烈推荐使用YAML。） 二、基本的swagger api定义方法2.1 最简单的例子我们从一个最简单（几乎没有东西）的API文档开始： 12345678910111213swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: &#123;&#125; 这个文档的内容分成四部分，下面分别来说明。 2.1.1 OpenAPI规范的版本号首先我们要通过一个swagger属性来声明OpenAPI规范的版本。 1swagger: "2.0" 你没看错，是swagger，上面已经介绍了，OpenAPI规范是基于Swagger的，在未来的版本中，这个属性可能会换成别的。 目前这个属性的值，暂时只能填写为2.0。 2.1.2 API描述信息然后我们需要说明一下API文档的相关信息，比如API文档版本（注意不同于上面的规范版本）、API文档名称已经可选的描述信息。 1234info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specification 2.1.3 API的URL作为web API，一个很重要的信息就是用来给消费者使用的根URL，可以用协议（http或者https）、主机名、根路径来描述： 1234schemes: - httpshost: simple.apibasePath: /openapi101 这这个例子中，消费者把https://simple.api/open101作为根节点来访问各种API。因为和具体环境有关，不涉及API描述的根本内容，所以这部分信息是可选的。 2.1.4 API的操作（operation）这个例子中，我们没有写API的操作，用一个YAML的空对象{}先占个位置。 2.2 定义一个API操作如果我们要展示一组用户信息，可以这样描述： 1234567891011121314151617181920212223242526272829303132swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.2.1 添加一个路径（path）我们添加一个/persons的路径，用来访问一组用户信息： 12paths: /persons: 2.2.2 在路径中添加一个HTTP方法在每个路径中，我们可以添加任意的HTTP动词来操作所需要的资源。 比如需要展示一组用户信息，我们可以在/persons路径中添加get方法，同时还可以填写一些简单的描述信息（summary）或者说明该方法的一段长篇大论（description）。 123get: summary: Gets some persons description: Returns a list containing all persons. 这样一来，我们调 get https://simple.api/open101/persons方法就能获取一个用户信息列表了。 2.2.3 定义响应（response）类型对于每个方法（或操作），我们都可以在响应(responses)中添加任意的HTTP状态码（比如200 OK 或者 404 Not Found等）。这个例子中我们添加上200的响应： 123responses: 200: description: A list of Person 2.2.4 定义响应内容get /persons这个接口返回一组用户信息，我们通过响应消息中的模式（schema）属性来描述清楚具体的返回内容。 一组用户信息就是一个用户信息对象的数组（array），每一个数组元素则是一个用户信息对象（object），该对象包含三个string类型的属性：姓氏、名字、用户名，其中用户名必须提供（required）。 123456789101112schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.3 定义请求参数（query parameters）用户太多，我们不想一股脑全部输出出来。这个时候，分页输出是个不错的选择，我们可以通过添加请求参数来实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################ responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.3.1 在get方法中增加请求参数首先我们在 get 方法中增加一个参数属性： 12345678paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters:# END ############################################################################ 2.3.2 添加分页参数在参数列表中，我们添加两个名字（name）分别叫做pageSize和pageNumber的整型（integer）参数，并作简单描述： 123456789101112 parameters:#START############################################################################ - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################## responses: 这样一来，消费者就可以通过 get /persons?pageSize=20&amp;pageNumber=2 来访问第2页的用户信息（不超过20条）了。 2.4 定义路径参数（path parameter）有时候我们想要根据用户名来查找用户信息，这时我们需要增加一个接口操作，比如可以添加一个类似 /persons/{username} 的操作来获取用户信息。注意，{username} 是在请求路径中的参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string#START############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 404: description: The Person does not exists.# END ############################################################################ 2.4.1 添加一个 get /persons/{username} 操作首先我们在 /persons 路径后面，增加一个 /persons/{username} 的路径，并定义一个 get （操作）方法。 12345678910111213141516171819202122swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: username: type: string#START############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username# END ############################################################################ 2.4.2 定义路径参数 username因为 {username} 是路径参数，我们需要先像请求参数一样将它添加到 parameters 属性中，注意名称应该同上面大括号（ { } ） 里面的名称一致。并通过 in 这个属性，来表示它是一个路径（path）参数。 123456parameters: - name: username in: path required: true description: The person's username type: string 定义路径参数时很容易出现的问题就是忘记：required: true，Swagger的自动完成功能中没有包含这个属性定义。 如果没有写 require 属性，默认值是 false，也就是说 username 参数时可选的。可事实上，作为路径参数，它是必需的。 2.4.3 定义响应消息别忘了获取单个用户信息也需要填写 200 响应消息，响应消息体的内容就是之前描述过的用户信息（用户信息列表中的一个元素）： 12345678910111213responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 当然，API的提供者会对 username 进行校验，如果查无此人，应该返回 404 的异常状态。所以我们再加上 404 状态的响应： 12404: description: The Person does not exists. 2.5 定义消息体参数（body parameter）当我们需要添加一个用户信息时，我们需要一个能够提供 post /persons 的API操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string#START############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: required: - username properties: firstName: type: string lastName: type: string username: type: string responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.# END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 404: description: The Person does not exists. 2.5.1 添加一个 post /persons 操作首先在 /persons 路径下廷加一个 post 操作： 12345paths: /persons: post: summary: Creates a person description: Adds a new person to the persons list. 2.5.2 定义消息体参数接下来我们给 post 方法添加参数，通过 in 属性显式说明参数是在 body 中的。参数的定义参考 get /persons/{username} 的 200 响应消息体参数，也就是包含用户的姓氏、名字、用户名。 1234567891011121314parameters: - name: person in: body description: The person to create. schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.5.3 定义响应消息最后不要忘记定义 post 操作的响应消息。 12345responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. 三、文档瘦身现在我们已经学会了编写API文档的基本方法。不过上面的例子中存在一些重复，这对于程序员的嗅觉来说，就是代码的“坏味道”。这一章我们一起学习如何通过抽取可重用的定义（definitions）来简化API文档。 3.1 简化数据模型我们认真观察第2章最后输出的API文档，很容易发现 Person 的定义出现了三次，非常的不 DRY☹。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: type: array items:#START 第1次定义################################################################### required: - username properties: firstName: type: string lastName: type: string username: type: string# END 第1次定义################################################################### post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema:#START 第2次定义################################################################### required: - username properties: firstName: type: string lastName: type: string username: type: string# END 第2次定义################################################################### responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema:#START 第3次定义################################################################### required: - username properties: firstName: type: string lastName: type: string username: type: string# END 第3次定义################################################################### 404: description: The Person does not exists. 现在，我们通过可重用的定义 （definition）来重构这个文档： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema:#START############################################################################ $ref: "#/definitions/Persons"# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema:#START############################################################################ $ref: "#/definitions/Person"# END ############################################################################ responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema:#START############################################################################ $ref: "#/definitions/Person"# END ############################################################################ 404: description: The Person does not exists.#START 新增定义####################################################################definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person"# END 新增定义#################################################################### 文档简化了很多。这得益于OpenAPI规范中关于定义（definition）的章节中允许我们“一处定义，处处使用”。 3.1.1 添加定义 （definitions）项我们首先在API文档的尾部添加一个定义 （definitions）项（其实它也可以放在文档的任意位置，只不过大家习惯放在文档末尾）： 12345 404: description: The Person does not exists.#START############################################################################definitions:# END ############################################################################ 3.1.2 增加一个可重用的（对象）定义然后我们增加一个 Person 对象的定义： 12345678910111213definitions:#START############################################################################ Person: required: - username properties: firstName: type: string lastName: type: string username: type: string# END ############################################################################ 3.1.3 引用一个定义来增加另一个定义在定义项中，我们可以立即引用刚才定义好的 Person 来增加另一个定义，Persons。Persons 是一个 Person 对象的数组。与之前直接定义的不同之处是，我们增加了一个引用（reference）属性，也就是 $ref 来引用 Person 。 1234Persons: type: array items: $ref: "#/definitions/Person" 3.1.4 在响应消息中使用定义一旦定义好了 Person ，我们可以把原来在响应消息中相应的定义字段替换掉。 3.1.4.1 get/persons原来： 12345678910111213responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string 现在： 12345responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons" 3.1.4.2 get/persons/{username}原来： 12345678910111213responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 现在： 12345responses: 200: description: A Person schema: $ref: "#/definitions/Person" 3.1.5 在参数中使用定义不仅仅在消息中可以使用定义，在参数中也可以使用。 3.1.5.1 post /persons原来： 1234567891011121314151617post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 现在： 123456789post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" 3.2 简化响应消息我们看到了引用 （$ref）的作用，接下来我们再把它用到响应消息的定义中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons"#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person"#START############################################################################ Error: properties: code: type: string message: type: stringresponses: Standard500ErrorResponse: description: An unexpected error occured. schema: $ref: "#/definitions/Error"# END ############################################################################ 3.2.1 定义可重用的HTTP 500 响应发生HTTP 500错误时，假如我们希望每一个API操作都返回一个带有错误码（error code）和描述信息（message）的响应，我们可以这样做： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons"#START############################################################################ 500: description: An unexpected error occured. schema: properties: code: type: string message: type: string# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.#START############################################################################ 500: description: An unexpected error occured. schema: properties: code: type: string message: type: string# END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.#START############################################################################ 500: description: An unexpected error occured. schema: properties: code: type: string message: type: string# END ############################################################################ 3.2.2 增加一个Error定义按照“一处定义、处处引用”的原则，我们可以在定义项中增加 Error 的定义： 1234567891011121314151617181920212223definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person"#START############################################################################ Error: properties: code: type: string message: type: string# END ############################################################################ 而且我们也学会了使用引用（$ref），所以我们可以这样写： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons" 500: description: An unexpected error occured. schema:#START############################################################################ $ref: "#/definitions/Error"# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. 500: description: An unexpected error occured. schema:#START############################################################################ $ref: "#/definitions/Error" # END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists. 500: description: An unexpected error occured. schema:#START############################################################################ $ref: "#/definitions/Error"# END ############################################################################ 3.2.3 定义一个可重用的响应消息上面的文档中，还是有一些重复的内容。我们可以根据OpenAPI规范中的responses章节的描述，通过定义一个可重用的响应消息，来进一步简化文档。 12345678910111213141516171819202122232425262728definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person" Error: properties: code: type: string message: type: string#START############################################################################responses: Standard500ErrorResponse: description: An unexpected error occured. schema: $ref: "#/definitions/Error"# END ############################################################################ 注意：响应消息中引用了 Error 的定义。 3.2.4 使用已定义的响应消息我们还是通过引用（$ref）来使用一个已经定义好的响应消息，比如： 3.2.4.1 get /users123456789 responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons"#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ 3.2.4.2 post/users123456789 responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ 3.2.4.3 get/users/{username}1234567891011 responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ 3.3 简化参数定义类似数据模型、响应消息的简化，参数定义的简化也很容易。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters: - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons" 500: $ref: "#/responses/Standard500ErrorResponse" post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Person succesfully created. 400: description: Person couldn't have been created. 500: $ref: "#/responses/Standard500ErrorResponse" /persons/&#123;username&#125;:#START############################################################################ parameters: - $ref: "#/parameters/username"# END ############################################################################ get: summary: Gets a person description: Returns a single person for its username. responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" delete: summary: Deletes a person description: Delete a single person identified via its username responses: 204: description: Person successfully deleted. 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" /persons/&#123;username&#125;/friends:#START############################################################################ parameters: - $ref: "#/parameters/username"# END ############################################################################ get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters: - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ responses: 200: description: A person's friends list schema: $ref: "#/definitions/Persons" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse"definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person" Error: required: - code - message properties: code: type: string message: type: stringresponses: Standard500ErrorResponse: description: An unexpected error occured. schema: $ref: "#/definitions/Error" PersonDoesNotExistResponse: description: Person does not exist.#START############################################################################parameters: username: name: username in: path required: true description: The person's username type: string pageSize: name: pageSize in: query description: Number of persons returned type: integer pageNumber: name: pageNumber in: query description: Page number type: integer# END ############################################################################ 3.3.1 路径参数只定义一次如果我们现在想要删除一个用户的信息，就需要增加一个 delete /persons/{username} 的操作，可以这样： 123456789101112131415161718192021222324252627282930313233343536373839 /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" delete: summary: Deletes a person description: Delete a single person identified via its username parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ responses: 204: description: Person successfully deleted. 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" 但是上面两次对参数 username 的定义，却让人有点难受。好消息是我们可以定义路径级别的参数（之前都是定义在操作级别。） 12345678910111213141516171819202122232425262728293031#START############################################################################ /persons/&#123;username&#125;: parameters: - name: username in: path required: true description: The person's username type: string# END ############################################################################ get: summary: Gets a person description: Returns a single person for its username. responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" delete: summary: Deletes a person description: Delete a single person identified via its username responses: 204: description: Person successfully deleted. 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" 3.3.2 定义可重用的参数如果我们想根据用户名查找该用户的朋友圈，可以添加一个 get /persons/{username}/friends 的操作。根据前面所学的内容，第一反应应该这样写： 12345678910111213141516171819202122232425262728/persons/&#123;username&#125;/friends: parameters: - name: username in: path required: true description: The person's username type: string get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A person's friends list schema: $ref: "#/definitions/Persons" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" 可以看到，关于 username 、pageSize 、pageNumber 的定义跟前面的 /person/{username} 、 get /persons 中的定义重复。如何消除重复呢？ 3.3.2.1 定义可重用的参数根据3.1和3.2中的内容，我们可以参考OpenAPI规范，融汇贯通。 1234567891011121314151617parameters: username: name: username in: path required: true description: The person's username type: string pageSize: name: pageSize in: query description: Number of persons returned type: integer pageNumber: name: pageNumber in: query description: Page number type: integer 3.3.2.2 使用定义参数借助万能的引用（$ref），这都是小菜一碟。比如： 3.3.2.2.1 get /persons原来： 123456789101112131415 /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################ 现在： 123456789 /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ 3.3.2.2.2 get 和 delete /persons/{username}原来： 123456789 /persons/&#123;username&#125;: parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ 现在： 12345 /persons/&#123;username&#125;: parameters:#START############################################################################ - $ref: "#/parameters/username"# END ############################################################################ 3.3.2.2.3 get /persons/{username}/friends原来： 1234567891011121314151617181920212223 /persons/&#123;username&#125;/friends: parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################ 现在： 12345678910111213 /persons/&#123;username&#125;/friends: parameters:#START############################################################################ - $ref: "#/parameters/username"# END ############################################################################ get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ 四、深入了解一下通过前面的练习，我们可以写出一篇结构清晰、内容精炼的API文档了。可是OpenAPI规范还给我们提供了更多的便利和惊喜，等着我们去了解和掌握。这一章主要介绍用于定义属性和数据模型的高级方法。 4.1 私人定制 Primitive data types in the Swagger Specification are based on the types supported by the JSON-Schema Draft 4. Models are described using the Schema Object which is a subset of JSON Schema Draft 4. 4.1.1 字符串（Strings）长度和格式当定义个字符串属性时，我们可以定制它的长度及格式： 属性 类型 描述 minLength number 字符串最小长度 maxLength number 字符串最大长度 pattern string 正则表达式 ，如果你暂时还不熟悉正则表达式 如果我们规定用户名是长度介于8~64，而且只能由小写字母和数字来构成，那么我们可以这样写： 12345username: type: string pattern: "[a-z0-9]&#123;8,64&#125;" minLength: 8 maxLength: 64 4.1.2 日期和时间日期和时间的处理参考 RFC 3339，我们唯一要做的就是写对格式： 格式 属性包含内容 属性示例 date ISO8601 full-date 2016-04-01 dateTime ISO8601 date-time 2016-04-16T16:06:05Z 如果我们在 Person 的定义中增加 生日 和 上次登录时间 时间戳，我们可以这样写： 123456dateOfBirth: type: string format: datelastTimeOnline: type: string format: dateTime 4.1.3 数字类型和范围当我们定义一个数字类型的属性时，我们可以[规定]它是一个整型、长型、浮点型或者双浮点型。 名称 类型 格式 integer integer int32 long integer int64 float number float double number double 和字符串一样，我们也可以定义数字属性的范围，比如： 属性 类型 描述 minimum number 最小值 maximum number 最大值 exclusiveMinimum boolean 数值必须 &gt; 最小值 exclusiveMaximum boolean 数值必须 &lt; 最大值 multipleOf number 数值必须是multipleOf的整数倍 如果我们规定 pageSize 必须是整数，必须 &gt; 0 且 &lt;=100，还必须是 10 的整数倍，可以这样写： 1234567891011pageSize: name: pageSize in: query description: Number of persons returned type: integer format: int32 minimum: 0 exclusiveMinimum: true maximum: 100 exclusiveMaximum: false multipleOf: 10 4.1.4 枚举类型我们还可以定义枚举类型，比如定义 Error 时，我们可以这样写： 123456code: type: string enum: - DBERR - NTERR - UNERR code 的值只能从三个枚举值中选择。 4.1.5 数值的大小和唯一性数字的大小和唯一性通过下面这些属性来定义： 属性 类型 描述 minItems number 数值中的最小元素个数 maxItem number 数值中的最大元素个数 uniqueItems boolean 标示数组中的元素是否唯一 比如我们定义一个用户数组 Persons，希望返回的用户信息条数介于10~100之间，而且不能有重复的用户信息，我们可以这样写： 123456789Persons: properties: items: type: array minItems: 10 maxItems: 100 uniqueItems: true items: $ref: "#/definitions/Person" 4.1.6 二进制数据可以用 string 类型来表示二进制数据： 格式 属性包含 byte Base64编码字符 binary 任意十进制的数据序列 比如我们需要在用户信息中增加一个头像属性（avatarBase64PNG）用base64编码的PNG图片来表示，可以这样写： 123avatarBase64PNG: type: string format: byte 4.2 高级数据定义4.2.1 读写操作同一定义的数据有时候我们读取资源信息的内容会比我们写入资源信息的内容（属性）更多，这很常见。是不是意味着我们必须专门为读取资源和写入资源分别定义不同的数据模型呢？幸运的是，OpenAPI规范中提供了 readOnly字段来帮我们解决整问题。比如： 1234lastTimeOnline: type: string format: dateTime readOnly: true 上面这个例子中，上次在线时间（lastTimeOnline ）是 Person 的一个属性，我们获取用户信息时需要这个属性。但是很明显，在创建用户时，我们不能把这个属性 post 到服务器。于是我们可以把它标记为 readOnly。 4.2.2 组合定义确保一致性一致性设计是在编写API文档时需要重点考虑的问题。比如我们在获取一组用户信息时，需要同时获取页面信息（ totalItems, totalPage, pageSize, currentPage）等，而且这些信息必须在根节点上。 怎么办呢？首先想到的做法就是： 1234567891011121314PagedPersonsV1: properties: items: type: array items: $ref: "#/definitions/Person" totalItems: type: integer totalPages: type: integer pageSize: type: integer currentPage: type: integer 如果其他API操作也需要这些页面信息，那就意味着这些属性必须一遍又一遍的定义。不仅重复体力劳动，而且还很危险：比如忘记了其中的一两个属性，或者需要添加一个新的属性进来，那就是霰弹式的修改，想想都很悲壮。 稍微好一点的做法，就是根据前面学习的内容，把这几个属性抽取出来，建立一个 Paging 模型，“一处定义、处处使用”： 12345678910111213141516171819PagedPersonsV2: properties: items: type: array items: $ref: "#/definitions/Person" paging: $ref: "#/definitions/Paging"Paging: properties: totalItems: type: integer totalPages: type: integer pageSize: type: integer currentPage: type: integer 但是，页面属性都不再位于 根节点！与我们前面设定的要求不一样了。怎么破？ JSON Schema v4 property中定义的allOf，能帮我们解围： 1234PagedPersons: allOf: - $ref: "#/definitions/Persons" - $ref: "#/definitions/Paging" 上面这个例子表示，PagedPersons 根节点下，具有将 Persons 和 Paging 展开 后的全部属性。 allOf同样可以使用行内的数据定义，比如： 1234567891011PagedCollectingItems: allOf: - properties: items: type: array minItems: 10 maxItems: 100 uniqueItems: true items: $ref: "#/definitions/CollectingItem" - $ref: "#/definitions/Paging" 4.2.3 数据模型的继承（TODO）目前各工具支持程度不高，待续 五、 输入输出模型这一章主要介绍如何定义高度精确化的参数和响应消息等。 5.1 高级参数定义5.1.1 必带参数和可选参数我们已经知道使用关键字required来定义一个必带参数。 5.1.1.1 定义必带参数和可选参数在一个参数中，required是一个 boolean 型的可选值。它的默认值是 false 。 比如在某个操作中，username 是必填参数： 12345678 username: name: username in: path#START############################################################################ required: true# END ############################################################################ description: The person's username type: string 5.1.1.2 定义必带属性和可选属性根据定义，required是一个字符串列表，列表中包含各必带参数名。如果某个参数在这张列表中找不到，那就说明它不是必带参数。如果没有定义required，就说明所有参数都是可选。如果required定义在一个HTTP请求上，这说明所有的请求参数都是必填。 在 POST 、persons 中有 Person 的定义，在这里 username 这个属性是必带的，我们可以指定它为required，其他非必带字段则不指定： 12345678910111213141516171819202122232425262728 Person:#START############################################################################ required: - username# END ############################################################################ properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time readOnly: true avatarBase64PNG: type: string format: byte default: data:image/png;base64,i…… spokenLanguages: $ref: '#/definitions/SpokenLanguages' 5.1.2 带默认值的参数通过关键字default，我们可以定义一个参数的默认值。当这个参数不可得（请求未带或者服务器未返回）时，这个参数就取默认值。因此设定了某个参数的默认值后，它是否required就没意义了。 5.1.2.1 定义参数的默认值我们定义参数 pageSize 的默认值为 20 ，那么如果请求时没有填写 pageSize ，服务器也会默认返回 20 个元素。 1234567891011121314 pageSize: name: pageSize in: query description: Number of persons returned type: integer format: int32 minimum: 0 exclusiveMinimum: true maximum: 100 exclusiveMaximum: false multipleOf: 10#START############################################################################ default: 20# END ############################################################################ 5.1.2.2 定义属性的默认值我们在定义 Person 对象时，希望给每个用户一个默认头像，也就是要给 avatarBase64PNG 属性一个默认值。 默认头像: … 12345678910111213141516171819202122232425262728 Person: required: - username properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time readOnly: true avatarBase64PNG: type: string format: byte#START############################################################################ default: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgA……rkJggg==# END ############################################################################ spokenLanguages: $ref: '#/definitions/SpokenLanguages' 5.1.3 带空值的参数在 GET /persons 时，如果我们想添加一个参数来过滤“是否通过实名认证”的用户，应该怎么做呢？首先想到的是这样：GET /persons?page=2&amp;includeVerifiedUsers=true ，问题是 includeVerifiedUsers 语义已经如此清晰，而让 “=true”显得很多余。我们能不能直接用：GET /persons?page=2&amp;includeVerifiedUsers 呢？ 要做到这种写法，我们需要一个关键字 allowEmptyValue。我们定义 includeVerifiedUsers 时允许它为空。那么如果我们请求 GET /persons?page=2&amp;includeVerifiedUsers 则表示需要过滤“实名认证”用户，如果我们直接请求 GET /persons?page=2 则表示不过滤： 12345678 includeNonVerifiedUsers: name: includeNonVerifiedUsers in: query type: boolean default: false#START############################################################################ allowEmptyValue: true# END ############################################################################ 5.1.4 参数组设计API的时候，我们经常会遇到在 GET 请求中需要携带一组请求参数的情况。如何在API文档章呈现呢？很简单，我们只需要设定参数类型（type） 为array，并选择合适的 组合格式（collectionFormat）就行了。 COLLECTIONFORMAT 描述 csv (default value) Comma separated values（逗号分隔） foo,bar ssv Space separated values（空格分隔） foo bar tsv Tab separated values（反斜杠分隔） foo\tbar pipes Pipes separated values（竖线分隔） `foo\ bar` multi 单属性可以取多个值，比如 foo=bar&amp;foo=baz. 只适用于查询参数和表单参数。 比如我们想根据多种参数（username ， firstname ， lastname ， lastTimeOnline ）等来对 Person 进行带排序的查询。我们需要一个这样的API请求： GET /persons?sort=-lastTimeOnline|+firtname|+lastname 。用于排序的参数是 sort ，+表示升序，-表示降序。 相应的API文档，可以这样写： 1234567891011sortPersons: name: sort in: query type: array uniqueItems: true minItems: 1 maxItems: 3 collectionFormat: pipes items: type: string pattern: '[-+](username|lastTimeOnline|firstname|lastname)' 现在我们就能搞定 GET /persons?sort=-lastTimeOnline|+firtname|+lastname 这种请求了。当然，我们还可以指定排序的默认值，锦上添花。 12345678910111213141516 sortPersons: name: sort in: query type: array uniqueItems: true minItems: 1 maxItems: 3 collectionFormat: pipes items: type: string pattern: '[-+](username|lastTimeOnline|firstname|lastname)'#START############################################################################ default: - -lastTimeOnline - +username# END ############################################################################ 5.1.5 消息头（Header）参数参数，按照位置来分，不仅仅包含路径参数、请求参数和消息体参数，还包括消息头参数和表单参数等。比如我们可以在HTTP请求的消息头上加一个 User-Agent （用于跟踪、调试或者其他），可以这样定义它： 12345userAgent: name: User-Agent type: string in: header required: true 然后像使用其他参数一样使用它： 1234paths: /persons: parameters: - $ref: '#/parameters/userAgent' 5.1.6 表单参数有些 js-less-browser 的老浏览器不支持 POST JSON数据，比如在创建用户时，只能够以这样个格式请求： 123POST /js-less-personsusername=apihandyman&amp;firstname=API&amp;lastname=Handyman 没有问题，丝袜哥可以搞定。我们只需要把各个属性的in关键字定义为formData，然后设置consumes的媒体类型为application/x-www-form-urlencoded即可。 123456789101112131415161718192021222324252627282930313233343536 post: summary: Creates a person description: For JS-less partners#START############################################################################ consumes: - application/x-www-form-urlencoded# END ############################################################################ produces: - text/html parameters: - name: username#START############################################################################ in: formData# END ############################################################################ required: true pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 type: string - name: firstname#START############################################################################ in: formData# END ############################################################################ type: string - name: lastname in: formData type: string - name: dateOfBirth#START############################################################################ in: formData# END ############################################################################ type: string format: date responses: '204': description: Person succesfully created. 5.1.7 文件参数当我们要处理一个请求，输入类型是 文件 时，我们需要： 使用 multipart/form-data 媒体类型； 设置参数的 in关键字为 formData； 设置参数的类型（type）为 file。 比如： 123456789101112131415161718/images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image consumes: - multipart/form-data parameters: - name: image in: formData type: file responses: '200': description: Image's ID schema: properties: imageId: type: string 有时候我们想限定输入文件的类型（后缀），很不幸的是，根据现在V2.0的规范暂时还做不到☹ The spec doesn’t allow specifying a content type for specific form data parameters. It’s a limitation of the spec. Ron Ratovsky comment in Swagger UI 609 issue 5.1.8 参数的媒体类型一个API可以消费各种不同的媒体类型，比如说最常见的是 application/json 类型的数据，当然这不是API唯一支持的类型。我们可以在文档的根节点 或者一个操作的根节点 下添加关键字 consumes，来定义这个操作能够消费的媒体类型。 比如我们的API全部都接受JSON和YAML的数据，那我们可以在文档的根节点下添加： 123consumes: - application/json - application/x-yaml 如果某个操作（比如上传图片的操作）很特殊，它可以通过自己添加 consumes来覆盖全局设置： 1234567891011121314151617181920 /images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image#START############################################################################ consumes: - multipart/form-data# END ############################################################################ parameters: - name: image in: formData type: file responses: '200': description: Image's ID schema: properties: imageId: type: string 5.2 高级响应消息定义5.2.1 不带消息体的响应消息不带消息体的响应很常见，比如HTTP 204 状态响应本身就表示服务器返回不带任何消息内容的成功消息。 要定义一个不带消息体的响应很简单，我们只需要写响应状态和描述就行了： 123456789101112131415 post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body required: true description: The person to create. schema: $ref: '#/definitions/Person' responses:#START############################################################################ '204': description: Person succesfully created.# END ############################################################################ 5.2.2 响应消息中的必带参数和可选参数与请求消息中类似，我们使用required参数来表示，比如请求一个用户信息时， 服务器必须返回username，可以这样写： 12345678910111213141516171819202122 Person:#START############################################################################ required: - username# END ############################################################################ properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time readOnly: true 5.2.3 响应消息头API的返回结果不仅仅体现下HTTP状态和响应消息体，还可以在响应消息头上做文章。比如我们可以限定一个API的使用次数和使用时间段，在响应消息头中，增加一个属性X-Rate-Limit-Remaining 来表示API可调用的剩余次数，增加另一个属性 X-Rate-Limit-Reset 来表示API的有效截止时间。 123456789101112131415161718192021 post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body required: true description: The person to create. schema: $ref: '#/definitions/Person' responses: '204': description: Person succesfully created. headers:#START############################################################################ X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time# END ############################################################################ 美中不足的是，对于这种响应消息头的修改，目前2.0规范暂时还不支持“一次定义、处处使用”☹ 5.2.4 默认响应消息我们在定义响应消息时，通常会列举不同的HTTP状态结果。如果有些状态不在我们API文档的定义范围（比如服务器需要返回 993 的状态），该怎么处理呢？这时需要通过关键字default来定义一个默认响应消息，用于各种定义之外的状态响应，比如： 1234567891011121314151617181920 delete: summary: Deletes a person description: Delete a single person identified via its username responses: '204': description: Person successfully deleted. headers: X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time '404': $ref: '#/responses/PersonDoesNotExistResponse' '500': $ref: '#/responses/Standard500ErrorResponse'#START############################################################################ default: $ref: '#/responses/TotallyUnexpectedResponse'# END ############################################################################ 目前这个配置也不支持“一次定义，处处使用” 。☹ 5.2.5 响应消息的媒体类型与请求消息一样，我们也可以定义响应消息所支持的媒体类型，不同的是我们要用到关键字 produces（与请求消息中的consumes相对，由此可见，API文档描述的主体是服务提供者）。 比如，我们可以在文档的根路径下全局设置： 123produces: - application/json - application/x-yaml 也可以在某个操作的根路径下覆盖设置： 123456789101112131415161718192021222324252627282930313233343536373839 /images/&#123;imageId&#125;: parameters: - $ref: '#/parameters/userAgent' get: summary: Gets an image parameters: - name: imageId in: path required: true type: string#START############################################################################ produces: - image/png - image/gif - image/jpeg - application/json - application/x-yaml# END ############################################################################ responses: '200': description: The image headers: X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time '404': description: Image do not exists headers: X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time '500': $ref: '#/responses/Standard500ErrorResponse' default: $ref: '#/responses/TotallyUnexpectedResponse' 5.3 定义某个参数只存在于响应消息中如前章节4.2.1中已经提到的，定义一个对象，其中某个属性我们只希望在响应消息中携带，而不希望在请求消息中携带，应该用readOnly关键字来表示。考虑到内容的完整性，这里再介绍一下。 比如 Person 对象中的 lastTimeOnline 这个属性，注册用户时我们不需要填写，但是在获取用户信息时，需要提供给服务消费者： 12345678910111213141516171819202122 Person: required: - username properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time#START############################################################################ readOnly: true# END ############################################################################ 六、 不要让API裸奔这一章主要介绍API文档中如何描述安全相关的内容。 6.1 定义安全安全相关内容的定义一般放在API文档根目录下的securityDefinition 中，它包括一组具体的命名安全项，每一个命名安全定义可能包括下面三种安全类型之一：basic，apiKey ，oauth2。 6.1.1 基础鉴权（Basic Authentication）要定义一个基础（basic） 鉴权，我们只需要将type设置为 basic 即可： 1234567securityDefinitions: UserSecurity: type: basic AdminSecurity: type: basic MediaSecurity: type: basic 这个例子中，我们定义了三种安全说明（UserSecurity，AdminSecurity， MediaSecurity），都属于基础鉴权。 6.1.2 API秘钥鉴权（API Key）要定义一个API秘钥鉴权，我们需要： 设置type为 apiKey 通过关键字in指示api秘钥所在位置。通常api秘钥会放在消息头、请求参数或者消息体中 给安全项命个名字 12345678910111213securityDefinitions: UserSecurity: type: apiKey in: header name: SIMPLE-API-KEY AdminSecurity: type: apiKey in: header name: ADMIN-API-KEY MediaSecurity: type: apiKey in: query name: MEDIA-API-KEY 在这个例子中,我们定义了三个apiKey类型的安全项： UserSecurity 定义了一个名为SIMPLE-API-KEY 的参数在消息头（header） AdminSecurity 定义了一个名为 ADMIN-API-KEY 的参数在消息头 （header） MediaSecurity 定义了一个名为MEDIA-API-KEY的参数在请求参数中 6.1.3 Oauth2鉴权6.1.3.1 流程（Flow）和URL当我们定义个 Oauth2 类型的安全项上，我们通常会定义Oauth2 的流程（flow）和并根据选定的流程配置相应的鉴权地址（authorizationUrl）和/或令牌地址（tokenUrl）。 流程 所需要的URL implicit authorizationUrl（鉴权地址） password tokenUrl（令牌地址） application tokenUrl accessCode authorizationUrl and tokenUrl 比如： 123456securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' 在这个例子中，我们定义了一个 Oauth2 的安全项，配置的流程是 accessCode 方式，同时配置了鉴权地址和令牌地址。 6.1.3.2 作用范围（scope）我们借助关键字scopes并通过哈希键值对来还可以配置 Oauth2 安全项的作用范围（scope），键值对的键表示作用范围名称；值是它的相关描述，比如： 12345678910securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope media: Media scope 在这个例子中，我们给 OauthSecurity 安全项添加了三个作用范围（admin ，user ，media）。 6.2 使用安全定义现在我们已经在securityDefinition 中定义好了安全项，现在我们可以将将它们应用到文档中了。使用的时候，我们通过security关键字，把安全项添加进去。 6.2.1 基础鉴权6.2.1.1 API级别12345678910111213securityDefinitions: UserSecurity: type: basic AdminSecurity: type: basic MediaSecurity: type: basic#START############################################################################security: - UserSecurity: [] # END ############################################################################paths: /persons: 在这个例子中，我们在API文档的根路径下直接使用了安全项 UserSecurity，它的作用范围是整个API文档。 6.2.1.2 操作级别比如我们在添加或者修改用户信息时，需要进行管理员鉴权，可以在 POST /persons 操作中增加安全项： 1234567 post: summary: Creates a person description: Adds a new person to the persons list.#START############################################################################ security: - AdminSecurity: []# END ############################################################################ 而在上传图片时，需要进行媒体操作鉴权，可以在 POST /images 操作中增加安全项： 123456789 /images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image#START############################################################################ security: - MediaSecurity: []# END ############################################################################ 6.2.2 API秘钥鉴权使用方式和基础鉴权一样，可以在API级别和操作级别使用： 123456789101112131415161718192021222324securityDefinitions: UserSecurity: type: apiKey in: header name: SIMPLE-API-KEY AdminSecurity: type: apiKey in: header name: ADMIN-API-KEY MediaSecurity: type: apiKey in: query name: media-api-key#START############################################################################security: - UserSecurity: [] # END ############################################################################paths: /persons: post: summary: Creates a person description: Adds a new person to the persons list. security: - AdminSecurity: [] 6.2.3 Oauth2鉴权Oauth2 鉴权的使用和上面的两种鉴权方式基本相同，不同之处在于我们可以指定它的哪一个作用范围（scope）。 比如API级别的鉴权： 1234567891011121314151617securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope media: Media scope#START############################################################################security: - OauthSecurity: - user# END ############################################################################paths: /persons: 操作级别的鉴权： 123456post: summary: Creates a person description: Adds a new person to the persons list. security: - OauthSecurity: - admin 在这个例子中，作用范围 admin 将覆盖全局配置的作用范围 user 。 6.3 使用多种安全配置OpenAPI规范并没有限定我们只能使用一种安全项。下面的例子将展示如何使用多种安全配置。 6.3.1 安全定义123456789101112131415securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope MediaSecurity: type: apiKey in: query name: media-api-key LegacySecurity: type: basic 这个例子中，我们定义了三种鉴权方式。 6.3.2 全局安全配置1234security: - OauthSecurity: - user - LegacySecurity: [] 这个配置的意思是用户可以通过两种方式中的任意一种来访问我们提供的API接口。 6.3.3 覆盖全局配置1234567post: summary: Creates a person description: Adds a new person to the persons list. security: - OauthSecurity: - admin - LegacySecurity: [] 在 POST /persons 操作中，OauthSecurity 的作用范围被覆写为admin。此时用户可以通过admin 的Oauth2*或者 *legacySecurity 来鉴权使用这个操作。 1234567/images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image security: - MediaSecurity: [] 在 POST/images 操作中，用MediaSecurity 整体覆写了全局安全项，用户只能通过 MediaSecurity 鉴权使用这个操作。 七、 让文档的可读性更好7.1 分类标签（Tags）通过关键字tags我们可以对文档中接口进行归类，tags的本质是一个字符串列表。tags定义在文档的根路径下。 7.1.1 单标签比如说 GET /persons 属于用户（Person） 这个分类的，那么我们可以给它贴个标签： 123456789101112paths: /persons: parameters: - $ref: '#/parameters/userAgent' get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. operationId: searchUsers#START############################################################################ tags: - Persons# END ############################################################################ 7.1.2 多标签一个操作也可以同时贴几个标签，比如： 1234567891011/js-less-consumer-persons: parameters: - $ref: '#/parameters/userAgent' post: summary: Creates a person description: For JS-less partners operationId: createUserJS deprecated: true tags: - JSLess - Persons 贴上标签后，在Swagger Editor和Swagger UI中能够自动归类，我们可以按照标签来筛选接口，试试吧？ 7.2 无处不在的描述文字（Descriptions）description这个属性几乎是无处不在，为了提高文档的可读性，我们应该在必要的地方都加上描述文字。 7.2.1 安全项的描述123456789101112131415161718securityDefinitions: OauthSecurity: description: New Oauth security system. Do not use MediaSecurity or LegacySecurity. type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope MediaSecurity: description: Specific media security for backward compatibility. Use OauthSecurity instead. type: apiKey in: query name: media-api-key LegacySecurity: description: Legacy security system for backward compatibility. Use OauthSecurity instead. type: basic 7.2.2 模式（Schema）的描述每一种模式（Schema），都会有一个标题（title）和一段描述，比如： 1234definitions: Person: title: Human description: A person which can be the user itself or one of his friend 7.2.3 属性的描述比如： 1234properties: firstName: description: first name type: string 7.2.4 参数的描述123456789101112131415161718192021paths: /persons: post: parameters: - name: person in: body required: true description: The person to create. schema: $ref: '#/definitions/Person'parameters: username: name: username in: path required: true description: The person's username type: string pageSize: name: pageSize in: query description: Number of persons returned 7.2.5 操作的概述（summary）、描述和操作ID（operationId）一个操作（Operation）通常都会包含概述和描述信息。而且我们还可以添加一个关键字operationId，这个关键字通常用来指示服务提供者对这个操作的处理函数的函数名。比如： 12345678paths: /persons: parameters: - $ref: '#/parameters/userAgent' get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. operationId: searchUsers 7.2.6 响应消息的描述12345678910111213paths: /persons: parameters: - $ref: '#/parameters/userAgent' get: …… …… responses: '200': description: A list of Personresponses: Standard500ErrorResponse: description: An unexpected error occured. 7.2.7 响应消息头的描述12345678headers: X-Rate-Limit-Remaining: description: How many calls consumer can do type: integer X-Rate-Limit-Reset: description: When rate limit will be reset type: string format: date-time 7.2.8 标签的描述我们在API文档的根路径下添加了tags的定义，对于其中的每一个标签，我们都可以添加描述信息： 123tags: - name: Persons description: Everything you need to handle users and friends 7.3 在描述中使用Markdown语法在绝大部分的description中，我们可以使用GFM（Github Flavored Markdown）语法。 7.3.1 多行描述使用符号 | 然后在新行中打一个tab（注意：YAML的tab是两个空格 ），就可以编辑多行描述，比如： 123456789'/persons/&#123;username&#125;/collecting-items': parameters: - $ref: '#/parameters/username' - $ref: '#/parameters/userAgent' get: summary: Gets a person's collecting items list description: | Returns a list containing all items this person is looking for. The list supports paging. 7.3.2 简单使用GFM比如我们要强调，可以这样写： 1234externalDocs: description: | **Complete** documentation describing how to use this API url: http://doc.simple.api/ 7.3.3 带信息组的描述12345678910111213141516171819CollectingItem: discriminator: itemType required: - itemType properties: itemType: description: | An item can be of different type: type | definition -----|----------- Vinyl| #/definitions/Vinyl VHS | #/definitions/VHS AudioCassette | #/definitions/AudioCassette type: string enum: - AudioCassette - Vinyl - VHS 7.3.4 带代码的描述1234567891011121314151617181920212223242526272829303132333435363738swagger: '2.0'info: version: 1.1.0 title: Simple API description: | A simple API to learn how to write OpenAPI Specification. This file uses almost every single aspect of the [Open API Specification](https://openapis.org/). This API will use JSON. JSON looks like this: ```JSON &#123; "key": "value", "anotherKey": "anotherValue" &#125;### 7.4 示例数据（Examples）我们已经知道了用Schema来描述参数和属性，有的时候，用示例数据更有表现了。我们可以使用关键字`example`来给原子属性或者对象添加示例数据。#### 7.4.1 原子属性的示例数据```YAML properties: firstName: description: first name type: string#START############################################################################ example: John# END ############################################################################ lastTimeOnline: description: The last time this person was connected to the service as a type: string format: date-time readOnly: true#START############################################################################ example: 2016-06-10T12:36:58.014Z# END ############################################################################ 7.4.2 对象属性的示例数据123456789101112131415161718192021222324252627 Persons: title: Humans description: A list of users or friends required: - items properties: items: description: Array containg the list type: array minItems: 10 maxItems: 100 uniqueItems: true items: $ref: '#/definitions/Person'#START############################################################################ example: - firstname: Robert lastname": Doe username": robdo dateOfBirth: 1970-01-28 lastTimeOnline: 2016-04-10T14:36:58.014Z - firstname: Jane lastname: Doe username: jdoe123 dateOfBirth: 1980-05-12 lastTimeOnline: 2016-05-12T19:23:59.014Z# END ############################################################################ 7.4.3 定义的示例数据跟普通属性一样，定义的对象也能添加示例数据： 1234567891011121314151617 MultilingualErrorMessage: title: MultiLingualMultiDeviceErrorMessage description: An multilingual error message (hashmap) with a long and a short description additionalProperties: $ref: '#/definitions/ErrorMessage' properties: defaultLanguage: $ref: '#/definitions/ErrorMessage'#START############################################################################ example: defaultLanguage: longMessage: We're deeply sorry but an error occured shortMessage: Error fr: longMessage: Nous sommes dÃ©solÃ© mais une erreur est survenu shortMessage: Erreur# END ############################################################################ 7.4.4 响应消息的示例数据我们甚至可以添加响应消息级别的示例数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 '/persons/&#123;username&#125;/collecting-items': …… get: …… responses: '200':#START############################################################################ examples: application/json: &#123; "totalItems": 10, "totalPage": 4, "pageSize": 3, "currentPage": 2, "items": [ &#123; "itemType": "Vinyl", "maxPrice": 20, "imageId": "98096838-04eb-4bac-b32e-cd5b7196de71", "albumName": "Captain Future Original Soundtrack", "artist": "Yuji Ohno" &#125;, &#123; "itemType": "VHS", "maxPrice": 10, "imageId": "b74469bc-e6a1-4a90-858a-88ef94079356", "movieTitle": "Star Crash", "director": "Luigi Cozzi" &#125;, &#123; "itemType": "AudioCassette", "maxPrice": 10, "imageId": "b74469bc-e6a1-4a90-858a-88ef94079356", "albumName": "Star Wars", "artist": "John Williams" &#125; ] &#125;# END ############################################################################ '404': $ref: '#/responses/PersonDoesNotExistResponse' '500': $ref: '#/responses/Standard500ErrorResponse' default: $ref: '#/responses/TotallyUnexpectedResponse' 7.4.5 示例数据的优先级如果我们在各个级别（比如参数、对象、定义、响应消息）都添加了示例数据。支持OpenAPI规范的各解析工具都是以 最高级别 的定义为准。 7.5 标记为弃用我们可以通过关键字deprecated置为 true 来标记接口的弃用状态，比如： 12345678/js-less-consumer-persons: parameters: - $ref: '#/parameters/userAgent' post: summary: Creates a person description: For JS-less partners operationId: createUserJS deprecated: true 7.6 链接到外部文档一般来说，项目中不光只有一篇API文档，还应该有些描述application key，测试用例，操作链以及其他内容的文档，这些文档一般是单独成篇的。如果在描述某个接口时，我们想链接这些文档，可以通过关键字externalDoc来添加，例如： 12345678910111213externalDocs: description: Complete documentation describing how to use this API url: http://doc.simple.api/ /images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image description: Upload an image, will return an image id. operationId: storeImage externalDocs: description: How to upload media url: http://doc.simple.api/media/upload 八、 分而治之根据前面几张的知识，我们已经可以轻松的构建一个复杂的API文档了。可是作为一个学过 Clean Code 的程序员，我们并不希望所有的接口、定义都在一个大而全的上帝文件里。这一章我们一起来学习拆分文件。 8.1 JSON指针我们在第2章已经知道了怎么定义一个可重用的对象，已经如何使用定义，重温一下： 123456789101112131415161718192021222324252627282930313233 /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person" 在使用的时候我们这样写：$ref: “#/definitions/Person” ，$ref就是JSON指针（参见定义RFC6901）。例子中的这个指针指向当前文档根路径（#）下的definitions 下的 Person。 JSON指针不仅可以指向当前文件，还可以指向其他文件。 8.2 基础拆分8.2.1 引用本地文件还是上面这个例子，我们在当前文档的同一目录下，新建的一个文件叫 person.yaml，然后把Person 的定义拆出来，放在其中。 12345678910Person: required: - username properties: firstName: type: string lastName: type: string username: type: string 在当前文档中把引用 Person 定义的地方修改为： 1$ref: "person.yaml#/Person" 8.2.2 编辑器报错把上面的代码放到Swagger Editor中编辑，编辑器预览中可能会报错： 提示找不到person.yaml文件。这是因为我们没有指定正确的编辑器的指针解析基础路径（Pointer Resolution Base Path） 。 解决的办法是：进入编辑器的 Preferences -&gt; Preferences 菜单，修改Pointer Resolution Base Path为： 考虑到缓存的原因，如果错误依然存在，请刷新浏览器。 8.2.3 文件夹如果引用子文件夹下的文件，我们可以这样写： 1$ref: "folder/person.yaml#/Person" 如果引用上级文件夹下的文件，我们可以这样写： 1234Persons: type: array items: $ref: "../folder/person.yaml#/Person" 8.2.4 引用远程文件如果我们想引用一个远程文件，应该怎么做呢？可以这样写： 1$ref: https://myserver.com/mypath/myfile.yaml#/example 但是需要注意的是：服务器必须提供跨域（CORS）访问服务。 如果要通过本地服务器上的8080端口引用文件，我们可以这样写： 1$ref: "http://localhost:8080/folder/person.yaml#/Person" 考虑两种比较特别的情况： 8.2.4.1 远端文件引用”本地文件”比如说，我们引用了： 1$ref: "http://localhost:8080/another-folder/persons.yaml#/Persons" 这个远端文件。但是persons.yaml又引用了与它在同一目录下的person.yaml文件，这个时候语法分析器会在localhost:8080上面找person.yaml文件，而不会查找我们本地的person.yaml。 8.2.4.2 远端文件引用 ”更“远端文件如果理解了8.2.4.1这个例子，我们就知道不管怎么引用，都是在相对于被引用的文件下来进行查找的。 8.2.5 整理一个文件中的多个定义比如我们在一个文件中，需要把定义分成几类，可以这样做： 1234567891011121314151617SomeDefinitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: stringOtherDefinitions: Persons: type: array items: $ref: "#/SomeDefinitions/Person" Person归属于SomeDefinitions这一类，Persons归属于OtherDefinitions这一类。那么我们在引用这两个定义时，分别应该这样书写： 12$ref: "definitions.yaml#/OtherDefinitions/Persons"$ref: "definitions.yaml#/SomeDefinitions/Person" 有点像命名空间的概念。 8.3 实战切分的思路 结构化切分思路： 先把API文档的头部info切下来，放在info.yaml，然后分别切分 paths.yaml，definitions.yaml，responses.yaml，parameters.yaml等文件，最后合并到main.yaml； 分层切分思路：分别按照功能和层次，将文件切分为base.yaml或common.yaml，然后是各个模块的xxx-paths.yaml，然后是各个定义的xxx-definitions.yaml。 其他思路…… 需要注意的是，不管怎么切分，有一个原则必须谨记： **切分出来的子文档，必须遵循OpenAPI规范，能够通过编辑器的校验，不然切分得再漂亮也是徒劳。]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 示例应用搭建]]></title>
    <url>%2F2019%2F04%2F12%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F3.Fabric%20%E7%A4%BA%E4%BE%8B%E5%BA%94%E7%94%A8%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[生成MSP身份文件配置crypto-config文件crypto-config.yaml文件： 1234567891011121314151617181920212223242526272829OrdererOrgs: - Name: Orderer Domain: example.com Specs: - Hostname: ordererPeerOrgs: - Name: Org1 Domain: org1.example.com CA: Hostname: ca # implicitly ca.org1.example.com Template: Count: 2 SANS: - "localhost" Users: Count: 1 - Name: Org2 Domain: org2.example.com CA: Hostname: ca Template: Count: 2 SANS: - "localhost" Users: Count: 1 生成MSP文件1cryptogen generate --config=./crypto-config.yaml # 生成msp文件 生成创世区块和channel事务创世区块中，包含了组织和联盟的配置信息等 配置configtx.yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758Profiles: TwoOrgsOrdererGenesis: Orderer: &lt;&lt;: *OrdererDefaults Organizations: - *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: &lt;&lt;: *ApplicationDefaults Organizations: - *Org1 - *Org2Organizations: - &amp;OrdererOrg Name: OrdererMSP ID: OrdererMSP MSPDir: crypto-config/ordererOrganizations/example.com/msp - &amp;Org1 Name: Org1MSP ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.example.com/msp AnchorPeers: - Host: peer0.org1.example.com Port: 7051 - &amp;Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.example.com/msp AnchorPeers: - Host: peer0.org2.example.com Port: 7051Orderer: &amp;OrdererDefaults OrdererType: solo Addresses: - orderer.example.com:7050 BatchTimeout: 2s BatchSize: MaxMessageCount: 10 AbsoluteMaxBytes: 98 MB PreferredMaxBytes: 512 KB Kafka: Brokers: - 127.0.0.1:9092 Organizations:Application: &amp;ApplicationDefaults Organizations: 生成创世区块1configtxgen -profile TwoOrgsOrdererGenesis -channelID order-channel -outputBlock genesis.block 生成channel配置事务1export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile TwoOrgsChannel -outputCreateChannelTx channel.tx -channelID $CHANNEL_NAME 生成Anchor peer 配置事务12export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate Org1MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org1MSPexport CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate Org2MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org2MSP 启动网络配置docker-compose文件完成两个ca， 两个组织(每个组织2个节点)，一个order节点和一个cli节点的配置 cli节点可以帮助初始化channel和链码的安装和初始化等工作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162version: '2'services: ca.org1.example.com: image: hyperledger/fabric-ca environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca.org1.example.com - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/0e729224e8b3f31784c8a93c5b8ef6f4c1c91d9e6e577c45c33163609fe40011_sk - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/0e729224e8b3f31784c8a93c5b8ef6f4c1c91d9e6e577c45c33163609fe40011_sk ports: - "7054:7054" command: sh -c 'fabric-ca-server start -b admin:adminpw -d' volumes: - ./channel/crypto-config/peerOrganizations/org1.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca.org1.example.com ca.org2.example.com: image: hyperledger/fabric-ca environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca.org2.example.com - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/a7d47efa46a6ba07730c850fed2c1375df27360d7227f48cdc2f80e505678005_sk - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/a7d47efa46a6ba07730c850fed2c1375df27360d7227f48cdc2f80e505678005_sk ports: - "8054:7054" command: sh -c 'fabric-ca-server start -b admin:adminpw -d' volumes: - ./channel/crypto-config/peerOrganizations/org2.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca.org2.example.com orderer.example.com: container_name: orderer.example.com image: hyperledger/fabric-orderer environment: - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/etc/hyperledger/configtx/genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/etc/hyperledger/crypto/orderer/msp - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/etc/hyperledger/crypto/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/etc/hyperledger/crypto/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/etc/hyperledger/crypto/orderer/tls/ca.crt, /etc/hyperledger/crypto/peerOrg1/tls/ca.crt, /etc/hyperledger/crypto/peerOrg2/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric/orderers command: orderer ports: - 7050:7050 volumes: - ./channel:/etc/hyperledger/configtx - ./channel/crypto-config/ordererOrganizations/example.com/orderers/orderer.example.com/:/etc/hyperledger/crypto/orderer - ./channel/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/:/etc/hyperledger/crypto/peerOrg1 - ./channel/crypto-config/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/:/etc/hyperledger/crypto/peerOrg2 peer0.org1.example.com: container_name: peer0.org1.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer0.org1.example.com - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_ADDRESS=peer0.org1.example.com:7051 ports: - 7051:7051 - 7053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com peer1.org1.example.com: container_name: peer1.org1.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer1.org1.example.com - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_ADDRESS=peer1.org1.example.com:7051 ports: - 8051:7051 - 8053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com peer0.org2.example.com: container_name: peer0.org2.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer0.org2.example.com - CORE_PEER_LOCALMSPID=Org2MSP - CORE_PEER_ADDRESS=peer0.org2.example.com:7051 ports: - 9051:7051 - 9053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com peer1.org2.example.com: container_name: peer1.org2.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer1.org2.example.com - CORE_PEER_LOCALMSPID=Org2MSP - CORE_PEER_ADDRESS=peer1.org2.example.com:7051 ports: - 10051:7051 - 10053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org2.example.com/peers/peer1.org2.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com cli: container_name: cli image: hyperledger/fabric-tools tty: true stdin_open: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock #- CORE_LOGGING_LEVEL=DEBUG - FABRIC_LOGGING_SPEC=INFO - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer0.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/bash -c "./scripts/script.sh" volumes: - /var/run/:/host/var/run/ - ./../chaincode/:/opt/gopath/src/github.com/chaincode - ./channel/crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/ - ./channel:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts depends_on: - orderer.example.com - peer0.org1.example.com - peer1.org1.example.com - peer0.org2.example.com - peer1.org2.example.com 运行docker-compose启动网络1docker-compose -f ./docker-compose.yaml up -d 接下来一个安装并示例好链码的网络环境就已经搭建好了， 接下来查看cli容器中进行了哪些步骤实现了对channel的创建和chaincode的安装和实例化 cli 客户端中执行的内容cli 容器中启动程序代码为， 在脚本中会进行channel的创建，组织节点加入channel，链码的安装，实例化等 启动执行脚本scripts.sh 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283CHANNEL_NAME="$1"DELAY="$2"LANGUAGE="$3"TIMEOUT="$4": $&#123;CHANNEL_NAME:="mychannel"&#125;: $&#123;DELAY:="3"&#125;: $&#123;LANGUAGE:="golang"&#125;: $&#123;TIMEOUT:="10"&#125;LANGUAGE=`echo "$LANGUAGE" | tr [:upper:] [:lower:]`COUNTER=1MAX_RETRY=5ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pemCC_SRC_PATH="github.com/chaincode/chaincode_example02/go/"if [ "$LANGUAGE" = "node" ]; then CC_SRC_PATH="/opt/gopath/src/github.com/chaincode/chaincode_example02/node/"fiecho "Channel name : "$CHANNEL_NAME# import utils. scripts/utils.shcreateChannel() &#123; setGlobals 0 1 if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer channel create -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/channel.tx &gt;&amp;log.txt res=$? set +x else set -x peer channel create -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/channel.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Channel creation failed" echo "===================== Channel \"$CHANNEL_NAME\" is created successfully ===================== " echo&#125;joinChannel () &#123; for org in 1 2; do for peer in 0 1; do joinChannelWithRetry $peer $org echo "===================== peer$&#123;peer&#125;.org$&#123;org&#125; joined on the channel \"$CHANNEL_NAME\" ===================== " sleep $DELAY echo done done&#125;## Create channelecho "Creating channel..."createChannel## Join all the peers to the channelecho "Having all peers join the channel..."joinChannel## Set the anchor peers for each org in the channelecho "Updating anchor peers for org1..."updateAnchorPeers 0 1echo "Updating anchor peers for org2..."updateAnchorPeers 0 2## Install chaincode on peer0.org1 and peer0.org2echo "Installing chaincode on peer0.org1..."installChaincode 0 1echo "Install chaincode on peer0.org2..."installChaincode 0 2## Install chaincode on peer1.org2echo "Installing chaincode on peer1.org1..."installChaincode 1 1echo "Installing chaincode on peer1.org2..."installChaincode 1 2# Instantiate chaincode on peer0.org2echo "Instantiating chaincode on peer0.org2..."instantiateChaincode 0 2 依赖脚本utils.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265verifyResult () &#123; if [ $1 -ne 0 ] ; then echo "!!!!!!!!!!!!!!! "$2" !!!!!!!!!!!!!!!!" echo "========= ERROR !!! FAILED to execute End-2-End Scenario ===========" echo exit 1 fi&#125;# Set OrdererOrg.Admin globalssetOrdererGlobals() &#123; CORE_PEER_LOCALMSPID="OrdererMSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/users/Admin@example.com/msp&#125;setGlobals () &#123; PEER=$1 ORG=$2 if [ $ORG -eq 1 ] ; then CORE_PEER_LOCALMSPID="Org1MSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp if [ $PEER -eq 0 ]; then CORE_PEER_ADDRESS=peer0.org1.example.com:7051 else CORE_PEER_ADDRESS=peer1.org1.example.com:7051 fi elif [ $ORG -eq 2 ] ; then CORE_PEER_LOCALMSPID="Org2MSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp if [ $PEER -eq 0 ]; then CORE_PEER_ADDRESS=peer0.org2.example.com:7051 else CORE_PEER_ADDRESS=peer1.org2.example.com:7051 fi elif [ $ORG -eq 3 ] ; then CORE_PEER_LOCALMSPID="Org3MSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/ca.crt CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp if [ $PEER -eq 0 ]; then CORE_PEER_ADDRESS=peer0.org3.example.com:7051 else CORE_PEER_ADDRESS=peer1.org3.example.com:7051 fi else echo "================== ERROR !!! ORG Unknown ==================" fi env |grep CORE&#125;updateAnchorPeers() &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer channel update -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/$&#123;CORE_PEER_LOCALMSPID&#125;anchors.tx &gt;&amp;log.txt res=$? set +x else set -x peer channel update -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/$&#123;CORE_PEER_LOCALMSPID&#125;anchors.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Anchor peer update failed" echo "===================== Anchor peers for org \"$CORE_PEER_LOCALMSPID\" on \"$CHANNEL_NAME\" is updated successfully ===================== " sleep $DELAY echo&#125;## Sometimes Join takes time hence RETRY at least for 5 timesjoinChannelWithRetry () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG set -x peer channel join -b $CHANNEL_NAME.block &gt;&amp;log.txt res=$? set +x cat log.txt if [ $res -ne 0 -a $COUNTER -lt $MAX_RETRY ]; then COUNTER=` expr $COUNTER + 1` echo "peer$&#123;PEER&#125;.org$&#123;ORG&#125; failed to join the channel, Retry after $DELAY seconds" sleep $DELAY joinChannelWithRetry $PEER $ORG else COUNTER=1 fi verifyResult $res "After $MAX_RETRY attempts, peer$&#123;PEER&#125;.org$&#123;ORG&#125; has failed to Join the Channel"&#125;installChaincode () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG VERSION=$&#123;3:-1.0&#125; set -x peer chaincode install -n mycc -v $&#123;VERSION&#125; -l $&#123;LANGUAGE&#125; -p $&#123;CC_SRC_PATH&#125; &gt;&amp;log.txt res=$? set +x cat log.txt verifyResult $res "Chaincode installation on peer$&#123;PEER&#125;.org$&#123;ORG&#125; has Failed" echo "===================== Chaincode is installed on peer$&#123;PEER&#125;.org$&#123;ORG&#125; ===================== " echo&#125;instantiateChaincode () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG VERSION=$&#123;3:-1.0&#125; # while 'peer chaincode' command can get the orderer endpoint from the peer (if join was successful), # lets supply it directly as we know it using the "-o" option if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer chaincode instantiate -o orderer.example.com:7050 -C $CHANNEL_NAME -n mycc -l $&#123;LANGUAGE&#125; -v $&#123;VERSION&#125; -c '&#123;"Args":["init","a","100","b","200"]&#125;' -P "OR ('Org1MSP.member','Org2MSP.member')" &gt;&amp;log.txt res=$? set +x else set -x peer chaincode instantiate -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -l $&#123;LANGUAGE&#125; -v 1.0 -c '&#123;"Args":["init","a","100","b","200"]&#125;' -P "OR ('Org1MSP.member','Org2MSP.member')" &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Chaincode instantiation on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' failed" echo "===================== Chaincode Instantiation on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' is successful ===================== " echo&#125;upgradeChaincode () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG set -x peer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c '&#123;"Args":["init","a","90","b","210"]&#125;' -P "OR ('Org1MSP.peer','Org2MSP.peer','Org3MSP.peer')" res=$? set +x cat log.txt verifyResult $res "Chaincode upgrade on org$&#123;ORG&#125; peer$&#123;PEER&#125; has Failed" echo "===================== Chaincode is upgraded on org$&#123;ORG&#125; peer$&#123;PEER&#125; ===================== " echo&#125;chaincodeQuery () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG EXPECTED_RESULT=$3 echo "===================== Querying on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME'... ===================== " local rc=1 local starttime=$(date +%s) # continue to poll # we either get a successful response, or reach TIMEOUT while test "$(($(date +%s)-starttime))" -lt "$TIMEOUT" -a $rc -ne 0 do sleep $DELAY echo "Attempting to Query peer$&#123;PEER&#125;.org$&#123;ORG&#125; ...$(($(date +%s)-starttime)) secs" set -x peer chaincode query -C $CHANNEL_NAME -n mycc -c '&#123;"Args":["query","a"]&#125;' &gt;&amp;log.txt res=$? set +x test $res -eq 0 &amp;&amp; VALUE=$(cat log.txt | awk '/Query Result/ &#123;print $NF&#125;') test "$VALUE" = "$EXPECTED_RESULT" &amp;&amp; let rc=0 done echo cat log.txt if test $rc -eq 0 ; then echo "===================== Query on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' is successful ===================== " else echo "!!!!!!!!!!!!!!! Query result on peer$&#123;PEER&#125;.org$&#123;ORG&#125; is INVALID !!!!!!!!!!!!!!!!" echo "================== ERROR !!! FAILED to execute End-2-End Scenario ==================" echo exit 1 fi&#125;# fetchChannelConfig &lt;channel_id&gt; &lt;output_json&gt;# Writes the current channel config for a given channel to a JSON filefetchChannelConfig() &#123; CHANNEL=$1 OUTPUT=$2 setOrdererGlobals echo "Fetching the most recent configuration block for the channel" if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL --cafile $ORDERER_CA set +x else set -x peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL --tls --cafile $ORDERER_CA set +x fi echo "Decoding config block to JSON and isolating config to $&#123;OUTPUT&#125;" set -x configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; "$&#123;OUTPUT&#125;" set +x&#125;# signConfigtxAsPeerOrg &lt;org&gt; &lt;configtx.pb&gt;# Set the peerOrg admin of an org and signing the config updatesignConfigtxAsPeerOrg() &#123; PEERORG=$1 TX=$2 setGlobals 0 $PEERORG set -x peer channel signconfigtx -f "$&#123;TX&#125;" set +x&#125;# createConfigUpdate &lt;channel_id&gt; &lt;original_config.json&gt; &lt;modified_config.json&gt; &lt;output.pb&gt;# Takes an original and modified config, and produces the config update tx which transitions between the twocreateConfigUpdate() &#123; CHANNEL=$1 ORIGINAL=$2 MODIFIED=$3 OUTPUT=$4 set -x configtxlator proto_encode --input "$&#123;ORIGINAL&#125;" --type common.Config &gt; original_config.pb configtxlator proto_encode --input "$&#123;MODIFIED&#125;" --type common.Config &gt; modified_config.pb configtxlator compute_update --channel_id "$&#123;CHANNEL&#125;" --original original_config.pb --updated modified_config.pb &gt; config_update.pb configtxlator proto_decode --input config_update.pb --type common.ConfigUpdate &gt; config_update.json echo '&#123;"payload":&#123;"header":&#123;"channel_header":&#123;"channel_id":"'$CHANNEL'", "type":2&#125;&#125;,"data":&#123;"config_update":'$(cat config_update.json)'&#125;&#125;&#125;' | jq . &gt; config_update_in_envelope.json configtxlator proto_encode --input config_update_in_envelope.json --type common.Envelope &gt; "$&#123;OUTPUT&#125;" set +x&#125;chaincodeInvoke () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG # while 'peer chaincode' command can get the orderer endpoint from the peer (if join was successful), # lets supply it directly as we know it using the "-o" option if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer chaincode invoke -o orderer.example.com:7050 -C $CHANNEL_NAME -n mycc -c '&#123;"Args":["invoke","a","b","10"]&#125;' &gt;&amp;log.txt res=$? set +x else set -x peer chaincode invoke -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -c '&#123;"Args":["invoke","a","b","10"]&#125;' &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Invoke execution on peer$&#123;PEER&#125;.org$&#123;ORG&#125; failed " echo "===================== Invoke transaction on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' is successful ===================== " echo&#125; 链代码内容链代码实现的主要方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177package mainimport ( "fmt" "strconv" "github.com/hyperledger/fabric/core/chaincode/shim" pb "github.com/hyperledger/fabric/protos/peer")// SimpleChaincode example simple Chaincode implementationtype SimpleChaincode struct &#123;&#125;func (t *SimpleChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response &#123; fmt.Println("ex02 Init") _, args := stub.GetFunctionAndParameters() var A, B string // Entities var Aval, Bval int // Asset holdings var err error if len(args) != 4 &#123; return shim.Error("Incorrect number of arguments. Expecting 4") &#125; // Initialize the chaincode A = args[0] Aval, err = strconv.Atoi(args[1]) if err != nil &#123; return shim.Error("Expecting integer value for asset holding") &#125; B = args[2] Bval, err = strconv.Atoi(args[3]) if err != nil &#123; return shim.Error("Expecting integer value for asset holding") &#125; fmt.Printf("Aval = %d, Bval = %d\n", Aval, Bval) // Write the state to the ledger err = stub.PutState(A, []byte(strconv.Itoa(Aval))) if err != nil &#123; return shim.Error(err.Error()) &#125; err = stub.PutState(B, []byte(strconv.Itoa(Bval))) if err != nil &#123; return shim.Error(err.Error()) &#125; return shim.Success(nil)&#125;func (t *SimpleChaincode) Invoke(stub shim.ChaincodeStubInterface) pb.Response &#123; fmt.Println("ex02 Invoke") function, args := stub.GetFunctionAndParameters() if function == "invoke" &#123; // Make payment of X units from A to B return t.invoke(stub, args) &#125; else if function == "delete" &#123; // Deletes an entity from its state return t.delete(stub, args) &#125; else if function == "query" &#123; // the old "Query" is now implemtned in invoke return t.query(stub, args) &#125; return shim.Error("Invalid invoke function name. Expecting \"invoke\" \"delete\" \"query\"")&#125;// Transaction makes payment of X units from A to Bfunc (t *SimpleChaincode) invoke(stub shim.ChaincodeStubInterface, args []string) pb.Response &#123; var A, B string // Entities var Aval, Bval int // Asset holdings var X int // Transaction value var err error if len(args) != 3 &#123; return shim.Error("Incorrect number of arguments. Expecting 3") &#125; A = args[0] B = args[1] // Get the state from the ledger // TODO: will be nice to have a GetAllState call to ledger Avalbytes, err := stub.GetState(A) if err != nil &#123; return shim.Error("Failed to get state") &#125; if Avalbytes == nil &#123; return shim.Error("Entity not found") &#125; Aval, _ = strconv.Atoi(string(Avalbytes)) Bvalbytes, err := stub.GetState(B) if err != nil &#123; return shim.Error("Failed to get state") &#125; if Bvalbytes == nil &#123; return shim.Error("Entity not found") &#125; Bval, _ = strconv.Atoi(string(Bvalbytes)) // Perform the execution X, err = strconv.Atoi(args[2]) if err != nil &#123; return shim.Error("Invalid transaction amount, expecting a integer value") &#125; Aval = Aval - X Bval = Bval + X fmt.Printf("Aval = %d, Bval = %d\n", Aval, Bval) // Write the state back to the ledger err = stub.PutState(A, []byte(strconv.Itoa(Aval))) if err != nil &#123; return shim.Error(err.Error()) &#125; err = stub.PutState(B, []byte(strconv.Itoa(Bval))) if err != nil &#123; return shim.Error(err.Error()) &#125; return shim.Success(nil)&#125;// Deletes an entity from statefunc (t *SimpleChaincode) delete(stub shim.ChaincodeStubInterface, args []string) pb.Response &#123; if len(args) != 1 &#123; return shim.Error("Incorrect number of arguments. Expecting 1") &#125; A := args[0] // Delete the key from the state in ledger err := stub.DelState(A) if err != nil &#123; return shim.Error("Failed to delete state") &#125; return shim.Success(nil)&#125;// query callback representing the query of a chaincodefunc (t *SimpleChaincode) query(stub shim.ChaincodeStubInterface, args []string) pb.Response &#123; var A string // Entities var err error if len(args) != 1 &#123; return shim.Error("Incorrect number of arguments. Expecting name of the person to query") &#125; A = args[0] // Get the state from the ledger Avalbytes, err := stub.GetState(A) if err != nil &#123; jsonResp := "&#123;\"Error\":\"Failed to get state for " + A + "\"&#125;" return shim.Error(jsonResp) &#125; if Avalbytes == nil &#123; jsonResp := "&#123;\"Error\":\"Nil amount for " + A + "\"&#125;" return shim.Error(jsonResp) &#125; jsonResp := "&#123;\"Name\":\"" + A + "\",\"Amount\":\"" + string(Avalbytes) + "\"&#125;" fmt.Printf("Query Response:%s\n", jsonResp) return shim.Success(Avalbytes)&#125;func main() &#123; err := shim.Start(new(SimpleChaincode)) if err != nil &#123; fmt.Printf("Error starting Simple chaincode: %s", err) &#125;&#125;]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ca 与 MSP 的介绍]]></title>
    <url>%2F2019%2F04%2F03%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2FCa%20%E4%B8%8E%20MSP%20%E7%9A%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[关于加密的介绍对称加密（共享密钥加密）在对称加密算法中，加密使用的密钥和解密使用的密钥是相同的。也就是说，加密和解密都是使用的同一个密钥。因此对称加密算法要保证安全性的话，密钥要做好保密，只能让使用的人知道，不能对外公开。这个和上面的公钥密码体制有所不同，公钥密码体制中加密是用公钥，解密使用私钥，而对称加密算法中，加密和解密都是使用同一个密钥，不区分公钥和私钥。 非对称加密（公开密钥加密 ）在非对称加密算法中，加密使用的密钥和解密使用的密钥是不相同的。也就是说加密使用的密钥和解密使用的密钥不同，因此它是一个非对称加密算法。 数字签名类似在纸质合同上进行签名以确认合同内容和证明身份，数字签名既可以证实某数字内容的完整性，又可以确认其来源 一个典型的场景是，Alice 通过信道发给 Bob 一个文件（一份信息），Bob 如何获知所收到的文件即为 Alice 发出的原始版本？Alice 可以先对文件内容进行摘要，然后用自己的私钥对摘要进行加密（签名），之后同时将文件和签名都发给 Bob。Bob 收到文件和签名后，用 Alice 的公钥来解密签名，得到数字摘要，与对文件进行摘要后的结果进行比对。如果一致，说明该文件确实是 Alice 发过来的（因为别人无法拥有 Alice 的私钥），并且文件内容没有被修改过（摘要结果一致）。 什么是RSA算法RSA是一种非对称加密算法，现在使用得很广泛。RSA公开密钥密码体制。所谓的公开密钥密码体制就是使用不同的加密密钥与解密密钥，是一种“由已知加密密钥推导出解密密钥在计算上是不可行的”密码体制。 公钥加密的内容可以并且只能由私钥进行解密，并且由私钥加密的内容可以并且只能由公钥进行解密。也就是说，RSA的这一对公钥、私钥都可以用来加密和解密，并且一方加密的内容可以由并且只能由对方进行解密。 X.509证书X.509证书标准 X.509是常见通用的证书格式，定义了数字证书的规范，专门定义了在目录访问中需要身份认证的证书的格式。所有的证书都符合为Public Key Infrastructure (PKI) 制定的 ITU-T X509 国际标准。 X.509 DER 编码(ASCII)的后缀是： .DER .CER .CRTX.509 PAM 编码(Base64)的后缀是： .PEM .CER .CRT X.509 - 这是一种证书标准,主要定义了证书中应该包含哪些内容.其详情可以参考RFC5280,SSL使用的就是这种证书标准. X.509编码格式 同样的X.509证书,可能有不同的编码格式,目前有以下两种编码格式.PEM - Privacy Enhanced Mail,打开看文本格式,以”—–BEGIN…”开头, “—–END…”结尾,内容是BASE64编码.查看PEM格式证书的信息:openssl x509 -in certificate.pem -text -nooutApache和 NIX服务器偏向于使用这种编码格式.DER - Distinguished Encoding Rules,打开看是二进制格式,不可读.查看DER格式证书的信息:openssl x509 -in certificate.der -inform der -text -nooutJava和Windows服务器偏向于使用这种编码格式. X.509相关的文件扩展名 这是比较误导人的地方,虽然我们已经知道有PEM和DER这两种编码格式,但文件扩展名并不一定就叫”PEM”或者”DER”,常见的扩展名除了PEM和DER还有以下这些,它们除了编码格式可能不同之外,内容也有差别,但大多数都能相互转换编码格式.CRT - CRT应该是certificate的三个字母,其实还是证书的意思,常见于NIX系统,有可能是PEM编码,也有可能是DER编码,大多数应该是PEM编码,相信你已经知道怎么辨别.CER - 还是certificate,还是证书,常见于Windows系统,同样的,可能是PEM编码,也可能是DER编码,大多数应该是DER编码.KEY - 通常用来存放一个公钥或者私钥,并非X.509证书,编码同样的,可能是PEM,也可能是DER.查看KEY的办法:openssl rsa -in mykey.key -text -noout如果是DER格式的话,同理应该这样了:openssl rsa -in mykey.key -text -noout -inform derCSR - Certificate Signing Request PFX/P12 - predecessor of PKCS#12,对nix服务器来说,一般CRT和KEY是分开存放在不同文件中的,但Windows的IIS则将它们存在一个PFX文件中,(因此这个文件包含了证书及私钥)这样会不会不安全？应该不会,PFX通常会有一个”提取密码”,你想把里面的东西读取出来的话,它就要求你提供提取密码,PFX使用的时DER编码,如何把PFX转换为PEM编码？openssl pkcs12 -in for-iis.pfx -out for-iis.pem -nodes这个时候会提示你输入提取代码. for-iis.pem就是可读的文本.生成pfx的命令类似这样:openssl pkcs12 -export -out certificate.pfx -inkey privateKey.key -in certificate.crt -certfile CACert.crt其中CACert.crt是CA(权威证书颁发机构)的根证书,有的话也通过-certfile参数一起带进去.这么看来,PFX其实是个证书密钥库.*JKS** - 即Java Key Storage,这是Java的专利,跟OpenSSL关系不大,利用Java的一个叫”keytool”的工具,可以将PFX转为JKS,当然了,keytool也能直接生成JKS,不过在此就不多表了. 证书编码的转换 PEM转为DER openssl x509 -in cert.crt -outform der -out cert.derDER转为PEM openssl x509 -in cert.crt -inform der -outform pem -out cert.pem(提示:要转换KEY文件也类似,只不过把x509换成rsa,要转CSR的话,把x509换成req…) PKCS 标准PKCS是由美国RSA数据安全公司及其合作伙伴制定的一组公钥密码学标准，其中包括证书申请、证书更新、证书作废表发布、扩展证书内容以及数字签名、数字信封的格式等方面的一系列相关协议 可以理解为对数字证书的特定封装形式，不同形式的封装用在不同的使用场合； 常用PKCS标准 PKCS 目前共发布过 15 个标准。 常用的有：PKCS#7 Cryptographic Message Syntax Standard ：定义一种通用的消息语法，包括数字签名和加密等用于增强的加密机制，PKCS#7与PEM兼容，所以不需其他密码操作，就可以将加密的消息转换成PEM消息。PKCS#10 Certification Request Standard：描述证书请求语法。PKCS#12 Personal Information Exchange Syntax Standard：描述个人信息交换语法标准。描述了将用户公钥、私钥、证书和其他相关信息打包的语法。 关于CA的介绍什么是证书“证书”洋文也叫“digital certificate”或“public key certificate”。 它是用来证明某某东西确实是某某东西的东西（是不是像绕口令？）。通俗地说，证书就好比例子里面的公章。通过公章，可以证明该介绍信确实是对应的公司发出的。 理论上，人人都可以找个证书工具，自己做一个证书。那如何防止坏人自己制作证书出来骗人捏？请看后续 CA 的介绍。 什么是CACA是Certificate Authority的缩写，也叫“证书授权中心”。 它是负责管理和签发证书的第三方机构，就好比例子里面的中介——C 公司。一般来说，CA必须是所有行业和所有公众都信任的、认可的。因此它必须具有足够的权威性。就好比A、B两公司都必须信任C公司，才会找 C 公司作为公章的中介。 什么是CA证书CA 证书，顾名思义，就是CA颁发的证书。 前面已经说了，人人都可以找工具制作证书。但是你一个小破孩制作出来的证书是没啥用处的。因为你不是权威的CA机关，你自己搞的证书不具有权威性。 这就好比上述的例子里，某个坏人自己刻了一个公章，盖到介绍信上。但是别人一看，不是受信任的中介公司的公章，就不予理睬。坏蛋的阴谋就不能得逞啦。 什么是根证书普通的证书一般包括三部分：用户信息，用户公钥，以及CA签名 那么我们要验证这张证书就需要验证CA签名的真伪。那么就需要CA公钥。而CA公钥存在于另外一张证书（称这张证书是对普通证书签名的证书）中。因此又需要验证另外一张证书（称这张证书是对另外一张证书签名的证书）的真伪。依次往下回溯，就得到一条证书链。那么这张证书链从哪里结束呢？就是在根证书结束（即验证到根证书结束）。根证书是个很特别的证书，它是CA中心自己给自己签名的证书（即这张证书是用CA公钥对这张证书进行签名）。信任这张证书，就代表信任这张证书下的证书链。 所有用户在使用自己的证书之前必须先下载根证书。 所谓根证书验证就是：用根证书公钥来验证该证书的颁发者签名。所以首先必须要有根证书，并且根证书必须在受信任的证书列表（即信任域）。 证书签发流程 向权威证书颁发机构申请证书， 把本地生成的申请证书(包含公钥)、组织信息、个人信息等 交给权威证书颁发机构,权威证书颁发机构对此进行签名,完成.（保留好csr,当权威证书颁发机构颁发的证书过期的时候,你还可以用同样的csr来申请新的证书,key保持不变.） CA 通过线上、线下等多种手段验证申请者提供信息的真实性，如组织是否存在、企业是否合法，是否拥有域名的所有权等； 如信息审核通过，CA 会向申请者签发认证文件-证书。证书包含以下信息：申请者公钥、申请者的组织信息和个人信息、签发机构 CA 的信息、有效时间、证书序列号等信息的明文，同时包含一个签名； 签名的产生算法：首先，使用散列函数计算公开的明文信息的信息摘要，然后，采用 CA 的私钥对信息摘要进行加密，密文即签名； 客户端证书验证流程 客户端请求服务端 ​ 客户端向服务端发送请求，服务端将自己的证书和用自己私钥加密的原文， 以及原文的摘要一并返回。 数字证书有效性验证​ 客户端 C 读取证书中的相关的明文信息，采用相同的散列函数计算得到信息摘要，然后，利用对应 CA 的公钥解密签名数据，对比证书的信息摘要，如果一致，则可以确认证书的合法性，即公钥合法； ​ 客户端然后验证证书相关的域名信息、有效时间等信息； 根证书验证​ 客户端会内置信任 CA 的证书信息(包含公钥)，如果CA不被信任，则找不到对应 CA 的证书，证书也会被判定非法。 CRL验证​ CRL是经过CA签名的证书作废列表，用于证书冻结和撤销。一般来说证书中有CRL地址，供HTTP或者LDAP方式访问，通过解析可得到CRL地址，然后下载CRL进行验证。​ 并且证书中有CRL生效日期以及下次更新的日期，因此CRL是自动更新的，因此会有延迟性。​ 于是呢，还有另外一种方式OSCP证书状态在线查询，可以即时的查询证书状态。 用https来说明CA认证的流程SSL 介绍SSL SSL - Secure Sockets Layer,现在应该叫”TLS”,但由于习惯问题,我们还是叫”SSL”比较多.http协议默认情况下是不加密内容的,这样就很可能在内容传播的时候被别人监听到,对于安全性要求较高的场合,必须要加密,https就是带加密的http协议,而https的加密是基于SSL的,它执行的是一个比较下层的加密,也就是说,在加密前,你的服务器程序在干嘛,加密后也一样在干嘛,不用动,这个加密对用户和开发者来说都是透明的 SSL是基于非对称加密的原理，在这之上还进行了对称加密的数据传输。当传送数据量过大的时候，客户端和服务器之间互相商定了一个对话密钥（session key），使用这个对话密钥来进行对称加密加快运算速度， 所以说SSL是基于RSA进行的数据传输上的优化，可以加速加密运算速度。 SSL 应用了RSA ， 数字签名，非对称加密等技术，解决了网络通讯被监听，伪装和篡改等问题，一般企业级应用，现在离不开SSL技术； OpenSSL OpenSSL - 简单地说,OpenSSL是SSL的一个实现,SSL只是一种规范.理论上来说,SSL这种规范是安全的,目前的技术水平很难破解,但SSL的实现就可能有些漏洞,如著名的”心脏出血”.OpenSSL还提供了一大堆强大的工具软件,强大到90%我们都用不到. http通信存在的问题 容易被监听 http通信都是明文，数据在客户端与服务器通信过程中，任何一点都可能被劫持。比如，发送了银行卡号和密码，hacker劫取到数据，就能看到卡号和密码，这是很危险的 被伪装 http通信时，无法保证通行双方是合法的，通信方可能是伪装的。比如你请求www.taobao.com,你怎么知道返回的数据就是来自淘宝，中间人可能返回数据伪装成淘宝。 被篡改 hacker中间篡改数据后，接收方并不知道数据已经被更改 https解决的问题https很好的解决了http的三个缺点（被监听、被篡改、被伪装），https不是一种新的协议，它是http+SSL(TLS)的结合体，SSL是一种独立协议，所以其它协议比如smtp等也可以跟ssl结合。https改变了通信方式，它由以前的http—–&gt;tcp，改为http——&gt;SSL—–&gt;tcp；https采用了共享密钥加密+公开密钥加密的方式 防监听 数据是加密的，所以监听得到的数据是密文，hacker看不懂。 防伪装 伪装分为客户端伪装和服务器伪装，通信双方携带证书，证书相当于身份证，有证书就认为合法，没有证书就认为非法，证书由第三方颁布，很难伪造 防篡改 https对数据做了摘要，篡改数据会被感知到。hacker即使从中改了数据也白搭。 https认证的详细流程 客户端发起HTTPS请求 这个没什么好说的，就是用户在浏览器里输入一个https网址，然后连接到server的443端口。 服务端的配置** 采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请。区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl就是个不错的选择，有1年的免费服务)。这套证书其实就是一对公钥和私钥。如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。 传送证书 服务端将证书发送给客户端，这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。 客户端解析证书 这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。如果证书没有问题，那么就生成一个随机值。然后用证书对该随机值进行加密。就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。 传送加密信息 这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。 服务段解密信息 服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密。所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。 传输加密后的信息 这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。 客户端解密信息 客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容。整个过程第三方即使监听到了数据，也束手无策。 总结整个过程： ​ 1）服务器向CA机构获取证书（假设这个证书伪造不了），当浏览器首次请求服务器的时候，服务器返回证书给浏览器。（证书包含：公钥+申请者与颁发者的相关信息+签名） ​ 2）浏览器得到证书后，开始验证证书的相关信息，证书有效（没过期等）。（验证过程，比较复杂，详见上文）。 ​ 3）验证完证书后，如果证书有效，客户端是生成一个随机数，然后用证书中的公钥进行加密，加密后，发送给服务器，服务器用私钥进行解密，得到随机数。之后双方便开始用该随机数作为钥匙，对要传递的数据进行加密、解密。 后续的问题 怎样保证公开密钥的有效性 你也许会想到，怎么保证客户端收到的公开密钥是合法的，不是伪造的，证书很好的完成了这个任务。证书由权威的第三方机构颁发，并且对公开密钥做了签名。 https的缺点 https保证了通信的安全，但带来了加密解密消耗计算机cpu资源的问题 ，不过，有专门的https加解密硬件服务器 各大互联网公司，百度、淘宝、支付宝、知乎都使用https协议，为什么？ 支付宝涉及到金融，所以出于安全考虑采用https这个，可以理解，为什么百度、知乎等也采用这种方式？为了防止运营商劫持！http通信时，运营商在数据中插入各种广告，用户看到后，怒火发到互联网公司，其实这些坏事都是运营商(移动、联通、电信)干的,用了https，运营商就没法插播广告篡改数据了。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric SDK 配置文件]]></title>
    <url>%2F2019%2F04%2F02%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F2.Fabric%20SDK%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[一、Fabric SDK配置Fabric区块链网络应用程序需要大量的参数，用于连接Fabric区块链网络。通常将Fabric区块链网络应用程序所需的参数放到一个配置文件进行管理，配置文件定义Fabric SDK Go的配置和用户自定义参数，指定了连接Fabric区块链网络所需的全部信息，例如Fabric区块链网络组件的主机名和端口等。Fabric SDK GO为应用程序提供的配置文件通常使用yaml文件格式编写，并命名为config.yaml，配置文件会在应用程序代码中被读取。Fabric SDK Go版本提供了config.yaml模板，开发者可以参考fabric-sdk-go/pkg/core/config/testdata/template/config.yaml，也可以根据fabric-sdk-go/test/fixtures/config/config_e2e.yaml实例进行改写。 二、version定义version用于定义config.yaml文件内容的版本，Fabric SDK Go会使用version匹配相应的解析规则。version: 1.0.0 三、channels定义channels部分描述已经存在的通道信息，每个通道包含哪些orderer、peer 。peer部分可以定义peer节点的角色属性，角色如下：endorsingPeer：可选。peer节点节点必须安装链码。peer节点是否会被发送交易提案进行背书。应用程序也可以使用本属性来决定发送链码安装请求到哪个peer节点。默认值：true。chaincodeQuery：可选。peer节点必须安装链码。peer节点是否会被发送交易提案只进行查询。应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true。ledgerQuery：可选。是否可以向peer节点发送不会进行链码查询的提案，如queryBlock（），queryTransaction（）等。默认值：true。eventSource：可选。peer节点是否为SDK事件监听器注册的目标，所有的peer节点都可以生产事件，但通常只需要连接一个对事件进行监听。默认值：true。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# 如果应用程序创建了通道，不需要本部分channels: # 如果没有定义channel配置或是定义的channel没有信息，使用默认channel # 如果channel没有定义peers，使用默认channel的peers # 如果channel没有定义orderes，使用默认channel的orderes # 如果channel没有定义背书策略，使用默认channel的背书策略 # 如果channel定义了背书策略，但某些背书策略缺失，缺失的背书策略会被默认channel填充 _default: # 可选，参与组织的peers列表 peers: peer1.org1.example.com: endorsingPeer: true chaincodeQuery: true ledgerQuery: true eventSource: true # 可选，应用程序使用下列选项执行通道操作，如获取通道配置 policies: #可选，获取通道配置区块 queryChannelConfig: #可选，成功响应节点的最小数量 minResponses: 1 # 可选 maxTargets: 1 # 可选，查询配置区块的重试选项 retryOpts: # 可选，重试次数 attempts: 5 # 可选， 第一次重试的后退间隔 initialBackoff: 500ms # 可选， 重试的最大后退间隔 maxBackoff: 5s backoffFactor: 2.0 # 可选，获取发现信息选项 discovery: maxTargets: 2 # 重试选项 retryOpts: # 可选，重试次数 attempts: 4 initialBackoff: 500ms maxBackoff: 5s backoffFactor: 2.0 # 可选，事件服务选项 eventService: # 可选 resolverStrategy指定连接到peer节点时选择peer节点的决策策略 # 可选值:PreferOrg（默认）, MinBlockHeight, Balanced # PreferOrg: # 基于区块高度滞后阀值决定哪些peer节点是合适的， 虽然会在当前组织中优先选择peer节点 # 如果当前组织中没有合适的peer节点，会从其它组织中选取 # MinBlockHeight: # 根据区块高度滞后阀值选择最好的peer节点， # 所有peer节点的最大区块高度是确定的，区块高度小于最大区块高度但在滞后阀值范围内的peer节点会被加载， # 其它peer节点不会被考虑。 # Balanced: # 使用配置的balancer选择peer节点 resolverStrategy: PreferOrg # 可选 balancer是选择连接到peer节点的负载均衡器 # 可选值: Random (default), RoundRobin balancer: Random # 可选，blockHeightLagThreshold设置区块高度滞后阀值，用于选择连接到的peer节点 # 如果一个peer节点滞后于最新的peer节点给定的区块数，会被排除在选择之外 # 注意：当minBlockHeightResolverMode设置为ResolveByThreshold时，本参数才可用 # 默认: 5 blockHeightLagThreshold: 5 # 可选，reconnectBlockHeightLagThreshold - 如果peer节点的区块高度落后于指定的区块数量， # 事件客户端会从peer节点断开，重新连接到一个性能更好的peer节点 # 如果peerMonitor设置为启用（默认），本参数才可用 # 默认值: 10 # 注意：设置值太低会导致事件客户端频繁断开或重连，影响性能 reconnectBlockHeightLagThreshold: 8 # 可选， peerMonitorPeriod是事件客户端从连接节点断开重新连接到另外一个节点的时间 # 默认: 对于Balanced resolverStrategy禁用，为0; 对于PreferOrg和MinBlockHeight为5s peerMonitorPeriod: 6s #如果_default没有定义，必选；如果_default已经定义，可选。 # 通道名称 assetchannel: # 如果_default peers没有定义，必选；如果_default peers已经定义，可选。 # 参与组织的peer节点列表 peers: peer0.org1.example.com: # 可选。peer节点是否会被发送交易提议只进行查询。peer节点必须安装链码。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true endorsingPeer: true # 可选。peer节点必须安装链码。peer节点是否会被发送交易提案只进行查询。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true chaincodeQuery: true # 可选。是否可以向peer节点发送不会进行链码查询的提案，如queryBlock（）， # queryTransaction（）等。默认值：true。 ledgerQuery: true # 可选。peer节点是否为SDK事件监听器注册的目标，所有的peer节点都可以生产事件， # 但通常只需要连接一个对事件进行监听。默认值：true。 eventSource: true 四、organizations定义organizations描述peer节点所属的组织(org)，证书颁发机构，以及组织的MSP ID。 123456789101112131415161718192021222324252627282930# Fabric区块链网络中参与的组织列表organizations: org1: mspid: Org1MSP # 组织的MSP存储位置，绝对路径或相对cryptoconfig的路径 cryptoPath: peerOrganizations/org1.example.com/users/&#123;username&#125;@org1.example.com/msp peers: - peer0.org1.example.com - peer1.org1.example.com # 可选，证书颁发机构签发×××明，Fabric-CA是一个特殊的证书管理机构，提供REST API支持动态证书管理，如登记、撤销、重新登记 # 下列部分只为Fabric-CA服务器设置 certificateAuthorities: - ca.org1.example.com org2: mspid: Org2MSP # 组织的MSP存储位置，相对于cryptoconfig的相对位置或绝对路径 cryptoPath: peerOrganizations/org2.example.com/users/&#123;username&#125;@org2.example.com/msp peers: - peer0.org2.example.com certificateAuthorities: - ca.org2.example.com # Orderer组织名称 ordererorg: # 组织的MSPID mspID: OrdererMSP # 加载用户需要的密钥和证书，绝对路径或相对路径 cryptoPath: ordererOrganizations/example.com/users/&#123;username&#125;@example.com/msp 五、orderers定义orderers必须指定要连接的Hyperledger Fabric区块链网络中所有orderder节点的主机名和端口。orderers对象可以包含多个orderder节点。 1234567891011121314151617181920# 发送交易请求或通道创建、更新请求到的orderers列表# 如果定义了超过一个orderer，SDK使用哪一个orderer由代码实现时指定orderers: # orderer节点，可以定义多个 orderer.example.com: url: orderer.example.com:7050 # 以下属性由gRPC库定义，会被传递给gRPC客户端构造函数 grpcOptions: ssl-target-name-override: orderer.example.com # 下列参数用于设置服务器上的keepalive策略，不兼容的设置会导致连接关闭 # 当keep-alive-time被设置为0或小于激活客户端的参数，下列参数失效 keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/ordererOrganizations/example.com/tlsca/tlsca.example.com-cert.pem 六、peers定义peers必须指定Hyperledger Fabric区块链网络中所有peer节点的主机名和端口，可能会在其它地方引用，如channels，organizations等部分。 123456789101112131415161718# peers节点列表peers: # peer节点定义，可以定义多个 peer0.org1.example.com: # URL用于发送背书和查询请求 url: peer0.org1.example.com:7051 grpcOptions: ssl-target-name-override: peer0.org1.example.com keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem 七、certificateAuthorities定义certificateAuthorities必须在Hyperledger Fabric区块链网络中指定证书颁发机构（CA）的主机名和端口，以便用于注册现有用户和注册新用户。 12345678910111213141516171819202122# Fabric-CA是Hyperledger Fabric提供了特殊的证书颁发机构，可以通过REST API管理证书。# 应用程序可以选择使用一个标准的证书颁发机构代替Fabric-CA，此时本部分不需要指定certificateAuthorities: # CA机构，可以定义多个 ca.org1.example.com: url: https://ca.org1.example.com:7054 tlsCACerts: # Comma-Separated list of paths path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem # 客户端和Fabric CA进行SSL握手的密钥和证书 client: key: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.key cert: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.crt # Fabric-CA支持通过REST API进行动态用户注册 registrar: enrollId: admin enrollSecret: adminpw # 可选，CA机构名称 caName: ca.org1.example.com 八、clientclient部分必需定义，客户端应用程序代表谁来和Fabric区块链网络来交互，可以定义超时选项。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#客户端定义client: # 客户端所属的组织，必须是organizations定义的组织 organization: org1 #定义日志服务 logging: level: debug #debug级别 # peer、事件服务、orderer超时的全局配置 # 本部分如果忽略，使用下列的值作为默认值 peer: timeout: connection: 10s response: 180s discovery: greylistExpiry: 10s eventService: timeout: registrationResponse: 15s orderer: timeout: connection: 15s response: 15s global: timeout: query: 180s execute: 180s resmgmt: 180s cache: connectionIdle: 30s eventServiceIdle: 2m channelConfig: 30m channelMembership: 30s discovery: 10s selection: 10m # MSP根目录 cryptoconfig: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric/AssetExchange/deploy/crypto-config # 某些SDK支持插件化的KV数据库, 通过指定credentialStore属性实现 credentialStore: # 可选，用于用户证书材料存储，如果所有的证书材料被嵌入到配置文件，则不需要 path: &quot;/tmp/state-store&quot; # 可选，指定Go SDK实现的CryptoSuite实现 cryptoStore: # 指定用于加密密钥存储的底层KV数据库 path: /tmp/msp # 客户端的BCCSP模块配置 BCCSP: security: enabled: true default: provider: &quot;SW&quot; hashAlgorithm: &quot;SHA2&quot; softVerify: true level: 256 tlsCerts: # 可选，当连接到peers，orderes时使用系统证书池，默认为false systemCertPool: true # 可选，客户端和peers与orderes进行TLS握手的密钥和证书 client: key: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.key cert: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.crt 九、config.yaml示例资产交易平台应用的配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170# Copyright SecureKey Technologies Inc. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0### The network connection profile provides client applications the information about the target# blockchain network that are necessary for the applications to interact with it. These are all# knowledge that must be acquired from out-of-band sources. This file provides such a source.name: &quot;assetchannel&quot;## Describe what the target network is/does.#description: &quot;asset exchange network&quot;#指定版本version: 1.0.0#客户端定义client: # 客户端所属的组织，必须是organizations定义的组织 organization: org1 #定义日志服务 logging: level: debug #debug级别 # MSP根目录 cryptoconfig: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric/AssetExchange/deploy/crypto-config # 某些SDK支持插件化的KV数据库, 通过指定credentialStore属性实现 credentialStore: # 可选，用于用户证书材料存储，如果所有的证书材料被嵌入到配置文件，则不需要 path: &quot;/tmp/state-store&quot; # 可选，指定Go SDK实现的CryptoSuite实现 cryptoStore: # 指定用于加密密钥存储的底层KV数据库 path: /tmp/msp # 客户端的BCCSP模块配置 BCCSP: security: enabled: true default: provider: &quot;SW&quot; hashAlgorithm: &quot;SHA2&quot; softVerify: true level: 256 tlsCerts: # 可选，当连接到peers，orderes时使用系统证书池，默认为false systemCertPool: true # 可选，客户端和peers与orderes进行TLS握手的密钥和证书 client: key: # path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.key cert: #path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.crt# 如果应用程序创建了通道，不需要本部分channels: # 如果没有定义channel配置或是定义的channel没有信息，使用默认channel # 如果channel没有定义peers，使用默认channel的peers # 如果channel没有定义orderes，使用默认channel的orderes # 如果channel没有定义背书策略，使用默认channel的背书策略 # 如果channel定义了背书策略，但某些背书策略缺失，缺失的背书策略会被默认channel填充 #如果_default没有定义，必选；如果_default已经定义，可选。 # 通道名称 assetchannel: # 如果_default peers没有定义，必选；如果_default peers已经定义，可选。 # 参与组织的peer节点列表 peers: peer0.org1.example.com: # 可选。peer节点是否会被发送交易提议只进行查询。peer节点必须安装链码。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true endorsingPeer: true # 可选。peer节点必须安装链码。peer节点是否会被发送交易提案只进行查询。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true chaincodeQuery: true # 可选。是否可以向peer节点发送不会进行链码查询的提案，如queryBlock（）， # queryTransaction（）等。默认值：true。 ledgerQuery: true # 可选。peer节点是否为SDK事件监听器注册的目标，所有的peer节点都可以生产事件， # 但通常只需要连接一个对事件进行监听。默认值：true。 eventSource: true # 可选，应用程序使用下列选项执行通道操作，如获取通道配置 policies: #可选，获取通道配置区块 queryChannelConfig: #可选，成功响应节点的最小数量 minResponses: 1 # 可选 maxTargets: 1 # 可选，查询配置区块的重试选项 retryOpts: # 可选，重试次数 attempts: 5 # 可选， 第一次重试的后退间隔 initialBackoff: 500ms # 可选， 重试的最大后退间隔 maxBackoff: 5s backoffFactor: 2.0# Fabric区块链网络中参与的组织列表organizations: org1: mspid: Org1MSP # 组织的MSP存储位置，绝对路径或相对cryptoconfig的路径 cryptoPath: peerOrganizations/org1.example.com/users/&#123;username&#125;@org1.example.com/msp peers: - peer0.org1.example.com - peer1.org1.example.com # 可选，证书颁发机构签发×××明，Fabric-CA是一个特殊的证书管理机构，提供REST API支持动态证书管理，如登记、撤销、重新登记 # 下列部分只为Fabric-CA服务器设置 certificateAuthorities: #- ca.org1.example.com # Orderer组织名称 ordererorg: # 组织的MSPID mspID: OrdererMSP # 加载用户需要的密钥和证书，绝对路径或相对路径 cryptoPath: ordererOrganizations/example.com/users/&#123;username&#125;@example.com/msp# 发送交易请求或通道创建、更新请求到的orderers列表# 如果定义了超过一个orderer，SDK使用哪一个orderer由代码实现时指定orderers: # orderer节点，可以定义多个 orderer.example.com: url: orderer.example.com:7050 # 以下属性由gRPC库定义，会被传递给gRPC客户端构造函数 grpcOptions: ssl-target-name-override: orderer.example.com # 下列参数用于设置服务器上的keepalive策略，不兼容的设置会导致连接关闭 # 当keep-alive-time被设置为0或小于激活客户端的参数，下列参数失效 keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 # path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/ordererOrganizations/example.com/tlsca/tlsca.example.com-cert.pem# peers节点列表peers: # peer节点定义，可以定义多个 peer0.org1.example.com: # URL用于发送背书和查询请求 url: peer0.org1.example.com:7051 grpcOptions: ssl-target-name-override: peer0.org1.example.com keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 #path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric网络搭建]]></title>
    <url>%2F2019%2F03%2F21%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F1.Fabric%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[环境搭建Docker 安装123curl -fsSl &lt;https://get.docker.com/&gt; | shsystemctl enable dockerservice docker start Docker-compose 安装1yum -y install epel-release &amp;&amp; yum -y install python-pip &amp;&amp; pip install docker-compose Go安装123456789wget https://dl.google.com/go/go1.11.5.linux-amd64.tar.gztar -C /usr/local -zxvf go1.11.5.linux-amd64.tar.gzecho &quot;export GOROOT=/usr/local/go&quot; &gt;&gt; /etc/profileecho &quot;export GOBIN=/opt/prod/go/bin&quot; &gt;&gt; /etc/profileecho &quot;export GOPATH=/opt/prod/go&quot; &gt;&gt; /etc/profileecho &quot;export PATH=/opt/prod/go/bin:$PATH&quot; &gt;&gt; /etc/profileecho &quot;export PATH=/usr/local/go/bin:$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilemkdir /opt/prod/go 下载Fabric组件bootstrap.sh文件可以通过网络下载 12curl -sSL https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh | bash 1.4.0 -s # 下载Fabric二进制文件 Image 以及示例文件cp fabric-sample/bin/* $/GOPATH/bin Fabric-sdk-go下载github地址: https://github.com/hyperledger/fabric-sdk-go.git sdk 编译， 最好在外网进行编译，用VPN容易超时 12make pupulatemake depend 网络配置创建目录结构创建一个目录用来存储Fabric的所有配置文件 12mkdir -p /opt/prod/Fabric/network # 存储Fabric所有配置文件mkdir -p /opt/prod/Fabric/network/channel-artifacts # 存放创世区块以及通道配置文件等 生成MSP创建MSP配置信息编辑MSP配置文件 crypto-config.yaml 12345678910111213141516171819202122# 排序节点配置OrdererOrgs: - Name: Orderer # 排序节点名称 Domain: myfab.com # 排序节点域名 Specs: - Hostname: orderer # 排序节点HOST PeerOrgs: - Name: Org1 # 节点名称 Domain: org1.myfab.com # 节点域名 EnableNodeOUs: true #如果设置了EnableNodeOUs，就在msp下生成config.yaml文件 Template: Count: 2 # 表示生成几个peer Users: Count: 3 # 表示生成几个user(admin 除外) - Name: Org2 Domain: org2.myfab.com EnableNodeOUs: true Template: Count: 2 Users: Count: 2 Template.Count 指定了所要生成的节点数量 Users.Count 指定了需要生成的初始化用户数量。 生成MSP文件系统1cryptogen generate --config=./crypto-config.yaml 将会生成一个名为crypto-config的目录，里面存储了order 和peer 的MSP文件系统； 使用该cryptogen工具为各种网络实体生成加密材料（x509证书和签名密钥）。这些证书代表身份，它们允许在我们的实体进行通信和交易时进行签名/验证身份验证。 Cryptogen使用文件 - crypto-config.yaml包含网络拓扑，并允许我们为组织和属于这些组织的组件生成一组证书和密钥。每个组织都配置了一个唯一的根证书（ca-cert），它将特定组件（同行和订购者）绑定到该组织。 MSP文件系统的目录结构如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223. ## Orderer 组织配置├── ordererOrganizations│ ││ └── example.com│ ││ ├── ca ## 存放组织根证书 及 私钥 (采用EC算法) 证书为【自签名】，组织内的实体将给予该根证书作为证书根│ ││ │ ├── 56d9c0c46acdda38a174a5ba3ffc44726a2c027e16bb22b460413acbcb9b3a90_sk│ │ ││ │ └── ca.example.com-cert.pem│ ││ ├── msp ## 存放该组织身份信息│ │ ││ │ ├── admincerts ## 组织管理员 身份验证证书，【被根证书签名】│ │ │ ││ │ │ └── Admin@example.com-cert.pem│ │ ││ │ ├── cacerts ## 组织的根证书 【和CA目录 里面一致】│ │ │ ││ │ │ └── ca.example.com-cert.pem│ │ │ │ │ └── tlscacerts ## 用于TLS的CA证书， 【自签名】│ │ │ │ │ └── tlsca.example.com-cert.pem │ ││ ││ ├── orderers ## 存放所有 Orderer 的身份信息 │ │ ││ │ └── orderer.example.com ## 第一个 Orderer 的信息 msp 及 tls│ │ ││ │ ├── msp│ │ │ │ │ │ │ ├── admincerts ## 组织管理员的身份验证证书。Peer将给予这些证书来确认交易签名是否为管理员签名 【和MSP.admincerts 一致】│ │ │ │ ││ │ │ │ └── Admin@example.com-cert.pem│ │ │ ││ │ │ ├── cacerts│ │ │ │ ││ │ │ │ └── ca.example.com-cert.pem ## 存放组织根证书，【和CA目录 里面一致】│ │ │ ││ │ │ ├── keystore ## 本节点的身份私钥，用来签名│ │ │ │ ││ │ │ │ └── 2ec1193fe048848eaa8e20666e26c527b791c4fb127d69cae65095bd31b6c80e_sk│ │ │ ││ │ │ ├── signcerts ## 验证本节点签名的证书，【被根证书签名】│ │ │ │ ││ │ │ │ └── orderer.example.com-cert.pem│ │ │ ││ │ │ └── tlscacerts ## TLS连接用的身份证书， 【和msp.tlscacerts 一致】│ │ │ │ │ │ │ └── tlsca.example.com-cert.pem│ │ │ │ │ └── tls ## tls 的相关信息│ │ ├── ca.crt ## 【组织的根证书】│ │ ├── server.crt ## 验证本节点签名的证书， 【被根证书签名】│ │ └── server.key ## 本节点的身份私钥，用来签名│ │ │ ├── tlsca ## 存放tls相关的证书和私钥│ │ │ │ │ ├── 2d66be83c519da67bb36b0972256a3b24357fa7f5b3a61f11405bc8b1f4d7c53_sk│ │ │ │ │ └── tlsca.example.com-cert.pem│ │ │ └── users ## 存放属于该组织的用户的实体│ │ │ └── Admin@example.com ## 管理员用户的信息，其中包括msp证书和tls证书两类│ │ │ ├── msp│ │ │ │ │ ├── admincerts ## 组织根证书作为管理员身份验证证书，【和MSP.admincerts 一致】│ │ │ │ │ │ │ └── Admin@example.com-cert.pem│ │ │ │ │ ├── cacerts ## 存放组织的根证书，【和CA目录 里面一致】│ │ │ │ │ │ │ └── ca.example.com-cert.pem│ │ │ │ │ ├── keystore ## 本用户的身份私钥，用来签名│ │ │ │ │ │ │ └── a3c1d7e1bc464faf2e3a205cb76ea231bd3ee7010655d3cd31dc6cb78726c4d0_sk│ │ │ │ │ ├── signcerts ## 管理员用户的身份验证证书，被组织根证书签名。要被某个Orderer认可，则必须放到该 Orderer 的msp/admincerts目录下│ │ │ │ │ │ │ └── Admin@example.com-cert.pem│ │ │ │ │ └── tlscacerts ## TLS连接用的身份证书，即组织TLS证书，【和msp.tlscacerts 一致】│ │ │ │ │ └── tlsca.example.com-cert.pem│ │ │ │ │ └── tls ## tls 的相关信息│ ├── ca.crt│ ├── client.crt ## 管理员的身份验证证书，【被 组织根证书签名】│ └── client.key ## 管理员的身份私钥，用来签名│ │ │ │ ## Peer 组织配置└── peerOrganizations │ ├── org1.example.com ## 第一个组织的所有身份证书 │ │ │ ├── ca ## 存放组织根证书及私钥 (采用EC算法) 证书为【自签名】，组织内的实体将给予该根证书作为证书根 │ │ │ │ │ ├── 496d6a41ae5f66bf120df3eab3a9d2dc4d268b2ab9a22af891d33d323bbdb5c8_sk │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ ├── msp ## 存放该组织身份信息 │ │ │ │ │ ├── admincerts ## 组织管理员 身份验证证书，【被根证书签名】 │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ ├── cacerts ## 组织的根证书 【和CA目录 里面一致】 │ │ │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ │ │ ├── config.yaml ## 记录 OrganizationalUnitIdentitifiers 信息，包括 根证书位置 和 ID信息 (主要是 crypto-config.yaml 的peer配置中配了 EnableNodeOUs: true ： 如果设置了EnableNodeOUs，就在msp下生成config.yaml文件) │ │ │ │ │ └── tlscacerts ## 用于TLS的CA证书， 【自签名】 │ │ │ │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ ├── peers ## 存放所有 Peer 的身份信息 │ │ │ │ │ ├── peer0.org1.example.com ## 第一个Peer的信息 msp 及 tls │ │ │ │ │ │ │ ├── msp │ │ │ │ │ │ │ │ │ ├── admincerts ## 组织管理员的身份验证证书。Peer将给予这些证书来确认交易签名是否为管理员签名 【和MSP.admincerts 一致】 │ │ │ │ │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ ├── cacerts ## 存放组织根证书，【和CA目录 里面一致】 │ │ │ │ │ │ │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ ├── config.yaml │ │ │ │ │ │ │ │ │ ├── keystore ## 本节点的身份私钥，用来签名 │ │ │ │ │ │ │ │ │ │ │ └── 0f0c2e1835086161f6a10c4bb38c2d89b2cee4e1128cee0fcda4433feb6eb6f8_sk │ │ │ │ │ │ │ │ │ │ │ │ │ │ ├── signcerts ## 验证本节点签名的证书，【被根证书签名】 │ │ │ │ │ │ │ │ │ │ │ └── peer0.org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ └── tlscacerts ## TLS连接用的身份证书， 【和msp.tlscacerts 一致】 │ │ │ │ │ │ │ │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ │ │ └──tls ## tls 的相关信息 │ │ │ ├── ca.crt ## 【组织的根证书】 │ │ │ ├── server.crt ## 验证本节点签名的证书， 【被根证书签名】 │ │ │ └── server.key ## 本节点的身份私钥，用来签名 │ │ │ │ │ │ │ │ └── peer1.org1.example.com │ │ │ │ │ │ │ │ │ ├── tlsca ## 存放tls相关的证书和私钥 │ │ │ │ │ ├── 3d39ea82dd5343c261b0480bc13d645a3cee13b7e7aa8c54fd2b5162f709671f_sk │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ │ │ └── users ## 存放属于该组织的用户的实体 │ │ │ ├── Admin@org1.example.com ## 管理员用户的信息，其中包括msp证书和tls证书两类 │ │ │ │ │ ├── msp │ │ │ │ │ │ │ ├── admincerts ## 组织根证书作为管理员身份验证证书，【和MSP.admincerts 一致】 │ │ │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ │ │ ├── cacerts ## 存放组织的根证书，【和CA目录 里面一致】 │ │ │ │ │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ │ │ │ │ ├── keystore ## 本用户的身份私钥，用来签名 │ │ │ │ │ │ │ │ │ └── 2b933c0740d857284be98ff218bf279261e55eff2b89d973e0a1f435f7c7d28b_sk │ │ │ │ │ │ │ ├── signcerts ## 管理员用户的身份验证证书，被组织根证书签名。要被某个Peer认可，则必须放到该Peer的msp/admincerts目录下 │ │ │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ │ │ └── tlscacerts ## TLS连接用的身份证书，即组织TLS证书，【和msp.tlscacerts 一致】 │ │ │ │ │ │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ └── tls ## tls 的相关信息 │ │ ├── ca.crt │ │ ├── client.crt ## 管理员的身份验证证书，【被 组织根证书签名】 │ │ └── client.key ## 管理员的身份私钥，用来签名 │ │ │ │ │ └── User1@org1.example.com │ ├── msp │ │ ├── admincerts ## 组织根证书作为管理员身份验证证书 │ │ │ └── User1@org1.example.com-cert.pem │ │ ├── cacerts ## 存放组织的根证书，【和CA目录 里面一致】 │ │ │ └── ca.org1.example.com-cert.pem │ │ ├── keystore ## 【参考admin】 │ │ │ └── 11ebc5afac42348f84a8882f329d18beee079efd4fd5d9b30389dc82053fc0c9_sk │ │ ├── signcerts ## 【参考admin】 │ │ │ └── User1@org1.example.com-cert.pem │ │ └── tlscacerts ## 【参考admin】 │ │ └── tlsca.org1.example.com-cert.pem │ └── tls ## 【参考admin】 │ ├── ca.crt │ ├── client.crt │ └── client.key │ └── org2.example.com 生成Order创世区块在 Orderer 节点上维护的有个 system chain, 这个创世区块实际上是这个 system chain 的创世区块。在 Fabric 的上下文中，chain，channel 基本上可以通用，下面有时会称 system chain 为 system channel。 创建system chain的创世区块创建创世区块的配置文件 configtx.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159## 一些列组织的定义，【被其他 部分引用】#### 【注意】：本文件中 &amp;KEY 均为 *KEY 所引用； xx：&amp;KEY 均为 &lt;&lt;: *KEY 所引用##Organizations: ## 定义Orderer组织 【&amp;OrdererOrg 这类语法类似 Go的中的指针及对象地址， 此处是被Profiles 中的 - *OrdererOrg 所引用，以下均为类似做法】 - &amp;OrdererOrg Name: OrdererOrg ## Orderer的组织的名称 ID: OrdererMSP ## Orderer 组织的ID （ID是引用组织的关键） MSPDir: crypto-config/ordererOrganizations/example.com/msp ## Orderer的 MSP 证书目录路径 AdminPrincipal: Role.ADMIN ## 【可选项】 组织管理员所需要的身份，可选项: Role.ADMIN 和 Role.MEMBER ## 定义Peer组织 1 - &amp;Org1 Name: Org1MSP ## 组织名称 ID: Org1MSP ## 组织ID MSPDir: crypto-config/peerOrganizations/org1.example.com/msp ## Peer的MSP 证书目录路径 AnchorPeers: ## 定义组织锚节点 用于跨组织 Gossip 通信 - Host: peer0.org1.example.com ## 锚节点的主机名 Port: 7051 ## 锚节点的端口号 ## 定义Peer组织 2 - &amp;Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.example.com/msp AnchorPeers: - Host: peer0.org2.example.com Port: 7051 ################################################################################## SECTION: Capabilities 【注意】： 该部分是 V1.1.0 版本提出来的， 不可以在更早的版本中使用## - 本节定义了 fabric 网络的功能. 这是v1.1.0中的一个新概念，不应在v1.0.x的peer和orderers中使用. # Capabilities 定义了存在于结构二进制文件中的功能，以便该二进制文件安全地参与结构网络. # 例如，如果添加了新的MSP类型，则较新的二进制文件可能会识别并验证此类型的签名，而没有此支持的旧二进制文件将无法验证这些事务. # 这可能导致具有不同世界状态的不同版本的结构二进制文件。 相反，为通道定义功能会通知那些没有此功能的二进制文件，# 它们必须在升级之前停止处理事务. 对于v1.0.x，如果定义了任何功能（包括关闭所有功能的配置），那么v1.0.x的peer 会主动崩溃.#################################################################################Capabilities: ## 通道功能适用于orderers and the peers，并且必须得到两者的支持。 将功能的值设置为true. Global: &amp;ChannelCapabilities ## V1.1 的 Global是一个行为标记，已被确定为运行v1.0.x的所有orderers和peers的行为，但其修改会导致不兼容。 用户应将此标志设置为true. V1_1: true ## Orderer功能仅适用于orderers，可以安全地操纵，而无需担心升级peers。 将功能的值设置为true Orderer: &amp;OrdererCapabilities ## Orderer 的V1.1是行为的一个标记，已经确定为运行v1.0.x的所有orderers 都需要，但其修改会导致不兼容。 用户应将此标志设置为true V1_1: true ## 应用程序功能仅适用于Peer 网络，可以安全地操作，而无需担心升级或更新orderers。 将功能的值设置为true Application: &amp;ApplicationCapabilities ## V1.2 for Application是一个行为标记，已被确定为运行v1.0.x的所有peers所需的行为，但其修改会导致不兼容。 用户应将此标志设置为true V1_2: true ################################################################################## SECTION: Application## - 应用通道相关配置，主要包括 参与应用网络的可用组织信息#################################################################################Application: &amp;ApplicationDefaults ## 自定义被引用的地址 Organizations: ## 加入通道的组织信息 ################################################################################## SECTION: Orderer## - Orderer 系统通道相关配置，包括 Orderer 服务配置和参与Orderer 服务的可用组织# # Orderer 默认是 solo 的 且不包含任何组织 【主要被 Profiles 部分引用】################################################################################Orderer: &amp;OrdererDefaults ## 自定义被引用的地址 OrdererType: solo ## Orderer 类型，包含 solo 和 kafka 集群 Addresses: ## 服务地址 - orderer.example.com:7050 BatchTimeout: 2s ## 区块打包的最大超时时间 (到了该时间就打包区块) BatchSize: ## 区块打包的最大包含交易数 MaxMessageCount: 10 ## 一个区块里最大的交易数 AbsoluteMaxBytes: 99 MB ## 一个区块的最大字节数， 任何时候都不能超过 PreferredMaxBytes: 512 KB ## 一个区块的建议字节数，如果一个交易消息的大小超过了这个值, 就会被放入另外一个更大的区块中 MaxChannels: 0 ## 【可选项】 表示Orderer 允许的最大通道数， 默认 0 表示没有最大通道数 Kafka: Brokers: ## kafka的 brokens 服务地址 允许有多个 - 127.0.0.1:9092 Organizations: ## 参与维护 Orderer 的组织，默认为空 ################################################################################## Profile ## - 一系列通道配置模板，包括Orderer 系统通道模板 和 应用通道类型模板#################################################################################Profiles: ## Orderer的 系统通道模板 必须包括 Orderer、 Consortiums 两部分 TwoOrgsOrdererGenesis: ## Orderer 系统的通道及创世块配置。通道为默认配置，添加一个OrdererOrg 组织， 联盟为默认的 SampleConsortium 联盟，添加了两个组织 【该名称可以自定义 ？？】 Capabilities: &lt;&lt;: *ChannelCapabilities Orderer: ## 指定Orderer系统通道自身的配置信息 &lt;&lt;: *OrdererDefaults ## 引用 Orderer 部分的配置 &amp;OrdererDefaults Organizations: - *OrdererOrg ## 属于Orderer 的通道组织 该处引用了 【 &amp;OrdererOrg 】位置内容 Capabilities: &lt;&lt;: *OrdererCapabilities Consortiums: ## Orderer 所服务的联盟列表 SampleConsortium: ## 创建更多应用通道时的联盟 引用 TwoOrgsChannel 所示 Organizations: - *Org1 - *Org2 ## 应用通道模板 必须包括 Application、 Consortium 两部分 TwoOrgsChannel: ## 应用通道配置。默认配置的应用通道，添加了两个组织。联盟为SampleConsortium Consortium: SampleConsortium ## 通道所关联的联盟名称 Application: ## 指定属于某应用通道的信息，主要包括 属于通道的组织信息 &lt;&lt;: *ApplicationDefaults Organizations: ## 初始 加入应用通道的组织 - *Org1 - *Org2 Capabilities: &lt;&lt;: *ApplicationCapabilities 在这个文件里有两个 Profile， 一个是 OneOrgOrdererGenesis, 一个是 OneOrgChannel。 一个 Profile 代表了一组配置, 里面包含了通道相关配置，Orderer 节点相关配置，联盟成员相关配置。通道相关配置确定了系统通道的一些权限策略，Orderer 节点配置确定了 Orderer 节点的类型（是 solo 还是 kafaka），Orderer 节点的访问地址，还有出块时间，区块大小，区块内允许包含的交易数量等。联盟配置确定了联盟的名称和联盟所包含的组织，对组织来说这里最为关键的是组织的 MSP ID 和路径，这些信息都会被包含到 system chain 中。 生成创世区块1configtxgen -profile OneOrgOrdererGenesis -channelID order-channel -outputBlock ./channel-artifacts/genesis.block 这个命令需要指定一个 channelID，注意这里的 channelID 为系统链(system chain)的 channelID。 生成通道配置区块现在来创建创世区块交易，通过交易将创世块上传到channel上 生成channel配置事务通过下面的命令，可以生成这样的一个交易。 1export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile OneOrgChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID $CHANNEL_NAME 该命令会读取和生成 Orderer 相同的配置文件，只是使用了 “OneOrgChannel” 这个 Profile，确定了通道的权限策略，创建通道的联盟和组织信息。 只是创建了通道，而不创建锚节点的化，通道区块数据就无法跨组织传播，所以一般还要通过下面的命令创建用来更新锚节点的交易。 为每个组织生成锚节点配置事务1export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile OneOrgChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org1MSP 上面的命令生成了为每个组织生成锚节点的交易。 启动网络接下来，就可以通过orderer命令或 docker 容器来启动网络了，orderer 启动的时候会查找一个名为 orderer.yaml 的配置文件。这个配置文件不是必须的，在找不到这个配置文件时，orderer 命令会使用默认配置。我们也可以通过环境变量或命令行参数的方式去对每个配置项进行覆盖。 配置各节点启动参数配置 yaml_config/docker-compose.yml 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: '2'volumes: orderer.example.com: peer0.org1.example.com: peer1.org1.example.com: peer0.org2.example.com: peer1.org2.example.com:networks: byfn:services: ca0: image: hyperledger/fabric-ca:$IMAGE_TAG environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca-org1 - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/CA1_PRIVATE_KEY ports: - "7054:7054" command: sh -c 'fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/CA1_PRIVATE_KEY -b admin:adminpw -d' volumes: - ./crypto-config/peerOrganizations/org1.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca_peerOrg1 networks: - byfn ca1: image: hyperledger/fabric-ca:$IMAGE_TAG environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca-org2 - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/CA2_PRIVATE_KEY ports: - "8054:7054" command: sh -c 'fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/CA2_PRIVATE_KEY -b admin:adminpw -d' volumes: - ./crypto-config/peerOrganizations/org2.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca_peerOrg2 networks: - byfn orderer.example.com: extends: file: base/docker-compose-base.yaml service: orderer.example.com container_name: orderer.example.com networks: - byfn peer0.org1.example.com: container_name: peer0.org1.example.com extends: file: base/docker-compose-base.yaml service: peer0.org1.example.com networks: - byfn peer1.org1.example.com: container_name: peer1.org1.example.com extends: file: base/docker-compose-base.yaml service: peer1.org1.example.com networks: - byfn peer0.org2.example.com: container_name: peer0.org2.example.com extends: file: base/docker-compose-base.yaml service: peer0.org2.example.com networks: - byfn peer1.org2.example.com: container_name: peer1.org2.example.com extends: file: base/docker-compose-base.yaml service: peer1.org2.example.com networks: - byfn 启动网络节点1docker-compose -f docker-compose.yml up -d ca.example.com orderer.example.com peer0.org1.example.com couchdb 注意: 如果启动ca， 之一要修改yaml文件中对应的ca私钥地址 运行Channel(示例为一个组织)这个阶段把网络启动后，还是啥事都做不了，节点之间也没有通信。 因为还没有生成peer节点互相通信的channel，也没有将节点加入到channel中 创建channel创建了一个channel的创世块，逻辑上就等于是创建了一个channel 1docker exec -e "CORE_PEER_LOCALMSPID=Org1MSP" -e "CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp" peer0.org1.example.com peer channel create -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/channel.tx 该命令是，通过Org1向order节点发送事务，order节点验证通过后，会生成mychannel的创世块 将组织加入到channel将组织加入到channel 1docker exec -e "CORE_PEER_LOCALMSPID=Org1MSP" -e "CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp" peer0.org1.example.com peer channel join -b mychannel.block 更新锚节点1docker exec -e "CORE_PEER_LOCALMSPID=Org1MSP" -e "CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp" peer0.org1.example.com peer channel update -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/Org1MSPanchors.tx 目前为止一个基本的网络就已经搭建好了。下面再介绍一些怎么更新我们的网络组织和channel成员 chain code 安装和使用chaincode 安装 将即将安装节点配置到环境变量中 1234CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspCORE_PEER_ADDRESS=peer0.org2.example.com:7051CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt 安装chaincode 1peer chaincode install -n mycc -v 1.0 -p github.com/chaincode/chaincode_example02/go/ 将chaincode实例化 一个channel只需要实例化一次 1peer chaincode instantiate -o orderer.example.com:7050 --tls --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C $CHANNEL_NAME -n mycc -v 1.0 -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;, &quot;100&quot;, &quot;b&quot;,&quot;200&quot;]&#125;&apos; -P &quot;AND (&apos;Org1MSP.peer&apos;,&apos;Org2MSP.peer&apos;)&quot; chaincode使用 查询 1peer chaincode query -C $CHANNEL_NAME -n mycc -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]&#125;&apos; 调用 1peer chaincode invoke -o orderer.example.com:7050 --tls true --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C $CHANNEL_NAME -n mycc --peerAddresses peer0.org1.example.com:7051 --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt --peerAddresses peer0.org2.example.com:7051 --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt -c &apos;&#123;&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]&#125;&apos; 创建新组织并添加到channel生成组织的证书和密钥创建一个目录: org3-artifacts, 用来存储生成org3最终配置和事务的配置文件和中间产物等； 该阶段操作都在org3-artifacts目录下进行 加密资料准备 生成组织的MSP文件 创建一个新org的org3-crypto.yaml文件用来生成新组织的MSP资料 org3-crypto.yaml文件内容 1234567891011PeerOrgs: # --------------------------------------------------------------------------- # Org3 # --------------------------------------------------------------------------- - Name: Org3 Domain: org3.example.com EnableNodeOUs: true Template: Count: 2 Users: Count: 1 1cryptogen generate --config=./org3-crypto.yaml 导出MSP数据 configtx.yaml文件内容 1234567891011121314151617Organizations: - &amp;Org3 # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: Org3MSP # ID to load the MSP definition as ID: Org3MSP MSPDir: crypto-config/peerOrganizations/org3.example.com/msp AnchorPeers: # AnchorPeers defines the location of peers which can be used # for cross org gossip communication. Note, this value is only # encoded in the genesis block in the Application section context - Host: peer0.org3.example.com Port: 11051 1export FABRIC_CFG_PATH=$PWD &amp;&amp; configtxgen -printOrg Org3MSP &gt; ../artifacts/org3.json 上面的命令创建一个JSON文件 - org3.json并将其输出到artifacts。此文件包含Org3的策略定义，以及以base 64格式呈现的三个重要证书：管理员用户证书（稍后将充当Org3的管理员），CA根证书和TLS根目录证书 移动order MSP数据到当前目录下 将Orderer Org的MSP材料移植到Org3 crypto-config目录中。特别是，我们关注的是Orderer的TLS根证书，它将允许Org3实体与网络订购节点之间的安全通信。 1cd ../ &amp;&amp; cp -r crypto-config/ordererOrganizations org3-artifacts/crypto-config/ 生成配置文件 进入cli应用 1docker exec -it cli bash 设置环境变量 接下来急需要使用order管理员的身份获取mychannel的区块文件 12export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pemexport CHANNEL_NAME=mychannel 获取channel的配置信息 1peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA 将配置转化为Json 1configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; config.json 这给我们留下了一个精简的JSON对象 - config.json 它将作为我们的配置更新的基线。 获得添加组织后的配置文件 将组织的配置json文件添加到channel的应用的分组字段，并输出到modified_config.json 1jq -s &apos;.[0] * &#123;&quot;channel_group&quot;:&#123;&quot;groups&quot;:&#123;&quot;Application&quot;:&#123;&quot;groups&quot;: &#123;&quot;Org3MSP&quot;:.[1]&#125;&#125;&#125;&#125;&#125;&apos; config.json ./channel-artifacts/org3.json &gt; modified_config.json 现在，在CLI容器中，我们有两个感兴趣的JSON文件 - config.json和modified_config.json。初始文件仅包含Org1和Org2材料，而“modified”文件包含所有三个Orgs。此时，只需重新编码这两个JSON文件并计算增量即可。 将config.json翻译回到config.pb 1configtxlator proto_encode --input config.json --type common.Config --output config.pb 将modified_config.json 编码成modified_config.pb: 1configtxlator proto_encode --input modified_config.json --type common.Config --output modified_config.pb 计算两个pb文件的增量 因为之前的组织MSP材料已经存在于通道的区块中，因此只需要计算和使用两个文件的增量 1configtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_config.pb --output org3_update.pb 将增量内容解码为json格式 1configtxlator proto_decode --input org3_update.pb --type common.ConfigUpdate | jq . &gt; org3_update.json 封装消息 1echo &apos;&#123;&quot;payload&quot;:&#123;&quot;header&quot;:&#123;&quot;channel_header&quot;:&#123;&quot;channel_id&quot;:&quot;mychannel&quot;, &quot;type&quot;:2&#125;&#125;,&quot;data&quot;:&#123;&quot;config_update&quot;:&apos;$(cat org3_update.json)&apos;&#125;&#125;&#125;&apos; | jq . &gt; org3_update_in_envelope.json 将最终的json文件转化为pb格式 1configtxlator proto_encode --input org3_update_in_envelope.json --type common.Envelope --output org3_update_in_envelope.pb 签名并提交配置更新 对更新做签名 在将配置写入分类帐之前，我们需要来自必需管理员用户的签名。 我们的渠道应用程序组的修改策略（mod_policy）设置为默认值“MAJORITY”，这意味着我们需要大多数现有组织管理员对其进行签名。 首先，让我们将此更新原型作为Org1管理员签名。请记住，CLI容器是使用Org1 MSP材质引导的，因此我们只需要发出 命令：peer channel signconfigtx 1peer channel signconfigtx -f org3_update_in_envelope.pb 最后一步是切换CLI容器的标识以反映Org2 Admin用户。我们通过导出特定于Org2 MSP的四个环境变量来实现此目的。 配置Org2环境变量： 123456789# you can issue all of these commands at onceexport CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspexport CORE_PEER_ADDRESS=peer0.org2.example.com:7051 签署 1peer channel signconfigtx -f org3_update_in_envelope.pb 发送更新请求 1peer channel update -f org3_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA 如果您的更新已成功提交，您应该会看到类似于以下内容的消息摘要指示： 12018-02-24 18:56:33.499 UTC [msp/identity] Sign -&gt; DEBU 00f Sign: digest: 3207B24E40DE2FAB87A2E42BC004FEAA1E6FDCA42977CB78C64F05A88E556ABA 您还将看到我们的配置事务的提交： 12018-02-24 18:56:33.499 UTC [channelCmd] update -&gt; INFO 010 Successfully submitted channel update 成功的频道更新呼叫向频道上的所有对等体返回新块 - 块5。如果您还记得，块0-2是初始通道配置，而块3和4是mycc链代码的实例 将新的节点组织加入到channel 配置领导者的选举 新的对等体不能利用gossip，因为它们无法验证其他对等体从其自己的组织转发的块，直到它们获得将该组织添加到该channel的配置事务。 因此，新添加的对等体必须具有以下配置之一，以便它们从订购服务接收块： 要使用静态领导模式，请将对等方配置为组织领导者： 12CORE_PEER_GOSSIP_USELEADERELECTION=falseCORE_PEER_GOSSIP_ORGLEADER=true 注意 对于添加到通道的所有新对等方，此配置必须相同。 要利用动态领导者选举，配置对等方使用领导者选举： 12CORE_PEER_GOSSIP_USELEADERELECTION=trueCORE_PEER_GOSSIP_ORGLEADER=false 注意 由于新添加的组织的对等方将无法形成成员资格视图，因此该选项将类似于静态配置，因为每个对等方将开始宣称自己是领导者。但是，一旦他们更新了将组织添加到渠道的配置事务，组织中将只有一个活跃的领导者。因此，如果您最终希望组织的同行使用领导者选举，建议使用此选项 将Org3 peer加入channel 打开org3 peer 此时，通道配置已更新为包含我们的新组织Org3- 意味着与其关联的对等方现在可以加入mychannel。 首先，让我们为Org3对等体和Org3特定的CLI启动容器。 打开一个新的终端并从first-network启动Org3 docker compose： compose文件内容: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192version: '2'volumes: peer0.org3.example.com: peer1.org3.example.com:networks: byfn:services: peer0.org3.example.com: container_name: peer0.org3.example.com extends: file: base/peer-base.yaml service: peer-base environment: - CORE_PEER_ID=peer0.org3.example.com - CORE_PEER_ADDRESS=peer0.org3.example.com:11051 - CORE_PEER_LISTENADDRESS=0.0.0.0:11051 - CORE_PEER_CHAINCODEADDRESS=peer0.org3.example.com:11052 - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:11052 - CORE_PEER_GOSSIP_BOOTSTRAP=peer1.org3.example.com:12051 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org3.example.com:11051 - CORE_PEER_LOCALMSPID=Org3MSP volumes: - /var/run/:/host/var/run/ - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/msp:/etc/hyperledger/fabric/msp - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls:/etc/hyperledger/fabric/tls - peer0.org3.example.com:/var/hyperledger/production ports: - 11051:11051 networks: - byfn peer1.org3.example.com: container_name: peer1.org3.example.com extends: file: base/peer-base.yaml service: peer-base environment: - CORE_PEER_ID=peer1.org3.example.com - CORE_PEER_ADDRESS=peer1.org3.example.com:12051 - CORE_PEER_LISTENADDRESS=0.0.0.0:12051 - CORE_PEER_CHAINCODEADDRESS=peer1.org3.example.com:12052 - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:12052 - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org3.example.com:11051 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org3.example.com:12051 - CORE_PEER_LOCALMSPID=Org3MSP volumes: - /var/run/:/host/var/run/ - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/msp:/etc/hyperledger/fabric/msp - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls:/etc/hyperledger/fabric/tls - peer1.org3.example.com:/var/hyperledger/production ports: - 12051:12051 networks: - byfn Org3cli: container_name: Org3cli image: hyperledger/fabric-tools:$IMAGE_TAG tty: true stdin_open: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - FABRIC_LOGGING_SPEC=INFO #- FABRIC_LOGGING_SPEC=DEBUG - CORE_PEER_ID=Org3cli - CORE_PEER_ADDRESS=peer0.org3.example.com:11051 - CORE_PEER_LOCALMSPID=Org3MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/bash volumes: - /var/run/:/host/var/run/ - ./../chaincode/:/opt/gopath/src/github.com/chaincode - ./org3-artifacts/crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./crypto-config/peerOrganizations/org1.example.com:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com - ./crypto-config/peerOrganizations/org2.example.com:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/ depends_on: - peer0.org3.example.com - peer1.org3.example.com networks: - byfn 1docker-compose -f docker-compose-org3.yaml up -d 此新组合文件已配置为跨越我们的初始网络，因此两个对等方和CLI容器将能够使用现有对等方和订购节点进行解析。现在运行三个新容器，执行特定于Org3的CLI容器： 1docker exec -it Org3cli bash 就像我们使用初始CLI容器一样，导出两个关键环境变量： 设置环境变量 就像我们使用初始CLI容器一样，导出两个关键环境变量：ORDERER_CA和CHANNEL_NAME： 1export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem &amp;&amp; export CHANNEL_NAME=mychannel 检查以确保已正确设置变量： 1echo $ORDERER_CA &amp;&amp; echo $CHANNEL_NAME 将节点加入到channel 1peer channel join -b mychannel.block 将第二个节点加入到channel 12export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls/ca.crtexport CORE_PEER_ADDRESS=peer1.org3.example.com:7051peer channel join -b mychannel.block 升级chaincode并更改认可策略更新chaincode的认可策略，添加org3到认可策略中 在org3 cli安装chaincode: 1peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/ 在org1 和 org2 的 cli上安装新版本chaincode peer0.org2上安装 1peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/ 跳到peer0.org1 123456789export CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/mspexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/ 更新认可策略 1peer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;90&quot;,&quot;b&quot;,&quot;210&quot;]&#125;&apos; -P &quot;OR (&apos;Org1MSP.peer&apos;,&apos;Org2MSP.peer&apos;,&apos;Org3MSP.peer&apos;)&quot;]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-动态分析破解Apk(Debug)]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E5%8A%A8%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk(Debug)%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、知识概要分析首先需要解释一下，这里为什么说是调试smali源码，不是Java源码，因为我们弄过反编译的人知道，使用apktool反编译apk之后，会有一个smali文件夹，这里就存放了apk对应的smali源码，关于smali源码这里不解释了，网上有介绍。 二、案例分析因为这一篇是一个教程篇，所以不能光说，那样会很枯燥的，所以这里用一个例子来介绍一下： 我们就用阿里2014年安全挑战赛的第一题：AliCrack_one.apk 看到这张图了，阿里还挺会制造氛围的，那么其实很简单，我们输入密码就可以破解了，下面我们就来看看如何获取这个密码。 第一步：使用apktool来破解apk1java -jar apktool_2.0.0rc4.jar d -d AliCraceme_1.apk -o out 这里的命令不做解释了。 但是有一个参数必须带上，那就是：-d 因为这个参数代表我们反编译得到的smali是java文件，这里说的文件是后缀名是java，如果不带这个参数的话，后缀名是smali的，但是Eclipse中是不会识别smali的，而是识别java文件的，所以这里一定要记得加上这个参数。 反编译成功之后，我们得到了一个out目录，如下： 源码都放在smali文件夹中，我们进入查看一下文件： 看到了，这里全是Java文件的，其实只是后缀名为java了，内容还是smali的： 第二步、修改AndroidManifest.xml中的debug属性和在入口代码中添加waitDebug上面我们反编译成功了，下面我们为了后续的调试工作，所以还是需要做两件事： 1》修改AndroidManifest.xml中的android:debuggable=”true” 关于这个属性，我们前面介绍run-as命令的时候，也提到了，他标识这个应用是否是debug版本，这个将会影响到这个应用是否可以被调试，所以这里必须设置成true。 2》在入口处添加waitForDebugger代码进行调试等待。 这里说的入口处，就是程序启动的地方，就是我们一般的入口Activity，查找这个Activity的话，方法太多了，比如我们这里直接从上面得到的AndroidManifest.xml中找到，因为入口Activity的action和category是固定的。 当然还有其他方式，比如aapt查看apk的内容方式，或者是安装apk之后用 adb dumpsys activity top 命令查看都是可以的。 找到入口Activity之后，我们直接在他的onCreate方法的第一行加上waitForDebugger代码即可，找到对应的MainActivity的smali源码： 然后添加一行代码： 1invoke-static &#123;&#125;, Landroid/os/Debug;-&gt;waitForDebugger()V 这个是smali语法的，其实对应的Java代码就是：android.os.Debug.waitForDebugger(); 这里把Java语言翻译成smali语法的，不难，网上有smali的语法解析，这里不想再解释了。 第三步：回编译apk并且进行签名安装1java -jar apktool_2.0.0rc4.jar b -d out -o debug.apk 还是使用apktool进行回编译 编译完成之后，将得到debug.apk文件，但是这个apk是没有签名的，所以是不能安装的，那么下面我们需要在进行签名，这里我们使用Android中的测试程序的签名文件和sign.jar工具进行签名： 1java -jar .\sign\signapk.jar .\sign\testkey.x509.pem .\sign\testkey.pk8 debug.apk debug.sig.apk 签名之后，我们就可以进行安装了。 第四步：在Eclipse中新建一个Java工程，导入smali源码这里我们新建一个Java工程，记住不是Android工程，因为我们最后调试其实是借助于Java的调试器，然后勾选掉Use default location选项，选择我们的smali源码目录，也就是我们上面反编译之后的out目录，点击完成 我们导入源码之后的项目工程结构： 主要看MainActivity类： 第五步：找到关键点，然后打断点这一步我们看到，其实说的比较广义了，这个要具体问题具体分析了，比如这个例子中，我们知道当我们输入密码之后，肯定要点击按钮，然后触发密码的校验过程，那么这里我们知道找到这个button的定义的地方，然后进入他的点击事件中就可以了。这里分为三步走： 1》使用Eclipse自带的View分析工具找到Button的ResId 点击之后，需要等待一会，分析View之后的结果： 看到了，这里我们能够看到整个当前的页面的全部布局，已经每个控件的属性值，我们需要找到button的resource-id 这里我们看到定义是@+id/button这个值。 2》我们得到这个resId之后，能否在smali工程中全局搜索这个值，就可以定位到这个button的定义的地方呢？ 然后我们看看搜到的结果： 这时候我们其实是在资源文件中搜到了这个id的定义，这个id值对应的是0x7F05003E。 当然除了这种方式，我们还有一种方式能快速找到这个id对应的整型值，那就是在反编译之后的values/public.xml文件中： 这个文件很有用的，他是真个apk中所有资源文件定义的映射内容，比如drawable/string/anim/attr/id 等这些资源文件定义的值，名字和整型值对应的地方： 这个文件很重要，是我们在寻找突破口的重要关键，比如我们有时候需要通过字符串内容来定位到关键点，这里就可以通过string的定义来找到对应的整型值即可。 当我们找到了button对应的id值了之后，我们就可以用这个id值在一次全局搜索一下，因为我们知道，Android中编译之后的apk，在代码中用到的resId都是用一个整型值代替的，这个整型值就是在R文件中做了定义，将资源的id和一个值对应起来，然后代码里面一般使用R.id.button这样的值，在编译出apk的时候，这个值就会被替换成对应的整型值，所以在全局搜索0x7F05003E 搜索的结果如下： 看到了，这里就定位到了代码中用到的这个button，我们进入代码看看： 在这里，看到了，使用了findViewById的方式定义Button,我们在往下面简单分析一下smali语法，下面是给button添加一个按钮事件，这里用的是内部类MainActivity$1，我们到这个类看看，他肯定实现了OnClickListener接口，那么直接搜onClick方法： 在这里我们就可以下个断点了，这里就是触发密码校验过程。 第六步：运行程序，设置远程调试工程在第五步中，我们找到了关键点，然后打上断点，下面我们就来运行程序，然后在Eclipse中设置远程调试的工程 首先我们运行程序，因为我们加入了waitForDebug的代码，所以启动的时候会出现一个Wait debug的对话框。不过，我测试的时候，我的手机没有出现这个对话框，而是一个白屏，不过这个不影响，程序运行起来之后，我们看看如何在Eclipse中设置远程调试工程，首先我们找到需要调试的程序对应远程调试服务端对应的端口： 这里我们看到有几个点： 1》在程序等待远程调试服务器的时候，前面会出现一个红色的小蜘蛛 2》在调试服务端这里我们会看到两个端口号：8600/8700，这里需要解释一下，为什么会有两个端口号呢？ 首先在这里的端口号，代表的是，远程调试服务器端的端口，下面在简单来看一下，Java中的调试系统： 这里我们看到，这里有三个角色： 111》JDB Client端(被调试的客户端)，这里我们可以认为我们需要破解的程序就是客户端，如果一个程序可以被调试，当启动的时候，会有一个jdwp线程用来和远程调试服务端进行通信 这里我们看到，我们需要破解的程序启动了JDWP线程，注意这个线程也只有当程序是debug模式下才有的，也就是AndroidManifest.xml中的debug属性值必须是true的时候，也就是一开始为什么我们要修改这个值的原因。 222》JDWP协议(用于传输调试信息的，比如调试的行号，当前的局部变量的信息等)，这个就可以说明，为什么我们在一开始的时候，反编译成java文件，因为为了Eclipse导入能够识别的Java文件，然后为什么能够调试呢？因为smali文件中有代码的行号和局部变量等信息，所以可以进行调试的。 333》JDB Server端(远程调试的服务端，一般是有JVM端)，就是开启一个JVM程序来监听调试端，这里就可以认为是本地的PC机，当然这里必须有端口用来监听，那么上面的8600端口就是这个作用，而且这里端口是从8600开始，后续的程序端口后都是依次加1的，比如其他调试程序： 那么有了8600端口，为什么还有一个8700端口呢？他是干什么的？ 其实他的作用就是远程调试端备用的基本端口，也就是说比如这里的破解程序，我们用8600端口可以连接调试，8700也是可以的，但是其他程序，比如demo.systemapi他的8607端口可以连接调试，8700也是可以的： 所以呀，可以把8700端口想象成大家都可以用于连接调试的一个端口，不过，在实际过程中，还是建议使用程序独有的端口号8600，我们可以查看8600和8700端口在远程调试端(本地pc机)的占用情况： 看到了，这里的8600端口和8700端口号都是对应的javaw程序，其实javaw程序就是启动一个JVM来进行监听的。 好了，到这里我们就弄清楚了，Java中的调试系统以及远程调试的端口号。 注意： 其实我们可以使用adb jdwp命令查看，当前设备中可以被调试的程序的进程号信息： 下面继续，我们知道了远程调试服务端的端口：8600，以及ip地址，这里就是本地ip：localhost/127.0.0.1 我们可以在Eclipse中新建一个远程调试项目，将我们的smali源码工程和设备中需要调试的程序关联起来： 右击被调试的项目=》选择Debug Configurations： 然后开始设置调试项目 选择Romote Java Application，在Project中选择被调试的smali项目，在Connection Type中选择SocketAttach方式，其实还有一种方式是Listener的，关于这两种方式其实很好理解： #Listner方式：是调试客户端启动就准备好一个端口，当调试服务端准备好了，就连接这个端口进行调试 #Attach方式：是调试服务端开始就启动一个端口，等待调试端来连接这个端口 我们一般都是选择Attach方式来进行操作的。 好了，我们设置完远程调试的工程之后，开始运行，擦发现，设备上的程序还是白屏，这是为什么呢？看看DDMS中调试程序的状态： 擦，关联到了这个进程，原因也很简单，我们是上面使用的是8700端口号，这时候我们选中了这个进程，所以就把smali调试工程关联到了这个进程，所以破解的进程没反应了，我们立马改一下，用8600端口： 好了，这下成功了，我们看到红色的小蜘蛛变成绿色的了，说明调试端已经连接上远程调试服务端了。 注意： 我们在设置远程调试项目的时候，一定要注意端口号的设置，不然没有将调试项目源码和调试程序关联起来，是没有任何效果的 第七步：开始运行调试程序，进入调试下面我们就开始操作了，在程序的文本框中输入：gggg内容，点击开始： 好了，到这里我们看到期待已久的调试界面出来了，到了我们开始的时候加的断点处，这时候我们就可以开始调试了，使用F6单步调试，F5单步跳入，F7单步跳出进行操作： 看到了，这里使用v3变量保存了我们输入的内容 这里有一个关键的地方，就是调用MainActivity的getTableFromPic方法，获取一个String字符串，从变量的值来看，貌似不是规则的字符串内容，这里先不用管了，继续往下走： 这里又遇到一个重要的方法：getPwdFromPic，从字面意义上看，应该是获取正确的密码，用于后面的密码字符串比对。 查看一下密码的内容，貌似也是一个不规则的字符串，但是我们可以看到和上面获取的table字符串内容格式很像，接着往下走： 这里还有一个信息就是，调用了系统的Log打印，log的tag就是v6保存的值：lil 这时候，我们看到v3是保存的我们输入的密码内容，这里使用utf-8获取他的字节数组，然后传递给access$0方法，我们使用F5进入这个方法： 在这个方法中，还有一个bytesToAliSmsCode方法，使用F5进入： 那么这个方法其实看上去还是很简单的，就是把传递进来的字节数组，循环遍历，取出字节值，然后转化成int类型，然后在调用上面获取到的table字符串的chatAt来获取指定的字符，使用StringBuilder进行拼接，然后返回即可。 按F7跳出，查看，我们返回来加密的内容是：日日日日，也就是说gggg=&gt;日日日日 最后再往下走，可以看到是进行代码比对的工作了。 那么上面我们就分析完了所有的代码逻辑，还不算复杂，我们来梳理一下流程： A&gt;调用MainActivity中的getTableFromPic方法，获取一个table字符串 我们可以进入看看这个方法的实现： 这里可以大体了解了，他是读取asset目录下的一个logo.png图片，然后获取图片的字节码，在进行操作，得到一个字符串，那么我们从上面的分析可以知道，其实这里的table字符串类似于一个密钥库。 B&gt;通过MainActivity中的getPwdFromPic方法，获取正确的密码内容 C&gt;获取我们输入内容的utf-8的字节码，然后调用access$0方法，获取加密之后的内容 D&gt;access$0方法中在调用bytesToAliSmsCode方法，获取加密之后的内容 这个方法是最核心的，我们通过分析知道，他的逻辑是，通过传递进来的字节数组，循环遍历数组，拿到字节转化成int类型，然后在调用密钥库字符串table的charAt得到字符，使用StringBuilder进行拼接。 通过上面的分析之后，我们知道获取加密之后的输入内容和正确的密码内容做比较，那么我们现在有的资源是：密钥库字符串和正确的加密之后的密码，以及加密的逻辑 那么我们的破解思路其实很简单了，相当于，我们知道了密钥库字符串，也知道了，加密之后的字符组成的字符串，那么可以通过遍历加密之后的字符串，循环遍历，获取字符，然后再去密钥库找到指定的index，然后在转成byte,保存到字节数组，然后用utf-8获取一个字符串，那么这个字符串就是我们要的密码。 下面我们就用代码来实现这个功能： 代码逻辑，很简单吧，其实这个函数相当于上面加密函数的bytesToAliSmsCode的反向实现，运行结果： OK，得到了正确的密码，下面来验证一下： 哈哈，不要太激动，成功啦啦~~。破解成功。 补充： 刚刚我们在断点调试的时候，看到了代码中用了Log来打印日志，tag是lil，那么我们可以打印这个log看看结果： 看到了，这里table是密钥库，pw是正确的加密之后的密码，enPassword是我们输入之后加密的密码。 所以从这里可以看到，这个例子，其实我们在破解apk的时候，有时候日志也是一个非常重要的信息。 三、思路整理1、我们通过apktool工具进行apk的反编译，得到smali源码和AndroidManifest.xml，然后修改AndroidManifest.xml中的debug属性为true，同时在入口处加上waitForDebug代码，进行debug等待，一般入口都是先找到入口Activity，然后在onCreate方法中的第一行这里需要注意的是：apktool工具一定要加上-d参数，这样反编译得到的文件是java文件，这样才能够被Eclipse识别，进行调试。 2、修改完成AndroidManifest.xml和添加waitForDebug之后，我们需要在使用apktool进行回编译，回编译之后得到的是一个没有签名的apk，我们还需要使用signapk.jar来进行签名，签名文件直接使用测试程序的签名文件就可以，最后在进行安装。 3、然后我们将反编译之后的smali源码导入到Eclipse工程中，找到关键点，进行下断点，这里的关键点，一般是我们先大致了解程序运行的结构，然后找到我们需要破解的地方，使用View分析工具，或者是使用jd-gui工具直接查看apk源码(使用dex2jar将dex文件转化成jar文件，然后用jd-gui进行查看)，找到代码的大体位置。然后下断点，这里我们可以借助Eclipse的DDMS自带的View分析工具找到对应控件的resid，然后在全局搜索这个控件的resid，或者直接在values/public.xml中查找，最终定位到这个控件位置，在查看他的点击事件即可。 4、设置远程调试工程，首先运行需要调试程序，然后在DDMS中找到对应的调试服务端的端口号，然后在Debug Configurations中设置远程调试项目，设置对应的调试端口和ip地址(一般都是本机pc,那就是localhost)，然后红色小蜘蛛变成绿色的，表示我们的远程调试项目连接关联上了调试程序，这里需要注意的是，一定需要关联正确，不然是没有任何效果的，关联成功之后，就可以进行操作。 5、操作的过程中，会进入到关键的断点处，通过F6单步，F5单步进入，F7单步跳出，来进行调试，找到关键方法，然后通过分析smali语法，了解逻辑，如果逻辑复杂的，可以通过查看具体的环境变量的值来观察，这里也是最重要的，也是最复杂的，同时这里也是没有规章可寻的，这个和每个人的逻辑思维以及破解能力有关系，分析关键的加密方法是需要功底的，当然这里还需要注意一个信息，就是Log日志，有时候也是很重要的一个信息。 6、最后一般当我们知道了核心方法的逻辑，要想得到正确的密码，还是需要自己用语言去实现逻辑的，比如本文中的加密方法，我们需要手动的code一下加密的逆向方法，才能得到正确的密码。 五、遗留问题1、使用apktool工具进行反编译有时候并不是那么顺利，比如像这样的报错： 这个一般都是apktool中解析出现了错误，其实这个都是现在apk为了抵抗apktool，做的apk加固策略，这个后面会写一篇文章如何应对这些加固策略，如何进行apk修复，其实原理就是分析apktool源码，找到指定的报错位置，进行apktool代码修复即可。 2、本文中说到了Java的调试系统，但是为了篇幅限制，没有详细的讲解了整个内容，后面会写一篇文章具体介绍Java中的调试系统以及Android的调试系统。 3、有时候我们还会遇到回编译成功了，然后遇到运行不起来的错误，这个就需要使用静态方式先去分析程序启动的逻辑，看看是不是程序做了什么运行限制，比如我们在静态分析那篇文章中，提到了应用为了防止反编译在回编译运行，在程序的入口处作了签名校验，如果校验失败，直接kill掉自己的进程，退出程序了，所以这时候我们还是需要使用静态方式去分析apk。 4、如何做到不修改AndroidManifest.xml中的debug属性就可以进行调试： 1》 修改boot.img,从而打开系统调试，这样就可以省去给app添加android:debuggable=”true”，再重打包的步骤了。2》直接修改系统属性，使用setpropex工具在已经root的设备上修改只读的系统属性。使用此工具来修改ro.secure和ro.debuggable的值。 这个也会在后面详细介绍这两种方法 四、总结这篇文章我们就介绍了如何使用Eclipse去动态调试反编译之后的smali源码，这种方式比静态方式高效很多的，比如本文中的这个例子，其实我们也可以使用静态方式进行破解的，但是肯定效率没有动态方式高效，所以以后我们又学会了一个技能，就是动态的调试smali源码来跟踪程序的核心点，但是现在市场上的大部分应用没有这么简单就破解了，比如核心的加密算法放到了native层去做，那么这时候就需要我们去动态调试so文件跟踪，这个是我们下一篇文章的内容，也有的时候，apk进行加固了，直接在apktool进行反编译就失败了，这时候我们就需要先进行apk修复，然后才能后续的操作，这个是我们下下篇的文章，如何应对apk的加固策略。通过这篇文章我们可以看到动态方式破解比静态方式高效的多，但是有时候我们还需要使用静态方式先做一些准备工作，所以在破解apk的时候，动静结合，才能做到完美的破解。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-静态分析破解Apk]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E9%9D%99%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、前言从这篇文章开始我们开始我们的破解之路，之前的几篇文章中我们是如何讲解怎么加固我们的Apk，防止被别人破解，那么现在我们要开始破解我们的Apk,针对于之前的加密方式采用相对应的破解技术，Android中的破解其实大体上可以分为静态分析和动态分析，对于这两种方式又可以细分为Java层(smail和dex)和native层(so)。所以我们今天主要来讲解如何通过静态分析来破解我们的apk，这篇文章我们会通过破解Java层和native层的例子来讲解。 二、准备工作第一、首先是基本知识： 1、了解Android中的Apk文件的结构。 2、了解Smail语法和dex文件格式 3、apk的签名机制 关于这三个知识点，这里就不做详细介绍了，不理解的同学可以自行网上学习，有很多资料讲解的。 第二、再者就是几个重要的工具 1、apktool：反编译的利器 2、dex2jar：将dex转化成jar 3、jd-gui：很好的查看jar文件的工具 4、IDA：收费的最全破解利器(分析dex和so都可以) 额外：上面四个工具是最基本的，但是现在网上也有一些更好的工具：JEB,GDA等。但是这些工具就是丰富了上面四个工具，所以说我们只要上面的四个工具就足够了。IDA工具我专门给了一个下载地址，其他的工具在我们提供的案例中。 三、技术原理准备工作完了，下面就来看一下今天的破解方式介绍： Android中的破解的静态分析说重要，也不重要，为什么这么说呢？ 因为我们到后面会介绍动态分析，那时候我们在破解一个Apk的时候，发现静态分析的方式几乎毫无用途，因为现在的程序加固的越来越高级，静态分析几乎失效，所以动态分析是必须的，但是要是说静态分析没有用，那么也错了，因为我们在有些场景下，只有静态分析能够开始破解之门，没有静态分析之后的结果，动态分析是无法开展的。这个下面会举例说明。所以说在破解的过程中，静态分析和动态分析一定会结合在一起的，只有这样我们才会勇往直前。下面就来看看我们如何通过静态分析来破解apk. 1. 静态分析的流程1、使用apktool来反编译apk 在这个过程中，我们会发现有些apk很轻易的被反编译了，但是有些apk每次反编译都会报各种错误，这个也是正常的，因为加固了吗。现在网上有很多对apk加密的方式，直接让反编译就通不过，比如Androidmanifest文件，dex文件等，因为apktool他需要解析这些重要的资源，一旦这些文件加密了那么就会终止，所以这里我们暂且都认为apk都能反编译的，因为我们今天是主要介绍怎么通过静态分析来破解，关于这里的反编译失败的问题，我后面会在用一篇文章详细介绍，到时候会列举一些反编译错误的例子。 2、得到程序的smail源码和AndroidManifest.xml文件 我们知道一个Android的程序入口信息都会在AndroidManifest.xml中，比如Application和入口Activity,所以我们肯定会先来分析这个文件，找到我们想要的信息，当然这里还有一个常用的命令需要记住： 1adb shell dumpsys activity top 能够获取到当前程序的Activity信息 之后我们会分析smail代码，进行代码逻辑的修改 3、直接解压apk文件得到classes.dex文件，然后用dex2jar工具得到jar,用jd-gui工具查看 这里我们主要很容易的查看代码，因为我们在第二步中得到了smail源码，就可以分析程序了，但是我们知道虽然smail语法不是很复杂，至少比汇编简单，但是怎么看着都是不方便的，还是看java代码比较方便，所以我们借助jd-gui工具查看代码逻辑，然后在smail代码中进行修改即可，上面说到的JEB工具，就加强了jd-gui工具的功能，它可以直接将smail源码翻译成java代码，这样我们就不需要先用jd-gui工具查看，再去smail源码中修改了，借助JEB即可。 4、分析native层代码 如果程序中有涉及到native层的话，我们可以用IDA打开指定的so文件。我们还是需要先看java代码，找到指定的so文件，在用IDA来静态分析so文件。 2、用到的技术上面介绍了静态分析的流程，下面来看一下静态分析的几个技术，我们在静态分析破解Apk的时候，首先需要找到突破点，找到关键的类和方法，当然这里就需要经验了，不是有方法可循的。但是我们会借助一些技术来加快破解。 1、全局查找关键字符串和日志信息 这个技术完全靠眼，我们在运行程序之后，会看到程序中出现的字符串，比如文本框，按钮上的文本，toast显示的信息等，都可能是重要信息，然后我们可以在jd-gui工具中全局搜索这个字符串，这样就会很快的定位到我们想要找的逻辑地方： 当然我们还有一个重要点就是Android中的Log信息，因为在一个大的项目中，会有多人开发，所以每个模块每个人开发，每个人都会调试信息，所以就会添加一些log信息，但是不是所有的人都会记得在项目发布的时候关闭项目中的所有log信息，这个也是我们在项目开发的过程中不好的习惯。这时候我们就可以通过程序运行起来之后，会打印一些log信息，那么我们可以通过这些信息获取突破点，Android中的log可以根据一个应用来进行过滤的，或者我们可以通过log信息中的字符串在jd-gui中进行全局搜索也是可以的。 2、代码的注入技术 在第一种方式中我们通过全局搜索一些关键的字符串来找突破点，但是这招有时候不好使，所以这时候我们需要加一些代码了来观察信息了，这里有一个通用的方法就是加入我们自己的log代码，来追踪代码的执行逻辑，因为这里讲的是静态分析技术，所以就用代码注入技术来跟踪执行逻辑，后面介绍了动态分析技术之后，那就简单了，我们可以随意的打断点来进行调试。这里的添加代码，就是修改smail代码，添加我们的日志信息即可，在下面我们会用例子来进行讲解，这个也是我们最常用的一种技术。 3、使用系统的Hook技术，注入破解程序进程，获取关键方法的执行逻辑 关于Android中的进程注入和Hook技术，这里就不做详细介绍了，技术介绍： 注入技术：http://blog.csdn.net/jiangwei0910410003/article/details/39292117 Hook技术：http://blog.csdn.net/jiangwei0910410003/article/details/41941393 Xposed使用：https://blog.csdn.net/xingkong_hdc/article/details/82531505 Xposed使用：https://www.jianshu.com/p/2d5f8e98d9f6 这两篇文章介绍了这两项技术，但是我们在实际操作过程中不用这两篇文章中用到的方式，因为这两篇文章只是介绍原理，技术还不是很成熟，关于这两个技术，网上有两个框架很成熟，也很实用，就是人们熟知的：Cydia和Xposed,关于这两个框架的话，网上的资料太多了，而且用起来也很容易，这里就不做太多的详细介绍了。 我们在实际的破解的过程中，这种方式用的有点少，因为这种方式效率有点低，所以只有在特定的场景下会使用。 4、使用IDA来静态分析so文件 这里终于用到了IDA工具了，本人是感觉这个工具太强大了，他可以查看so中的代码逻辑，我们看到的的可能是汇编指令，所以这里就有一个问题了，破解so的时候，我们还必须掌握一项技能，就是能看懂汇编指令，不然用IDA来破解程序，会很费经的，关于汇编指令，大学的时候，我们接触过了，但是我们当时感觉这东西又难，而且用的地方也很少，所以就没太在意，其实不然呀，真正懂汇编的人才是好的程序员： 看到些汇编指令，头立马就大了，不过这个用多了，破解多了，还是可以的。我们可以看到左边栏中有我们的函数，我们可以找到指定函数的定义的地方进行查看即可。其实IDA最强大的地方是在于他动态调试so文件，下一篇文章会介绍怎么动态调试so文件。当然IDA可也是可以直接查看apk文件的： 可以查看apk文件中的所有文件，我们可以选择classes.dex文件： 但是这里我们可能会遇到一个问题，就是如果应用程序太大的话，这个打开的过程中会很慢的，有可能IDA停止工作，所以要慢慢等啦： 打开之后，我们可以看到我们的类和方法名，这里还可以支持搜索类名和方法名Ctrl+F，也可以查看字符串内容(Shirt+F12): 我们发现IDA也是一个分析Java代码的好手，所以说这个工具太强大了啦啦~~ 四、案例分析上面讲解了静态分析的破解技术，那么下面就开始使用一个例子来看看静态分析的技术。 1. 反编译apk，获取smail文件首先我们拿到我们需要破解的Apk,使用apktool.jar工具来反编译： 1java -jar apktool.jar d xxx.apk 这个apk很是容易就被反编译了，看来并没有进行任何的加固。那就好办了，我们这里来改一下他的AndroidManifest.xml中的信息，改成可调式模式，这个是我们后面进行动态调试的前提，一个正式的apk，在AndroidManifest.xml中这个值是false的。 我们看看他的AndroidManifest.xml文件： 我们把这个值改成true.在回编译，这时候我们就可以动态调试这个apk了，所以在这点上我们可以看到，静态分析是动态分析的前提，这个值不修改的话，我们是办法进行后续的动态调试的。 2. 重新打包apk， 并安装修改成功之后，我们进行回编译： 首先进入到apktools工作目录， 进行反编译： 12345java -jar apktool.jar b -d sq -o debug.apk# sq是之前反编译的目录，debug.apk是回编译之后的文件# 这时候，debug.apk是不能安装运行的，因为没有签名，Android中是不允许安装一个没有签名的apk 下面还要继续签名，我们用系统自带的签名文件即可签名： 12java -jar .\sign\signapk.jar .\sign\testkey.x509.pem .\sign\testkey.pk8 debug.apk debug.sig.apk# signapk.jar， testkey.x509.pem, testkey.pk8 需要提前下载 注：其实我们在用IDE工具开发android项目的时候，工具就是用这个签名文件进行签名的，只是这个过程IDE帮我们做了。 后面就是直接安装这个apk,然后运行这个Apk。这个过程中我们只需要知道应用的包名和入口Activity名称即可，这个信息我们在AndroidManifest.xml中也是可以获取到的 当然我们用：adb shell dumpsys activity top 命令也可以得到： 3. 通过jd-gui查看代码，代码注入， Hook使用， 推导代码逻辑回编译之后，我们运行程序，发现有问题，就是点击程序的icon，没反应，运行不起来，我们在查看log中的异常信息，发现也没有抛出任何异常，那么这时候，我们就判断，他内部肯定做了什么校验工作，这个一般回编译之后的程序运行不起来的话，那就是内部做校验了，一般做校验的话，有两种： 1、对dex做校验，防止修改dex的 2、对apk的签名做校验，防止重新打包 那我们就需要从新看看他的代码，来看看是不是做了校验： 我们在分析代码的时候，肯定先看看他有没有自己定义Application,如果有定义的话，就需要看他自己的Application类，这里我们看到他定了自己的Application：com.shuqi.application.ShuqiApplication 我们解压apk,得到dex,然后dex2jar进行转化，得到jar，再用jd-gui查看这个类： 这里我们看到他的代码做混淆了，但是一些系统回调方法肯定不能混淆的，比如onCreate方法，但是这里我们一般找的方法是： 1、首先看这个类有没有静态方法和静态代码块，因为这类的代码会在对象初始化之前运行，可能在这里加载so文件，或者是加密校验等操作 2、再看看这个类的构造方法 3、最后再看生命周期方法 我们这里看到他的核心代码在onCreate中，调用了很多类的方法，猜想这里的某个方法做工作了？ 下面来看看我们怎么添加我们的日志信息，其实很简单，就是添加日志，需要修改smail文件，我们在去查看smail源码： 关于smail语法，本人认为不是很难，所以大家自己网上去搜一些资料学习一下即可，这里我们可以很清晰的看到调用了这些方法，那么我们就在每个方法加上我们的日志信息，这里加日志有两种方式，一种就是直接在这里调用系统的log方法，但是有两个问题： 1、需要导入包，在smail中修改 2、需要定义一个两个参数，一个是tag,msg,才能正常的打印log出来 明显这个方法有点麻烦，这里我们就自己定义一个MyLog类，然后反编译，得到MyLog的smail文件，添加到这个ShuqiApplication.smail的root目录下，然后在代码中直接调用即可，至于为何要放到root目录下，这样在代码中调用就不需要导入包了，比如SuqiApplication.smail中的一些静态方法调用： 编写日志类MyLog，这里就不粘贴代码了，我们新建一个项目之后，反编译得到MyLog.smail文件，放到目录中： 我们得到这个文件的时候，一定要注意，把MyLog.smail的包名信息删除，因为我们放到root目录下的，意味着这个MyLog类是没有任何包名的，这个需要注意，不然最后加的话，也是报错的。 我们在ShuqiApplication的onCreate方法中插入我们的日志方法： invoke-static {}, LMyLog;-&gt;print()V 但是我们在加代码的时候，需要注意的是，要找对地方加，所谓找对地方，就是在上个方法调用完之后添加，比如： invoke-virtual,invoke-static等，而且这些指令后面不能有：move-result-object，因为这个指令是获取方法的返回值，所以我们一般是这么加代码的： 1、在invoke-static/invoke-virtual指令他的返回类型是V之后可以加入 2、在invoke-static/invoke-virtual指令返回类型不是V,之后的move-result-object命令之后可以加入 加好了我们的日志代码之后，下面我们就回编译执行，在这个过程可能会遇到samil语法错误，这个就对应指定的文件修改就可以了，我们得到回编译的apk之后，可以在反编译一下，看看他的java代码： 我们看到了，我们添加的代码，在每个方法之后打印信息。 下面我们运行程序，同时开启我们的log的tag：adb logcat -s JW 看到我们打印的日志了，我们发现打印了三个log,这里需要注意的是，这里虽然打印了三个log,但是都是在不同的进程中，所以说一个进程中的log的话，只打印了一个，所以我们判断，问题出现在vr.h这个方法 我们查看这个方法源码： 果然，这个方法做了签名验证，不正确的话，直接退出程序。那么我们现在要想正常的运行程序的话，很简单了，直接注释这行代码：vr.h(this) 然后回编译，在运行，果然不报错了，这里就不在演示了： 好了，上面就通过注入代码，来跟踪问题，这个方法是很常用，也是很实在的。 4. 静态分析Native代码下面继续来介绍一下，如何使用IDA来静态分析native代码，这里一定要熟悉汇编指令，不然看起来很费劲的。 我们在反编译之后，看到他的onCreate方法中有一个加载so的代码 看看这个代码： 获取密码的方法，是native的，我们就来看看那个getDbPassword方法，用IDA打开libpsProcess.so文件： 我们看看这个函数的实现，我们一般直接看BL/BLX等信息，跳转逻辑，还有就是返回值，我们在函数的最后部分，发现一个重点，就是：BL __android_log_print 这个是在native层调用log的函数，我们在往上看，发现：tag是System.out.c 我们运行程序看起log看看，但是我们此时也可以在java层添加日志的：我们全局搜索这个方法，在yi这个类中调用的 我们修改yi.smail代码： 回编译，在运行程序，开启log： adb logcat -s JW adb logcat -s System.out.c 发现，返回的密码java层和native层是一样的。说明我们静态分析native还是有效的。 五、未解决的问题1、如何搞定apktool工具反编译出错的问题 这个我在开始的时候也说了，这里出错的原因大部分是apk进行加固了，所以后面我会专门介绍一下如何解决这样的问题 2、如何搞定让一个Apk可以调试 我们在上面看到一个apk想要能调试的话，需要修改android:debug的值，但是有时候，我们会遇到修改失败，导致程序不能运行，后面会专门介绍有几种方式来让一个发布后的apk可以调试 六、技术总结这篇文章我们介绍了如何使用静态方式去破解一个apk,我们在破解一个apk的时候，其实就是改点代码，然后能够运行起来，达到我们想要的功能，一般就是： 1、注释特定功能，比如广告展示等 2、得到方法的返回值，比如获取用户的密码 3、添加我们的代码，比如加入我们自己的监测代码和广告等 我们在静态分析代码的时候，需要遵循的大体路线： 1、首先能够反编译，得到AndroidManifest.xml文件，找到程序入口代码 2、找到我们想要的代码逻辑，一般会结合界面分析，比如我们想得让用户登录成功，我们肯定想要得到用户登录界面Activity,这时候我们可以用adb shell dumpsys activity top命令得到Activity名称，然后用Eclipse自带的程序当前视图分析工具：得到控件名称，或者是在代码中获取layout布局文件，一般是setContentView方法的调用地方，然后用布局文件结合代码得到用户登录的逻辑，进行修改 3、在关键的地方通过代码注入技术来跟踪代码执行逻辑 4、注意方法的返回值，条件判断等比较显眼的代码 5、对于有些apk中的源码，可能他有自己的加密算法，这时候我们需要获取到这个加密方法，如果加密方法比较复杂的话，我们就需要大批的测试数据来获取这个加密方法的逻辑，一般是输入和输出作为一个测试用例，比如阿里安全第一届比赛的第一题就可以用静态分析的方式破解，它内部就是一个加密算法，我们需要用测试数据来破解。 6、对于那些System.loadLibrary加载so文件的代码，我们只需要找到这个so文件，然后用IDA打开进行静态分析，因为有些apk中把加密算法放到了so中了，这时候我们也可以通过测试数据来获取加密算法。 7、通过上面的例子，我们可以总结一个方式，就是现在很多apk会做一些校验工作，一般在代码中包含：“signature”字符串信息，所以我们可以全局搜索一下，也许可以获取一些重要信息。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-动态分析破解Apk(破解加固)]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E5%8A%A8%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk(%E7%A0%B4%E8%A7%A3%E5%8A%A0%E5%9B%BA)%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、前言介绍一下如何应对现在市场中一些加固的apk的破解之道，现在市场中加固apk的方式一般就是两种：一种是对源apk整体做一个加固，放到指定位置，运行的时候在解密动态加载，还有一种是对so进行加固，在so加载内存的时候进行解密释放。我们今天主要看第一种加固方式，就是对apk整体进行加固。 二、案例分析按照国际惯例，咋们还是得用一个案例来分析讲解，这次依然采用的是阿里的CTF比赛的第三题： 题目是：要求输入一个网页的url，然后会跳转到这个页面，但是必须要求弹出指定内容的Toast提示，这个内容是：祥龙！ 了解到题目，我们就来简单分析一下，这里大致的逻辑应该是，输入的url会传递给一个WebView控件，进行展示网页，如果按照题目的逻辑的话，应该是网页中的Js会调用本地的一个Java方法，然后弹出相应的提示，那么这里我们就来开始操作了。 按照我们之前的破解步骤： 第一步：肯定是先用解压软件搞出来他的classes.dex文件，然后使用dex2jar+jd-gui进行查看java代码 擦，这里我们看到这里只有一个Application类，从这里我们可以看到，这个apk可能被加固了，为什么这么说呢？因为我们知道一个apk加固，外面肯定得套一个壳，这个壳必须是自定义的Application类，因为他需要做一些初始化操作，那么一般现在加固的apk的壳的Application类都喜欢叫StubApplication。而且，这里我们可以看到，除了一个Application类，没有其他任何类了，包括我们的如可Activity类都没有了，那么这时候会发现，很蛋疼，无处下手了。 第二步：我们会使用apktool工具进行apk的反编译，得到apk的AndroidManifest.xml和资源内容 反编译之后，看到程序会有一个入口的Activity就是MainActivity类，我们记住一点就是，不管最后的apk如何加固，即使我们看不到代码中的四大组件的定义，但是肯定会在AndroidManifest.xml中声明的，因为如果不声明的话，运行是会报错的。那么这里我们也分析完了该分析的内容，还是没发现我们的入口Activity类，而且我们知道他肯定是放在本地的一个地方，因为需要解密动态加载，所以不可能是放在网上的，肯定是本地，所以这里就有一些技巧了： 当我们发现apk中主要的类都没有了，肯定是apk被加固了，加固的源程序肯定是在本地，一般会有这么几个地方需要注意的： 1、应用程序的asset目录，我们知道这个目录是不参与apk的资源编译过程的，所以很多加固的应用喜欢把加密之后的源apk放到这里 2、把源apk加密放到壳的dex文件的尾部，这个肯定不是我们这里的案例，但是也有这样的加固方式，这种加固方式会发现使用dex2jar工具解析dex是失败的，我们这时候就知道了，肯定对dex做了手脚 3、把源apk加密放到so文件中，这个就比较难了，一般都是把源apk进行拆分，存到so文件中，分析难度会加大的。 一般都是这三个地方，其实我们知道记住一点：就是不管源apk被拆分，被加密了，被放到哪了，只要是在本地，我们都有办法得到他的。 好了，按照这上面的三个思路我们来分析一下，这个apk中加固的源apk放在哪了？通过刚刚的dex文件分析，发现第二种方式肯定不可能了，那么会放在asset目录中吗？我们查看asset目录： 看到asset目录中的确有两个jar文件，而且我们第一反应是使用jd-gui来查看jar，可惜的是打开失败，所以猜想这个jar是经过处理了，应该是加密，所以这里很有可能是存放源apk的地方。但是我们上面也说了还有第三种方式，我们去看看libs目录中的so文件： ** 擦，这里有三个so文件，而我们上面的Application中加载的只有一个so文件：libmobisec.so，那么其他的两个so文件很有可能是拆分的apk文件的藏身之处。 通过上面的分析之后，我们大致知道了两个地方很有可能是源apk的藏身地方，一个是asset目录，一个是libs目录，那么分析完了之后，我们发现现在面临两个问题： 第一个问题：asset目录中的jar文件被处理了，打不开，也不知道处理逻辑 第二个问题：libs目录中的三个so文件，唯一加载了libmobisec.so文件了 那么这里现在的唯一入口就是这个libmobisec.so文件了，因为上层的代码没有，没法分析，下面来看一下so文件： 擦，发现蛋疼的是，这里没有特殊的方法，比如Java_开头的什么，所以猜测这里应该是自己注册了native方法，混淆了native方法名称，那么到这里，我们会发现我们遇到的问题用现阶段的技术是没法解决了。 三、获取正确的dex内容分析完上面的破解流程之后，发现现在首要的任务是先得到源apk程序，通过分析知道，处理的源apk程序很难找到和分析，所以这里就要引出今天说的内容了，使用动态调试，给libdvm.so中的函数：dvmDexFileOpenPartial 下断点，然后得到dex文件在内存中的起始地址和大小，然后dump处dex数据即可。 那么这里就有几个问题了： 第一个问题：为何要给dvmDexFileOpenPartial 这个函数下断点？ 因为我们知道，不管之前的源程序如何加固，放到哪了，最终都是需要被加载到内存中，然后运行的，而且是没有加密的内容，那么我们只要找到这的dex的内存位置，把这部分数据搞出来就可以了，管他之前是如何加固的，我们并不关心。那么问题就变成了，如何获取加载到内存中的dex的地址和大小，这个就要用到这个函数了：dvmDexFileOpenPartial 因为这个函数是最终分析dex文件，加载到内存中的函数： int dvmDexFileOpenPartial(const void* addr, int len, DvmDex** ppDvmDex); 第一个参数就是dex内存起始地址，第二个参数就是dex大小。 第二个问题：如何使用IDA给这个函数下断点 我们在之前的一篇文章中说到了，在动态调试so，下断点的时候，必须知道一个函数在内存中的绝对地址，而函数的绝对地址是：这个函数在so文件中的相对地址+so文件映射到内存中的基地址，这里我们知道这个函数肯定是存在libdvm.so文件中的，因为一般涉及到dvm有关的函数功能都是存在这个so文件中的，那么我们可以从这个so文件中找到这个函数的相对地址，运行程序之后，在找到libdvm.so的基地址，相加即可，那么我们如何获取到这个libdvm.so文件呢？这个文件是存放在设备的/system/lib目录下的： 那么我们只需要使用adb pull 把这个so文件搞出来就可以了。 好了，解决了这两个问题，下面就开始操作了： 第一步：运行设备中的android_server命令，使用adb forward进行端口转发 这里的android_server工具可以去ida安装目录中dbgsrv文件夹中找到 第二步：使用命令以debug模式启动apk adb shell am start -D -n com.ali.tg.testapp/.MainActivity 因为我们需要给libdvm.so下断点，这个库是系统库，所以加载时间很早，所以我们需要像之前给JNI_OnLoad函数下断点一样，采用debugger模式运行程序，这里我们通过上面的AndroidManifest.xml中，得到应用的包名和入口Activity： 而且这里的android:debuggable=true，可以进行debug调试的。 第三步：双开IDA，一个用于静态分析libdvm.so，一个用于动态调试libdvm.so 通过IDA的Debugger菜单，进行进程附加操作： 第四步：使用jdb命令启动连接attach调试器 1jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 但是这里可能会出现这样的错误： 这个是因为，我们的8700端口没有指定，这时候我们可以通过Eclipse的DDMS进行端口的查看： 看到了，这里是8600端口，但是基本端口8700不在，所以这里我们有两种处理方式，一种是把上面的命令的端口改成8600，还有一种是选中这个应用，使其具有8700端口： 点击这个条目即可，这时候我们在运行上面的jdb命令： 处于等待状态。 第四步：给dvmDexFileOpenPartial函数下断点 使用一个IDA静态分析得到这个函数的相对地址：43308 在动态调试的IDA解密，使用Ctrl+S键找到libdvm.so的在内存中的基地址：41579000 然后将两者相加得到绝对地址：43308+41579000=415BC308，使用G键，跳转： 跳转到dvmDexFileOpenPartial函数处，下断点： 第五步：点击运行按钮或者F9运行程序 之前的jdb命令就连接上了： IDA出现如下界面，不要理会，一路点击取消按钮即可 运行到了dvmDexFileOpenPartial函数处： 使用F8进行单步调试，但是这里需要注意的是，只要运行过了PUSH命令就可以了，记得不要越过下面的BL命令，因为我们没必要走到那里，当执行了PUSH命令之后，我们就是使用脚本来dump处内存中的dex数据了，这里有一个知识点，就是R0~R4寄存器一般是用来存放一个函数的参数值的，那么我们知道dvmDexFileOpenPartial函数的第一个参数就是dex内存起始地址，第二个参数就是dex大小： 那么这里就可以使用这样的脚本进行dump即可： static main(void){ auto fp, dex_addr, end_addr; fp = fopen(“F:\dump.dex”, “wb”); end_addr = r0 + r1; for ( dex_addr = r0; dex_addr &lt; end_addr; dex_addr ++ ) fputc(Byte(dex_addr), fp);} 脚本不解释了，非常简单，而且这个是固定的格式，以后dump内存中的dex都是这段代码，我们将dump出来的dex保存到F盘中。 然后这时候，我们使用：Shirt+F2 调出IDA的脚本运行界面： 点击运行，这里可能需要等一会，运行成功之后，我们去F盘得到dump.dex文件，其实这里我们的IDA使命就完成了，因为我们得到了内存的dex文件了，下面开始就简单了，只要分析dex文件即可 四、分析正确的dex文件内容我们拿到dump.dex之后，使用dex2jar工具进行反编译： 可惜的是，报错了，反编译失败，主要是有一个类导致的，开始我以为是dump出来的dex文件有问题，最后我用baksmali工具得到smali文件是可以的，所以不是dump出来的问题，我最后用baksmali工具将dex转化成smali源码： j 1ava -jar baksmali-2.0.3.jar -o C:\classout/ dump.dex 得到的smali源码目录classout在C盘中： 我们得到了指定的smali源码了。 那么下面我们就可以使用静态方式分析smali即可了： 首先找到入口的MainActivity源码： 这里不解释了，肯定是找按钮的点击事件代码处，这里是一个btn_listener变量，看这个变量的定义： 是MainActivity$1内部类定义，查看这个类的smali源码，直接查看他的onClick方法： 这里可以看到，把EditText中的内容，用Intent传递给WebViewActivity中，但是这里的intent数据的key是加密的。 下面继续看WebViewActivity这个类： 我们直接查找onCreate方法即可，看到这里是初始化WebView，然后进行一些设置，这里我们看到一个@JavascriptInterface 这个注解，我们在使用WebView的时候都知道，他是用于Js中能够访问的设置了这个注解的方法，没有这个注解的方法Js是访问不了的 注意： 我们知道这个注解是在SDK17加上的，也就是Android4.2版本中，那么在之前的版本中没有这个注解，任何public的方法都可以在JS代码中访问，而Java对象继承关系会导致很多public的方法都可以在JS中访问，其中一个重要的方法就是 getClass()。然后JS可以通过反射来访问其他一些内容。那么这里就有这个问题了：比如下面的一段JS代码： function findobj(){ for (var obj in window) { if ("getClass" in window[obj]) { return window[obj] } } } 看到了，这段js代码很危险的，使用getClass方法，得到这个对象(java中的每个对象都有这个方法的)，用这个方法可以得到一个java对象，然后我们就可以调用这个对象中的方法了。这个也算是WebView的一个漏洞了。 所以通过引入 @JavascriptInterface注解，则在JS中只能访问 @JavascriptInterface注解的函数。这样就可以增强安全性。 回归到正题，我们上面分析了smali源码，看到了WebView的一些设置信息，我们可以继续往下面看： 这里的我们看到了一些重要的方法，一个是addJavascriptInterface，一个是loadUrl方法。 我们知道addjavaascriptInterface方法一般的用法： mWebView.addJavascriptInterface(new JavaScriptObject(this), “jiangwei”); 第一个参数是本地的Java对象，第二个参数是给Js中使用的对象的名称。然后js得到这个对象的名称就可以调用本地的Java对象中的方法了。 看了这里的addjavaascriptInterface方法代码，可以看到，这里用 ListViewAutoScrollHelpern;-&gt;decrypt_native(Ljava/lang/String;I)Ljava/lang/String; 将js中的名称进行混淆加密了，这个也是为了防止恶意的网站来拦截url，然后调用我们本地的Java中的方法。 注意： 这里又存在一个关于WebView的安全问题，就是这里的js访问的对象的名称问题，比如现在我的程序中有一个Js交互的类，类中有一个获取设备重要信息的方法，比如这里获取设备的imei方法，如果我们的程序没有做这样名称的混淆的话，破解者得到这个js名称和方法名，然后就伪造一个恶意url，来调用我们程序中的这个方法，比如这样一个例子： 然后在设置js名称： 我们就可以伪造一个恶意的url页面来访问这个方法，比如这个恶意的页面代码如下： 运行程序： 看到了，这里恶意的页面就成功的调用了程序中的一个重要方法。 所以，我们可以看到，对Js交互中的对象名称做混淆是必要的，特别是本地一些重要的方法。 回归到正题，我们分析完了WebView的一些初始化和设置代码，而且我们知道如果要被Js访问的方法，那么必须要有@JavascriptInterface注解 因为在Java中注解也是一个类，所以我们去注解类的源码看看那个被Js调用的方法： 这里看到了有一个showToast方法，展示的内容：\u7965\u9f99\uff01 ，我们在线转化一下： 擦，这里就是题目要求展示的内容。 好了，到这里我们就分析完了apk的逻辑了，下面我们来整理一下： 1、在MainActivity中输入一个页面的url，跳转到WebViewActivity进行展示 2、WebViewActivity有Js交互，需要调用本地Java对象中的showToast方法展示消息 问题： 因为这里的js对象名称进行了加密，所以这里我们自己编写一个网页，但是不知道这个js对象名称，无法完成showToast方法的调用 五、破解的方法下面我们就来分析一下如何解决上面的问题，其实解决这个问题，我们现有的方法太多了 第一种方法：修改smali源码，把上面的那个js对象名称改成我们自己想要的，比如：jiangwei，然后在自己编写的页面中直接调用：jiangwei.showToast方法即可，不过这里需要修改smali源码，在使用smali工具回编译成dex文件，在弄到apk中，在运行。方法是可行的，但是感觉太复杂，这里不采用 第二种方法：利用Android4.2中的WebView的漏洞，直接使用如下Js代码即可 这里根本不需要任何js对象的名称，只需要方法名就可以完成调用，所以这里可以看到这个漏洞还是很危险的。 第三种方法：我们看到了那个加密方法，我们自己写一个程序，来调用这个方法，尽然得到正确的js对象名称，这里我们就采用这种方式，因为这个方式有一个新的技能，所以这里我就讲解一下了。 那么如果用第三种方法的话，就需要再去分析那个加密方法逻辑了： android.support.v4.widget.ListViewAutoScrollHelpern在这个类中，我们再去查找这个smali源码： 这个类加载了libtranslate.so库，而且加密方法是native层的，那么我们用IDA查看libtranslate.so库： 我们搜一下Java开头的函数，发现并没有和decrypt_native方法对应的native函数，说明这里做了native方法的注册混淆，我们直接看JNI_OnLoad函数： 这里果然是自己注册了native函数，但是分析到这里，我就不往下分析了，为什么呢？因为我们其实没必要搞清楚native层的函数功能，我们知道了Java层的native方法定义，那么我们可以自己定义一个这么个native方法来调用libtranslate.so中的加密函数功能： 我们新建一个Demo工程，仿造一个ListViewAutoScrollHelpern类，内部在定义一个native方法： 然后我们在MainActivity中加载libtranslate.so： 然后调用那个native方法，打印结果： 这里的方法的参数可以查看smali源码中的那个方法参数： 点击运行，发现有崩溃的，我们查看log信息： 是libtranslate.so中有一个PagerTitleStripIcsn类找不到，这个类应该也有一个native方法，我们在构造这个类： 再次运行，还是报错，原因差不多，还需要在构造一个类：TaskStackBuilderJellybeann 好了，再次点击运行： OK了，成功了，从这个log信息可以看出来了，解密之后的js对象名称是：SmokeyBear，那么下面就简单了，我们在构造一个url页面，直接调用：SmokeyBear.showToast即可。 注意： 这里我们看到，如果知道了Java层的native方法的定义，那么我们就可以调用这个native方法来获取native层的函数功能了，这个还是很不安全的，但是我们如何防止自己的so被别人调用呢？可以在so中的native函数做一个应用的签名校验，只有属于自己的签名应用才能调用，否则直接退出。 六，开始测试上面已经知道了js的对象名称，下面我们就来构造这个页面了： 那么这里又有一个问题了，这个页面构造好了？放哪呢？有的同学说我有服务器，放到服务器上，然后输入url地址就可以了，的确这个方法是可以的，但是有的同学没有服务器怎么办呢？这个也是有方法的，我们知道WebView的loadUrl方法是可以加载本地的页面的，所以我们可以把这个页面保存到本地，但是需要注意的是，这里不能存到SD卡中，因为这个应用没有读取SD的权限，我们可以查看他的AndroidManifest.xml文件： 我们在不重新打包的情况下，是没办法做到的，那么放哪呢？其实很简单了，放在这个应用的/data/data/com.ali.tg.testapp/目录下即可，因为除了SD卡位置，这个位置是最好的了，那么我们知道WebView的loadUrl方法在加载本地的页面的格式是： file:///data/data/com.ali.tg.testapp/crack.html 那么我们直接输入即可 注意： 这里在说一个小技巧：就是我们在一个文本框中输入这么多内容，是不是有点蛋疼，我们其实可以借助于命令来实现输入的，就是使用：adb shell input text ”我们需要输入的内容“。 具体用法很简单，打开我们需要输入内容的EditText，点击调出系统的输入法界面，然后执行上面的命令即可： 不过这里有一个小问题，就是他不识别分号： 不过我们直接修改成分号点击进入： 运行成功，看到了toast的展示。 七、内容整理到这里我们就破解成功了，下面来看看整理一下我们的破解步骤： 1、破解的常规套路 我们按照破解惯例，首先解压出classses.dex文件，使用dex2jar工具查看java代码，但是发现只有一个Application类，所以猜测apk被加壳了，然后用apktool来反编译apk，得到他的资源文件和AndroidManifest.xml内容，找到了包名和入口的Activity类。 2、加固apk的源程序一般存放的位置 知道是加固apk了，那么我们就分析，这个加固的apk肯定是存放在本地的一个地方，一般是三个地方： 1》应用的asset目录中 2》应用的libs中的so文件中 3》应用的dex文件的末尾 我们分析了一下之后，发现asset目录中的确有两个jar文件，但是打不开，猜测是被经过处理了，所以我们得分析处理逻辑，但是这时候我们也没有代码，怎么分析呢？所以这时候就需要借助于dump内存dex技术了： 不管最后的源apk放在哪里，最后都是需要经历解密动态加载到内存中的，所以分析底层加载dex源码，知道有一个函数：dvmDexFileOpenPartial 这个函数有两个重要参数，一个是dex的其实地址，一个是dex的大小，而且知道这个函数是在libdvm.so中的。所以我们可以使用IDA进行动态调试获取信息 3、双开IDA开始获取内存中的dex内容 双开IDA，走之前的动态破解so方式来给dvmDexFileOpenPartial函数下断点，获取两个参数的值，然后使用一段脚本，将内存中的dex数据保存到本地磁盘中。 4、分析获取到的dex内容 得到了内存中的dex之后，我们在使用dex2jar工具去查看源码，但是发现保存，以为是dump出来的dex格式有问题，但是最后使用baksmali工具进行处理，得到smali源码是可以的，然后我们就开始分析smali源码。 5、分析源码了解破解思路 通过分析源码得知在WebViewActivity页面中会加载一个页面，然后那个页面中的js会调用本地的Java对象中的一个方法来展示toast信息，但是这里我们遇到了个问题：Js的Java对象名称被混淆加密了，所以这时候我们需要去分析那个加密函数，但是这个加密函数是native的，然后我们就是用IDA去静态分析了这个native函数，但是没有分析完成，因为我们不需要，其实很简单，我们只需要结果，不需要过程，现在解密的内容我们知道了，native方法的定义也知道了，那么我们就去写一个简单的demo去调用这个so的native方法即可，结果成功了，我们得到了正确的Js对象名称。 6、了解WebView的安全性 WebView的早期版本的一个漏洞信息，在Android4.2之前的版本WebView有一个漏洞，就是可以执行Java对象中所有的public方法，那么在js中就可以这么处理了，先获取geClass方法获取这个对象，然后在调用这个对象中的一些特定方法即可，因为Java中所有的对象都有一个getClass方法，而这个方法是public的，同时能够返回当前对象。所以在Android4.2之后有了一个注解： @JavascriptInterface ，只有这个注解标识的方法才能被Js中调用。 7、获取输入的新技能 验证结果的过程中我们发现了一个技巧，就是我们在输入很长的文本的时候，比较繁琐，可以借助adb shell input text命令来实现。 八、技术点概要1、通过dump出内存中的dex数据，可以佛挡杀佛了，不管apk如何加固，最终都是需要加载到内存中的。 2、了解到了WebView的安全性的相关知识，比如我们在WebView中js对象名称做一次混淆还是有必要的，防止被恶意网站调用我们的本地隐私方法。 3、可以尝试调用so中的native方法，在知道了这个方法的定义之后 4、adb shell input text 命令来辅助我们的输入 九、总结这里就介绍了Android中如何dump出那些加固的apk程序，其实核心就一个：不管上层怎么加固，最终加载到内存的dex肯定不是加固的，所以这个dex就是我们想要的，这里使用了IDA来动态调试libdvm.so中的dvmDexFileOpenPartial函数来获取内存中的dex内容，同时还可以使用gdb+gdbserver来获取，这个感兴趣的同学自行搜索吧。结合了之前的两篇文章，就算善始善终，介绍了Android中大体的破解方式，当然这三种方式不是万能的，因为加固和破解是相生相克的，没有哪个有绝对的优势，只是两者相互进步罢了，当然还有很多其他的破解方式，后面如果遇到的话，会在详细说明。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android逆向</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-动态分析破解Apk(调试so)]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E5%8A%A8%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk(%E8%B0%83%E8%AF%95so)%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、知识准备**我们在介绍如何调试so文件的时候，先来看一下准备知识： 1. native方法的定义和调用什么是NDKAndroid 平台从一开就已经支持了C/C++了。我们知道Android的SDK主要是基于Java的，所以导致了在用Android SDK进行开发的工程师们都必须使用Java语言。不过，Google从一开始就说明Android也支持JNI编程方式，也就是第三方应用完成可以通过JNI调用自己的C动态度。于是NDK就应运而生了。 Android NDK 是一套允许您使用原生代码语言(例如C和C++) 实现部分应用的工具集。在开发某些类型应用时，这有助于您重复使用以这些语言编写的代码库。 Android NDK 就是一套工具集合，允许你使用C/C++语言来实现应用程序的部分功能。 NDK 是Native Develop Kit的含义，从含义很容易理解，本地开发。大家都知道，Android 开发语言是Java，不过我们也知道，Android是基于Linux的，其核心库很多都是C/C++的，比如Webkit等。那么NDK的作用，就是Google为了提供给开发者一个在Java中调用C/C++代码的一个工作。NDK本身其实就是一个交叉工作链，包含了Android上的一些库文件，然后，NDK为了方便使用，提供了一些脚本，使得更容易的编译C/C++代码。总之，在Android的SDK之外，有一个工具就是NDK，用于进行C/C++的开发。一般情况，是用NDK工具把C/C++编译为.co文件，然后在Java中调用。 从NDK到.so 从上图这个Android系统框架来看，我们上层通过JNI来调用NDK层的，使用这个工具可以很方便的编写和调试JNI的代码。因为C语言的不跨平台，在Mac系统的下使用NDK编译在Linux下能执行的函数库——so文件。其本质就是一堆C、C++的头文件和实现文件打包成一个库。目前Android系统支持以下七种不用的CPU架构，每一种对应着各自的应用程序二进制接口ABI：(Application Binary Interface)定义了二进制文件(尤其是.so文件)如何运行在相应的系统平台上，从使用的指令集，内存对齐到可用的系统函数库。 什么是JNIJava调用C/C++在Java语言里面本来就有的，并非Android自创的，即JNI。JNI就是Java调用C++的规范。当然，一般的Java程序使用的JNI标准可能和android不一样，Android的JNI更简单。 JNI，全称为Java Native Interface，即Java本地接口，JNI是Java调用Native 语言的一种特性。通过JNI可以使得Java与C/C++机型交互。即可以在Java代码中调用C/C++等语言的代码或者在C/C++代码中调用Java代码。由于JNI是JVM规范的一部分，因此可以将我们写的JNI的程序在任何实现了JNI规范的Java虚拟机中运行。同时，这个特性使我们可以复用以前用C/C++写的大量代码JNI是一种在Java虚拟机机制下的执行代码的标准机制。代码被编写成汇编程序或者C/C++程序，并组装为动态库。也就允许非静态绑定用法。这提供了一个在Java平台上调用C/C++的一种途径，反之亦然。 实现JNI的过程第1步：在Java中先声明一个native方法 第2步：编译Java源文件javac得到.class文件 第3步：通过javah -jni命令导出JNI的.h头文件 第4步：使用Java需要交互的本地代码，实现在Java中声明的Native方法（如果Java需要与C++交互，那么就用C++实现Java的Native方法。） 第5步：将本地代码编译成动态库(Windows系统下是.dll文件，如果是Linux系统下是.so文件，如果是Mac系统下是.jnilib) 第6步：通过Java命令执行Java程序，最终实现Java调用本地代码。 在Java开发中的静态类定义方法12345678public class NDKTools &#123; static &#123; System.loadLibrary("ndkdemotest-jni"); &#125; public static native String getStringFromNDK();&#125; 2. 在python中怎么调用.so文件在linux系统中可以使用python的ctypes库来导入并调用.sh静态库 12345from ctypes import cdll cur = cdll.LoadLibrary('./libmax.so') a = cur.``max(1, 2) 3. IDA工具的使用 这里有多个窗口，也有多个视图，用到最多的就是： 1、Function Window对应的so函数区域：这里我们可以使用ctrl+f进行函数的搜索 2、IDA View对应的so中代码指令视图：这里我们可以查看具体函数对应的arm指令代码 3、Hex View对应的so的十六进制数据视图：我们可以查看arm指令对应的数据等 当然在IDA中我们还需要知道一些常用的快捷键： 1、强大的F5快捷键可以将arm指令转化成可读的C语言，帮助分析 首先选中需要翻译成C语言的函数，然后按下F5： 看到了，立马感觉清爽多了，这些代码看起来应该会好点了。 下面我们还需要做一步，就是还原JNI函数方法名一般JNI函数方法名首先是一个指针加上一个数字，比如v3+676。然后将这个地址作为一个方法指针进行方法调用，并且第一个参数就是指针自己，比如(v3+676)(v3…)。这实际上就是我们在JNI里经常用到的JNIEnv方法。因为Ida并不会自动的对这些方法进行识别，所以当我们对so文件进行调试的时候经常会见到却搞不清楚这个函数究竟在干什么，因为这个函数实在是太抽象了。解决方法非常简单，只需要对JNIEnv指针做一个类型转换即可。比如说上面提到a1和v4指针： 我们可以选中a1变量，然后按一下y键： 然后将类型声明为：JNIEnv*。 确定之后再来看： 修改之后，是不是瞬间清晰了很多？另外有人（ 貌似是看雪论坛上的）还总结了所有JNIEnv方法对应的数字，地址以及方法声明： 2、Shirt+F12快捷键，速度打开so中所有的字符串内容窗口 有时候，字符串是一个非常重要的信息，特别是对于破解的时候，可能就是密码，或者是密码库信息。 3、Ctrl+S快捷键，有两个用途，在正常打开so文件的IDA View视图的时候，可以查看so对应的Segement信息 可以快速得到，一个段的开始位置和结束位置，不过这个位置是相对位置，不是so映射到内存之后的位置，关于so中的段信息，不了解的同学可以参看这篇文章：Android中so文件格式详解 这篇文章介绍的很很清楚了，这里就不在作介绍了。 当在调试页面的候，ctrl+s可以快速定位到我们想要调试的so文件映射到内存的地址： 因为一般一个程序，肯定会包含多个so文件的，比如系统的so就有好多的，一般都是在/system/lib下面，当然也有我们自己的so，这里我们看到这里的开始位置和结束位置就是这个so文件映射到内存中： 这里我们可以使用cat命令查看一个进程的内存映射信息：cat /proc/[pid]/maps 我们看到映射信息中有多so文件，其实这个不是多个so文件，而是so文件中对应的不同Segement信息被映射到内存中的，一般是代码段，数据段等，因为我们需要调试代码，所以我们只关心代码段，代码段有一个特点就是具有执行权限x，所以我们只需要找到权限中有x的那段数据即可。 4、G快捷键：在IDA调试页面的时候，我们可以使用S键快速跳转到指定的内存位置 这里的跳转地址，是可以算出来的，比如我现在想跳转到A函数，然后下断点，那么我们可以使用上面说到的ctrl+s查找到so文件的内存开始的基地址，然后再用IDA View中查看A函数对应的相对地址，相加就是绝对地址，然后跳转到即可，比如这里的： Java_cn_wjdiankong_encryptdemo_MainActivity_isEquals 函数的IDA View中的相对地址(也就是so文件的地址)：E9C 上面看到so文件映射到内存的基地址：74FE4000 那么跳转地址就是：74FE4000+E9C=74FE4E9C 注意： 一般这里的基地址只要程序没有退出，在运行中，那么他的值就不会变，因为程序的数据已经加载到内存中了，基地址不会变的，除非程序退出，又重新运行把数据加载内存中了，同时相对地址是永远会变的，只有在修改so文件的时候，文件的大小改变了，可能相对地址会改变，其他情况下不会改变，相对地址就是数据在整个so文件中的位置。 这里我们可以看到函数映射到内存中的绝对地址了。 注意： 有时候我们发现跳转到指定位置之后，看到的全是DCB数据，这时候我们选择函数地址，点击P键就可以看到arm指令源码了： 5、调试快捷键：F8单步调试，F7单步进入调试 上面找到函数地址之后，我们可以下断点了，下断点很简单，点击签名的绿色圈点，变成红色条目即可，然后我们可以点击F9快捷键，或者是点击运行按钮，即可运行程序： 其中还有暂停和结束按钮。我们运行之后，然后在点击so的native函数，触发断点逻辑： 这时候，我们看到进入调试界面，点击F8可以单步调试，看到有一个PC指示器，其实在arm中PC是一个特殊的寄存器，用来存储当前指令的地址，这个下面会介绍到。 好了到这里，我们就大致说了一下关于IDA在调试so文件的时候，需要用到的快捷键： 1&gt;、Shift+F12快速查看so文件中包含的字符串信息 2&gt;、F5快捷键可以将arm指令转化成可读的C代码，这里同时可以使用Y键，修改JNIEnv的函数方法名 3&gt;、Ctrl+S有两个用途，在IDA View页面中可以查看so文件的所有段信息，在调试页面可以查看程序所有so文件映射到内存的基地址 4&gt;、G键可以在调试界面，快速跳转到指定的绝对地址，进行下断点调试，这里如果跳转到目的地址之后，发现是DCB数据的话，可以在使用P键，进行转化即可，关于DCB数据，下面会介绍的。 5&gt;、F7键可以单步进入调试，F8键可以单步调试 4.常用的ARM指令集知识我们在上面看到IDA打开so之后，看到的是纯种的汇编指令代码，所以这就要求我们必须会看懂汇编代码，就类似于我们在调试Java层代码的时候一样，必须会smali语法，庆幸的是，这两种语法都不是很复杂，所以我们知道一些大体的语法和指令就可以了，下面我们来看看arm指令中的寻址方式，寄存器，常用指令，看完这三个知识点，我们就会对arm指令有一个大体的了解，对于看arm指令代码也是有一个大体的认知了。 1、arm指令中的寻址方式 1&gt;. 立即数寻址也叫立即寻址，是一种特殊的寻址方式，操作数本身包含在指令中，只要取出指令也就取到了操作数。这个操作数叫做立即数，对应的寻址方式叫做立即寻址。例如：MOV R0,#64 ；R0 ← 642&gt;. 寄存器寻址寄存器寻址就是利用寄存器中的数值作为操作数，也称为寄存器直接寻址。例如： ADD R0，R1， R2 ；R0 ← R1 + R23&gt;. 寄存器间接寻址寄存器间接寻址就是把寄存器中的值作为地址，再通过这个地址去取得操作数，操作数本身存放在存储器中。例如：LDR R0，[R1] ；R0 ←[R1]4&gt;. 寄存器偏移寻址这是ARM指令集特有的寻址方式，它是在寄存器寻址得到操作数后再进行移位操作，得到最终的操作数。例如：MOV R0，R2，LSL #3 ；R0 ← R2 * 8 ，R2的值左移3位，结果赋给R0。5&gt;. 寄存器基址变址寻址寄存器基址变址寻址又称为基址变址寻址，它是在寄存器间接寻址的基础上扩展来的。它将寄存器（该寄存器一般称作基址寄存器）中的值与指令中给出的地址偏移量相加，从而得到一个地址，通过这个地址取得操作数。例如：LDR R0，[R1，#4] ；R0 ←[R1 + 4]，将R1的内容加上4形成操作数的地址，取得的操作数存入寄存器R0中。6&gt;. 多寄存器寻址这种寻址方式可以一次完成多个寄存器值的传送。例如：LDMIA R0，{R1，R2，R3，R4} ；R1←[R0]，R2←[R0+4]，R3←[R0+8]，R4←[R0+12]7&gt;. 堆栈寻址堆栈是一种数据结构，按先进后出（First In Last Out，FILO）的方式工作，使用堆栈指针（Stack Pointer, SP）指示当前的操作位置，堆栈指针总是指向栈顶。堆栈寻址举例如下：STMFD SP！，｛R1－R7, LR｝ ；将R1－R7, LR压入堆栈。满递减堆栈。LDMED SP！，｛R1－R7, LR｝ ；将堆栈中的数据取回到R1－R7, LR寄存器。空递减堆栈。 2、ARM中的寄存器 R0-R3:用于函数参数及返回值的传递R4-R6, R8, R10-R11:没有特殊规定，就是普通的通用寄存器R7:栈帧指针(Frame Pointer).指向前一个保存的栈帧(stack frame)和链接寄存器(link register， lr)在栈上的地址。R9:操作系统保留R12:又叫IP(intra-procedure scratch )R13:又叫SP(stack pointer)，是栈顶指针R14:又叫LR(link register)，存放函数的返回地址。R15:又叫PC(program counter)，指向当前指令地址。 3、ARM中的常用指令含义 ADD 加指令SUB 减指令STR 把寄存器内容存到栈上去LDR 把栈上内容载入一寄存器中.W 是一个可选的指令宽度说明符。它不会影响为此指令的行为，它只是确保生成 32 位指令。Infocenter.arm.com的详细信息BL 执行函数调用，并把使lr指向调用者(caller)的下一条指令，即函数的返回地址BLX 同上，但是在ARM和thumb指令集间切换。CMP 指令进行比较两个操作数的大小 4、ARM指令简单代码段分析 C代码： #include &lt;stdio.h&gt;int func(int a, int b, int c, int d, int e, int f){ ​ int g = a + b + c + d + e + f;​ return g;} 对应的ARM指令： add r0, r1 将参数a和参数b相加再把结果赋值给r0ldr.w r12, [sp] 把最的一个参数f从栈上装载到r12寄存器add r0, r2 把参数c累加到r0上ldr.w r9, [sp, #4] 把参数e从栈上装载到r9寄存器add r0, r3 累加d累加到r0add r0, r12 累加参数f到r0add r0, r9 累加参数e到r0 二、构造so案例好了，关于ARM指令的相关知识，就介绍这么多了，不过我们在调试分析的时候，肯定不能做到全部的了解，因为本身ARM指令语法就比较复杂，不过幸好大学学习了汇编语言，所以稍微能看懂点，如果不懂汇编的同学那就可能需要补习一下了，因为我们在使用IDA分析so文件的时候，不会汇编的话，那是肯定行不通的，所以我们必须要看懂汇编代码的，如果遇到特殊指令不了解的同学，可以网上搜一下即可。 上面我们的准备知识做完了，一个是IDA工具的时候，一个是ARM指令的了解，下面我们就来开始操刀了，为了方便开始，我们先自己写一个简单的Android native层代码，然后进行IDA进行分析即可。 这里可以使用AndroidStudio中进行新建一个简单工程，然后创建JNI即可： 这里顺便简单说一下AndroidStudio中如何进行NDK的开发吧： 第一步：在工程中新建jni目录 第二步：使用javah生成native的头文件 注意： javah执行的目录，必须是类包名路径的最上层，然后执行： javah 类全名 注意没有后缀名java哦 第三步：配置项目的NDK目录 选择模块的设置选线：Open Module Settings： 设置NDK目录即可 第四步：copy头文件到jni目录下，然后配置gradle中的ndk选项 这里只需要设置编译之后的模块名，就是so文件的名称，需要产生那几个平台下的so文件，还有就是需要用到的lib库，这里我们看到我们用到了Android中打印log的库文件。 第五步：编译运行，在build目录下生成指定的so文件，copy到工程的libs目录下即可 好了，到这里我们就快速的在AndroidStudio中新建了一个Native项目，这里关于native项目的代码不想解释太多，就是Java层 传递了用户输入的密码，然后native做了校验过程，把校验结果返回到Java层即可： 具体的校验过程这里不再解释了。我们运行项目之后，得到apk文件，那么下面我们就开始我们的破解旅程了 三、开始破解so文件开始破解我们编译之后的apk文件 第一、首先我们可以使用最简单的压缩软件，打开apk文件，然后解压出他的so文件 我们得到libencrypt.so文件之后，使用IDA打开它： 我们知道一般so中的函数方法名都是：Java_类名_方法名 那么这里我们直接搜：Java关键字即可，或者使用jd-gui工具找到指定的native方法 双击，即可在右边的IDA View页面中看到Java_cn_wjdiankong_encryptdemo_MainActivity_isEquals 函数的指令代码： 我们可以简单的分析一下这段指令代码： 1&gt;、PUSH {r3-r7,lr} 是保存r3,r4,r5,r6,r7,lr 的值到内存的栈中，那么最后当执行完某操作后，你想返回到lr指向的地方执行，当然要给pc了，因为pc保留下一条CPU即将执行的指令，只有给了pc，下一条指令才会执行到lr指向的地方 pc：程序寄存器，保留下一条CPU即将执行的指令lr: 连接返回寄存器，保留函数返回后，下一条应执行的指令 这个和函数最后面的POP {r3-r7,pc}是相对应的。 2&gt;、然后是调用了strlen,malloc,strcpy等系统函数，在每次使用BLX和BL指令调用这些函数的时候，我们都发现了一个规律：就是在调用他们之前一般都是由MOV指令，用来传递参数值的，比如这里的R5里面存储的就是strlen函数的参数，R0就是is_number函数的参数，所以我们这样分析之后，在后面的动态调试的过程中可以得到函数的入口参数值，这样就能得到一些重要信息 3&gt;、在每次调用有返回值的函数之后的命令，一般都是比较指令，比如CMP，CBZ，或者是strcmp等，这里是我们破解的突破点，因为一般加密再怎么牛逼，最后比较的参数肯定是正确的密码(或者是正确的加密之后的密码)和我们输入的密码(或者是加密之后的输入密码)，我们在这里就可以得到正确密码，或者是加密之后的密码： 到这里，我们就分析完了native层的密码比较函数：Java_cn_wjdiankong_encryptdemo_MainActivity_isEquals 如果觉得上面的ARM指令看的吃力，可以使用F5键，查看他的C语言代码： 我们这里看到其实有两个函数是核心点： 1&gt;is_number函数，这个函数我们看名字应该猜到是判断是不是数字，我们可以使用F5键，查看他对应的C语言代码： 这里简单一看，主要是看return语句和if判断语句，看到这里有一个循环，然后获取_BYTE这里地址的值，并且自增加一，然后存到v2中，如果v3为’\0’的话，就结束循环，然后做一次判断，就是v2-48是否大于9，那么这里我们知道48对应的是ASCII中的数字0，所以这里可以确定的是就是：用一个循环遍历_BYTE这里存的字符串是否为数字串。 2&gt;get_encrypt_str函数，这个函数我们看到名字可以猜测，他是获取我们输入的密码加密之后的值，再次使用F5快捷键查看： 这里我们看到，首先是一个if语句，用来判断传递的参数是否为NULL，如果是的话，直接返回，不是的话，使用strlen函数获取字符串的长度保存到v2中，然后使用malloc申请一块堆内存，首指针保存到result，大小是v2+1也就是传递进来的字符串长度+1，然后就开始进入循环，首指针result，赋值给i指针，开始循环，v3是通过v1-1获取到的，就是函数传递进来字符串的地址，那么v6就是获取传递进来字符串的字符值，然后减去48，赋值给v7，这里我们可以猜到了，这里想做字符转化，把char转化成int类型，继续往下看，如果v6==48的话，v7=1，也就是说这里如果遇到字符’0’，就赋值1，在往下看，看到我们上面得到的v7值，被用来取key_src数组中的值，那么这里我们双击key_src变量，就跳转到了他的值地方，果不其然，这里保存了一个字符数组，看到他的长度正好是18，那么这里我们应该明白了，这里通过传递进来的字符串，循环遍历字符串，获取字符，然后转化成数字，在倒序获取key_src中的字符，保存到result中。然后返回。 好了，到这里我们就分析完了这两个重要的函数的功能，一个是判断输入的内容是否为数字字符串，一个是通过输入的内容获取密码内容，然后和正确的加密密码：ssBCqpBssP 作比较。 第二、开始使用IDA进行调试设置那么下面我们就用动态调试来跟踪传入的字符串值，和加密之后的值，这里我们看到没有打印log的函数，所以很难知道具体的参数和寄存器的值，所以这里需要开始调试，得知每个函数执行之后的寄存器的值，我们在用IDA进行调试so的时候，需要以下准备步骤： 1、在IDA安装目录下获取android_server命令文件 在IDA安装目录\dbgsrv\android_server 其实是使用gdb和gdbserver来做到的，gdb和gdbserver在调试的时候，必须注入到被调试的程序进程中，但是非root设备的话，注入别的进程中只能借助于run-as这个命令了，所以我们知道，如果要调试一个应用进程的话，必须要注入他内部，那么IDA调试so也是这个原理，他需要注入(Attach附加)进程，才能进行调试，但是IDA没有自己弄了一个类似于gdbserver这样的工具，那就是android_server了，所以他需要运行在设备中，保证和PC端的IDA进行通信，比如获取设备的进程信息，具体进程的so内存地址，调试信息等。 所以我们把android_server保存到设备的/data目录下，修改一下他的运行权限，然后必须在root环境下运行，因为他要做注入进程操作，必须要root。 注意： 这里把他放在了/data目录下，然后运行./android_server，这里提示了IDA Android 32-bit，所以后面我们在打开IDA的时候一定要是32位的IDA，不是64位的，不然保存，IDA在安装之后都是有两个可执行的程序，一个是32位，一个是64位的，如果没打开正确会报这样的错误： 同样还有一类问题： error: only position independent executables (PIE) are supported 这个主要是Android5.0以上的编译选项默认开启了pie，在5.0以下编译的原生应用不能运行，有两种解决办法，一种是用Android5.0以下的手机进行操作，还有一种就是用IDA6.6+版本即可。 然后我们再看，这里开始监听了设备的23946端口，那么如果要想让IDA和这个android_server进行通信，那么必须让PC端的IDA也连上这个端口，那么这时候就需要借助于adb的一个命令了： adb forward tcp:远端设备端口号(进行调试程序端) tcp:本地设备端口(被调试程序端) 那么这里，我们就可以把android_server端口转发出去： 然后这时候，我们只要在PC端使用IDA连接上23946这个端口就可以了，这里面有人好奇了，为什么远程端的端口号也是23946，因为后面我们在使用IDA进行连接的时候，发现IDA他把这个端口设置死了，就是23946，所以我们没办法自定义这个端口了。 我们可以使用netstat命令查看端口23946的使用情况，看到是ida在使用这个端口 2、上面就准备好了android_server，运行成功，下面就来用IDA进行尝试连接，获取信息，进行进程附加注入 我们这时候需要在打开一个IDA，之前打开一个IDA是用来分析so文件的，一般用于静态分析，我们要调试so的话，需要在打开一个IDA来进行，所以这里一般都是需要打开两个IDA，也叫作双开IDA操作。动静结合策略。 这里记得选择go这个选项，就是不需要打开so文件了，进入是一个空白页： 我们选择Debugger选项，选择Attach，看到有很多debugger，所以说IDA工具真的很强大，做到很多debugger的兼容，可以调试很多平台下的程序。这里我们选择Android debugger： 这里看到，端口是写死的：23946，不能进行修改，所以上面的adb forward进行端口转发的时候必须是23946。这里PC本地机就是调试端，所以host就是本机的ip地址：127.0.0.1，点击确定： 这里可以看到设备中所有的进程信息就列举出来的，其实都是android_server干的事，获取设备进程信息传递给IDA进行展示。 注意： 如果我们当初没有用root身份去运行android_server: 这里就会IDA是不会列举出设备的进程信息： 还有一个注意的地方，就是IDA和android_server一定要保持一致。 我们这里可以ctrl+F搜索我们需要调试的进程，当然这里我们必须运行起来我们需要调试的进程，不然也是找不到这个进程的 双击进程，即可进入调试页面： 这里为什么会断在libc.so中呢？ android系统中libc是c层中最基本的函数库，libc中封装了io、文件、socket等基本系统调用。所有上层的调用都需要经过libc封装层。所以libc.so是最基本的，所以会断在这里，而且我们还需要知道一些常用的系统so,比如linker： 我们知道，这个linker是用于加载so文件的模块，所以后面我们在分析如何在.init_array处下断点 还有一个就是libdvm.so文件，他包含了DVM中所有的底层加载dex的一些方法： 我们在后面动态调试需要dump出加密之后的dex文件，就需要调试这个so文件了。 3、找到函数地址，下断点，开始调试 我们使用Ctrl+S找到需要调试so的基地址：74FE4000 然后通过另外一个IDA打开so文件，查看函数的相对地址：E9C 那么得到了函数的绝对地址就是：74FE4E9C，使用G键快速跳转到这个绝对地址： 跳转到指定地址之后，开始下断点，点击最左边的绿色圆点即可下断点： 然后点击左上角的绿色按钮，运行，也可以使用F9键运行程序： 我们点击程序中的按钮： 触发native函数的运行： 看到了，进入调试阶段了，这时候，我们可以使用F8进行单步调试，F7进行单步进入调试： 我们点击F8进行单步调试，达到is_number函数调用出，看到R0是出入的参数值，我们可以查看R0寄存器的内容，然后看到是123456，这个就是Java层传入的密码字符串，接着往下走： 这里把is_number函数返回值保存到R0寄存中，然后调用CBZ指令，判断是否为0，如果为0就跳转到locret_74FE4EEC处，查看R0寄存器的值不是0，继续往下走： 看到了get_encrypt_str函数的调用，函数的返回值保存在R1寄存器中，查看内容：zytyrTRAB了，那么看到，上层传递的：123456=》zytyrTRAB了，前面我们静态分析了get_encrypt_str函数的逻辑，继续往下看： 看到了，这里把上面得到的字符串和ssBCqpBssP作比较，那么这里ssBCqpBssP就是正确的加密密码了，那么我们现在的资源是： 正确的加密密码：ssBCqpBssP，加密密钥库：zytyrTRA*BniqCPpVs，加密逻辑get_encrypt_str 那么我们可以写一个逆向的加密方法，去解析正确的加密密码得到值即可，这里为了给大家一个破解的机会，这里就不公布正确答案了，这个apk我随后会上传，手痒的同学可以尝试破解一下。 第三、总结IDA调试的流程到这里，我们就分析了如何破解apk的流程，下面来总结一下： 1、我们通过解压apk文件，得到对应的so文件，然后使用IDA工具打开so,找到指定的native层函数 2、通过IDA中的一些快捷键：F5,Ctrl+S,Y等键来静态分析函数的arm指令，大致了解函数的执行流程 3、再次打开一个IDA来进行调试so 1&gt;将IDA目录中的android_server拷贝到设备的指定目录下，修改android_server的运行权限，用Root身份运行android_server 2&gt;使用adb forward进行端口转发，让远程调试端IDA可以连接到被调试端 3&gt;使用IDA连接上转发的端口，查看设备的所有进程，找到我们需要调试的进程。 4&gt;通过打开so文件，找到需要调试的函数的相对地址，然后在调试页面使用Ctrl+S找到so文件的基地址，相加之后得到绝对地址，使用G键，跳转到函数的地址处，下好断点。点击运行或者F9键。 5&gt;触发native层的函数，使用F8和F7进行单步调试，查看关键的寄存器中的值，比如函数的参数，和函数的返回值等信息 总结就是：在调试so的时候，需要双开IDA，动静结合分析。 四、使用IDA来解决反调试问题那么到这里我们就结束了我们这期的破解旅程了？答案是否定的，因为我们看到上面的例子其实是我自己先写了一个apk,目的就是为了给大家演示，如何使用IDA来进行动态调试so，那么下面我们还有一个操刀动手的案例，就是2014年，阿里安全挑战赛的第二题：AliCrackme_2： 阿里真会制造氛围，还记得我们破解的第一题吗，这次看到了第二题，好吧，下面来看看破解流程吧： 首先使用aapt命令查看他的AndroidManifest.xml文件，得到入口的Activity类： 然后使用dex2jar和jd-gui查看他的源码类：com.yaotong.crackme.MainActivity： 看到，他的判断，是securityCheck方法，是一个native层的，所以这时候我们去解压apk文件，获取他的so文件，使用IDA打开查看native函数的相对地址：11A8 这里的ARM指令代码不在分析了，大家自行查看即可，我们直接进入调试即可： 在打开一个IDA进行关联调试： 选择对应的调试进程，然后确定： 使用Ctrl+S键找到对应so文件的基地址：74EA9000 和上面得到的相对地址相加得到绝对地址：74EA9000+11A8=74EAA1A8 使用G键直接跳到这个地址： 下个断点，然后点击F9运行程序： 擦，IDA退出调试页面了，我们再次进入调试页面，运行，还是退出调试页面了，好了，这下蛋疼了，没法调试了。 这里其实是阿里做了反调试侦查，如果发现自己的程序被调试了，就直接退出程序，那么这里有问题了，为什么知道是反调试呢？这个主要还是看后续自己的破解经验了，没技术可言，还有一个就是阿里如何做到的反调试策略的，这里限于篇幅，只是简单介绍一下原理： 前面说到，IDA是使用android_server在root环境下注入到被调试的进程中，那么这里用到一个技术就是Linux中的ptrace，关于这个这里也不解释了，大家可以自行的去搜一下ptrace的相关知识，那么Android中如果一个进程被另外一个进程ptrace了之后，在他的status文件中有一个字段：TracerPid 可以标识是被哪个进程trace了，我们可以使用命令查看我们的被调试的进行信息：**status文件在：/proc/[pid]/status** 看到了，这里的进程被9187进程trace了，我们在用ps命令看看9187是哪个进程： 果不其然，是我们的android_server进程，好了，我们知道原理了，也大致猜到了阿里在底层做了一个循环检测这个字段如果不为0，那么代表自己进程在被人trace，那么就直接停止退出程序，这个反检测技术用在很多安全防护的地方，也算是一个重要的知识点了。 那么下面就来看看如何应对这个反调试？ 我们刚刚看到，只要一运行程序，就退出了调试界面，说明，这个循环检测程序执行的时机非常早，那么我们现在知道的最早的两个时机是：一个是.init_array，一个是JNI_OnLoad .init_array是一个so最先加载的一个段信息，时机最早，现在一般so解密操作都是在这里做的 JNI_OnLoad是so被System.loadLibrary调用的时候执行，他的时机要早于哪些native方法执行，但是没有.init_array时机早 那么知道了这两个时机，下面我们先来看看是不是在JNI_OnLoad函数中做的策略，所以我们需要先动态调试JNI_OnLoad函数 我们既然知道了JNI_OnLoad函数的时机，如果阿里把检测函数放在这里的话，我们不能用之前的方式去调试了，因为之前的那种方式时机太晚了，只要运行就已经执行了JNI_OnLoad函数，所以就会退出调试页面 幸好这里IDA提供了在so文件load的时机，我们只需要在Debug Option中设置一下就可以了： 在调试页面的Debugger 选择 Debugger Option选项： 然后勾选Suspend on library load/unload即可 这样设置之后，还是不行，因为我们程序已经开始运行，就在static代码块中加载so文件了，static的时机非常早，所以这时候，我们需要让程序停在加载so文件之前即可。 那么我想到的就是添加代码waitForDebugger代码了，这个方法就是等待debug，我们还记得在之前的调试smali代码的时候，就是用这种方式让程序停在了启动出，然后等待我们去用jdb进行attach操作。 那么这一次我们可以在System.loadLibrary方法之前加入waitForDebugger代码即可，但是这里我们不这么干了，还有一种更简单的方式就是用am命令，本身am命令可以启动一个程序，当然可以用debug方式启动： adb shell am start -D -n com.yaotong.crackme/.MainActivity 这里一个重要参数就是-D,用debug方式启动 运行完之后，设备是出于一个等待Debugger的状态： 这时候，我们再次使用IDA进行进程的附加，然后进入调试页面，同时设置一下Debugger Option选项，然后定位到JNI_OnLoad函数的绝对地址。 但是我们发现，这里没有RX权限的so文件，说明so文件没有加载到内存中，想一想还是对的，以为我们现在的程序是wait Debugger，也就是还没有走System.loadLibrary方法，so文件当然没有加载到内存中，所以我们需要让我们程序跑起来，这时候我们可以使用jdb命令去attach等待的程序，命令如下： jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 其实这条命令的功能类似于，我们前一篇说到用Eclipse调试smali源码的时候，在Eclipse中设置远程调试工程一样，选择Attach方式，调试机的ip地址和端口，还记得8700端口是默认的端口，但是我们运行这个命令之后，出现了一个错误： 擦，无法连接到目标的VM，那么这种问题大部分都出现在被调试程序不可调试，我们可以查看apk的android:debuggable属性： 果不其然，这里没有debug属性，所以这个apk是不可以调试的，所以我们需要添加这个属性，然后在回编译即可： 回编译：java -jar apktool.jar b -d out -o debug.apk 签名apk：java -jar .\sign\signapk.jar .\sign\testkey.x509.pem .\sign\testkey.pk8 debug.apk debug.sig.apk 然后在次安装，使用am 命令启动： 第一步：运行：adb shell am start -D -n com.yaotong.crackme/.MainActivity 出现Debugger的等待状态 第二步：启动IDA 进行目标进程的Attach操作 第三步：运行：jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 第三步：设置Debugger Option选项 第四步：点击IDA运行按钮，或者F9快捷键，运行 看到了，这次jdb成功的attach住了，debug消失，正常运行了， 但是同时弹出了一个选择提示： 这时候，不用管它，全部选择取消按钮，然后就运行到了linker模块了： 这时候，说明so已经加载进来了，我们再去获取JNI_OnLoad函数的绝对地址 Ctrl+S查找到了基地址：7515A000 用静态方式IDA打开so查看相对地址：1B9C 相加得到绝对地址：7515A000+1B9C=7515BB9C，然后点击S键，跳转： 跳转到指定的函数位置： 这时候再次点击运行，进入了JNI_OnLoad处的断点： 下面咋们就开始单步调试了，但是当我们每次到达BLX R7这条指令执行完之后，就JNI_OnLoad就退出了： 经过好几次尝试都是一样的结果，所以我们发现这个地方有问题，可能就是反调试的地方了 我们再次进入调试，看见BLX跳转的地方R7寄存器中是pthread_create函数，这个是Linux中新建一个线程的方法 所以阿里的反调试就在这里开启一个线程进行轮训操作，去读取/proc/[pid]/status文件中的TrackerPid字段值，如果发现不为0，就表示有人在调试本应用，在JNI_OnLoad中直接退出。其实这里可以再详细进入查看具体代码实现的，但是这里限于篇幅问题，不详细解释了，后续在写一篇文章我们自己可以实现这种反调试机制的。本文的重点是能够动态调试即可。 那么问题找到了，我们现在怎么操作呢？ 其实很简单，我们只要把BLX R7这段指令干掉即可，如果是smali代码的话，我们可以直接删除这行代码即可，但是so文件不一样，他是汇编指令，如果直接删除这条指令的话，文件会发生错乱，因为本身so文件就有固定的格式，比如很多Segement的内容，每个Segement的偏移值也是有保存的，如果这样去删除会影响这些偏移值，会破坏so文件格式，导致so加载出错的，所以这里我们不能手动的去删除这条指令，我们还有另外一种方法，就是把这条指令变成空指令，在汇编语言中，nop指令就是一个空指令，他什么都不干，所以这里我们直接改一下指令即可，arm中对应的nop指令是：00 00 00 00 那么我们看到BLX R7对应的指令位置为：1C58 查看他的Hex内容是：37 FF 2F E1 我们可以使用一些二进制文件软件进行内容的修改，这里使用010Editor工具进行修改： 这里直接修改成00 00 00 00： 这时候，保存修改之后的so文件，我们再次使用IDA进行打开查看： 哈哈，指令被修改成了：ANDEQ R0，R0，R0了 那么修改了之后，我们在替换原来的so文件，再次重新回编译，签名安装，再次按照之前的逻辑给主要的加密函数下断点，这里不需要在给JNI_OnLoad函数下断点了，因为我们已经修改了反调试功能了，所以这里我们只需要按照这么简单几步即可： 第一步：启动程序 第二步：使用IDA进行进程的attach 第三步：找到Java_com_yaotong_crackme_MainActivity_securityCheck函数的绝对地址 第四步：打上断点，点击运行，进行单步调试 看到了吧，这里我们可以单步调试进来了啦啦，说明我们修改反调试指令成功了。 下面就继续F8单步调试： 调试到这里，发现一个问题，就是CMP指令之后，BNE 指令就开始跳转到loc_74FAF2D0处了，那么我们就可以猜到了，CMP指令比较的应该就是我们输入的密码和正确的密码，我们再次从新调试，看看R3和R1寄存器的值 看到了这里的R3寄存器的值就是用寄存器寻址方式，赋值字符串的，这里R2寄存器就是存放字符串的地址，我们看到的内容是aiyou…但是这里肯定不是全部字符串，因为我们没看到字符串的结束符：’\0’，我们点击R2寄存器，进入查看完整内容： 这里是全部内容：aiyou,bucuoo 我们继续查看R1寄存器的内容： 这里也是同样用寄存器寻址，R0寄存器存储的是R1中字符串的地址，我们看到这里的字符串内容是：jiangwei 这个就是我输入的内容，那么这里就可以豁然开朗了，密码是上面的：aiyou,bucuoo 我们再次输入这个密码： 哈哈哈，破解成功啦啦~~ 五、技术总结到这里我们算是讲解完了如何使用IDA来调试so代码，从而破解apk的知识了，因为这里IDA工具比较复杂，所以这篇文章篇幅有点长，所以同学们可以多看几遍，就差不多了。下面我们来整理一下这篇文章中涉及到的知识点吧： 第一、IDA中的常用快捷键使用 1、Shift+F12可以快速查看so中的常量字符串内容，有时候，字符串内容是一个很大的突破点 2、使用强大的F5键，可以查看arm汇编指令对应的C语言代码，同时可以使用Y键，进行JNIEnv*方法的还原 3、使用Ctrl+S键，可以在IDA View页面中查看so的所有段信息，在调试页面可以查找对应so文件映射到内存的基地址，这里我们还可以使用G键，进行地址的跳转 4、使用F8进行单步调试，F7进行单步跳入调试，同时可以使用F9运行程序 第二、ARM汇编指令相关知识 1、了解了几种寻址方式，有利于我们简单的读懂arm汇编指令代码 2、了解了arm中的几种寄存器的作用，特别是PC寄存器 3、了解了arm中常用的指令，比如：MOV，ADD，SUB，LDR，STR，CMP，CBZ，BL，BLX 第三、使用IDA进行调试so的步骤，这里分两种情况 1、IDA调试无反调试的so代码步骤： 1》把IDA安装目录中的android_server拷贝到设备的指定目录中，修改android_server的权限，并且用root方式运行起来，监听23946端口 2》使用adb forward命令进行端口的转发，将设备被调试端的端口转发到远程调试端中 3》双开IDA工具，一个是用来打开so文件，进行文件分析，比如简单分析arm指令代码，知道大体逻辑，还有就是找到具体函数的相对位置等信息，还有一个IDA是用来调试so文件的，我们在Debugger选项中设置Debugger Option，然后附加需要调试的进程 4》进入调试页面之后，通过Ctrl+S和G快捷键，定位到需要调试的关键函数，进行下断点 5》点击运行或者快捷键F9，触发程序的关键函数，然后进入断点，使用F8单步调试，F7单步跳入调试，在调试的过程中主要观察BL，BLX指令，以及CMP和CBZ等比较指令，然后在查看具体的寄存器的值。 2、IDA调试有反调试的so代码步骤： 1》查看apk是否为可调式状态，可以使用aapt命令查看他的AndroidManifest.xml文件中的android:debuggeable属性是否为true，如果不是debug状态，那么就需要手动的添加这个属性，然后回编译，在签名打包从新安装 2》使用adb shell am start -D -n com.yaotong.crackme/.MainActivity 命令启动程序，出于wait Debug状态 3》打开IDA，进行进程附加，进入到调试页面 4》使用 jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 命令attach之前的debug状态，让程序正常运行 5》设置Debug Option选项，设置Suspend on library start/exit/Suspend on library load/unload/Suspend on process entry point选项 6》点击运行按钮或者F9键，程序运行停止在linker模块中，这时候表示so文件加载进来了，我们通过Ctrl+S和G键跳转到JNI_OnLoad函数出，进行下断点 7》然后继续运行，进入JNI_OnLoad断点处，使用F8进行单步调试，F7进行单步跳入调试，找到反调试代码处 8》然后使用二进制软件修改反调试代码为nop指令，即00值 9》修改之后，在替换原来的so文件，进行回编译，从新签名打包安装即可 10》按照上面的无反调试的so代码步骤即可 第四、学习了如何做到反调试检测 现在很多应用防止别的进程调试或者注入，通常会用自我检测装置，原理就是： 循环检测/proc/[mypid]/status文件，查看他的TracerPid字段是否为0，如果不为0，表示被其他进程trace了 那么这时候就直接退出程序。因为现在的IDA调试时需要进程的注入，进程注入现在都是使用Linux中的ptrace机制，那么这里的TracePid就可以记录trace的pid，我们可以发现我们的程序被那个进程注入了，或者是被他在调试。进而采取一些措施。 第五、IDA调试的整体原理 我们知道了上面的IDA调试步骤，其实我们可以仔细想一想，他的调试原理大致是这样的： 首先他得在被调试端安放一个程序，用于IDA端和调试设备通信，这个程序就是android_server，因为要附加进程，所以这个程序必须要用root身份运行，这个程序起来之后，就会开启一个端口23946，我们在使用adb forward进行端口转发到远程调试端，这时候IDA就可以和调试端的android_server进行通信了。后面获取设备的进程列表，附加进程，传递调试信息，都可以使用这个通信机制完成即可。IDA可以获取被调试的进程的内存数据，一般是在 /proc/[pid]maps 文件中，所以我们在使用Ctrl+S可以查看所有的so文件的基地址，可以遍历maps文件即可做到。 破解法则：**时刻需要注意关键的BL/BLX等跳转指令，在他们执行完之后，肯定会有一些CMP/CBZ等比较指令，这时候就可以查看重要的寄存器内容来获取重要信息。**]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android反编译尝试(裁判文书app示例)]]></title>
    <url>%2F2019%2F02%2F27%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E5%8F%8D%E7%BC%96%E8%AF%91%E7%88%AC%E8%99%AB((%E8%A3%81%E5%88%A4%E6%96%87%E4%B9%A6app%E7%A4%BA%E4%BE%8B))%2F</url>
    <content type="text"><![CDATA[一、工具准备 fiddler 安卓模拟器 android studio enjarify jd-gui python3 jdk apktool adb SignApk.jar apk签名工具 IDA 二、 抓包查看apk的请求逻辑1）设置fiddler下载fiddler后，打开菜单栏 Tools=》Options=》HTTPS， 勾选【Decrypt HTTPS traffic】选项，对于【Ignore server certificate errors (unsafe)】选项可以不必勾选，然后点击【Actions】点击【Export Root Certificate to Desktop】这时候就会将Fiddler根证书FiddlerRoot.cer保存到桌面上，这个根证书在如果开启了Fiddler的HTTPS解密的时候火狐浏览器访问HTTPS地址时候出现【您的连接并不安全】的错误页面时候使用。 然后点击HTTPS标签栏旁边的Connections标签， 这里我们要记得【Fiddler listen on port】中显示的端口号（关于这个端口号，如果当前默认的8888端口号已经被占用了，那么需要重新设置另外的端口号），然后将【Allow remote computers to connect】前面的勾打上。点击确定，然后重新启动Fiddler。重新启动后，打开Fiddler后，在Fiddler界面的右上角的三角形上点击就会显示一个【Online】图标，把鼠标放到【Online】图标上，会显示当前机器的IP地址， 该地址直接用来和后面安卓设备的链接 2） 设置安卓设备不同的安卓设备会有一些区别，但是设置的思路一样 点击桌面上的【系统应用】=》【设置】=》【WLAN】， 鼠标放到当前已经连接的网络上长按， 在弹出的消息窗口中点击【修改网络】，输入上面我们得到的IP地址和端口号，点击保存。 然后在模拟器中打开浏览器，输入：http://ipv4.fiddler:8888 ，出现下面的页面说明我们刚刚设置的http代理正确，然后点击红线框的【FiddlerRoot certificate】，下载Fiddler的根证书： 然后我们来到桌面【系统应用】=》【设置】=》【安全】=》【从SD卡安装】中找到我们刚刚下载的证书： 点击证书，然后输入证书名称点击【确定】 安卓设备设置完成，打开浏览器可以检验一下fiddler中是否可以检测到网络请求，如果检测到说明配置成功 3）操作apk并进行请求和抓包抓包目的 下载apk，并进行相应的请求，查找所需数据的网络请求包和必要的请求参数、以及数据书否被加密等 找到数据包 以裁判文书Apk为例， 找到按关键字搜索文书的数据结果 Apk主页 在搜索框中输入关键字 “阿里”, 点击搜索按钮，在fiddler中找到包含数据的请求 第一个主要的请求主要是从服务端获取reqtoken，该参数作为后面请求的表单参数 第二个主要的请求，便是从服务端获取搜索的结果 点击列表页的item，获取详情内容，抓取数据包； 可以确定获取详情内容的数据包为红色框内请求，需携带详情的fieldId; 确定破解目标 综上分析，从抓包上来看，在请求中不确定的参数主要有，headers中的devid，signature，nonce， timespan参数，并且第二次对列表页的请求的响应数据并不是明文，也就是加密后的结果，所以只有经过解密后才能获得真正的结果 接下来将要对其apk代码进行分析和debug调试，需要完成以下两个任务： 1.获取请求参数的生成方式，devid，signature，nonce, timespan 2.详情页的fieIdId生成方式； 3.列表页和详情页响应内容的解密方式 三、反编译获取Java代码关于apk获取静态java代码以及分析请参考《Android逆向-静态分析破解Apk》 1）配置环境，apk下载使用enjarify进行反编译， 需要提前配置好python3运行环境 配置好python3开发环境后，下载将要进行反编译的apk 2）运行enjarify进行反编译将下载好的apk导入到同enjarify同一级目录下 输入命令 1python3 -O -m enjarify.main yourapp.apk 反编译效果， 反编译生成的文件就是 wenshuapp-enjarify.jar 3）JD-GUI查看java源码这一步不用过多介绍，将反编译生成的jar文件导入进来就可以查看其源码部分 如图就是通过反编译工具获取的java class 源代码， 开始寻找我们需要分析的代码 可以看到大部分的逻辑源码都在红色区域内 下面开始动态调试，通过和源码内容的结合来分析整个apk请求和解析数据的逻辑 enjarify工具下载地址：https://github.com/google/enjarify jd-gui工具下载地址: http://jd.benow.ca/ 四、制作debug模式Apk关于debug模式的apk制作和smali的基础语法，和调试方法请参考《Android逆向-动态分析破解Apk(Debug)》 1）使用apktool来破解apk1java -jar apktool/apktool.jar d wenshu/wenshuapp.apk -o wenshu_out 这里的命令不做解释了。 反编译成功之后，我们得到了一个out目录，如下： 源码都放在smail文件夹中，我们进入查看一下文件： 2）修改AndroidManifest.xml中的debug属性和在入口代码中添加waitDebug上面我们反编译成功了，下面我们为了后续的调试工作，所以还是需要做两件事： 1》修改AndroidManifest.xml中的android:debuggable=”true” 关于这个属性，我们前面介绍run-as命令的时候，也提到了，他标识这个应用是否是debug版本，这个将会影响到这个应用是否可以被调试，所以这里必须设置成true。 当然还有其他方式，比如aapt查看apk的内容方式，或者是安装apk之后用 adb dumpsys activity top 命令查看都是可以的。 2》在入口处添加waitForDebugger代码进行调试等待。 这里说的入口处，就是程序启动的地方，就是我们一般的入口Activity，查找这个Activity的话，方法太多了，比如我们这里直接从上面得到的AndroidManifest.xml中找到，因为入口Activity的action和category是固定的, 如上图第二个红框。 找到入口Activity之后，我们直接在他的onCreate方法的第一行加上waitForDebugger代码即可，找到对应的MainActivity的smali源码，然后添加一行代码： 1invoke-static&#123;&#125;, Landroid/os/Debug;-&gt;waitForDebugger()V 这个是smali语法的，其实对应的Java代码就是：android.os.Debug.waitForDebugger(); 这里把Java语言翻译成smali语法的，网上有smali的语法解析。 3）回编译apk并且进行签名安装1java -jar apktool.jar b out -o wenshu_debug.apk 还是使用apktool进行回编译 编译完成之后，将得到debug.apk文件，但是这个apk是没有签名的，所以是不能安装的，那么下面我们需要在进行签名，这里我们使用Android中的测试程序的签名文件和sign.jar工具进行签名： 1java -jar signApk/signapk.jar ./signApk/testkey.x509.pem ./signApk/testkey.pk8 wenshu_debug.apk wenshu_debug.sig.apk 签名之后，我们就可以进行安装了。 五、进行调试1) android studio配置下载 smalidea 下载地址: https://bitbucket.org/JesusFreke/smali/downloads/ 安装smalidea 打开AndroidStudio，点击Preferences… | Plugins, 选择Install plugin from disk 2）将反编译的文件夹导入Android Studio选择Import Project 选择Create preject from existing sources 一直选择“Next”，直至导入工程完成 然后在AndroidStudio工程中右键点击smali文件夹，设定Mark Directory as -&gt; Sources Root AndroidStudio的File -&gt; Project Structure, 配置JDK。 3）配置项目远程调试在AndroidStudio里面配置远程调试的选项，选择Run -&gt; Edit Configurations， 并增加一个Remote调试的调试选项，端口选择:8700 4） 将进程和android studio之间建立连接启动apk 启动模拟器上重新打包并安装好的apk， apk将会卡在启动过程中等待接入debug 查看进程端口信息 找到apk进程的端口号 1adb shell ps | grep com.lawyee.wenshuapp 建立进程和android studio之间的链接 1adb forward tcp:5005 jdwp:29685 5) 开始调试启动程序调试 点击程序调试按钮, 控制台如果显示connect 成功，表示成功启动 六、参数破解代码都是混淆过后的，所以没什么办法，耐心的调试吧，找到想要破解的各个参数的生成位置， 找到生成参数的位置 获取请求参数生成方法因为参数最终都是要经过网络请求发包，所以可以从网络请求发起处入手，然后反向追溯参数的生成过程 比如文书app的网络请求所在位置为com.lawyee.wenshuapp.util.a.a 可以通过调试找到参数生成的位置，再去生成的方法里面去看详细的生成逻辑就好，每个参数的生成 各参数对应的生成方法 timespan 就是当前的datetime去掉所有符号格式 nonce生成方法 devid生成方法 signature生成方法 获取列表\详情的解密方法获取解密方法 找到解密响应数据的位置: 方法: com.lawyee.wenshuapp.util.aa.a， 用jd-gui查看一下该方法的源码 位置: 粗略一看可以看出是通过AES进行的解密，但是方法接收了3个参数现在还不确定其生成方式，下面寻找三个参数的生成方法 获取方法入参 paramString1：通过ide，通过调试窗口可获得其为请求的响应值，即加密后待解密的内容 paramString2：为以下函数的返回值 com.lawuee.wenshuapp.vo.DevInfoVO.getIvParameter， 作为aes解密参数iv param； paramString3：为以下方法的返回值 com.lawyee.wenshuapp.config.ApplicationSet.a， 作为aes解密参数key， 接收发送请求的timespan参数为入参 this实例属性 更新this.f 属性的方法i，通过方法i 向this.f绑定20个生成key的方法； this.f是一个解密方法的集合，在初始化时更新this.f，其内容为20个获取key的方法，在解密的时候，会选择其中一个用来生成key，20个方法就不贴上了 在这20个生成key的方法中，其中用到了三个native方法，如下。 这三个静态方法来自静态库 lienutil-lib, 下面需要从apktool反编译出来的文件中找到该静态文件，将其反编译成汇编，破解这三个方法 分析so文件 关于IDA的使用方法参考《Android逆向-动态分析破解Apk(调试so)》 用IDA打开 libenutil.so文件， 分别选中三个需要分析的方法 按F5键，将方法翻译成c代码， 然后再翻译成目标打码即可 StrToLong方法 StrToLong2方法 StrToLong3方法 目前所有的请求参数和解密过程中涉及的方法都找到了，下面可以实现破解了 七、破解代码生成直接将上一步找到的参数生成方法，和解密方法等翻译成目标代码，python为例 请求参数生成方法 123456789101112131415161718192021222324252627282930import hashlibimport datetimeimport randomimport hashlib# 生成signaturedef get_signature(timespan, nonce, devid): string_buffer = [timespan, nonce, devid] string_buffer.sort() str_join = "".join(string_buffer) m = hashlib.md5() m.update(str_join.encode()) return m.hexdigest()# 生成noncedef get_nonce(): str_array = "abcdefghijklmnopqrstuvwxyz0123456789" nonce = "" for _ in range(4): index_choose = random.randint(0, len(str_array) - 1) nonce = nonce + str_array[index_choose] return nonce# 生成deviddef get_uuid(): return uuid.uuid4().replace("-", "")# 生成timespandef get_timespan(): return datetime.datetime.now().strftime("%Y%m%d%H%M%S") 解密数据方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding=utf-8from appcrack.f_methods import e, f, g, h, k, l, m, n, o, p, q, r, s, t, u, v, w, x, j # 获取key参数的20的方法，暂不列举from appcrack.f_methods import i as iifrom Crypto.Cipher import AESimport base64import hashlib# 3个native方法的python实现def str2long(param_str, param_int): v10 = param_int v4 = 0 v5 = param_str if not param_str.strip(): return v4 v6 = len(v5) v4 = v10 if v6 &gt;= 2: v7 = 1 while v7 &lt; v6: v4 += v10 + v7 + ord(v5[v7] % 256 &lt;&lt; (v7 &amp; 7)) - ord(v5[v7]) % 256 v7 += 1 return v4def str2long2(param_str, param_int): v10 = param_int v4 = 0 v5 = param_str if not param_str.strip(): return v4 v6 = len(v5) v4 = v10 if v6 &gt;= 2: v7 = 1 while v7 &lt; v6: v4 += v10 + v7 + ord(v5[v7] % 256 &lt;&lt; (v7 &amp; 15)) - ord(v5[v7]) % 256 v7 += 1 return v4def str2long3(param_str, param_int): v10 = param_int v4 = 0 v5 = param_str if not param_str.strip(): return v4 v6 = len(v5) v4 = v10 if v6 &gt;= 2: v7 = 1 while v7 &lt; v6: v4 += v10 + v7 + ord(v5[v7] % 256 &lt;&lt; (v7 &amp; 15)) + v7 % 4 - ord(v5[v7]) % 256 v7 += 1 return v4# 数据解密iv param(paramString2)参数生成方法def get_iv_param(devid): if len(devid.strip()) &lt; 16: return None return devid[len(devid) - 16]# key(paramString3)的生成方法def get_key(timespan, token): key_producer_list = [e, f, q, r, s, t, u, v, w, x, g, h, ii, j, k, l, m, n, o, p] i = str2long(token, 1) % 20 return key_producer_list[i](token + timespan)# aes解密方法实现def aes_decrypt(param_str1, param_str2, param_str3): block_size = AES.block_size pkc5Padding = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size) \ if (block_size - len(s) % block_size) else s + block_size * chr(16) unpkc5Padding = lambda s: s[:-ord(s[len(s) - 1:])] mode = AES.MODE_CBC localCipher = AES.new(key=param_str2.encode('ascii'), mode=mode, iv=param_str3.encode('utf8')) cryptedStr = base64.b64decode(param_str1) recovery = unpkc5Padding(localCipher.decrypt(cryptedStr)) return recovery.decode('utf-8')# 数据解密方法接口def decrypt(devid, token, timespan, context): iv_param = get_iv_param(devid) key = get_key(timespan, token) return aes_decrypt(context, iv_param, key) 好了，目前app请求过程中的所有参数的生成方法，和响应数据的解密方法都已经构建好了，接下来就可以随意的编写爬虫进行数据爬取了。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链密码学]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F2.%E5%8C%BA%E5%9D%97%E9%93%BE%E5%AF%86%E7%A0%81%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[密码学算法Hash算法定义 Hash（哈希或散列）算法，又常被称为指纹（fingerprint）或摘要（digest）算法，是非常基础也非常重要的一类算法。可以将任意长度的二进制明文串映射为较短的（通常是固定长度的）二进制串（Hash 值），并且不同的明文很难映射为相同的 Hash 值，且算法不可逆，例如: MD5、SHA-1、SHA-2、SHA-256； 比如判断两个字符串是否相等，只需要判断两字符串的hash值是否相等就可以基本断定两字符串是否相等； Hash 算法并不是一种加密算法，不能用于对信息的保护。但 Hash 算法可被应用到对登录口令的保存上。例如网站登录时需要验证用户名和密码，如果网站后台直接保存用户的口令原文，一旦发生数据库泄露后果不堪设想（事实上，网站数据库泄露事件在国内外都不少见）。利用 Hash 的防碰撞特性，后台数据库可以仅保存用户口令的 Hash 值，这样每次通过 Hash 值比对，即可判断输入口令是否正确。即便数据库泄露了，攻击者也无法轻易从 Hash 值还原回口令。 性质 正向快速：给定原文和 Hash 算法，在有限时间和有限资源内能计算得到 Hash 值； 逆向困难：给定（若干）Hash 值，在有限时间内无法（基本不可能）逆推出原文； 输入敏感：原始输入信息发生任何改变，新产生的 Hash 值都应该发生很大变化； 碰撞避免：很难找到两段内容不同的明文，使得它们的 Hash 值一致（即发生碰撞）。 常见hash算法 目前常见的 Hash 算法包括国际上的 Message Digest（MD）系列和 Secure Hash Algorithm（SHA）系列算法，以及国内的 SM3 算法。 MD 算法主要包括 MD4 和 MD5 两个算法。MD4（RFC 1320）是 MIT 的 Ronald L. Rivest 在 1990 年设计的，其输出为 128 位。MD4 已证明不够安全。MD5（RFC 1321）是 Rivest 于 1991 年对 MD4 的改进版本。它对输入仍以 512 位进行分组，其输出是 128 位。MD5 比 MD4 更加安全，但过程更加复杂，计算速度要慢一点。MD5 已于 2004 年被成功碰撞，其安全性已不足应用于商业场景。。 SHA 算法由美国国家标准与技术院（National Institute of Standards and Technology，NIST）征集制定。首个实现 SHA-0 算法于 1993 年问世，1998 年即遭破解。随后的修订版本 SHA-1 算法在 1995 年面世，它的输出为长度 160 位的 Hash 值，安全性更好。SHA-1 设计采用了 MD4 算法类似原理。SHA-1 已于 2005 年被成功碰撞，意味着无法满足商用需求。 为了提高安全性，NIST 后来制定出更安全的 SHA-224、SHA-256、SHA-384，和 SHA-512 算法（统称为 SHA-2 算法）。新一代的 SHA-3 相关算法也正在研究中。 目前， MD5和SHA1已经被破解，一半推荐使用SHA2-256或更安全的算法 对称加密算法定义 对称加密算法，顾名思义，加密和解密过程的密钥是相同的。该类算法的优点是加解密效率，和加密强度强度都很高。 缺点是参与方需要提前持有密钥，一旦有人泄露则系统安全性被破坏；另外如何在不安全通道中提前分发密钥也是个问题，需要借助额外的 Diffie–Hellman 协商协议或非对称加密算法来实现。 对称密码从实现原理上可以分为两种：分组密码和序列密码。前者将明文切分为定长数据块作为基本加密单位，应用最为广泛。后者则每次只对一个字节或字符进行加密处理，且密码不断变化，只用在一些特定领域（如数字媒介的加密）。 分组对称加密算法 DES（Data Encryption Standard）：经典的分组加密算法，最早是 1977 年美国联邦信息处理标准（FIPS）采用 FIPS-46-3，将 64 位明文加密为 64 位的密文。其密钥长度为 64 位（包括 8 位校验码），现在已经很容易被暴力破解； 3DES：三重 DES 操作：加密 –&gt; 解密 –&gt; 加密，处理过程和加密强度优于 DES，但现在也被认为不够安全； AES（Advanced Encryption Standard）：由美国国家标准研究所（NIST）采用，取代 DES 成为对称加密实现的标准，1997~2000 年 NIST 从 15 个候选算法中评选 Rijndael 算法（由比利时密码学家 Joan Daemon 和 Vincent Rijmen 发明）作为 AES，标准为 FIPS-197。AES 也是分组算法，分组长度为 128、192、256 位三种。AES 的优势在于处理速度快，整个过程可以数学化描述，目前尚未有有效的破解手段； IDEA（International Data Encryption Algorithm）：1991 年由密码学家 James Massey 与来学嘉共同提出。设计类似于 3DES，密钥长度增加到 128 位，具有更好的加密强度。 序列加密算法 RC4算法: 可以通过“一次性密码本”的对称加密处理。即通信双方每次使用跟明文等长的随机密钥串对明文进行加密处理。序列密码采用了类似的思想，每次通过伪随机数生成器来生成伪随机密钥串。 加解密原理 用相同的密钥对原文进行加密和解密 加密过程: 密钥 + 原文 =&gt; 密文 解密过程: 密文 + 密钥 =&gt; 原文 缺点 无法确保密钥被安全的传递 无法保证数据不会被篡改 非对称加密定义 非对称加密是现代密码学的伟大发明，它有效解决了对称加密需要安全分发密钥的问题。 顾名思义，非对称加密中，加密密钥和解密密钥是不同的，分别被称为公钥（Public Key）和私钥（Private Key）。私钥一般通过随机数算法生成，公钥可以根据私钥生成。 其中，公钥一般是公开的，他人可获取的；私钥则是个人持有并且要严密保护，不能被他人获取。 非对称加密算法优点是公私钥分开，无需安全通道来分发密钥。缺点是处理速度（特别是生成密钥和解密过程）往往比较慢，一般比对称加解密算法慢 2~3 个数量级；同时加密强度也往往不如对称加密。 非对称加密算法的安全性往往基于数学问题，包括大数质因子分解、离散对数、椭圆曲线等经典数学难题。 代表算法包括：RSA、ElGamal、椭圆曲线（Elliptic Curve Crytosystems，ECC）、SM2 等系列算法。 常用非对称加密算法 RSA：经典的公钥算法，1978 年由 Ron Rivest、Adi Shamir、Leonard Adleman 共同提出，三人于 2002 年因此获得图灵奖。算法利用了对大数进行质因子分解困难的特性，但目前还没有数学证明两者难度等价，或许存在未知算法可以绕过大数分解而进行解密。 ElGamal：由 Taher ElGamal 设计，利用了模运算下求离散对数困难的特性，比 RSA 产生密钥更快。被应用在 PGP 等安全工具中。 椭圆曲线算法（Elliptic Curve Cryptography，ECC）：应用最广也是强度最早的系列算法，基于对椭圆曲线上特定点进行特殊乘法逆运算（求离散对数）难以计算的特性。最早在 1985 年由 Neal Koblitz 和 Victor Miller 分别独立提出。ECC 系列算法具有多种国际标准（包括 ANSI X9.63、NIST FIPS 186-2、IEEE 1363-2000、ISO/IEC 14888-3 等），一般被认为具备较高的安全性，但加解密过程比较费时。其中，密码学家 Daniel J.Bernstein 于 2006 年提出的 Curve25519/Ed25519/X25519 等算法（分别解决加密、签名和密钥交换），由于其设计完全公开、性能突出等特点，近些年引起了广泛关注和应用。 SM2（ShangMi 2）：中国国家商用密码系列算法标准，由中国密码管理局于 2010 年 12 月 17 日发布，同样基于椭圆曲线算法，一般认为其安全强度优于 RSA 系列算法。 非对称加密算法适用于签名场景或密钥协商过程，但不适于大量数据的加解密。除了 SM2 之外，大部分算法的签名速度要比验签速度慢（1~2个数量级）。 RSA 类算法被认为已经很难抵御现代计算设备的破解，一般推荐商用场景下密钥至少为 2048 位。如果采用安全强度更高的椭圆曲线算法，256 位密钥即可满足绝大部分安全需求。 性质 公钥用于加密, 私钥用于解密 公钥由私钥生成，私钥可以推导出公钥 从公钥无法推导出私钥 优点 解决了密钥传输中的安全性问题 解决了数据源确定性的问题 缺点 没有解决数据可以中途被篡改的问题 消息认证码与数字签名消息认证码和数字签名技术通过对消息的摘要进行加密，可以防止消息被篡改和认证身份。 消息认证码定义 消息认证码（Hash-based Message Authentication Code，HMAC），利用对称加密，对消息完整性（Integrity）进行保护。 基本过程为对某个消息，利用提前共享的对称密钥和 Hash 算法进行处理，得到 HMAC 值。该 HMAC 值持有方可以向对方证明自己拥有某个对称密钥，并且确保所传输消息内容未被篡改。 典型的 HMAC 生成算法包括 K，H，M 三个参数。K 为提前共享的对称密钥，H 为提前商定的 Hash 算法（如 SHA-256），M 为要传输的消息内容。三个参数缺失了任何一个，都无法得到正确的 HMAC 值。 消息认证码可以用于简单证明身份的场景。如 Alice、Bob 提前共享了 K 和 H。Alice 需要知晓对方是否为 Bob，可发送一段消息 M 给 Bob。Bob 收到 M 后计算其 HMAC 值并返回给 Alice，Alice 检验收到 HMAC 值的正确性可以验证对方是否真是 Bob。当然，消息认证码起作用的前提是网络中没有中间人攻击的情况，假设网络是安全的，因此在公网中，一半不会使用消息认证码来进行身份验证； 消息认证码的主要问题是需要提前共享密钥，并且当密钥可能被多方同时拥有（甚至泄露）的场景下，无法追踪消息的真实来源。如果采用非对称加密算法，则能有效的解决这个问题，即数字签名。 缺点 不能解决密钥安全传递的问题 数字签名定义 类似在纸质合同上进行签名以确认合同内容和证明身份，数字签名既可以证实某数字内容的完整性，又可以确认其来源（即不可抵赖，Non-Repudiation）。 一个典型的场景是，Alice 通过信道发给 Bob 一个文件（一份信息），Bob 如何获知所收到的文件即为 Alice 发出的原始版本？Alice 可以先对文件内容进行摘要，然后用自己的私钥对摘要进行加密（签名），之后同时将文件和签名都发给 Bob。Bob 收到文件和签名后，用 Alice 的公钥来解密签名，得到数字摘要，与对文件进行摘要后的结果进行比对。如果一致，说明该文件确实是 Alice 发过来的（因为别人无法拥有 Alice 的私钥），并且文件内容没有被修改过（摘要结果一致）。 理论上所有的非对称加密算法都可以用来实现数字签名，实践中常用算法包括 1991 年 8 月 NIST 提出的 DSA（Digital Signature Algorithm，基于 ElGamal 算法）和安全强度更高的 ECSDA（Elliptic Curve Digital Signature Algorithm，基于椭圆曲线算法）等。 除普通的数字签名应用场景外，针对一些特定的安全需求，产生了一些特殊数字签名技术，包括盲签名、多重签名、群签名、环签名等。 数字签名的类型 盲签名 盲签名（Blind Signature），1982 年由 David Chaum 在论文《Blind Signatures for Untraceable Payment》中提出。签名者需要在无法看到原始内容的前提下对信息进行签名。 盲签名可以实现对所签名内容的保护，防止签名者看到原始内容；另一方面，盲签名还可以实现防止追踪（Unlinkability），签名者无法将签名内容和签名结果进行对应。典型的实现包括 RSA 盲签名算法等。 多重签名 多重签名（Multiple Signature），即 n 个签名者中，收集到至少 m 个（n &gt;= m &gt;= 1）的签名，即认为合法。 其中，n 是提供的公钥个数，m 是需要匹配公钥的最少的签名个数。 多重签名可以有效地被应用在多人投票共同决策的场景中。例如双方进行协商，第三方作为审核方。三方中任何两方达成一致即可完成协商。 比特币交易中就支持多重签名，可以实现多个人共同管理某个账户的比特币交易。 群签名 群签名（Group Signature），即某个群组内一个成员可以代表群组进行匿名签名。签名可以验证来自于该群组，却无法准确追踪到签名的是哪个成员。 群签名需要存在一个群管理员来添加新的群成员，因此存在群管理员可能追踪到签名成员身份的风险。 群签名最早在 1991 年由 David Chaum 和 Eugene van Heyst 提出。 环签名 环签名（Ring Signature），由 Rivest，Shamir 和 Tauman 三位密码学家在 2001 年首次提出。环签名属于一种简化的群签名。 签名者首先选定一个临时的签名者集合，集合中包括签名者自身。然后签名者利用自己的私钥和签名集合中其他人的公钥就可以独立的产生签名，而无需他人的帮助。签名者集合中的其他成员可能并不知道自己被包含在最终的签名中。 环签名在保护匿名性方面也具有很多用途。 优点 解决了密钥传输中的安全性问题 解决了数据中途可能被篡改的问题 解决了数据源验证的问题 证书授权中心-CA定义 对于非对称加密算法和数字签名来说，很重要的步骤就是公钥的分发。理论上任何人都可以获取到公开的公钥。然而这个公钥文件有没有可能是伪造的呢？传输过程中有没有可能被篡改呢？一旦公钥自身出了问题，则整个建立在其上的的安全性将不复成立。 数字证书机制正是为了解决这个问题，它就像日常生活中的证书一样，可以确保所记录信息的合法性。比如证明某个公钥是某个实体（个人或组织）拥有，并且确保任何篡改都能被检测出来，从而实现对用户公钥的安全分发。 根据所保护公钥的用途，数字证书可以分为加密数字证书（Encryption Certificate）和签名验证数字证书（Signature Certificate）。前者往往用于保护用于加密用途的公钥；后者则保护用于签名用途的公钥。两种类型的公钥也可以同时放在同一证书中。 一般情况下，证书需要由证书认证机构（Certification Authority，CA）来进行签发和背书。权威的商业证书认证机构包括 DigiCert、GlobalSign、VeriSign 等。用户也可以自行搭建本地 CA 系统，在私有网络中进行使用。 CA解决的问题 CA解决了电子商务中公钥的可信度的问题 负责证明 “我确实是我” CA是受信任的第三方，公钥的合法性校验 CA证书的内容 证书的持有人的公钥 证书授权中心的名称 证书的有效期 证书授权中心的数字签名 区块链的密码学应用区块链使用的密码学函数区块链技术的运行中使用了多项密码学函数，其中最主要的函数包括以下算法 哈希算法 数字签名 零知识证明 哈希算法在区块链系统中的应用哈希函数的特性 确定性：无论在同一个哈希函数中解析多少次，输入同一个A总是能得到相同的输出h(A)。 高效运算：计算哈希值的过程是高效的。 抗原像攻击（隐匿性）：对一个给定的输出结果h(A)，想要逆推出输入A，在计算上是不可行的。 抗碰撞性（抗弱碰撞性）：对任何给定的A和B，找到满足B≠A且h(A)=h(B)的B，在计算上是不可行的。 细微变化影响：任何输入端的细微变化都会对哈希函数的输出结果产生剧烈影响。 谜题友好性：对任意给定的Hash码Y和输入值x而言，找到一个满足h(k|x)=Y的k值在计算上是不可行的。 区块链数据结构区块链账本的数据结构和链表结构比较类似， 只不过区块指向的是老区块头部的hash值； 区块链的构成如下图： 区块链本质上是一个链表，其中的每个新区块都包含一个哈希指针。指针指向前一区块及其含有的所有数据的哈希值。假设修改某一个区块内容，即使是很微小的变化，那也会对其hash值产生很大的影响，导致区块不能和下一个区块组成链； 因此，每一个区块之间都有了不可篡改的特性。 梅克耳树默克尔树（又叫哈希树）是一种典型的二叉树结构，由一个根节点、一组中间节点和一组叶节点组成。默克尔树最早由 Merkle Ralf 在 1980 年提出，曾广泛用于文件系统和 P2P 系统中。 其主要特点为： 最下面的叶节点包含存储数据或其哈希值。 非叶子节点（包括中间节点和根节点）都是它的两个孩子节点内容的哈希值。 进一步地，默克尔树可以推广到多叉树的情形，此时非叶子节点的内容为它所有的孩子节点的内容的哈希值。 默克尔树逐层记录哈希值的特点，让它具有了一些独特的性质。例如，底层数据的任何变动，都会传递到其父节点，一层层沿着路径一直到树根。这意味树根的值实际上代表了对底层所有数据的“数字摘要”。 默克尔树主要功能有: 1. 快速比较大量数据； 2. 快速定位修改； 3. 零知识证明； 在区块链中每个区块都有自己的梅克尔根（Merkle Root）。现在，正如你已知道的，每个区块里都包含多笔交易。如果将这些交易按线性存储，那么在所有交易中寻找一笔特定交易的过程会变得无比冗长。 这就是我们使用梅克尔树的原因， 让交易在区块中变得容易寻找，由于hash的微细变化影响的特性，也让交易不能被修改。 在梅克尔树中，所有的交易通过哈希算法都能向上追溯至同一根。这就使得搜索变得非常容易。因此，如果想要在区块里面找到某一个指定的数据，可以直接通过梅克尔树里的哈希值来进行搜索，而不需要遍历所有的交易进行搜索。 比如，先要验证红色圆圈的交易数据是否在区块内，只需要提供蓝色圆圈的交易，就可以根据最终生成的hash值和梅克耳根对比，如果结果相同，就说明交易确实属于该区块，并且是正确的。 挖矿加密谜题被用来挖掘新的区块，因此哈希算法仍然至关重要。其工作原理是调整难度值的设定。随后，一个被命名为“nonce”的随机字符串被添加到新区块的哈希值上，然后被再次哈希。接着，再来检验其是否低于已设定的难度值水平。如果低于，那么产生的新区块会被添加至链上，而负责挖矿的矿工就会获得奖励。如果没有低于，则矿工继续修改随即字符串“nouce”，直至低于难度值水平的值出现。 正如你所见，哈希算法是区块链和加密经济学中一个至关重要的部分。 工作量证明概念 当矿工们通过“挖矿”来产生新区块并添加至区块链上时，其中验证及添加区块涉及到的共识系统被称为“工作量证明”。矿工们使用庞大的计算机算力来解决这道密码学谜题，而难度值决定了这道题的所需要的计算量。这是区块链技术中最具开拓意义的机制之一。早期的去中心化点对点数字货币系统之所以会失败，是由于“拜占庭将军问题”导致的，而工作量证明的共识系统为该问题提供了一种解决方案。 工作量证明实际上就是对hash算法消耗的算力的应用，让攻击者很难去在规定时间集中如此大的算力篡改数据内容； 拜占庭将军问题 假设有一群拜占庭将军想要攻打一座城市，他们将面临两个不同的问题： 每个将军及其军队在地理上相距甚远，因此不可能通过中央来统一指挥，这使得协同作战变得异常困难。 被攻打的城市拥有一只庞大的军队，他们能获得胜利的唯一方式是所有人在同一时刻一同发起进攻。 为了让合作成功，位于城堡左边的军队派遣一位信使，向城堡右边的军队发送了一则内容为“周三攻击”的信息， 如果所有军队都准备好了， 那就可以确定周三攻击。但是，如果邮编的军队没有做好攻击准备，并让信使携带一则内容为“不，周五攻击”的信息返回。而信使需要通过穿越被攻打的城市返回到左边的军队； 问题就来了。这位信使身上可能会发生很多事情。比如，他有可能被抓获、泄露信息、或被攻打的城市杀害后将其替换了。这将导致军队获得被篡改过的信息，从而使作战计划无法达成一致而失败。 上述例子对区块链有明显借鉴意义。区块链是一个巨型网络，你要如何信任他们呢？如果你想从钱包里发送4个以太币给某人，你如何确认网络中的某人不会篡改信息，将4个以太币改成40个， 或者更改了其接受者？中本聪发明了工作量证明机制来绕过拜占庭将军问题。其运行原理是：假设左边的军队想要发送内容为“周三进攻”的信息给右边的军队，他们需要执行如下步骤： 首先，他们会给初始文本添加一个“nonce”，这个nonce可以是任何一个随机十六进制值。 其次，他们将添加了“nonce”的文本进行哈希，得到一个结果。假设说他们决定仅当哈希结果前5位是零的时候，才进行信息共享。 如果哈希结果满足条件，他们就会让信使带着有哈希结果的信息出发。否则，他们会持续随机改变nonce的值，直到得到想要的结果。这一过程不仅冗长耗时，且占用大量的算力。 如果敌人抓到了信使，并企图篡改信息，那么根据哈希函数的特性，哈希结果将会剧烈变化。如果城市右边的将军看到信息没有以规定数量的0作为开头，那么他们就会叫停攻击。 工作量证明的本质 寻找一个符合哈希目标的nonce值，是一个非常困难且耗时的过程， 想要篡改需要浪费大量的算例。 然而，验证结果中是否有作恶行为却是非常简单的。 数字签名在区块链系统中的应用概念假设Alan想把信息“m”发送出去，Alan有一把私钥Ka-和一把公钥Ka+。那么，当他把信息发送给Tyrone时，他会用私钥将该条信息加密，于是信息变成了Ka-(m)。当Tyrone收到这条信息时，他可以使用Alan的公钥来取回信息，Ka+(Ka-(m))，于是便得到了原始信息“m”; 签名的作用就是用来保证信息在传输的过程中没有被修改过， 也可以用来验证信息确实是从发送者处发送出来。 数字签名使用的过程 Alan有一笔交易“m”，并且Tyrone知道他正在接收该笔交易。 Alan对m进行哈希运算，得到h(m)。 Alan用自己的私钥对哈希结果进行加密，得到Ka-(h(m))。 Alan将加密数据发送给Tyrone。 Tyrone使用Alan的公钥来解密，Ka+(Ka-(h(m)))，并得到原来的哈希结果h(m)。 Tyrone用已知的“m”进行哈希运算，可以得到h(m)。 哈希函数的确定性特征决定了如果h(m)=h(m)，就意味着这笔交易是真实有效的。 数字签名的特性 可验证性：如果加密信息能够用Alan的公钥进行解密，那就可以100%确定是Alan发送了该条信息。 不可伪造性：如果说有其他人，例如Bob，拦截了该条信息，并用自己的私钥发送了一条自己的信息，那么Alan的公钥将无法对其解密。Alan的公钥只能用来解密Alan用自己的私钥加密过的信息。 不可抵赖性：同样的，如果Alan宣称，“我没有发送信息，是Bob发的”，但Tyrone却能够用Alan的公钥来解密信息，那就证明Alan在撒谎。如此，Alan就无法收回他之前发出的信息，并将其归咎于他人。 零知识证明概念(ZKP)零知识证明是这样一个过程， 证明着不向验证着提供任何额外的信息的前提下， 使验证着相信某个论断是正确的。 ZKP意味着A可以向B证明，他知道特定的信息，而不必告诉对方自己具体知道些什么，这尤为有用，因为这将为证明者提供一层额外的隐私保护。 在区块链系统中，为了做到完全的隐私，对数据的验证可以通过零知识证明，对交易的详情等可以做到完全的隐藏，避免泄露信息； 零知识证明具备的性质 完整性：如果陈述属实，那么诚实的验证者能被诚实的证明者说服。 可靠性：如果证明者不诚实，他们无法通过说谎来说服验证者相信陈述是可靠的。零知识：如果陈述属实，那么验证者无法得知陈述的内容是什么。 举个栗子，证明者（P）对验证者（V）说，他知道洞穴后面暗门的密码，并提出在不向验证者透露密码的情况下证明此事。那么，其验证过程如下图所示： 证明者可以走路径A或者路径B，假设他们一开始决定通过路径A到达暗门。同时，验证者V来到入口，他对证明者选择哪条路径并不知情，并宣称他们希望见到证明者在路径B出现。 如图所示，证明者确实出现在路径B上，但万一这仅是巧合呢？也有可能是证明者凭运气在出发时选择了路径B，却因不知道密码被困在了门口。 所以，我们需要通过多次试验来确定测试的有效性。如果证明者每次都能出现在正确的路径上，那么证明者的确可以在不向验证者透露密码的情况下，证明自己知道密码。 零知识证明使用实例许多基于区块链的技术都在使用Zk-Snarks。事实上，以太坊在大都会阶段就计划引入Zk-Snarks，并且将其加入以太坊的功能库。Zk-Snarks是“零知识简洁无交互知识认证”的简称，是一种在无需泄露数据本身情况下证明某些数据运算的一种零知识证明。 以上内容可用来生成一个证明，通过对每笔交易创建一个简单的快照来验证其有效性。这足以向信息接收方证明交易的有效性，而无需泄露交易的实质内容。 通过零知识证明，解决了区块链的一些因为问题: 1.实现了交易的完整性和隐私性; 2.实现了系统的抽象性。由于无需展示整个交易内部的工作方式，因此系统非常易用。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊技术栈]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F4.%E4%BB%A5%E5%A4%AA%E5%9D%8A%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[以太坊的介绍以太坊简介以太坊（Ethereum）项目的最初目标，是打造一个运行智能合约的平台（Platform for Smart Contract）。该平台支持图灵完备的应用，按照智能合约的约定逻辑自动执行，理想情况下将不存在故障停机、审查、欺诈，以及第三方干预等问题。 以太坊平台目前支持 Golang、C++、Python 等多种语言实现的客户端。由于核心实现上基于比特币网络的核心思想进行了拓展，因此在很多设计特性上都与比特币网络十分类似。 基于以太坊项目，以太坊团队目前运营了一条公开的区块链平台——以太坊网络。智能合约开发者使用官方提供的工具和以太坊专用应用开发语言 Solidity，可以很容易开发出运行在以太坊网络上的“去中心化”应用（Decentralized Application，DApp）。这些应用将运行在以太坊的虚拟机（Ethereum Virtual Machine，EVM）里。用户通过以太币（Ether）来购买燃料（Gas），维持所部署应用的运行。 以太坊和比特币的区别以太坊的目的是创建一种去中心化应用的协议，提供一套对大量去中心化应用程序非常有用的新方案，特别强调快速开发，对小的和少数人使用的应用也非常安全（小而使用人少的应用容易被51%攻破），以及不同应用程序之间能够有效的互动。以太坊用过建立在本质上是抽象的基础层来完成这一工作； 一个区块链其内置了图灵完备的编程语言，允许任何人编写智能合约和去中心化的应用程序，在这些应用程序中，他们可以创建任意的属于他们自己的规则、交易格式和状态转换函数。名字币的一个简单版本在以太坊可以用两行代码来编写完成，而其他协议如货币和信用系统则可以用不到 20 行的代码来构建。智能合约-包含价值而且只有满足某些条件才能打开的加密箱子-也能在我们的平台上构建，并且因为图灵完备性、价值知晓（value-awareness）、区块链知晓（blockchain-awareness）和多状态所增加的力量，远比特币脚本所能提供的功能强大得多； 以太坊主要特点以太坊区块链底层也是一个类似比特币网络的 P2P 网络平台，智能合约运行在网络中的以太坊虚拟机里。网络自身是公开可接入的，任何人都可以接入并参与网络中数据的维护，提供运行以太坊虚拟机的资源。 跟比特币项目相比，以太坊区块链的技术特点主要包括： 支持图灵完备的智能合约，设计了编程语言 Solidity 和虚拟机 EVM； 选用了内存需求较高的哈希函数，避免出现强算力矿机、矿池攻击； 叔块（Uncle Block）激励机制，降低矿池的优势，并减少区块产生间隔（10 分钟降低到 15 秒左右）； 采用账户系统和世界状态，而不是 UTXO，容易支持更复杂的逻辑； 通过 Gas 限制代码执行指令数，避免循环执行攻击； 支持 PoW 共识算法，并计划支持效率更高的 PoS 算法。 此外，开发团队还计划通过分片（Sharding）方式来解决网络可扩展性问题。 这些技术特点，解决了比特币网络在运行中被人诟病的一些问题，让以太坊网络具备了更大的应用潜力。 以太坊的核心概念智能合约什么是智能合约 智能合约（Smart Contract）是以太坊中最为重要的一个概念，即以计算机程序的方式来缔结和运行各种合约。最早在上世纪 90 年代，Nick Szabo 等人就提出过类似的概念，但一直依赖因为缺乏可靠执行智能合约的环境，而被作为一种理论设计。区块链技术的出现，恰好补充了这一缺陷。 以太坊支持通过图灵完备的高级语言（包括 Solidity、Serpent、Viper）等来开发智能合约。智能合约作为运行在以太坊虚拟机（Ethereum Virual Machine，EVM）中的应用，可以接受来自外部的交易请求和事件，通过触发运行提前编写好的代码逻辑，进一步生成新的交易和事件，可以进一步调用其它智能合约。 智能合约的执行结果可能对以太坊网络上的账本状态进行更新。这些修改由于经过了以太坊网络中的共识，一旦确认后无法被伪造和篡改。 只能合约作用 智能合约是在以太坊虚拟机上运行的应用程序。这是一个分布的“世界计算机”，计算能力由所有以太坊节点提供。提供计算能力的任何节点都将以Ether数字货币作为资源支付。 他们被命名为智能合约，因为您可以编写满足要求时自动执行的“合同”。 例如，想象一下在以太坊之上建立一个类似Kickstarter的众筹服务。有人可以建立一个以太坊智能合约，将资金汇集到别人身上。这个智能合约可以写成这样的话：当将100,000美元的货币添加到池中时，它将全部发送给收件人。或者，如果一个月内没有达到100,000美元的门槛，所有的货币都将被发回给货币的原始持有人。当然，这将使用以太币代替美元。 这一切都将根据智能合同代码进行，智能合同代码可自动执行交易，而无需可信任的第三方持有货币并签署交易。例如，Kickstarter在5％的付款处理费之上收取5％的费用，这意味着在$100,000的众筹项目中将收取8000到10000美元的费用。智能合约不需要向像Kickstarter这样的第三方支付费用。 智能合约可以用于许多不同的事情。开发人员可以创建智能合约，为其他智能合约提供功能，类似于软件库的工作方式。或者，智能合约可以简单地用作应用程序来存储以太坊区块链上的信息。 为了真正执行智能合同代码，有人必须发送足够的以太网代币作为交易费 - 多少取决于所需的计算资源。这为以太坊节点参与并提供计算能力付出了代价。 账户ACCOUNT账户介绍 在之前章节中，笔者介绍过比特币在设计中并没有账户（Account）的概念，而是采用了 UTXO 模型记录整个系统的状态。任何人都可以通过交易历史来推算出用户的余额信息。而以太坊则采用了不同的做法，直接用账户来记录系统状态。每个账户存储余额信息、智能合约代码和内部数据存储等。以太坊支持在不同的账户之间转移数据，以实现更为复杂的逻辑。 具体来看，以太坊账户分为两种类型：合约账户（Contracts Accounts）和外部账户（Externally Owned Accounts，或 EOA）。 合约账户：存储执行的智能合约代码，只能被外部账户来调用激活； 外部账户：以太币拥有者账户，对应到某公钥。账户包括 nonce、balance、storageRoot、codeHash 等字段，由个人来控制。 在以太坊中，全网的状态是由被“账户”的对象组成的，账户之间可以直接的进行价值和信息的转移，一个以太坊的账户包含下面 4 个字段: 随机数, 一个计数器，用以确保每个交易都只会被处理一次 账户当前的以太币额度 账户的合约代码, 如果有的话 这个账户的 存储 (默认空) 合约账户外部账户之间关系 当合约账户被调用时，存储其中的智能合约会在矿工处的虚拟机中自动执行，并消耗一定的燃料。燃料通过外部账户中的以太币进行购买。 对于大多数用户来说，最基本的区别在于，用户掌握着 EOA 账号，因为用户掌握着控制 EOA 账号的私钥。而合约账号由内部程序代码来控制的，当然掌控私钥的 EOA 账户可以通过编写特定的程序代码来掌控合约账户。流行的术语“智能合约”就是合约账号中的代码，当一个交易被发送到该账户时，合约中的代码就会被执行。用户可以通过把代码部署到区块链中来创建一个新合约，也即创建了一个新的合约账户。 合约账户只有在 EOA 账户发出一个指令的时候才会去执行一个操作。所以一个合约账户是不可能自己去执行一个操作的，如生产一个随机数或执行一个 API 调用等，它只有在 EOA 账户作出确认的情况下才会去做这些事情。这是因为以太坊要求节点能够对计算的结果无论对错都达成一致，这就对操作有了一个必定会执行的要求。 从外部拥有账户到合约账户的消息会激活合约账户的代码，允许它执行各种动作。（比如转移代币，写入内部存储，挖出一个新代币，执行一些运算，创建一个新的合约等等）。 以太币以太币（Ether）是以太坊网络中的货币。 以太币主要用于购买燃料，支付给矿工，以维护以太坊网络运行智能合约的费用。以太币最小单位是 wei，一个以太币等于 10^18 个 wei。 以太币同样可以通过挖矿来生成，成功生成新区块的以太坊矿工可以获得 3 个以太币的奖励，以及包含在区块内交易的燃料费用。用户也可以通过交易市场来直接购买以太币。 目前每年大约可以通过挖矿生成超过一千万个以太币，单个以太币的市场价格目前超过 300 美金。 燃料Gas燃料（Gas），控制某次交易执行指令的上限。每执行一条合约指令会消耗固定的燃料。当某个交易还未执行结束，而燃料消耗完时，合约执行终止并回滚状态。 Gas 可以跟以太币进行兑换。需要注意的是，以太币的价格是波动的，但运行某段智能合约的燃料费用可以是固定的，通过设定 Gas 价格等进行调节。 以太坊虚拟机以太坊是一个可编程的区块链，不仅仅是给用户一些预定义操作（如比特币只交易），以太坊允许用户创建属于他们自己的复杂的操作。以太坊作为一个平台为不同的区块链应用提供服务。 狭义来说，以太坊是一系列协议，其核心就是一个以太坊虚拟机，能执行遵守协议的任何复杂的代码。以太坊虚拟机是图灵完备的，开发者可以在虚拟机上使用像 javascript，python 这样的友好的编程语言来创建应用。 和任何的区块链一样，以太坊包含了一个点对点的网络协议。这以太坊区块链是被链接着这个网络的各个节点维护和更新的。网络中的各个节点的虚拟机都执行相同的指令来共同维护区块数据库，因为这个原因，以太坊有时候被人称为“世界计算机”。 以太坊全网的大规模并行计算不是只为了提计算效率，而是为了保证全网的数据一致性。实际上，这使得在以太网上的 运算要比传统的电脑慢的多，成本也昂贵得多。全网中的每一台虚拟机的运行都是为确保全网数据库的一致性。去中心化的一致性给全网极端的容错能力;抗审查能力和永不宕机能力等。 以太坊账户以太坊的基本单元是账号。每一个账户都有一个 20 个字节长度的地址 。以太坊区块链跟踪每一个账号的状态，区块链上所有状态的转移都是账户之间的令牌（令牌即以太币）和信息的转移。以太坊有 2 种账户类型： 外部账号，简称 EOA,是由私钥来控制的。 有一个以太币的余额 可以发送交易(以太币转账或者激活合约代码) 通过私钥控制 没有相关联的代码 合约帐户,由合约代码来控制,且只能由一个 EOA 账号来操作 有一个以太币余额 相关联的代码 代码执行是通过交易后者其他的合约发送的call来激活 当被执行时 – 运行在随机复杂度(图灵完备性) – 只能操作其拥有的特定存储， 例如可以拥有其永久的state – 可以call其他合约 运行环境和语言运行环境以太坊采用以太坊虚拟机作为智能合约的运行环境。以太坊虚拟机是一个隔离的轻量级虚拟机环境，运行在其中的智能合约代码无法访问本地网络、文件系统或其它进程。 对同一个智能合约来说，往往需要在多个以太坊虚拟机中同时运行多份，以确保整个区块链数据的一致性和高度的容错性。另一方面，这也限制了整个网络的容量。 开发语言以太坊为编写智能合约设计了图灵完备的高级编程语言，降低了智能合约开发的难度。 目前 Solidity 是最常用的以太坊合约编写语言之一。 智能合约编写完毕后，用编译器编译为以太坊虚拟机专用的二进制格式（EVM bytecode），由客户端上传到区块链当中，之后在矿工的以太坊虚拟机中执行。 消息和交易以太坊交易交易（Transaction），在以太坊中是指从一个账户到另一个账户的消息数据。消息数据可以是以太币或者合约执行参数。 名词“交易”在以太坊中是指签名的数据包，这个数据包中存储了从外部账户发送的消息，以太坊是用交易作为操作的最小单位，交易包含以下内容: 消息的接受者 一个可以识别发送者的签名 发送方给接收方的以太币数量 一个可选的数据字段 一个 STARTGAS 值, 表示执行这个交易允许消耗的最大计算步骤 一个 GASPRICE 值, 表示发送方的每个计算步骤的费用 前面三个是每一个加密货币都有的标准字段。默认情况下第四个数据字段没有任何功能，但是合约可以访问这里的数据；举个例子，如果一个合约是在一个区块链上提供域名注册服务的，那么它就会想把这数据字段中的数据解析成 2 个字段，第一个字段是域名，第二个字段是域名对应的 IP 地址。这个合约会从数据字段中读取这些值，然后适当把它们保存下来。 这个 STARTGAS 和 GASPRICE 字段 是以太坊的预防拒绝式攻击用的，非常重要。为了防止在代码中出现意外或敌对的无限循环或其他计算浪费，每个交易都需要设置一个限制，以限制它的计算总步骤是一个明确的值。这计算的基本单位是“汽油（gas）”； 通常，一个计算成本是一个 1 滴汽油，但是一些操作需要消耗更多的汽油，因为它们的计算成本更高。在交易数据中每一个字节需要消耗 5 滴汽油。这样做的目的是为了让攻击者为他们所消耗的每一种资源，包括计算，带宽和存储支付费用；所以消耗网络资源越多，则交易成本就越大。 交易模型UTXO模型UTXO 模型中，交易只是代表了 UTXO 集合的变更。而账户和余额的概念是在 UTXO 集合上更高的抽象，账号和余额的概念只存在于钱包中。 优点： 计算是在链外的，交易本身既是结果也是证明。节点只做验证即可，不需要对交易进行额外的计算，也没有额外的状态存储。交易本身的输出 UTXO 的计算是在钱包完成的，这样交易的计算负担完全由钱包来承担，一定程度上减少了链的负担。 除 Coinbase 交易外，交易的 Input 始终是链接在某个 UTXO 后面。交易无法被重放，并且交易的先后顺序和依赖关系容易被验证，交易是否被消费也容易被举证。 UTXO 模型是无状态的，更容易并发处理。 对于 P2SH 类型的交易，具有更好的隐私性。交易中的 Input 是互不相关联的，可以使用 CoinJoin 这样的技术，来增加一定的隐私性。 缺点： 无法实现一些比较复杂的逻辑，可编程性差。对于复杂逻辑，或者需要状态保存的合约，实现难度大，且状态空间利用率比较低。 当 Input 较多时，见证脚本也会增多。而签名本身是比较消耗 CPU 和存储空间的。 ACCOUNT模型出于智能合约的便利考虑，以太坊采用了账户的模型，状态可以实时的保存到账户里，而无需像比特币的 UXTO 模型那样去回溯整个历史。 对于 Account 模型，Account 模型保存了世界状态，链的状态一般在区块中以 StateRoot 和 ReceiptRoot 等形式进行共识。交易只是事件本身，不包含结果，交易的共识和状态的共识本质上可以隔离的。 优点： 合约以代码形式保存在 Account 中，并且 Account 拥有自身状态。这种模型具有更好的可编程性，容易开发人员理解，场景更广泛。 批量交易的成本较低。设想矿池向矿工支付手续费，UTXO 中因为每个 Input 和 Out 都需要单独 Witness script 或者 Locking script，交易本身会非常大，签名验证和交易存储都需要消耗链上宝贵的资源。而 Account 模型可以通过合约的方式极大的降低成本。 缺点： Account 模型交易之间没有依赖性，需要解决重放问题。 对于实现闪电网络/雷电网络，Plasma 等，用户举证需要更复杂的 Proof 证明机制，子链向主链进行状态迁移需要更复杂的协议。 UTXO和ACCOUNT区别 计算问题 UTXO 交易本身对于区块链并没有复杂的计算，这样简单的讲其实并不完全准确，原因分有两个，一是 Bitcoin 本身的交易多为 P2SH，且 Witness script 是非图灵完备的，不存在循环语句。而对于 Account 模型，例如 Ethereum，由于计算多在链上，且为图灵完备，一般计算较为复杂，同时合约安全性就容易成为一个比较大的问题。当然是否图灵完备对于是否是账户模型并没有直接关联。但是账户模型引入之后，合约可以作为一个不受任何人控制的独立实体存在，这一点意义重大。 UTXO更易并发 在 UTXO 模型中，世界状态即为 UTXO 的集合，节点为了更快的验证交易，需要在内存中存储所有的 UTXO 的索引，因此 UTXO 是非常昂贵的。对于长期不消费的 UTXO，会一直占用节点的内存。所以对于此种模型，理论上应该鼓励用户减少生产 UTXO，多消耗 UTXO。但是如果要使用 UTXO 进行并行交易则需要更多的 UTXO 作为输入，同时要产生更多的 UTXO 来保证并发性，这本质上是对网络进行了粉尘攻击。并且由于交易是在钱包内构造，所以需要钱包更复杂的设计。反观 Account 模型，每个账户可以看成是单独的互不影响的状态机，账户之间通过消息进行通信。所以理论上用户发起多笔交易时，当这些交易之间不会互相调用同一 Account 时，交易是完全可以并发执行的。 Account模型的交易重放 Ethereum 使用了在 Account 中增加 nonce 的方式，每笔交易对应一个 nonce，nonce 每次递增。这种方式虽然意在解决重放的问题，但是同时引入了顺序性问题，同时使得交易无法并行。例如在 Ethereum中，用户发送多笔交易，如果第一笔交易打包失败，将引起后续多笔交易都打包不成功。在 CITA 中我们使用了随机 nonce 的方案，这样用户的交易之间没有顺序性依赖，不会引起串联性失败，同时使得交易有并行处理的可能。 存储问题 因为 UTXO 模型中，只能在交易中保存状态。而 Account 模型的状态是在节点保存，在 Ethereum 中使用 MPT 的方式存储，Block 中只需要共识 StateRoot 等即可。这样对于链上数据，Account 模型实际更小，网络传输的量更小，同时状态在节点本地使用 MPT 方式保存，在空间使用上也更有效率。例如 A 向 B 转账，如果在 UTXO 中假设存在 2 个 Input 和2个 Output，则需要 2 个 Witness script 和 2 个 Locking script；在 Account 模型中则只需要一个签名，交易内容只包含金额即可。在最新的隔离见证实现后，Bitcoin 的交易数据量也大大减少，但是实际上对于验证节点和全节点仍然需要针对 Witness script 进行传输和验证。 轻节点获取地址状态难易 例如钱包中，需要向全节点请求所有关于某个地址的所有 UTXO，全节点可以发送部分 UTXO，钱包要验证该笔 UTXO 是否已经被消费，有一定的难度，而且钱包很难去证明 UTXO 是全集而不是部分集合。而对于 Account 模型则简单很多，根据地址找到 State 中对应状态，当前状态的 State Proof 则可以证明合约数据的真伪。当然对于 UTXO 也可以在每个区块中对 UTXO 的 root 进行验证，这一点与当前 Bitcoin 的实现有关，并非 UTXO 的特点。 综上 ​ 综上来看，Account 模型在可编程性，灵活性等方面更有优势；在简单业务和跨链上，UTXO 有其非常独到和开创性的优点。对于选择何种模型，要从具体的业务场景进行出发。 UTXO和ACCOUNT的对比 以太坊消息合约具有发送”消息”到其他合约的能力。消息是一个永不串行且只在以太坊执行环境中存在的虚拟对象。他们可以被理解为函数调用（function calls）。 一个消息包括： 明确的消息发送者 消息的接收者 一个可选的数据域，这是合约实际上的输入数据 一个GASLIMIT值，用来限制这个消息出发的代码执行可用的最大gas数量 总的来说，一个消息就像是一个交易，除了它不是由外部账户生成，而是合约账户生成。当合约正在执行的代码中运行了CALL 或者DELEGATECALL这两个命令时，就会生成一个消息。消息有的时候也被称为”内部交易”。与一个交易类似，一个消息会引导接收的账户运行它的代码。因此，合约账户可以与其他合约账户发生关系，这点和外部账户一样。有许多人会误用交易这个词指代消息，所以可能消息这个词已经由于社区的共识而慢慢退出大家的视野，不再被使用。 以太坊状态转移函数 以太坊的状态转移函数 APPLY(S,TX) -&gt; S’ 可以被定义成下面的: 检查这个交易是不是合法的 ,签名是不是合法的, 这随机数是不是匹配这个发送者的账户，如果答案是否定的，那返回错误。 用 STARTGAS * GASPRICE 计算交易的费用，并且从签名中确定这个发送者的地址。 从发送者的余额中减去费用，并且增加发送者的随机值。如果余额不够，则返回错误。 初始化 GAS = STARTGAS, 并根据这交易中的字节数拿走一定量的汽油。 把交易的值从发送的账户转移到接收者的账户。如果接收者的账户还不存在，就创建一个。如果这个接收者的账户是一个合约，那么就运行合约的代码直到完成，或者报汽油消耗光的异常。 如果值转移失败了，因为发送者没有足够多的余额，或代码执行消耗光了汽油，恢复除了支付的费用外的所有的状态，并且把这个费用添加到矿工的账户上。 另外,把所有剩下的汽油退还给发送者，然后把用于支付费用的汽油发送给矿工。举例，假设合约的代码是这样的:if !self.storage[calldataload(0)]: self.storage[calldataload(0)] = calldataload(32)注意，真实的合约代码是用底层的 EVM 代码编写的；这个列子是用一个叫 Serpent 的高级语言写的。假设这个合约的存储开始是空的，并且发送了一个交易，其中包含 10 个以太币，2000 个汽油，汽油价格是 0.001 比特币，和 64 字节的数据，其中 0-31 字节代表数字 2,32-63 字节代表字符串 CHARLIE。在这个案例中，这状态转移函数的处理如下： 检查者交易是否有效并且格式完好。 检查者交易的发送者是否至少有 2000 * 0.001 = 2 以太币。如果有，则从发送者的账户中减去 2 以太币。 初始化 汽油（gas）= 2000;假设这个交易是 170 个字节长度并且每个字节的费用是 5，那么减去 850，汽油还剩 1150。 从发送者的账户减去 10 个以太币，并且添加到合约的账户中。 运行合约的代码. 在这里例子中:检查合约的存储的第 2 个索引是否已经被使用，注意到它没 有，然后就把这数据存储的第二个索引的值设置为 CHARLIE. 假设这个操作消耗了 187 个汽 油，那么剩下的汽油总量是 1150 – 187 = 963 把 963 * 0.001 = 0.963 以太币加到发送者的账户，然后反正结果状态。 如果交易的接收端没有合约，那么这总的交易费用就简单的等于汽油的价格乘以这个交易的字节长 度，与交易一起发送的数据字段的数据将无关重要。 注意，在恢复这个方面，消息和交易的处理方式是相同的： 如果一个消息执行消耗光了汽油，那么 这消息的执行和其他被触发的执行都会被恢复，但是父类的执行不会恢复。 Gas的详细介绍什么是gas以太坊在区块链上实现了一个运行环境，被称为以太坊虚拟机（EVM）。每个参与到网络的节点都会运行都会运行EVM作为区块验证协议的一部分。他们会验证区块中涵盖的每个交易并在EVM中运行交易所触发的代码。每个网络中的全节点都会进行相同的计算并储存相同的值。合约执行会在所有节点中被多次重复，这个事实得使得合约执行的消耗变得昂贵，所以这也促使大家将能在链下进行的运算都不放到区块链上进行。对于每个被执行的命令都会有一个特定的消耗，用单位gas计数。每个合约可以利用的命令都会有一个相应的gas值。这里列了一些命令的gas消耗。 交易消耗的gas每笔交易都被要求包括一个gas limit（或startGas）和一个交易愿为单位gas支付的费用。矿工可以有选择的打包这些交易并收取这些费用。在现实中，今天所有的交易最终都是由矿工选择的，但是用户所选择支付的交易费用多少会影响到该交易被打包所需等待的时长。如果该交易由于计算，包括原始消息和一些触发的其他消息，需要使用的gas数量小于或等于所设置的gas limit，那么这个交易会被处理。如果gas总消耗超过gas limit，那么所有的操作都会被复原，但交易是成立的并且交易费任会被矿工收取。区块链会显示这笔交易完成尝试，但因为没有提供足够的gas导致所有的合约命令都被复原。所以交易里没有被使用的超量gas都会以以太币的形式打回给交易发起者。因为gas消耗一般只是一个大致估算，所以许多用户会超额支付gas来保证他们的交易会被接受。这没什么问题，因为多余的gas会被退回给你。 区块的gas limit是由在网络上的矿工决定的。与可调整的区块gas limit协议不同的是一个默认的挖矿策略，即大多数客户端默认最小区块gas limit为4,712,388。 以太坊上的矿工需要用一个挖矿软件，例如ethminer。它会连接到一个geth或者Parity以太坊客户端。Geth和Pairty都有让矿工可以更改配置的选项。这里是geth挖矿命令行选项以及Parity的选项。 估算交易消耗一个交易的交易费由两个因素组成： gasUsed：该交易消耗的总gas数量 gasPrice：该交易中单位gas的价格（用以太币计算） 交易费 = gasUsed * gasPrice gasUsed 每个EVM中的命令都被设置了相应的gas消耗值。gasUsed是所有被执行的命令的gas消耗值总和。 gasPrice 一个用户可以构建和签名一笔交易，但每个用户都可以各自设置自己希望使用的gasPrice，甚至可以是0。然而，以太坊客户端的Frontier版本有一个默认的gasPrice，即0.05e12 wei。矿工为了最大化他们的收益，如果大量的交易都是使用默认gasPrice即0.05e12 wei，那么基本上就很难又矿工去接受一个低gasPrice交易，更别说0 gasPrice交易了。 以太坊挖矿以太坊挖矿过程 这以太坊的区块链和比特币的区块链有很多相似的地方，也有很多不同的地方。这个以太坊和比特币在区块链体系中最重要的不同点是 ：以太坊的区块同时包含了交易列表和最近区块的状态。除此之外，2 个其他的值，区块的编号和难度值也存在在区块中。以太坊中最基本的区块验证算法如下： 检查上一个区块是否存在和其有效性。 检测这区块的时间戳，是不是比上一个区块的大，并且小于 15 分钟 检查这区块编号，难度值，交易根（transaction root） , 叔根（uncle root）和汽油限制是否有效 检查这区块的工作证明是否有效 把 S[0] 设置成上一个区块的末端的状态 让 TX 成为这区块的交易列表，如果有 n 个交易。则做 for 循环 For i in 0…n-1, 设置 S[i+1] = APPLY(S[i],TX[i]). 如果任何一个应用发生错误，或这区块中汽油的总的消耗达到了 GASLIMIT, 则返回一个错误. 让 S_FINAL 等于 S[n], 但是把支付给矿工的奖励添加到这区块里。 检查这个状态 S_FINAL 的默克尔树树根是不是和区块头信息中所提供的状态根是一样的。如果是，则区块有效，不然则无效。 乍看上去，这种方法似乎效率很低，因为它需要将整个状态存储在每个块中，但在现实中，效率应该与比特币相当。原因在于，状态存储在树结构中，并且每个块后，只需要修改树的一小部分。此外，由于所有的状态信息都是最后一个区块的一部分，所以不需要存储整个区块链的历史——这一策略，如果它可以应用于比特币，那么它的磁盘空间将节省 5-20 倍。 以太坊网络中交易会被验证这网络的节点收集起来。这些“矿工”在以太坊网络中收集、传播、验证和执行交易，然后整理归档这些交易，打包成一个区块，与别的矿工竞争将区块添加到区块链中，添加成功的矿工将收到奖励。通过这样的措施，鼓励人们为区块链全网提供更多的硬件和电力支持。 共识机制以太坊目前采用了基于成熟的 PoW 共识的变种算法 Ethash 协议作为共识机制。 为了防止 ASIC 矿机矿池的算力攻击，跟原始 PoW 的计算密集型 Hash 运算不同，Ethash 在执行时候需要消耗大量内存，反而跟计算效率关系不大。这意味着很难制造出专门针对 Ethash 的芯片，反而是通用机器可能更加有效。 虽然，Ethash 相对原始的 PoW 进行了改进，但仍然需要进行大量无效的运算，这也为人们所诟病。 社区已经有计划在未来采用更高效的 Proof-of-Stake（PoS）作为共识机制。相对 PoW 机制来讲，PoS 机制无需消耗大量无用的 Hash 计算，但其共识过程的复杂度要更高一些，还有待进一步的检验。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币技术栈]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F3.%E6%AF%94%E7%89%B9%E5%B8%81%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[比特币设计原理比特币网络是一个分布式的点对点网络，网络中的矿工通过“挖矿”来完成对交易记录的记账过程，维护网络的正常运行。 区块链网络提供一个公共可见的记账本，通过共识机制所有节点共同维护同一份账本，该记账本并非记录每个账户的余额，而是用来记录发生过的交易的历史信息。该设计可以避免重放攻击，即某个合法交易被多次重新发送造成攻击。 其交易是通过销毁“未被花费的交易输出”(即UTXO)， 和创建新的UTXO来实现资金的流转； 比特币可以理解为: BTC = UTXO + 共识机制 + 区块链账本 比特币交易比特币交易系统—状态转移系统比特币中没有账户的概念。因此，每次发生交易，用户需要将交易记录写到比特币网络账本中，等网络确认后即可认为交易完成。 除了挖矿获得奖励的 coinbase 交易只有输出，正常情况下每个交易需要包括若干输入和输出，未经使用（引用）的交易的输出（Unspent Transaction Outputs，UTXO）可以被新的交易引用作为其合法的输入。被使用过的交易的输出（Spent Transaction Outputs，STXO），则无法被引用作为合法输入。 因此，比特币网络中一笔合法的交易，必须是引用某些已存在交易的 UTXO（必须是属于付款方才能合法引用）作为新交易的输入，并生成新的 UTXO（将属于收款方）。 那么，在交易过程中，付款方如何证明自己所引用的 UTXO 合法？比特币中通过“签名脚本”来实现，并且指定“输出脚本”来限制将来能使用新 UTXO 者只能为指定收款方。对每笔交易，付款方需要进行签名确认。并且，对每一笔交易来说，总输入不能小于总输出。总输入相比总输出多余的部分称为交易费用（Transaction Fee），为生成包含该交易区块的矿工所获得。目前规定每笔交易的交易费用不能小于 0.0001 BTC，交易费用越高，越多矿工愿意包含该交易，也就越早被放到网络中。交易费用在奖励矿工的同时，也避免了网络受到大量攻击。 交易中金额的最小单位是“聪”，即一亿分之一（10^-8）比特币。 下图展示了一些简单的示例交易。更一般情况下，交易的输入、输出可以为多方。 交易 目的 输入 输出 签名 差额 T0 A 转给 B 他人向 A 交易的输出 B 账户可以使用该交易 A 签名确认 输入减输出，为交易服务费 T1 B 转给 C T0 的输出 C 账户可以使用该交易 B 签名确认 输入减输出，为交易服务费 … X 转给 Y 他人向 X 交易的输出 Y 账户可以使用该交易 X 签名确认 输入减输出，为交易服务费 需要注意，刚放进网络中的交易（深度为 0）并非是实时得到确认的。进入网络中的交易存在被推翻的可能性，一般要再生成几个新的区块后（深度大于 0）才认为该交易被确认。 下面分别介绍比特币网络中的重要概念和主要设计思路。 账户(地址)比特币采用了非对称的加密算法，用户自己保留私钥，对自己发出的交易进行签名确认，并公开公钥。 比特币的账户地址其实就是用户公钥经过一系列 Hash（HASH160，或先进行 SHA256，然后进行 RIPEMD160）及编码运算后生成的 160 位（20 字节）的字符串。 一般地，也常常对账户地址串进行 Base58Check 编码，并添加前导字节（表明支持哪种脚本）和 4 字节校验字节，以提高可读性和准确性。 注：账户并非直接是公钥内容，而是 Hash 后的值，避免公钥过早公开后导致被破解出私钥。 UTXO基本概念在比特币中，一比交易’’在黑盒子里” 实际运作的方式是: 花费一种东西的集合，这种东西被称为“未被花费的交易输出”(即UTXO) ， 这些输出由一个或多个之前的交易所创造，并在其后制造出一比或多笔新的UTXO，可以在未来的交易中花费。每一笔UTXO他有面额、所有者。 而且，一笔交易若要有效，必须满足的两个规则是： 1）该交易必须包含一个有效的签名，来自它所花费的 UTXO 的拥有者； 2）被花费的 UTXO 的总面额必须等于或者大于该交易产生的 UTXO 的总面额。一个用户的余额因此并不是作为一个数字储存起来的；而是用他占有的 UTXO 的总和计算出来的。 如果一个用户想要发送一笔交易，发送 X 个币到一个特定的地址，有时候，他们拥有的 UTXO 的一些子集组合起来面值恰好是 X，在这种情况下，他们可以创造一个交易：花费他们的 UTXO 并创造出一笔新的、价值 X 的 UTXO ，由目标地址占有。当这种完美的配对不可能的时候，用户就必须打包其和值 大于 X 的 UTXO 输入集合，并添加一笔拥有第二个目标地址的 UTXO ，称为“零钱输出”，分配剩下的币到一个由他们自己控制的地址。 UTXO具有如下性质 UTXO， 用比特币拥有者的公钥(锁定)(加密)的一个数字 UTXO == 比特币 比特币系统里没有比特币， 只有UTXO 比特币系统中没有账户，只有UTXO(公钥锁定) 比特币系统里没有账户余额， 只有UTXO( 账户余额只是比特币钱包的概念 ) UTXO存在前节点的数据库里 转账将消耗掉属于你自己的UTXO， 同时生成新的UTXO， 并用接受者的公钥锁定。 交易的原理一笔交易包含的信息 交易的输入的交易 ID（UTXO） 交易的金额: 多少钱，和输入的差额为交易的服务费 时间戳: 交易合适能生效 锁定脚本（用接受者的公钥哈希）- 将比特币地址锁定到接收者 解锁脚本（发送者用私钥对交易的数字签名, 发送者的公钥）- 用来证明比特币确实属于发送者，并保证内容不被篡改 交易的生成和验证过程 1）钱包软件生成交易，并想临近节点传播 2）节点对收到的交易进行验证，并丢弃不合法的交易 交易是否已经处理过 交易的size要小于区块size的上限 （比如比特币之前的大小限制是1M ） 交易的输入UTXO是存在的 交易输入UTXO没有被其他交易引用-防止双花（Double Spending） 输入总金额 &gt; 输出的总金额 解锁脚本的验证 将合格的交易加入到本地的Transaction数据库中，并将合法的交易转给临近节点 3）旷工将合格的交易打包进区块 交易脚本脚本系统 即便没有任何扩展，比特币的协议实际上却是促进了一个弱化版的”智能合约”的概念。。UTXO 在比特币中不是只被一个公钥持有, 而是还被在一个基于堆栈的程序语言组成的复杂的脚本所持有着。 在这个范例中，一个交易消耗的 UTXO 必须提供满足脚本的数据。实际上，这最基本的公钥所有权机制也是通过一个脚本来实现的：这个脚本使用一个椭圆曲线签名作为一个输入，验证拥有这个 UTXO 的交易和地址，如果验证成功，则返回 1，不然则返回 0。其他，更加复杂的脚本存在于各种复杂的用例中。 例如，你可以构造一个脚本，它要求从给定的三个私钥中，至少要选其中的 2 个来做签名验证（“多重签名”），这个对公司账本，储蓄账户等来说非常有用。脚本也能用来对解决计算问题的用户支付报酬。人们甚至可以创建这样的脚本“如果你能够提供你已经发送一定数额的的狗币给我的简化确认支付证明，这一比特币就是你的了”，本质上，比特币系统允许不同的密码学货币进行去中心化的兑换。然而，在比特币中的脚本语言有几个重要的限制： 缺乏图灵-完备 那就是说，虽然比特币脚本语言支持的计算方式很多，但是它不是所有的都支持。在主要类别中缺失循环。 它这样做的目的是为了防止对交易的验证出现死循环；理论上，它的脚本是可以克服这个障碍的，因为任何的循环都可以通过 if 语句重复多次底层代码来模拟，但是这样的脚本运行效率非常低下。 值的盲区 一个 UTXO 脚本没有办法提供资金的颗粒度可控的出金操作。比如, 一个预言合约（oracle contract ）的其中一个强大的用例就是一个套期保值的合约，A 和 B 都把价值1000$的 BTC 放到合约中，30 天后，这个合约把价值 1000$的 BTC 发给了 A，剩下的发给了B。 这就需要合约要确定 1BTC 以美元计值多少钱。然而，因为 UTXO 是不可分割的，为实现此合约，唯一的方法是非常低效地采用许多有不同面值的 UTXO（例如有 2^k 的 UTXO，其中 K可以最大到 30)并使预言合约挑出正确的 UTXO 发送给 A 和 B。 状态缺失 UTXO 要么被使用了，要么没有被使用；这会使得多阶段的合约和脚本没有机会保持任何其他的内部状态。这使得制作多阶段的期权合约、去中心化的交换协议或两阶段加密承诺协议变得困难(对于安全计算奖金来说是必要的)。这也意味着，UTXO 只能用于构建简单的、一次性的合约，而不是更复杂的“有状态”的合约，比如去中心化的组织，并且使得元协议难以实现。 区块链盲区 UTXO 对某些区块链数据视而不见，比如随机数和之前的区块的哈希。这严重限制了博彩和其他一些类别的应用，因为它剥夺了一种潜在的有价值的脚本语言：随机数！也就是在比特币的脚本中是没有随机数的。 因此，我们看到了在加密货币之上构建高级应用程序的三种方法：一，构建一个新的区块链，二，在比特币之上使用脚本，三，在比特币之上构建一个元协议。 构建一个新的区块链可以无限制扩展功能集，但是这样做非常消耗时间。使用脚本很容易实现和标准化，但在其功能上非常有限，而元协议虽然容易，但在可伸缩性方面却存在缺陷。在以太坊中，我们打算建立一个替代性的框架，它提供了更大的开发和更强大的轻客户属性，同时允许应用程序共享一个经济环境和区块链安全。 比特币的锁定脚本和解锁脚本脚本（Script）是保障交易完成（主要用于检验交易是否合法）的核心机制，当所依附的交易发生时被触发。通过脚本机制而非写死交易过程，比特币网络实现了一定的可扩展性。比特币脚本语言是一种非图灵完备的语言。 一般每个交易都会包括两个脚本：负责输入的解锁脚本（scriptSig）和负责输出的锁定脚本（scriptPubKey）。 输出脚本一般由付款方对交易设置锁定，用来对能动用这笔交易的输出（例如，要花费该交易的输出）的对象（收款方）进行权限控制，例如限制必须是某个公钥的拥有者才能花费这笔交易。 认领脚本则用来证明自己可以满足交易输出脚本的锁定条件，即对某个交易的输出（比特币）的拥有权。 输出脚本目前支持两种类型： P2PKH：Pay-To-Public-Key-Hash，允许用户将比特币发送到一个或多个典型的比特币地址上（证明拥有该公钥），前导字节一般为 0x00； P2SH：Pay-To-Script-Hash，支付者创建一个输出脚本，里边包含另一个脚本（认领脚本）的哈希，一般用于需要多人签名的场景，前导字节一般为 0x05； 以 P2PKH 为例，输出脚本的格式为 1scriptPubKey: OP_DUP OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG 其中，OP_DUP 是复制栈顶元素；OP_HASH160 是计算 hash 值；OP_EQUALVERIFY 判断栈顶两元素是否相等；OP_CHECKSIG 判断签名是否合法。这条指令实际上保证了只有 pubKey 的拥有者才能合法引用这个输出。 另外一个交易如果要花费这个输出，在引用这个输出的时候，需要提供认领(输入)脚本格式为 1scriptSig: &lt;sig&gt; &lt;pubKey&gt; 其中， 是拿 pubKey 对应的私钥对交易（全部交易的输出、输入和脚本）Hash 值进行签名，pubKey 的 Hash 值需要等于 pubKeyHash。 进行交易验证时，会按照先 scriptSig 后 scriptPubKey 的顺序进行依次入栈处理，即完整指令为： 1&lt;sig&gt; &lt;pubKey&gt; OP_DUP OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG 读者可以按照栈的过程来进行推算，理解整个脚本的验证过程。 引入脚本机制带来了灵活性，但也引入了更多的安全风险。比特币脚本支持的指令集十分简单，基于栈的处理方式，并且非图灵完备，此外还添加了额外的一些限制（大小限制等）。 比特币脚本语言-基于栈的脚本语言 栈（stack）- 操作数据的一种结构 只能从一端操作数据，后进先出LIFO 如同子弹匣， 先压如的子弹最后打出 压栈（PUSH）， 出栈（POP） 基于栈的脚本语言 对栈的操作： OP_DUP 逻辑运算符： OP_EQUALIVERIFY 加密运算符： OP_HASH160, OP_CHECKSIG 算数运算符: OP_ADD, OP_SUB, OP_MUL, OP_DIV 交易验证-锁定脚本和解锁脚本 锁定脚本 OP_DUP OP_HASH160 &lt;发送者的公钥哈希&gt; OP_EQUALVERIFY OP_CHECKSIG 解锁脚本 &lt;发送者的签名&gt; &lt;发送者的公钥&gt; 交易验证 运行解锁脚本 + 锁定脚本 =&gt; True 比特币区块区块的基本信息比特币一个区块大小不能超过1MB， 主要包括以下内容: 区块大小：4 字节； 区块头：80 字节： 交易个数计数器：1~9 字节； 所有交易的具体内容，可变长，匹配 Merkle 树叶子节点顺序。 区块头包含信息 版本号：4 字节； 上一个区块头的 Hash 值：链接到上一个合法的块上，对其区块头进行两次 SHA256 操作，32 字节； 本区块所包含的所有交易的 Merkle 树根的哈希值：两次 SHA256 操作，32 字节； 时间戳：4 字节； 难度指标：4 字节； Nonce：4 字节，PoW 问题的答案。 可见，要对区块链的完整性进行检查，只需要检验各个区块头部信息即可，无需获取到具体的交易内容，这也是简单交易验证（Simple Payment Verification，SPV）的基本原理。另外，通过头部的链接，提供时序关系的同时加大了对区块中数据进行篡改的难度。 区块的生成 旷工在挖矿前要组件区块 将coinbase交易打包进区块 将交易池中高优先级的交易打包进去快 优先级 = 交易的额度 * UTXO的深度 / 交易的size 创建去开的头部 挖矿成功后，将计算出来的随机数nonce填入区块头部， 并向临近节点传播 区块的验证p2p节点接受到新区块后，立即做以下的检查 验证POW的nonce值是否符合难度值 检查时间戳是否小于当前时间2小时 检查Merkle树根是否正确 检查区块size要小于区块的size上限 第一笔交易必须是coinbase交易 验证每个交易 默克尔树 在默克尔树中只要提供少数的介个节点就可以给出 一个分支的有效性证明，比如验证12c5是否有效，只需要提供标记为蓝色的几点就可以证明12c5这个交易是否有效 试图改变默克尔树的任意一部分都会导致链条上在某处放生不一致的情况 比特币的一个重要特性，即区块是存在一个多级数据结构中的 。一个区块的“哈希值”实际上只是这个区块的头信息的哈希值，一个大约 200 个字节的数据，其中包含了时间戳，随机数，上一个区块的哈希和一个存储了这个区块中所有交易的称之为默克尔树的数据结构的根哈希。 默克尔树是一种二叉树，包含了一组节点，它们的含有基础信息的树根有大量的叶子节点，一组中间节点，每一个节点都是它的 2 个子节点的哈希，然后，最终的一个根节点，也是由它的 2 个子节点的哈希形成，代表着这树的“顶端”。 这个默克尔树的目的是允许在一个区块中的数据能够被零散的传递: 一个节点只能从一个源来下载一个区块的头信息，树的一小部分关联着另一个源 ，并且任然可以保证所有的数据都是正确的。之所以这样做行得通，是因为哈希值都是向上传导的: 如果一个恶意的用户试图在默克尔树的底部替换一个假的交易, 这个更改将导致上面的节点发生变化，然后上面的节点的变化又会导致上上面的节点发生变化，最终改变这个数根节点，因此也改变了这区块的哈希，导致这个协议把它注册成一个完全不同的区块 (几乎可以肯定是一个无效的工作证明).这默克尔树协议对比特币的长期可持续发展是必不可少的。比特币网络中的一个“完整节点” , 截止到 2014 年，占用了大约 15G 的磁盘空间，并且每月正在以 10 亿字节的速度递增。目前，这对于电脑来说是没有问题的，但是在手机上却是不现实的。在以后的将来，只有商业的和业余爱好者才能参与玩比特币。一个称之为 “简化支付验证（simplified payment verification）” (SPV)的协议 允许另一种类型的节点存在，这种节点称之为 “轻节点（light nodes）”, 其下载区块的头信息,在这区块头信息上验证工作证明，然后只下载与之交易相关的“分支” 。 这使得轻节点只要下载整个区块链的一小部分，就可以安全地确定任何一笔比特币交易的状态和账户的当前余额。 挖矿挖矿的基本原理了解比特币，最应该知道的一个概念就是“挖矿”。挖矿是参与维护比特币网络的节点，通过协助生成新区块来获取一定量新增的比特币的过程。 当用户向比特币网络中发布交易后，需要有人将交易进行确认，形成新的区块，串联到区块链中。在一个互相不信任的分布式系统中，该由谁来完成这件事情呢？比特币网络采用了“挖矿”的方式来解决这个问题。 目前，每 10 分钟左右生成一个不超过 1 MB 大小的区块（记录了这 10 分钟内发生的验证过的交易内容），串联到最长的链尾部，每个区块的成功提交者可以得到系统 12.5 个比特币的奖励（该奖励作为区块内的第一个交易，一定区块数后才能使用），以及用户附加到交易上的支付服务费用。即便没有任何用户交易，矿工也可以自行产生合法的区块并获得奖励。 每个区块的奖励最初是 50 个比特币，每隔 21 万个区块自动减半，即 4 年时间，最终比特币总量稳定在 2100 万个。因此，比特币是一种通缩的货币。 挖矿的过程 如果我们有一个可信任的中央服务器, 那么实现这个系统是一件很简单的事情; 就按照需求所描述的去编写代码即可，把状态记录在中央服务器的硬盘上。 然而，与比特币一样，我们试图去建立一个去中心化的货币系统, 所以，我们需要把状态转移系统和一致性系统结合起来，以确保每个人都同意这交易的顺序。 比特币的去中心化的一致性处理进程要求网络中的节点连续不断的去尝试对交易进行打包，这些被打成的包就称为“区块”。 这个网络会故意的每隔 10 分钟左右就创建一个区块, 每一个区块里都包含一个时间戳，一个随机数，一个对上一个区块的引用 ，和从上一个区块开始的所有交易的列表。随着时间的推移，这会创建一个持久的，不断增长的区块链，这个区块链不断的被更新，使其始终代表着最新的比特币总账的状态。 验证一个区块是否有效的算法如下: 检查其引用的上一个区块是否存在并且有效. 检查这个区块的时间戳是否大于上一个区块的时间戳 并且小于 2 小时之内 检查这区块上的工作证明是否有效. 让 S[0] 成为上一个区块的最末端的状态. 假设 TX 是这个区块的交易列表，且有 n 个交易。 做 for 循环，把 i 从 0 加到到 n-1， 设置 S[i+1] = APPLY(S[i],TX[i]) 如果任何一个应用(APPLY)返回错误，则退出并且返回。 返回 true,并且把 S[n] 设置成这个区块最末端的状态。 从本质上说，区块中的每一个交易都必须提供一个有效的状态，从交易执行前的标准状态到执行后的一个新的状态。 注意，状态并没有以任何方式编码进区块中;它纯粹是一个被验证节点所记住的抽象，并且它只能用来被从创世区块起的每一个区块进行安全的计算，然后按照顺序的应用在每一个区块中的每一次交易中。此外，请注意矿工把交易打包进区块的顺序是很重要的; 如果一个区块中有 2 个交易 A 和 B,B 花了一个由 A 创建的 UTXO, 那么如果 A 比 B 更早的进入区块，那么这个区块将是有效的，不然就是无效的。 在上述列出的验证条件中，“工作证明” 这一明确的条件就是每一个区块的 2 次 SHA256 哈希值, 它作为一个 256 位的数字,必须小于一个动态调整的目标值, 截止到本文写作的时间，该动态调整的值的大小大约是 2 的 187 次方。这样做的目的是为了让创建区块的算法变难, 从而，阻止幽灵攻击者从对它们有利的角度出来，来对区块链进行整个的改造。因为 SHA256 被设计成一个完全不可预测的伪随机函数, 这创建一个有效区块的唯一的方法只有是不断的尝试和出错， 不断对随机数进行递增，然后查看新的哈希值是否匹配。 按照当前的目标值 2 的 187 次方，这个网络在找到一个有效的区块前，必须进行 2 的 69 次方次的尝试; 一般来说,每隔 2016 个区块，这个目标值就会被网络调整一次 ，因此网络中平均每隔 10 分钟就会有一些节点产生出一个新的区块。为了补偿这些矿工的计算工作, 每一个区块的矿工有权要求包含一笔发给他们自己的 12.5BTC（不知道从哪来的）的交易。另外,如果任何交易，它的总的输入的面值比总的输出要高，这一差额会作为“交易费用”转给矿工。顺便提一下，对矿工的奖励是比特币发行的唯一途径，创世状态中并没有比特币。 负反馈调解比特币网络在设计上，很好的体现了负反馈的控制论基本原理。 比特币网络中矿工越多，系统就越稳定，比特币价值就越高，但挖到矿的概率会降低。 反之，网络中矿工减少，会让系统更容易导致被攻击，比特币价值越低，但挖到矿的概率会提高。 因此，比特币的价格理论上应该稳定在一个合适的值（网络稳定性也会稳定在相应的值），这个价格乘以挖到矿的概率，恰好达到矿工的收益预期。 从长远角度看，硬件成本是下降的，但每个区块的比特币奖励每隔 4 年减半，最终将在 2140 年达到 2100 万枚，之后将完全依靠交易的服务费来鼓励矿工对网络的维护。 注：比特币最小单位是“聪”，即 10^(-8) 比特币，总“聪”数为 2.1E15。对于 64 位处理器来说，高精度浮点计数的限制导致单个数值不能超过 2^53 约等于 9E15。 共识机制共识机制介绍比特币网络是完全公开的，任何人都可以匿名接入，因此共识协议的稳定性和防攻击性十分关键。 比特币区块链采用了 Proof of Work（PoW）的机制来实现共识，该机制最早于 1998 年在 B-money 设计中提出。 目前，Proof of X 系列中比较出名的一致性协议包括 PoW、PoS 和 DPoS 等，都是通过经济惩罚来限制恶意参与。 工作量证明工作量证明，通过计算来猜测一个数值（nonce），使得拼凑上交易数据后内容的 Hash 值满足规定的上限（来源于 hashcash）。由于 Hash 难题在目前计算模型下需要大量的计算，这就保证在一段时间内，系统中只能出现少数合法提案。反过来，能够提出合法提案，也证明提案者确实已经付出了一定的工作量。 同时，这些少量的合法提案会在网络中进行广播，收到的用户进行验证后，会基于用户认为的最长链基础上继续难题的计算。因此，系统中可能出现链的分叉（Fork），但最终会有一条链成为最长的链。 Hash 问题具有不可逆的特点，因此，目前除了暴力计算外，还没有有效的算法进行解决。反之，如果获得符合要求的 nonce，则说明在概率上是付出了对应的算力。谁的算力多，谁最先解决问题的概率就越大。当掌握超过全网一半算力时，从概率上就能控制网络中链的走向。这也是所谓 51% 攻击的由来。 参与 PoW 计算比赛的人，将付出不小的经济成本（硬件、电力、维护等）。当没有最终成为首个算出合法 nonce 值的“幸运儿”时，这些成本都将被沉没掉。这也保障了，如果有人尝试恶意破坏，需要付出大量的经济成本。也有设计试图将后算出结果者的算力按照一定比例折合进下一轮比赛考虑。 权益证明权益证明（Proof of Stake，PoS），最早在 2013 年被提出，类似现实生活中的股东机制，拥有股份越多的人越容易获取记账权（同时越倾向于维护网络的正常工作）。 典型的过程是通过保证金（代币、资产、名声等具备价值属性的物品即可）来对赌一个合法的块成为新的区块，收益为抵押资本的利息和交易服务费。提供证明的保证金（例如通过转账货币记录）越多，则获得记账权的概率就越大。合法记账者可以获得收益。 PoS 试图解决在 PoW 中大量资源被浪费的缺点，受到了广泛关注。恶意参与者将存在保证金被罚没的风险，即损失经济利益。 一般的，对于 PoS 来说，需要掌握超过全网 1/3 的资源，才有可能左右最终的结果。这个也很容易理解，三个人投票，前两人分别支持一方，这时候，第三方的投票将决定最终结果。 PoS 也有一些改进的算法，包括授权股权证明机制（DPoS），即股东们投票选出一个董事会，董事会中成员才有权进行代理记账。这些算法在实践中得到了不错的验证，但是并没有理论上的证明。 2017 年 8 月，来自爱丁堡大学和康涅狄格大学的 Aggelos Kiayias 等学者在论文《Ouroboros: A Provably Secure Proof-of-Stake Blockchain Protocol》中提出了 Ouroboros 区块链共识协议，该协议可以达到诚实行为的近似纳什均衡，认为是首个可证实安全的 PoS 协议。 隔离见证什么是隔离见证隔离见证，即 Segregated Witness（简称SegWit），由Pieter Wuille（比特币核心开发人员、Blockstream联合创始人）在2015年12月首次提出。 见证（Witness） 见证，在比特币里指的是对交易合法性的验证。举个例子，Alice发起一笔交易，给Bob支付1个BTC，该笔交易信息由三部分组成： a.元数据：交易信息格式的版本号；交易锁定时间等 b.付款人：Alice用于付款的BTC来源，一般来源于某历史区块上某笔交易的输出（详 见UTXO）；证明Alice拥有该笔交易的输出，即见证（Witness）数据 c.收款人：Bob的收款地址和金额 可见，见证数据包含在交易信息里头。 隔离（Segregated） 指的就是把见证数据从交易信息里抽离出来，单独存放。 隔离见证的来源为什么要把见证数据隔离出来呢，或者说这样做有什么好处呢？这就涉及到比特币里的另一个概念–扩容。 扩容，指的是增加比特币每秒的交易量。比特币每10分钟左右挖出一个大小小于1MB的区块，每笔交易平均250字节，即每个区块最多放进4000笔交易，这样算下来，比特币每秒处理的交易数不超过7个。对比其它交易平台，PayPal每秒数百笔、Visa每秒数千笔、支付宝能达到每秒数万笔，可见比特币是一个非常低效的交易系统。如果使用人数增多，则会造成比特币的拥堵。 如何解决拥堵呢？ 有两种方式，一是简单的增加每个区块的大小，比如将区块大小增加到8M；另一种就是隔离见证+闪电网络啦。 扩容方案一: 增加区块大小如果将区块大小增至8M，简单思考一下，比特币每秒处理的交易数似乎也增加到原来的8倍，即56笔每秒。如果每个区块1个GB，比特币每秒将处理7000笔交易，拥堵问题不就解决了吗？ 中本聪可没那么傻，之所以将区块大小设定为1M，是有重要原因的。比特币白皮书的标题为：一种点对点的电子现金系统，相比于传统货币系统，比特币的核心价值在于实现了一种去中心而且安全的货币。如果区块的大小过大，则会危害到比特币的安全模型，作为一种货币应用，这显然是不能令人接受的。 为什么这么说呢？ POW机制的安全基础，是假设一个人的算力无法超过全网算力的50%。如果增大区块，可能一个人的算力超过全网的1/3，就危害到了比特币的安全。举个例子，为了达到每秒7000笔的交易速度，我们把区块的大小增加到1GB： a.假设1GB的区块从产生到广播到全网节点需要10分钟； b.有一个叫Byzantium的节点，拥有的算力超过全网1/3； c.当Byzantium节点挖出一个新区块时，假设该时间点为0秒，那么Byzantium节点 获取新区块的时间点为0秒；根据假设a，全网最后一个获取新区块的节点的获取时间 为600秒，如果获取速度是线性的，全网其它节点获取新区块的平均时间是300秒。 d.因为在新区块上挖坑的算力才是有效算力；根据c，全网其它节点的有效算力只剩下 一半，也就是说，全网其它节点的有效算力小于1/3 e.根据b和d，这种情况下，Byzantium节点算力超过全网其它节点算力，如果Byzantium 节点在自己挖出的区块上继续挖矿且不公布广播，则Byzantium节点上没公布的区块 长度，会大于全网区块长度；一旦Byzantium节点公布这些区块，则全网其它节点挖 出的区块全部作废。 可见，区块设计过大，会威胁到比特币的安全。换句话说，比特币区块的大小是有上限的，《On Scaling Decentralized Blockchain》这篇论文指出，在目前的互联网环境下，如果十分钟产生一个区块，区块的大小最好不能超过4MB。这样看来，增加区块大小这种扩容方案，效果就十分有限了。 扩容方案二: 隔离见证 + 闪电网络隔离见证为什么能扩容呢？先来看看比特币区块的数据结构： 每笔交易平均250字节，见证部分的数据约为150字节，其余部分100字节。如果将见证数据隔离出来，原来1MB空间的区块可以放下10000笔交易（原来为4000笔），交易速度约提升2.5倍。隔离出来的见证数据放到了区块末尾，大小为1.5到2MB，所以隔离见证的整个区块大小为2.5到3MB左右。 隔离见证的意义: 解决了交易延展性问题； 为闪电网络铺路 其他优化 交易延展性 中本聪在设计比特币的时候直接把这两个信息直接放在了区块内，所以一个区块就承载不了更多的交易信息，如果隔离了“见证信息”，那么区块链只记录交易信息，那么一个区块可承载的交易更多交易。中本聪设计比特币时，并没有把两部分资料分开处理，因此导致交易ID的计算混合了交易和见证。因为见证本身包括签名，而签名不可能对其自身进行签名，因此见证可以由任何人在未得到交易双方同意的情况下进行改变，造成所谓的交易可塑性（malleability）。在交易发出后、确认前，交易ID可以被任意更改，因此基于未确认交易的交易是绝对不安全的。在2014年就曾有人利用这个漏洞大规模攻击比特币网络。 指的是一笔交易发起后，交易数据中的见证部分可以被篡改，而且篡改后的交易仍然有效。具体的说，见证的实现依靠一种签名算法，比如椭圆曲线数字签名算法（ECDSA），这种算法下签名（r，s）和签名（r，-s（mod n））都是有效的，所以可以把一种有效见证数据篡改成另一种有效见证数据，该笔交易仍然是有效的。每笔交易有个交易ID，交易ID是对整个交易数据的Hash值，为该笔交易的唯一标识。通过对见证数据的篡改，可以改变Hash值，从而改变该笔交易的唯一标识。隔离见证通过把见证数据隔离移出，生成交易ID时Hash的数据不包括见证数据，因此也就无法改变交易ID值。 从此以后，只有发出交易的人才可以改变交易ID，没有任何第三方可以做到。如果是多重签名交易，就只有多名签署人同意才能改变交易ID。这可以保证一连串的未确认交易的有效性，是双向支付通道或闪电网络所必须的功能。有了双向支付通道或闪电网络，二人或多人之间就可以实际上进行无限次交易，而无需把大量零碎交易放在区块链，大为减低区块空间压力。 闪电网络 通过增加区块大小无法从根本上解决比特币的扩容问题。闪电网络通过在比特币基础上，构建第二层网络，将交易转移到链下的方式，来减轻公链负担，以实现扩容的效果。目前看来，在公链基础上构建协议层网络，是解决公链拥堵问题最合适也是最有前景的方案 隔离见证所带来的改变，为闪电网络的实现提供了一些便利，主要有3点： a.交易延展性的解决，让交易无法被干扰，闪电网络白皮书中提到的“人质状态” （hostage situation），得以避免； b.在通道的生命周期上，隔离见证让闪电网络的通道永久开启更方便实现； c.虽然从理论上系统是安全的，但用户还是要查看区块链中的交易是否广播撤回，防止交易方的欺诈行为，隔离见证使得这项活动可以外包出去，只要给服务器传送少量信息， 就能代替你完成这一过程。 d.此外，隔离见证给比特币带来了一些细节优化，比如增加了脚本版本（Script Version），使得脚本语言可以以一种向后兼容的方式来发展；签名算法复杂度有了较大优化等等。 比特币的缺陷比特币的缺陷 交易确认时间长，吞吐量低 POW挖矿浪费计算资源 ASIC矿机出现是全民参与性降低，算力集中 不完全匿名 无法存储太多的数字资产 不支持复杂的脚本语言 缩短交易时间的方法 缩短平均产生区块的时间 中心化服务 信任地址多重签名 开放交易和联合服务器 POS和DPOS Segwit与闪电网络]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[供应链金融的痛点]]></title>
    <url>%2F2019%2F01%2F19%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%E7%9A%84%E7%97%9B%E7%82%B9%2F</url>
    <content type="text"><![CDATA[供应链的介绍什么是供应链金融？ 供应链金融是指将供应链上的核心企业以及与其相关的上下游企业看做一个整体，以核心企业为依托，以真实的贸易为前提，运用自偿性贸易融资的方式，对供应链上下游企业提供的综合性金融产品服务。 根据融资担保品的不同，金融机构将供应链金融分为应收账款类、预付类和存货类融资，其中应收账款类的规模尤为巨大。 供应链金融痛点1. 供应链上的中小企业融资难供应链上的中小企业融资难，成本高。由于银行依赖的是核心企业的控货能力和调节销售能力，出于风控的考虑，银行仅愿意对核心企业有 直接，应付账款义务的上游供应商(仅限于一级供应商)提供保理服务，或者对其下游经销商（一级经销商），提供预付款或者存货融资。 这就导致有巨大融资需求的二级、三级等供应商/经销商需求得不到满足，供应链金融的业务量受到限制，而中小企业得不到及时的融资易导致产品质量问题，会伤害到整个供应链体系。 区块链解决方法 我们在区块链上发行，运行一种数字票据，可以在公开透明、多方见证的情况下进行随意的拆分和转移。 这种模式相当于把整个商业体系中的信用将变得可传导、可追溯，为大量原本无法融资的中小企业提供了融资机会，极大地提高票据的流转效率和灵活性，降低中小企业的资金成本。 据统计，过去传统的供应链金融公司大约仅能为15%的供应链上的供应商们（中小企业）提供融资服务，而采用区块链技术以后，85%的供应商们都能享受到融资便利。 2. 转让难度较大作为供应链金融的主要融资工具，现阶段的商业汇票、银行汇票使用场景受限，转让难度较大。商业汇票的使用受限制与企业的信誉，银行汇票贴现的到账时间难以把控。同时，如果要把这些汇票进行转让，难度也不小。 因为在实际金融操作中，银行非常关注应收账款债券”转让通知”的法律效应，如果核心企业无法签回，银行不会愿意授信。据了解，银行对于签署这个债券“转让通知”的法律效应很谨慎，甚至要求核心企业的法人代表去银行当面签署，显然这种方式操作难度是极大的。 区块链解决方案: 银行与核心企业之间可以打造一个联盟链，提供给供应链上的所有成员企业使用，利用区块链多方签名、不可篡改的特点，使得债权转让得到多方共识，降低操作难度。 当然，系统设计要能达到债券转让的法律通知效果。同时，银行还可以追溯每个节点的交易，勾画出可视性的交易流程图。 3. 系统难以自证清白供应链金融平台/核心企业系统难以自证清白，导致资金端风控成本居高不下。 目前的供应链金融业务中，银行或其他资金端除了担心企业的还款能力和还款意愿以外，也很担心交易信息本身的真实性，而交易信息是由核心企业的ERP系统所记录的。 虽然ERP篡改难度大，但是也并非绝对可信，银行依然担心核心企业和供应商/经销商勾结篡改信息，因而需要投入人力物力去验证交易的真伪，这就增加了额外的风控成本。 区块链解决方案: 区块链作为“信任的机器”，具有可溯源、共识和去中心化的特性，且区块链上的数据都带有时间戳，即使某个节点的数据被修改，也无法只手遮天，因而区块链能够提供绝对可信的环境，减少资金端的风控成本，解决银行对于被信息篡改的疑虑。 区块链 + 供应链金融 (应收账款)供应链金融需求需要供应链的行业在供应链金融方面，典型的客户不只是金融企业，比如说银行；也包括产业的龙头，比如物流行业的顺丰。核心企业只要有上下游，供应商和经销商都会有复杂交易的问题，这些公司其实是最需要区块链供应链金融的。 区块链适合的企业 需要多方参与的复杂交易，并且需要方便快速的交易 需要方便的进行账本的溯源 不适合区块链的企业以银行账户为例 本身不需要快速，是通过各种机构间的对账来确保账户的正确性； 也不需要很大的溯源，除非账户有错，通过复式记账法也可以溯源，只是没有那么方便而已。 区块链可以解决供应链金融的问题 票据的拆分转让 现阶段的融资工资普遍是商业贴现票据，和应收账款不同的是，应收账款是有核心企业来签发的，商业贴现票据是有银行来签发的，并且商业贴现票据是不能拆分的。应收账款的情况更适合1+N的供应链金融模式，就是一个核心企业带来N个小企业。 现在，通过区块链技术，记录拆分的动作，并通过签名技术，能够实现票据的拆分，并且可以让交易更加迅速。 应收账款的溯源 通过区块链账本的记账功能和不可篡改的特性，可以让应收账款可以实现快速的溯源功能。 通过区块链技术能够让供应链金融运行的更加快速票据可拆分和可溯源，贷款的需求就可以旺盛起来 通过区块链的不可篡改的特性，可以让供应链金融的票据实现安全快速的拆分转让，并且可溯源，可以解决部分供应链的小企业融资和转让难度大的问题，但是还是解决不了数据源头去伪的问题，即还是解决不了系统数据源头的清白，这部分痛点可以依靠物料网来解决。 如何应用区块链切入供应链金融在市场选择上，区块链初创公司应选择天花板足够高的细分领域，比如家电、汽车零售、服装、药品行业等。 切入区块链金融的两种模式1. 直接与核心企业/平台合作直接与核心企业/平台合作，为其提供区块链底层的解决方案，在积累足够多数据之后，通过搭建联盟链，对接资金方提供金融服务。(联盟链模式) 由于区块链本身不能解决风控问题，现阶段企业级的风控还是需要围绕着强势的核心企业，同时，获得核心企业的支持还可以有效解决获客问题，因为一家大型核心企业一般都会有上千家的各类供应商。 2. 第二种方式是从供应链管理服务入手比如溯源、追踪、可视化等，将信息流、物流和资金流整合到一起，在此基础之上从事金融服务。(私有链模式) 这种模式相当于同去看连搭建起了一个 应用场景。 打造供应链金融交易所步骤 前提， 我们需要先打造一个区块链+供应链金融的联盟，联盟的参与者包括供应链金融平台，核心企业、专业的金融中介机构、资金方、保理机构等。 每个参与者都需要承担相应的义务，比如平台负责提供供应链信息，客户信息这些类似水电的基础服务，而核心企业了解行业状况，对供应链上的企业具有掌控力，负责风险控制。 专业的金融中介机构可以对平台信息进行整合分析，提供定制化的供应链金融产品，比如个性化的区块链电子票据。资金方包括银行、互联网金融机构等负责对接相应风险偏好的客户。 1. 数据上链将供应链联盟里的数据放到链上，利用区块链的特性使其不可篡改，并提供数据的确权，溯源等服务。 2. 资产数字化将资产数字化，把仓单、合同、以及课代表融资需求的区块链票据都变为数字资产，且具有唯一、不可篡改、不可复制等特点。 3. 数字资产的交易数字资产的交易，供应链金融平台将转变成一个金融资产交易所，将非标的企业贷款需求转变为标准化的金融产品，进行代币化，对接投融资需求，进行价值交易。 最终，区块链技术将能有效地增强供应链金融资产的流动性，调动新型的融资工具和风控体系帮助覆盖中小企业融资的长尾市场，催生供应链金融服务。]]></content>
      <categories>
        <category>区块链</category>
        <category>供应链金融</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>供应链金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[供应链金融三种模式]]></title>
    <url>%2F2019%2F01%2F12%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[什么是供应链金融供应链金融其本质是对供应链结构特点、交易细节的把握，将核心企业和上下游企业联系在一起提供灵活运用的金融产品和服务的一种融资模式。也就是把资金作为供应链的一个溶剂，增加其流动性。 供应链金融促进供应链的发展体现在资金注入和信用注入两方面：一方面解决相对弱势的上、下游配套中小企业融资难和供应链地位失衡的问题；另一方面，将银行信用融入上、下游配套企业，实现其商业信用增级，促进配套企业与核心企业建立起长期战略协同关系，从而提升整个供应链的竞争能力。 传统融资模式中银行与供应链成员的关系银行围绕某一家核心企业，从原材料采购，到最终产品，最后由销售网络把产品送到消费者手中这一条供应链，将供应商、制造商、分销商、零售商直到最终用户连成一个整体，全方位的为链条上的N个企业提供融资服务，通过相关企业的职能分工合作，实现整个供应链的不断增值 供应链金融使银行从新的视角评估中小企业的信用风险，从专注于对中小企业本身信用风险的评估，转变为对整个供应链及其交易的评估，这样既真正评估了业务的真实风险，同时也使更多的中小企业能够进入银行的服务范围。一方面，将资金有效注入处于相对弱势的上下游配套中小企业，解决供应链失衡；另一方面，将银行信用融入上下游企业的购销行为，增强其商业信用，改善其谈判地位，使供应链成员更加平等的协商和逐步建立长期战略协同关系，提升供应链的竞争能力，促进了整个供应链的持续稳定发展； 供应链金融的三种模式三种分类介绍供应链金融的实质是帮助企业盘活流动资产，即应收、预付和存货。因此通常将产品分为三类：应收类、预付类和存货类。 应收类 应收类产品帮助上游企业将应收账款转换成现金或应付票据; 预付类 预付类产品则帮助下游企业扩大了单次采购额，提高了采购能力，将本应即期支出的现金资产转换为短期借款或应付票据; 存货类 现货质押更为直接，以企业的存货作为担保方式，换取流动性更强的现金资产。虽然现货融资通常没有核心企业参与，但因该业务涉及对货权的控制和物流监管企业的管理，从管理上与供应链金融流行的预付款渠道融资相近。 应收账款融资以未到期的应收账款向金融机构办理融资的行为，称为应收账款融资，这种模式使得企业可以及时获得机构的短期融资，不但有利于解决融资企业短期资金的需求，加快中小企业健康稳定的发展和成长，而且有利于整个供应链的持续高效运作。 应收类产品主要应用于核心企业的上游融资，如果销售已经完成，但尚未收妥货款，则适用产品为保理或应收账款质押融资;如融资目的是为了完成订单生产，则为订单融资，担保方式为未来应收账款质押，实质是信用融资。 应收账款质押融资是是指企业与银行等金融机构签订合同。以应收账款作为质押品。在合同规定的期限和信贷限额条件下，采取随用随支的方式，向银行等金融机构取得短期借款的融资方式。其中放款需要发货来实现物权转移，促使合同生效。同时也需要告知核心企业，得到企业的确权。 风险要点： 应在《应收账款质押登记公示系统》内登记，避免重复质押 供应商和加工企业之间应有长期稳定贸易关系 货物应有明确验收标准，交割标准，准确付款依据 供应商发货后，银行方应定期、不定期与生产企业进行对账 核心企业付款应转入银行方监管的专用账户 保理和应收账款质押区别 保理业务是以债权人转让其应收账款为前提，集应收账款催收、管理、坏账担保及融资于一体的综合性金融服务。与应收账款质押融资的差别是，保理是一种债权的转让行为，适用于《合同法》，而应收账款质押是一种物权转让行为，适用于《物权法》。银行拥有对货物物权的处置权。 票据池 票据池业务是银行一种常见的供应链金融服务。票据是供应链金融使用最多的支付工具，银行向客户提供的票据托管、委托收款、票据池授信等一揽子结算、融资服务。票据池授信是指客户将收到的所有或部分票据做成质押或转让背书后，纳入银行授信的资产支持池，银行以票据池余额为限向客户授信。用于票据流转量大、对财务成本控制严格的生产和流通型企业，同样适用于对财务费用、经营绩效评价敏感并追求报表优化的大型企业、国有企业和上市公司。 对客户而言. 票据池业务将票据保管和票据托收等工作全部外包给银行，减少了客户自己保管和到期托收票据的工作量。而且，票据池融资可以实现票据拆分、票据合并、短票变长票等效果，解决了客户票据收付过程中期限和金额的不匹配问题。对银行而言，通过票据的代保管服务，可以吸引票据到期后衍生的存款沉淀。 未来提货权融资未来货权融资（又称为保兑仓融资）是下游购货商向平台申请贷款，用于支付上游核心供应商在未来一段时期内交付货物的款项，同时供应商承诺对未被提取的货物进行回购，并将提货权交由金融机构控制的一种融资模式。 融资企业通过保兑仓业务获得的是分批支付货款并分批提取货物的权利，因而不必一次性支付全额货款，有效缓解了企业短期的资金压力，实现了融资企业的杠杆采购和供应商的批量销售。 预付类产品则主要用于核心企业的下游融资，即主要为核心企业的销售渠道融资，包含两种主要业务模式： 一是，银行给渠道商融资，预付采购款项给核心企业，核心企业发货给银行指定的仓储监管企业，然后仓储监管企业按照银行指令逐步放货给借款的渠道商，此即为所谓的未来货权融资或者先款后货融资; 二是，核心企业不再发货给银行指定的物流监管企业，而是本身承担了监管职能，按照银行指令逐步放货给借款的渠道商，此即所谓的保兑仓业务模式。 先货后款(或先票后货)是指买方从银行取得授信，在交纳一定比例保证金的前提下，向卖方支付全额货款;卖方按照购销合同以及合作协议书的约定发运货物，货物到达后设定抵质押，作为银行技信的担保。一些热销产品的库存往往较少，因此企业的资金需求集中在预付款领域。同时，该产品因为涉及到卖家及时发货、发货不足的退款、到货通知及在途风险控制等环节，因此客户对卖家的谈判地位也是操作该产品的条件之一。 保兑仓(又称为担保提货授信)是在客户交纳一定保证金的前提下，银行贷出金额货款供客户(买方)向核心企业(卖方)采购，卖方出具金额提单作为授信的抵质押物。随后，客户分次向银行提交提货保证金，银行再分次通知卖方向客户发货。卖方就发货不足部分的价值承担向银行的退款责任。该产品又被称为卖方担保买方信贷模式。 保兑仓是基于特殊贸易背景，如：1、客户为了取得大批量采购的析扣，采取一次性付款方式，而厂家因为排产问题无怯一次性发货;2、客户在淡季向上游打款，支持上游生产所需的流动资金，并锁定优惠的价格。然后在旺季分次提货用于销售;3、客户和上游都在异地，银行对在途物流和到货后的监控缺乏有效手段。保兑仓是一项可以让买方、核心企业和银行均收益的业务。 融通仓融资所谓融通仓即存货融资，是企业以存货作为质押向金融机构办理融资业务的行为。所以融通仓服务不仅可以为企业提供高水平的物流服务，又可以为中小型企业解决融资问题，解决企业运营中现金流的资金缺口，以提高供应链的整体绩效。 存货类融资主要分为现货融资和仓单融资两大类，现货质押又分为静态质押和动态质押，仓单融资里又包含普通仓单和标准仓单。 静态抵质押授信是指客户以自有或第三人合法拥有的动产为抵质押，银行委托第三方物流公司对客户提供的抵质押的商品实行监管，抵质押物不允许以货易货，客户必须打款赎货。此项业务适用于除了存货以外没有其他合适的抵质押物的客户，而且客户的购销模式为批量进货、分次销售。利用该产品，客户得以将原本积压在存货上的资金盘活，扩大经营规模。 动态抵质押授信是静态抵质押授信的延伸产品——它是指客户以自有或第三人合法拥有的动产为抵质押，银行对于客户抵质押的商品价值设定最低限额，允许在限额以上的商品出库，客户可以以货易货。该产品适用于库存隐定、货物品类较为一致、抵质押物的价值核定较为容易的客户。同时，对于一些客户的存货进出颇繁，难以采用静态抵质押授信的情况，也可运用这类产品。该产品多用于生产型客户。对于客户而言，由于可以以货易货，因此抵质押设定对于生产经营活动的影响相对较小。特别对于库存稳定的客户而言，在合理设定抵质押价值底线的前提下，授信期间内几乎无须启动追加保证金赎货的梳程，因此对盘活存货的作用非常明显。 仓单质押是以仓单为标的物而成立的一种质权。仓单质押作为一种新型的服务项目，为仓储企业拓展服务项目。仓单是保管人收到仓储物后给存货人开付的提取仓储物的凭证。仓单除作为已收取仓储物的凭证和提取仓储物的凭证外，还可以通过背书，转让仓单项下货物的所有权，或者用于出质。存货人在仓单上背书并经保管人签字或者盖章，转让仓单始生效力。存货人以仓单出质应当与质权人签订质押合同，在仓单上背书并经保管人签字或者盖章，将仓单交付质权人后，质押权始生效力。 标准仓单质押授信是指客户以自有或第三人合法拥有的标准仓单为质押的授信业务。标准仓单是指符合交易所统一要求的、由指定交割仓库在完成入库商品验收、确认合格后签发给货主用干提取商品的、并经交易所注册生效的标准化提货凭证。该产品适用于通过期货交易市场进行采购或销售的客户，以及通过期货交易市场套期保值、规避经营风险的客户。对于客户而言，相比动产抵质押，标准仓单质押手续简便、成本较低。对银行而言，成本和风险都较低。此外，由于标准仓单的流动性很强，也利于银行在客户违约情况下对货押物的处置。 普通仓单质押授信是指客户提供由仓库或其他第三方物流公司提供的非期货交割用仓单作为质押物，并对仓单作出质背书，银行提供融资的一种银行产品。在涉及货押的融资模式里，目前最大的问题是监管企业的职责边界、风险认定和收益权衡问题。监管企业承担的责任法律界定模糊，当监管企业获取的收益较低时，承担过大的风险与其并不匹配。]]></content>
      <categories>
        <category>区块链</category>
        <category>供应链金融</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>供应链金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链简介]]></title>
    <url>%2F2019%2F01%2F08%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F1.%E5%8C%BA%E5%9D%97%E9%93%BE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[区块链介绍区块链的历史上世纪 80 年代和 90 年代的匿名电子现金协议主要使用了一种被称为“乔姆盲签（Chaumian Blinding）”的加密技术，这种技术为这些新货币提供了很高的隐私保护，但是由于他们的基础协议在很大程序上需要依赖于一个中央中介，因此未能获得支持。1998 年，戴伟（Wei Dai）的 b-money 首次引入了通过解决计算难题和去中心化的共识来创造货币的思想，但是该建议并未给出实现去中心化的共识的具体方法。2005 年，芬尼（Hal Finney）引入了“可重复的工作证明”（reusable proofs of work）概念，它同时使用 b-money 的思想和 Adam Back 提出的计算困难的哈希现金（Hashcash）难题来创造密码学货币。但是，这种概念再次迷失于理想化，因为它需要可信任的计算作为后端。在2009 年，一种去中心化的货币由中本聪在实践中首次得到实现，通过使用公共密钥密码来管理所有权，并过一种一致性算法来跟踪货币的持有者，这种算法被称为“工作证明”。这“工作证明” 的算法是一种突破，因为它同时决了 2 个问题。首先，它提供了一种简单的、有节制的有效的共识算法，允许网络中的节点一起同意比特币总帐状一组更新。其次，它提供了一种机制，允许任何节点自由进入共识的处理进程，从而解决了谁来影响共识的政治难题，同时阻止女巫攻击。 这参与共识投票的节点的投票权比重是直接和节点它们自己的算力挂钩的。 这就是比特币，其核心思想就是建立一个共识机制，不会由某一个服务中心作为计算后端。 区块链的基本概念区块链是一种分布式账本，由不可更改的数字记录数据包构成。这些数据包被称作为区块。然后使用加密签名呢将每个区块 “链接” 到下一个区块。这样就使区块链像分类账本一样被使用，可以有具有适当权限的任何人共享和访问。 在区块中，我们会写入如下信息 谁在转账给谁 交易金额 签名等其他信息 区块的链接方式 ​ 假设我们有 3 个区块，包含如下信息： 区块 1 包含的信息为 I1，I1 的哈希值为 H1。 区块 2 包含的信息为 I2 ，I2 的哈希值为 H2。 区块 3 包含的信息为 I3 ，I3 的哈希值为 H3。 H2 是由 H1 和 I2 结合起来算出的。同样地，H3 是由 H2 和 I3 结合起来算出的，依此类推。 分布式去中心化分类账区块链是一个分布式的去中心化分类账，用来存储交易信息等数据，这些数据为整个区块链网络中的节点所共享。 分类账分类账是承载区块列表的主要记录载体。 存储数据区块能存储数据（信息）。此处对数据的定义很广泛，可以是我们能想到的任何数据。我们就拿交易信息这一数据来举个例子。 分布式的去中心化分类账数据处理通常由一个中心机器负责。但是区块链里有很多机器（因此它不是中心化的），且所有机器都是点对点相互连接。另外，这些机器维护的是同一本分类账。因此，区块链被称为分布式的去中心化分类账。 换句话说，因为同一区块链网络中的所有人都共享同一本分类账，所以说区块链是分布式的。每个人都有整个分类账的副本，一旦有什么东西添加进去，副本马上就会更新。 为整个区块链网络中的节点所共享在区块链网络中，所有机器全都相互连接。每个节点（机器）都持有相同的分类账副本。这就意味着整个区块链网络中的节点都共享一本分类账。 区块链是如何确保安全性的区块链利用密码学来生成数字签名，并通过数字签名的方式防止数据完整性。 当创建一个交易的时候，利用自己的私钥对信息进行加密创建一个数字签名 然后把交易(内含信息、公钥以及数字签名)提交到其他的临近节点进行审批。 在这一过程中，网络会利用公钥来解密数字签名，并从签名中提取信息。 如果原信息与上图所示的签名中提取出来的信息相匹配，就可以通过审批，否则就无法通过。 如果量信息不匹配， 就可能是一下原因: 原信息在中途被操控了。 生成数字签名时所用的私钥与所提供的公钥不匹配。 这就是区块链网络如何能够防止篡改的方法，因此区块链相对上是安全的。 区块链技术的应用 区块链货币兑换 通过电子加密货币可以低成本的实现货币兑换和汇款 数据存储 区块链的实质是分布式账本，并且具有不可更改的特性，因此非常适合金融科技行业的数据存储。从另一个角度看，区块链也是分布式数据库，可以满足个人用户数据存储的需要。 区块链物联网 通过分部署账本记录某个设备与其他设备、web服务、或者人类用户之间的数据交换，就可以跟踪他的历史。 投票系统 区块链很好的解决了无需依赖第三方而达成信任的问题，因此特别适合实现公众投票系统。 预测平台 通常情况下，群体的智慧大于少数个体。对未来发生的时间，群体的预测结果通常会更加准确。利用区块链全名参与、只能合约等特性，可以创造新型的预测平台 支付 、借贷 通过电子加密货币可以实现低成本的跨境支付与个人借贷。 区块链技术栈 共识机制 用来筛选和竞争出让哪个节点来进行记账，并广播给所有节点进行同步的机制； 密码算法 用来计算区块的哈希值来关联各个区块， 和计算每个区块交易事务的哈希值(梅克尔根) 网络路由 用来发现各个节点，实现节点之间的相互通讯，完成各节点数据的同步； 脚本系统(可以类比以太坊智能合约和比特币的交易过程) 一般用来驱动数据的收发，即一套数据的收发和处理规则；在不同的系统中也可以通过改编脚本系统程序。拓展区块链系统功能，比如以太坊就可以根据自定义功能的脚本系统，进而实现智能合约的功能； 区块链账本 用来记录数据，将数据以区块的方式记录下来, 每一个区块一般包含[1. 区块头；2.前一区块头的哈希值 ； 3. 梅克耳根(交易事务的哈希值) ]， 根据下图, 一般六个区块就能确定 一比交易成功； 中心服务器和区块链的区别第三方介入的缺点 交易成本高 对小额交易不友好 调解纠纷的成本被平摊到交易手续费 跨地域交易时需要额外费用 对卖家不公平 当发生纠纷时通常保护买家的利益 某些产品可以轻易复制，如文档，包含源代码的程序等。当产生退货时对卖家不利； 交易时间长 第三方核实交易需要时间较长 隐私暴露 商家会向客户索要完全不必要的个人信息 如何去除第三方 电子加密货币的核心思想 全民参与 让全网尽可能多的节点参与核实，记录交易 给参与见证的节点讲理，维持整个系统持续 运行 任何人都可以参与挖矿，拥有代币，发起交易 匿名&amp;公开 交易的双方无需透露个人信息 交易信息是公开，非加密的。根据钱包地址可查所有的交易记录 去中心化 采用p2p网络，所有节点都来自于互联网，解决了第三方单独进行数据处理的问题 区块链分类根据网络范围分类 公有链 完全对外开放，任何人都可以任意使用； 没有权限的设定，有没有身份的认证； 所有的数据参与其中的任何人都可以任意查看，完全的公开透明； 节点数量不固定，节点是否在线也无法控制，通过网络中大多数节点承认的链就是主链； 公有链中共识机制一般是工作量证明（POW）和权益证明（POS）; 最具代表的就是比特币； 私有链 不对外开发，仅仅在组织内部使用； 通常需要注册，或者需要身份认证； 数据只对私链中的节点可见； 节点的数量和节点的状态通常的可控的; 因为私有链的节点都有很高的信任度，一般不需要通过竞争的方式来筛选数据打包者，可以采用更加节能的方式进行筛选； 比较代表的应用就是企业的票据管理，账务审计，供应链管理等系统； 联盟链 联盟链的网络范围介于公有链和私有链之间， 仅限联盟成员使用； 可以给不同的联盟成员设定不同的权限，所以记账规则和数据权限都可以私人订制； 一般也是具有身份认证和权限设置的； 节点的数量和节点的状态通常的可控的； 联盟链几乎不采用工作量证明共识机制而是采用权益证明或PBTF等共识算法; 代表的应用有银行之间的支付结算、企业之间的物流、政府机关的对外数据公开系统等； 根据部署环境分类 主链 部署在正式生产环境的区块链系统； 测试链 部署在测试环境，用来开发和测试Bug的区块链系统； 根据对接类型分类 单链 能够单独运行的区块链系统都可以称之为 “单链”，例如比特币主链，测试链， 这些区块链系统拥有完备的组件模块，自称一个体系； 侧链 区块链系统与侧链系统本身就是一个独立的链系统，两者之间可以按照一定的协议进行数据互动，通过这种方式，侧链能够起到一个对主链进行功能拓展的作用，很多在主链中不方便实现的功能可以实现在侧链中，而侧链再通过与主链的数据交互增强自己的可靠性； 互联链(多链) 就是区块链系统之间的互联，通过各自的优势，彼此互补，可以大大增强了系统的可靠性以及性能； 区块链技术特点 数据不可篡改性 区块链系统不是一个中心化软件设施，因此数据没有被某一家机构控制，数据肯定不可能被第三方篡改； 分布式存储 在区块链系统中，每个运行的节点都拥有一份完整的数据副本，这样的设计不仅避免了存储的单节点故障问题，还可以让每个节点能够独立的验证和检索数据，大大增加了整个系统的可靠性，节点之间的数据副本还可以互相保持同步，并使用类似梅克尔树这样的技术结构保证数据的完整性和一致性； 匿名性 使用传统的服务软件时，通常都是需要注册一个用户名，绑定手机号等，进行一些认证等；但是在区块链系统中，目前几乎所有的区块链产品都是使用所谓的地址来表示用户的，不需要提供其他的任何能表示出用户身份的信息，地址通常也是通过公开密钥算法生成的公钥转换而来的，这通常就是一串如乱码一般的字符串，因为即使这些公链系统是完全公开透明的，我们却不知道背后的操作者是谁； 价值传递 以比特币为例，比特币是一种数字资产，他是由比特币软件组成的网络所维护的，在这个网络中，不需要其他的第三方，自己可以根据规则发型比特币，并且能够确保发行的比特币是具有价值的(工作量证明)，而这种价值的认定是通过网络中所有的节点来自动进行验证的，节点之间打成公式就算认可了，整个过程都是自成一个体系来运行的，人们在交易比特币的时候就产生了价值传递； 区块链系统是可以自己创造信任机制的，无需第三方信任的环境中，大大简化了各种交易个过程，降低了交易的成本； 自动网络共识 生活中一半得事情都需要双方或者多方达成共识，比如签订一份合同，都是在多方达成共识，并且需要做各种确认；比特币从发型到转账交易，都是由网络中的节点自动及逆行身份认证和一系列的检查的，检查通过后就打成了网络共识，因为每个节点都遵守一份共同的约定和规则，只要意向交易符合所有的约定规则就能被确认； 可编程合约 比如比特币，在比特币系统中，并不是想银行账户一样，将金额存储在账户下就代表用户拥有的，而是通过脚本解锁和锁定一比资产，简单地说，就是让资产具备更强的编程可控能力，比如配置程序，让一比资产需要多个人共同签名才能被转移或者需要达到某个条件的时候才能被使用，这就是编程合约的思想。区块链系统具有数据不可篡改，价值传递等能力，加上编程合约，就能完成商业上各种的需求； 软分叉和硬分叉软分叉软分叉：是指区块链网络系统版本或协议升级后，旧的节点并不会意识到比特币代码发生改变，并继续接受由新节点创造的区块，新老节点始终还是在同一条链上工作。 硬分叉是指比特币区块格式或交易格式（共识机制）发生改变时，未升级的节点拒绝验证已经升级的节点产生的区块，然后大家各自延续自己认为正确的链，所以分成两条链。 产生两个币种，如BCC]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine原理以及channel使用]]></title>
    <url>%2F2019%2F01%2F05%2F11.Golang%2FGolang%E7%89%B9%E6%80%A7%2Fgoroutine%E5%8E%9F%E7%90%86%E5%92%8Cchannel%2F</url>
    <content type="text"><![CDATA[一、goroutine作用goroutine的本质是协程，是实现并行计算的核心，和python的async编程类似，但是又不需要我们手动的去创建携程并通过实践循环来启动。goroutine异步执行函数，只需使用go关键字+函数名即可启动一个协程，函数则直接处于异步执行的状态。 1go func()//通过go关键字启动一个协程来运行函数 二、goroutine的原理携程概念介绍协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此，协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。线程和进程的操作是由程序触发系统接口，最后的执行者是系统；协程的操作执行者则是用户自身程序，goroutine也是协程。 调度模型简介groutine是通过GPM调度模型实现，下面就来解释下goroutine的调度模型。 Go的调度器内部有四个重要的结构：M，P，S，Sched，如上图所示（Sched未给出）M:M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。P:P全称是Processor，处理器，它的主要用途就是用来执行goroutine的，所以它也维护了一个goroutine队列，里面存储了所有需要它来执行的goroutineSched：代表调度器，它维护有存储M和G的队列以及调度器的一些状态信息等。 调度实现 从上图中看，有2个物理线程M，每一个M都拥有一个处理器P，每一个也都有一个正在运行的goroutine。P的数量可以通过GOMAXPROCS()来设置，它其实也就代表了真正的并发度，即有多少个goroutine可以同时运行。图中灰色的那些goroutine并没有运行，而是出于ready的就绪态，正在等待被调度。P维护着这个队列（称之为runqueue），Go语言里，启动一个goroutine很容易：go function 就行，所以每有一个go语句被执行，runqueue队列就在其末尾加入一个goroutine，在下一个调度点，就从runqueue中取出 当一个OS线程M0陷入阻塞时(例如爬虫或者其他的io操作， 这里就和python的多线程类似)（如下图)，P转而在运行M1，图中的M1可能是正被创建，或者从线程缓存中取出。 当MO返回时，它必须尝试取得一个P来运行goroutine，一般情况下，它会从其他的OS线程那里拿一个P过来，如果没有拿到的话，它就把goroutine放在一个global runqueue里，然后自己睡眠（放入线程缓存里）。所有的P也会周期性的检查global runqueue并运行其中的goroutine，否则global runqueue上的goroutine永远无法执行。 另一种情况是P所分配的任务G很快就执行完了（分配不均），这就导致了这个处理器P很忙，但是其他的P还有任务，此时如果global runqueue没有任务G了，那么P不得不从其他的P里拿一些G来执行。一般来说，如果P从其他的P那里要拿任务的话，一般就拿runqueue的一半，这就确保了每个OS线程都能充分的使用，如下图： 三、goroutine使用基本使用设置goroutine运行的CPU数量，最新版本的go已经默认已经设置了。 12num := runtime.NumCPU() //获取主机的逻辑CPU个数runtime.GOMAXPROCS(num) //设置可同时执行的最大CPU数，即设置多少个P 使用示例 123456789101112131415161718192021222324252627282930package mainimport ( "fmt" "time")func cal(a int , b int ) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c)&#125;func main() &#123; for i :=0 ; i&lt;10 ;i++&#123; go cal(i,i+1) //启动10个goroutine 来计算 &#125; time.Sleep(time.Second * 2) // sleep作用是为了等待所有任务完成&#125; //结果//8 + 9 = 17//9 + 10 = 19//4 + 5 = 9//5 + 6 = 11//0 + 1 = 1//1 + 2 = 3//2 + 3 = 5//3 + 4 = 7//7 + 8 = 15//6 + 7 = 13 goroutine异常捕捉当启动多个goroutine时，如果其中一个goroutine异常了，并且我们并没有对进行异常处理，那么整个程序都会终止，所以我们在编写程序时候最好每个goroutine所运行的函数都做异常处理，异常处理采用recover 123456789101112131415161718192021222324252627282930313233343536package mainimport ( "fmt" "time")func addele(a []int ,i int) &#123; defer func() &#123; //匿名函数捕获错误 err := recover() if err != nil &#123; fmt.Println("add ele fail") &#125; &#125;() a[i]=i fmt.Println(a)&#125;func main() &#123; Arry := make([]int,4) for i :=0 ; i&lt;10 ;i++&#123; go addele(Arry,i) &#125; time.Sleep(time.Second * 2)&#125;//结果add ele fail[0 0 0 0][0 1 0 0][0 1 2 0][0 1 2 3]add ele failadd ele failadd ele failadd ele failadd ele fail 同步的goroutine由于goroutine是异步执行的，那很有可能出现主程序退出时还有goroutine没有执行完，此时goroutine也会跟着退出。此时如果想等到所有goroutine任务执行完毕才退出，go提供了sync包和channel来解决同步问题，当然如果你能预测每个goroutine执行的时间，你还可以通过time.Sleep方式等待所有的groutine执行完成以后在退出程序(如上面的列子)。 方法一：使用sync包同步goroutine 这种方法和python的Thread.join() 很类似WaitGroup 等待一组goroutinue执行完毕. 主程序调用 Add 添加等待的goroutinue数量. 每个goroutinue在执行结束时调用 Done ，此时等待队列数量减1.，主程序通过Wait阻塞，直到等待队列为0. 123456789101112131415161718192021222324252627282930313233package mainimport ( "fmt" "sync")func cal(a int , b int ,n *sync.WaitGroup) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c) defer n.Done() //goroutinue完成后, WaitGroup的计数-1&#125;func main() &#123; var go_sync sync.WaitGroup //声明一个WaitGroup变量 for i :=0 ; i&lt;10 ;i++&#123; go_sync.Add(1) // WaitGroup的计数加1 go cal(i,i+1,&amp;go_sync) &#125; go_sync.Wait() //等待所有goroutine执行完毕&#125;//结果9 + 10 = 192 + 3 = 53 + 4 = 74 + 5 = 95 + 6 = 111 + 2 = 36 + 7 = 137 + 8 = 150 + 1 = 18 + 9 = 17 示例二：通过channel实现goroutine之间的同步。 这种方法类似于python的Thread.Queue()通讯，来实现异步程序和主程序之间的数据交换实现方式：通过channel能在多个groutine之间通讯，当一个goroutine完成时候向channel发送退出信号,等所有goroutine退出时候，利用for循环channe去channel中的信号，若取不到数据会阻塞原理，等待所有goroutine执行完毕，使用该方法有个前提是你已经知道了你启动了多少个goroutine。 12345678910111213141516171819202122232425package mainimport ( "fmt" "time")func cal(a int , b int ,Exitchan chan bool) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c) time.Sleep(time.Second*2) Exitchan &lt;- true&#125;func main() &#123; Exitchan := make(chan bool,10) //声明并分配管道内存 for i :=0 ; i&lt;10 ;i++&#123; go cal(i,i+1,Exitchan) &#125; for j :=0; j&lt;10; j++&#123; &lt;- Exitchan //取信号数据，如果取不到则会阻塞 &#125; close(Exitchan) // 关闭管道&#125; goroutine之间的通讯goroutine本质上是协程，可以理解为不受内核调度，而受go调度器管理的线程。goroutine之间可以通过channel进行通信或者说是数据共享，当然你也可以使用全局变量来进行数据共享。 示例：使用channel模拟消费者和生产者模式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport ( "fmt" "sync")func Productor(mychan chan int,data int,wait *sync.WaitGroup) &#123; mychan &lt;- data fmt.Println("product data：",data) wait.Done()&#125;func Consumer(mychan chan int,wait *sync.WaitGroup) &#123; a := &lt;- mychan fmt.Println("consumer data：",a) wait.Done()&#125;func main() &#123; datachan := make(chan int, 100) //通讯数据管道 var wg sync.WaitGroup for i := 0; i &lt; 10; i++ &#123; go Productor(datachan, i,&amp;wg) //生产数据 wg.Add(1) &#125; for j := 0; j &lt; 10; j++ &#123; go Consumer(datachan,&amp;wg) //消费数据 wg.Add(1) &#125; wg.Wait()&#125;//结果consumer data： 4product data： 5product data： 6product data： 7product data： 8product data： 9consumer data： 1consumer data： 5consumer data： 6consumer data： 7consumer data： 8consumer data： 9product data： 2consumer data： 2product data： 3consumer data： 3product data： 4consumer data： 0product data： 0product data： 1 四、Golang - channel简介channel俗称管道，用于数据传递或数据共享，其本质是一个先进先出的队列，使用goroutine+channel进行数据通讯简单高效，同时也线程安全，多个goroutine可同时修改一个channel，不需要加锁。 channel可分为三种类型：只读channel：只能读channel里面数据，不可写入只写channel：只能写数据，不可读一般channel：可读可写 channel使用定义和声明12345678910// 定义var readOnlyChan &lt;-chan int // 只读chanvar writeOnlyChan chan&lt;- int // 只写chanvar mychan chan int //读写channel//声明read_only := make (&lt;-chan int,10)//定义只读的channel， 有缓存write_only := make (chan&lt;- int,10)//定义只写的channel， 有缓存read_write := make (chan int,10)//可同时读写， 有缓存read_write := make (chan int)//可同时读写， 无缓存 读写数据需要注意 如果没有使用goroutine，如果未关闭channel，读取数据会产生deadlock异常 管道如果关闭后再继续写入数据会pannic 如果是有缓存channel，当管道中没有数据时候再行读取会读取到默认值，如int类型默认值是0 1234ch &lt;- "wd" //写数据a := &lt;- ch //读取数据a, ok := &lt;-ch //优雅的读取数据for v := range ch &#123;&#125; // 遍历已经关闭的channel 循环管道需要注意 使用range循环管道，如果管道未关闭会引发deadlock错误。 如果采用for range 循环已经关闭的管道，当管道没有数据时候，读取的数据会是管道的默认值，并且循环不会退出。 12345678910111213141516171819package mainimport ( "fmt" "time")func main() &#123; mychannel := make(chan int,10) for i := 0;i &lt; 10;i++&#123; mychannel &lt;- i &#125; close(mychannel) //关闭管道 fmt.Println("data lenght: ",len(mychannel)) for v := range mychannel &#123; //循环管道 fmt.Println(v) &#125; fmt.Printf("data lenght: %d",len(mychannel))&#125; 带缓冲区channe和不带缓冲区channel带缓冲区channel：定义声明时候制定了缓冲区大小(长度)，可以保存多个数据。不带缓冲区channel：只能存一个数据，并且只有当该数据被取出才能存下个数据 12ch := make(chan int) //不带缓冲区ch := make(chan int ,10) //带缓冲区 不带缓冲区示例： 12345678910111213141516171819202122232425262728293031323334353637383940package mainimport "fmt"func test(c chan int) &#123; for i := 0; i &lt; 10; i++ &#123; fmt.Println("send ", i) c &lt;- i &#125;&#125;func main() &#123; ch := make(chan int) go test(ch) for j := 0; j &lt; 10; j++ &#123; fmt.Println("get ", &lt;-ch) &#125;&#125;//结果：send 0send 1get 0get 1send 2send 3get 2get 3send 4send 5get 4get 5send 6send 7get 6get 7send 8send 9get 8get 9 Channel的deadlock一个死锁的例子: 1234func main() &#123; ch := make(chan int) &lt;- ch // 阻塞main goroutine, 信道ch被锁&#125; 执行这个程序你会看到Go报这样的错误: 1fatal error: all goroutines are asleep - deadlock! *何谓死锁? *操作系统有讲过的，所有的线程或进程都在等待资源的释放。如上的程序中, 只有一个goroutine, 所以当你向里面加数据或者存数据的话，都会锁死信道， 并且阻塞当前 goroutine, 也就是所有的goroutine(其实就main线一个)都在等待信道的开放(没人拿走数据信道是不会开放的)，也就是死锁咯。 其实，总结来看，为什么会死锁？非缓冲信道上如果发生了流入无流出，或者流出无流入，也就导致了死锁。或者这样理解 Go启动的所有goroutine里的非缓冲信道一定要一个线里存数据，一个线里取数据，要成对才行 那么死锁的解决办法呢？最简单的，把没取走的数据取走，没放入的数据放入， 因为无缓冲信道不能承载数据，那么就赶紧拿走！ Go的deadlock 信道数据你也许发现，上面的代码一个一个地去读取信道简直太费事了，Go语言允许我们使用range来读取信道: 12345678910func main() &#123; ch := make(chan int, 3) ch &lt;- 1 ch &lt;- 2 ch &lt;- 3 for v := range ch &#123; fmt.Println(v) &#125;&#125; 如果你执行了上面的代码，会报死锁错误的，原因是range不等到信道关闭是不会结束读取的。也就是如果 缓冲信道干涸了，那么range就会阻塞当前goroutine, 所以死锁咯。 那么，我们试着避免这种情况，比较容易想到的是读到信道为空的时候就结束读取: 12345678910ch := make(chan int, 3)ch &lt;- 1ch &lt;- 2ch &lt;- 3for v := range ch &#123; fmt.Println(v) if len(ch) &lt;= 0 &#123; // 如果现有数据量为0，跳出循环 break &#125;&#125; 以上的方法是可以正常输出的，但是注意检查信道大小的方法不能在信道存取都在发生的时候用于取出所有数据，这个例子 是因为我们只在ch中存了数据，现在一个一个往外取，信道大小是递减的。 另一个方式是显式地关闭信道: 1234567891011ch := make(chan int, 3)ch &lt;- 1ch &lt;- 2ch &lt;- 3// 显式地关闭信道close(ch)for v := range ch &#123; fmt.Println(v)&#125; 被关闭的信道会禁止数据流入, 是只读的。我们仍然可以从关闭的信道中取出数据，但是不能再写入数据了。 channel实现作业池我们创建三个channel，一个channel用于接受任务，一个channel用于保持结果，还有个channel用于决定程序退出的时候。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( "fmt")func Task(taskch, resch chan int, exitch chan bool) &#123; defer func() &#123; //异常处理 err := recover() if err != nil &#123; fmt.Println("do task error：", err) return &#125; &#125;() for t := range taskch &#123; // 处理任务 fmt.Println("do task :", t) resch &lt;- t // &#125; exitch &lt;- true //处理完发送退出信号&#125;func main() &#123; taskch := make(chan int, 20) //任务管道 resch := make(chan int, 20) //结果管道 exitch := make(chan bool, 5) //退出管道 go func() &#123; for i := 0; i &lt; 10; i++ &#123; taskch &lt;- i &#125; close(taskch) &#125;() for i := 0; i &lt; 5; i++ &#123; //启动5个goroutine做任务 go Task(taskch, resch, exitch) &#125; go func() &#123; //等5个goroutine结束 for i := 0; i &lt; 5; i++ &#123; &lt;-exitch &#125; close(resch) //任务处理完成关闭结果管道，不然range报错 close(exitch) //关闭退出管道 &#125;() for res := range resch&#123; //打印结果 fmt.Println("task res：",res) &#125;&#125; 只读channel和只写channel一般定义只读和只写的管道意义不大，更多时候我们可以在参数传递时候指明管道可读还是可写，即使当前管道是可读写的。 1234567891011121314151617181920212223242526package mainimport ( "fmt" "time")//只能向chan里写数据func send(c chan&lt;- int) &#123; for i := 0; i &lt; 10; i++ &#123; c &lt;- i &#125;&#125;//只能取channel中的数据func get(c &lt;-chan int) &#123; for i := range c &#123; fmt.Println(i) &#125;&#125;func main() &#123; c := make(chan int) go send(c) go get(c) time.Sleep(time.Second*1)&#125;//结果 select-case实现非阻塞channel原理通过select+case加入一组管道，当满足（这里说的满足意思是有数据可读或者可写)select中的某个case时候，那么该case返回，若都不满足case，则走default分支。 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt")func send(c chan int) &#123; for i :=1 ; i&lt;10 ;i++ &#123; c &lt;-i fmt.Println("send data : ",i) &#125;&#125;func main() &#123; resch := make(chan int,20) strch := make(chan string,10) go send(resch) strch &lt;- "wd" select &#123; case a := &lt;-resch: fmt.Println("get data : ", a) case b := &lt;-strch: fmt.Println("get data : ", b) default: fmt.Println("no channel actvie") &#125;&#125;//结果：get data : wd channel频率控制在对channel进行读写的时，go还提供了非常人性化的操作，那就是对读写的频率控制，通过time.Ticke实现 示例： 123456789101112131415161718192021222324package mainimport ( "time" "fmt")func main()&#123; requests:= make(chan int ,5) for i:=1;i&lt;5;i++&#123; requests&lt;-i &#125; close(requests) limiter := time.Tick(time.Second*1) for req:=range requests&#123; &lt;-limiter fmt.Println("requets",req,time.Now()) //执行到这里，需要隔1秒才继续往下执行，time.Tick(timer)上面已定义 &#125;&#125;//结果：requets 1 2018-07-06 10:17:35.98056403 +0800 CST m=+1.004248763requets 2 2018-07-06 10:17:36.978123472 +0800 CST m=+2.001798205requets 3 2018-07-06 10:17:37.980869517 +0800 CST m=+3.004544250requets 4 2018-07-06 10:17:38.976868836 +0800 CST m=+4.000533569]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang特性</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang reflect interface 理解]]></title>
    <url>%2F2018%2F12%2F22%2F11.Golang%2FGolang%E7%89%B9%E6%80%A7%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%8F%8D%E5%B0%84%E7%9A%84%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[一、Golang的interface interface是方法的集合， 其方法不需要实现 interface是一种类型，并且是指针类型 interface的更重要的作用在于多态实现 interface定义方法12345type 接口名称 interface &#123;method1 (参数列表) 返回值列表method2 (参数列表) 返回值列表...&#125; interface使用 接口的使用不仅仅针对结构体，自定义类型、变量等等都可以实现接口。 如果一个接口没有任何方法，我们称为空接口，由于空接口没有方法，所以任何类型都实现了空接口。 要实现一个接口，必须实现该接口里面的所有方法。 1234567891011121314151617181920212223242526272829303132package mainimport "fmt"//定义接口type Skills interface &#123; Running() Getname() string&#125;type Student struct &#123; Name string Age int&#125;// 实现接口func (p Student) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Student) Running() &#123; // 实现 Running方法 fmt.Printf("%s running",p.Name)&#125;func main() &#123; var skill Skills var stu1 Student stu1.Name = "wd" stu1.Age = 22 skill = stu1 skill.Running() //调用接口&#125; interface嵌套go语言中的接口可以嵌套，可以理解为继承，子接口拥有父接口的所有方法，并且想要使用该子接口的话，必须将父接口和子接口的所有方法都实现。 12345678910type Skills interface &#123; Running() Getname() string&#125;type Test interface &#123; sleeping() Skills //继承Skills&#125; 多态的概念上面提到了，go语言中interface是实现多态的一种形式，所多态，就是一种事物的多种形态，就像python调用方法的时候，我们不需要关注传入参数的类型，我们只需要让传入的参数可以被方法使用就可以。 同一个interface，不同的类型实现，都可以进行调用，它们都按照统一接口进行操作。 在上面的示例中，我们增加一个Teacher结构体，同样实现接口进行说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport "fmt"type Skills interface &#123; Running() Getname() string&#125;type Student struct &#123; Name string Age int&#125;type Teacher struct &#123; Name string Salary int&#125;func (p Student) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Student) Running() &#123; // 实现 Running方法 fmt.Printf("%s running",p.Name)&#125;func (p Teacher) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Teacher) Running() &#123; // 实现 Running方法 fmt.Printf("\n%s running",p.Name)&#125;func main() &#123; var skill Skills var stu1 Student var t1 Teacher t1.Name = "wang" stu1.Name = "wd" stu1.Age = 22 skill = stu1 skill.Running() skill = t1 t1.Running()&#125;//wd running//wang running 通过interface 判断类型&amp;类型转换由于接口是一般类型，当我们使用接口时候可能不知道它是那个类型实现的，基本数据类型我们有对应的方法进行类型转换，当然接口类型也有类型转换。 当然我们也可以用这个方式来进行类型的判断。 类型转换示例： 12345var s intvar x interfacex = sy , ok := x.(int) //将interface 转为int,ok可省略 但是省略以后转换失败会报错，true转换成功，false转换失败, 并采用默认值 类型判断示例： 12345678910111213141516package mainimport "fmt"func main() &#123; var x interface&#123;&#125; s := "WD" x = s y,ok := x.(int) z,ok1 := x.(string) fmt.Println(y,ok) fmt.Println(z,ok1)&#125;//0 false//WD true 类型判断示例： 1234567891011121314151617181920212223242526272829303132333435package mainimport "fmt"type Student struct &#123; Name string&#125;func TestType(items ...interface&#123;&#125;) &#123; for k, v := range items &#123; switch v.(type) &#123; case string: fmt.Printf("type is string, %d[%v]\n", k, v) case bool: fmt.Printf("type is bool, %d[%v]\n", k, v) case int: fmt.Printf("type is int, %d[%v]\n", k, v) case float32, float64: fmt.Printf("type is float, %d[%v]\n", k, v) case Student: fmt.Printf("type is Student, %d[%v]\n", k, v) case *Student: fmt.Printf("type is Student, %d[%p]\n", k, v) &#125;&#125;&#125;func main() &#123; var stu StudentTestType("WD", 100, stu,3.3)&#125;//type is string, 0[WD]//type is int, 1[100]//type is Student, 2[&#123;&#125;]//type is float, 3[3.3] interface 和 reflect反射Golang的变量包含 type 和value两部分 type 包括 static type和concrete type. static type就类似python的不可变类型(如int、string)，concrete type是runtime系统看见的类型 类型断言能否成功，取决于变量的concrete type，而不是static type. Golang的静态类型和interface类型 Golang的指定类型的变量的type是静态的（也就是指定int、string这些的变量，它的type是static type），在创建变量的时候就已经确定 Golang的interface的type是concrete type。 在Golang的实现中，每个interface变量都有一个对应pair，pair中记录了实际变量的值和类型(value, type): Golang的反射 interface及其pair的存在，是Golang中实现反射的前提，理解了pair，就更容易理解反射。反射就是用来检测存储在接口变量内部(值value；类型concrete type) pair对的一种机制。 value是实际变量值，type是实际变量的类型。一个interface{}类型的变量包含了2个指针，一个指针指向值的类型【对应concrete type】，另外一个指针指向实际的值【对应value】。 二、反射reflect反射是程序执行时检查其所拥有的结构。尤其是类型的一种能力。这是元编程的一种形式。它同一时候也是造成混淆的重要来源。 每一个语言的反射模型都不同， python比较类似反射的例子就是类的 hasattr 方法， 用来检测类是否包含某个方法或者属性 go语言中的反射通过refect包实现，reflect包实现了运行时反射，允许程序操作任意类型的对象。 reflect.TypeType：Type类型用来表示一个go类型。 不是所有go类型的Type值都能使用所有方法。请参见每个方法的文档获取使用限制。在调用有分类限定的方法时，应先使用Kind方法获知类型的分类。调用该分类不支持的方法会导致运行时的panic。 获取Type对象的方法： 1func TypeOf(i interface&#123;&#125;) Type 示例： 123456789101112package mainimport ( "reflect" "fmt")func main() &#123; str := "wd" res_type := reflect.TypeOf(str) fmt.Println(res_type) //string&#125; reflect.Type的方法通用方法： 12345678910111213141516171819202122232425262728293031323334// 通用方法func (t *rtype) String() string // 获取 t 类型的字符串描述，不要通过 String 来判断两种类型是否一致。func (t *rtype) Name() string // 获取 t 类型在其包中定义的名称，未命名类型则返回空字符串。func (t *rtype) PkgPath() string // 获取 t 类型所在包的名称，未命名类型则返回空字符串。func (t *rtype) Kind() reflect.Kind // 获取 t 类型的类别。func (t *rtype) Size() uintptr // 获取 t 类型的值在分配内存时的大小，功能和 unsafe.SizeOf 一样。func (t *rtype) Align() int // 获取 t 类型的值在分配内存时的字节对齐值。func (t *rtype) FieldAlign() int // 获取 t 类型的值作为结构体字段时的字节对齐值。func (t *rtype) NumMethod() int // 获取 t 类型的方法数量。func (t *rtype) Method() reflect.Method // 根据索引获取 t 类型的方法，如果方法不存在，则 panic。// 如果 t 是一个实际的类型，则返回值的 Type 和 Func 字段会列出接收者。// 如果 t 只是一个接口，则返回值的 Type 不列出接收者，Func 为空值。func (t *rtype) MethodByName(string) (reflect.Method, bool) // 根据名称获取 t 类型的方法。func (t *rtype) Implements(u reflect.Type) bool // 判断 t 类型是否实现了 u 接口。func (t *rtype) ConvertibleTo(u reflect.Type) bool // 判断 t 类型的值可否转换为 u 类型。func (t *rtype) AssignableTo(u reflect.Type) bool // 判断 t 类型的值可否赋值给 u 类型。func (t *rtype) Comparable() bool // 判断 t 类型的值可否进行比较操作####注意对于：数组、切片、映射、通道、指针、接口 func (t *rtype) Elem() reflect.Type // 获取元素类型、获取指针所指对象类型，获取接口的动态类型 示例： 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; inf := new(Skills) stu_type := reflect.TypeOf(stu1) inf_type := reflect.TypeOf(inf).Elem() // 特别说明，引用类型需要用Elem()获取指针所指的对象类型 fmt.Println(stu_type.String()) //main.Student fmt.Println(stu_type.Name()) //Student fmt.Println(stu_type.PkgPath()) //main fmt.Println(stu_type.Kind()) //struct fmt.Println(stu_type.Size()) //24 fmt.Println(inf_type.NumMethod()) //2 fmt.Println(inf_type.Method(0),inf_type.Method(0).Name) // &#123;reading main func() &lt;invalid Value&gt; 0&#125; reading fmt.Println(inf_type.MethodByName("reading")) //&#123;reading main func() &lt;invalid Value&gt; 0&#125; true&#125; 其他方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 数值func (t *rtype) Bits() int // 获取数值类型的位宽，t 必须是整型、浮点型、复数型------------------------------// 数组func (t *rtype) Len() int // 获取数组的元素个数------------------------------// 映射func (t *rtype) Key() reflect.Type // 获取映射的键类型------------------------------// 通道func (t *rtype) ChanDir() reflect.ChanDir // 获取通道的方向------------------------------// 结构体func (t *rtype) NumField() int // 获取字段数量func (t *rtype) Field(int) reflect.StructField // 根据索引获取字段func (t *rtype) FieldByName(string) (reflect.StructField, bool) // 根据名称获取字段func (t *rtype) FieldByNameFunc(match func(string) bool) (reflect.StructField, bool) // 根据指定的匹配函数 math 获取字段func (t *rtype) FieldByIndex(index []int) reflect.StructField // 根据索引链获取嵌套字段------------------------------// 函数func (t *rtype) NumIn() int // 获取函数的参数数量func (t *rtype) In(int) reflect.Type // 根据索引获取函数的参数信息func (t *rtype) NumOut() int // 获取函数的返回值数量func (t *rtype) Out(int) reflect.Type // 根据索引获取函数的返回值信息func (t *rtype) IsVariadic() bool // 判断函数是否具有可变参数。// 如果有可变参数，则 t.In(t.NumIn()-1) 将返回一个切片。 示例： 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; stu_type := reflect.TypeOf(stu1) fmt.Println(stu_type.NumField()) //2 fmt.Println(stu_type.Field(0)) //&#123;Name string 0 [0] false&#125; fmt.Println(stu_type.FieldByName("Age")) //&#123;&#123;Age int 16 [1] false&#125; true&#125; reflect.Value不是所有go类型值的Value表示都能使用所有方法。请参见每个方法的文档获取使用限制。在调用有分类限定的方法时，应先使用Kind方法获知该值的分类。调用该分类不支持的方法会导致运行时的panic。 Value为go值提供了反射接口，获取Value对象方法： 1func ValueOf(i interface&#123;&#125;) Value 示例： 123str := "wd"val := reflect.ValueOf(str)//wd reflect.Value的方法reflect.Value.Kind()：获取变量类别，返回常量 123456789101112package mainimport ("reflect" "fmt")func main() &#123; str := "wd" val := reflect.ValueOf(str).Kind() fmt.Println(val)//string&#125; 用于获取值方法： 1234567891011121314151617181920212223242526272829303132333435363738func (v Value) Int() int64 // 获取int类型值，如果 v 值不是有符号整型，则 panic。func (v Value) Uint() uint64 // 获取unit类型的值，如果 v 值不是无符号整型（包括 uintptr），则 panic。func (v Value) Float() float64 // 获取float类型的值，如果 v 值不是浮点型，则 panic。func (v Value) Complex() complex128 // 获取复数类型的值，如果 v 值不是复数型，则 panic。func (v Value) Bool() bool // 获取布尔类型的值，如果 v 值不是布尔型，则 panic。func (v Value) Len() int // 获取 v 值的长度，v 值必须是字符串、数组、切片、映射、通道。func (v Value) Cap() int // 获取 v 值的容量，v 值必须是数值、切片、通道。func (v Value) Index(i int) reflect.Value // 获取 v 值的第 i 个元素，v 值必须是字符串、数组、切片，i 不能超出范围。func (v Value) Bytes() []byte // 获取字节类型的值，如果 v 值不是字节切片，则 panic。func (v Value) Slice(i, j int) reflect.Value // 获取 v 值的切片，切片长度 = j - i，切片容量 = v.Cap() - i。// v 必须是字符串、数值、切片，如果是数组则必须可寻址。i 不能超出范围。func (v Value) Slice3(i, j, k int) reflect.Value // 获取 v 值的切片，切片长度 = j - i，切片容量 = k - i。// i、j、k 不能超出 v 的容量。i &lt;= j &lt;= k。// v 必须是字符串、数值、切片，如果是数组则必须可寻址。i 不能超出范围。func (v Value) MapIndex(key Value) reflect.Value // 根据 key 键获取 v 值的内容，v 值必须是映射。// 如果指定的元素不存在，或 v 值是未初始化的映射，则返回零值（reflect.ValueOf(nil)）func (v Value) MapKeys() []reflect.Value // 获取 v 值的所有键的无序列表，v 值必须是映射。// 如果 v 值是未初始化的映射，则返回空列表。func (v Value) OverflowInt(x int64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是有符号整型。func (v Value) OverflowUint(x uint64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是无符号整型。func (v Value) OverflowFloat(x float64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是浮点型。func (v Value) OverflowComplex(x complex128) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是复数型。 设置值方法： 12345678910111213141516171819func (v Value) SetInt(x int64) //设置int类型的值func (v Value) SetUint(x uint64) // 设置无符号整型的值func (v Value) SetFloat(x float64) // 设置浮点类型的值func (v Value) SetComplex(x complex128) //设置复数类型的值func (v Value) SetBool(x bool) //设置布尔类型的值func (v Value) SetString(x string) //设置字符串类型的值func (v Value) SetLen(n int) // 设置切片的长度，n 不能超出范围，不能为负数。func (v Value) SetCap(n int) //设置切片的容量func (v Value) SetBytes(x []byte) //设置字节类型的值func (v Value) SetMapIndex(key, val reflect.Value) //设置map的key和value，前提必须是初始化以后，存在覆盖、不存在添加 其他方法： 123456789101112131415161718192021222324252627282930##########结构体相关：func (v Value) NumField() int // 获取结构体字段（成员）数量func (v Value) Field(i int) reflect.Value //根据索引获取结构体字段func (v Value) FieldByIndex(index []int) reflect.Value // 根据索引链获取结构体嵌套字段func (v Value) FieldByName(string) reflect.Value // 根据名称获取结构体的字段，不存在返回reflect.ValueOf(nil)func (v Value) FieldByNameFunc(match func(string) bool) Value // 根据匹配函数 match 获取字段,如果没有匹配的字段，则返回零值（reflect.ValueOf(nil)）########通道相关：func (v Value) Send(x reflect.Value)// 发送数据（会阻塞），v 值必须是可写通道。func (v Value) Recv() (x reflect.Value, ok bool) // 接收数据（会阻塞），v 值必须是可读通道。func (v Value) TrySend(x reflect.Value) bool // 尝试发送数据（不会阻塞），v 值必须是可写通道。func (v Value) TryRecv() (x reflect.Value, ok bool) // 尝试接收数据（不会阻塞），v 值必须是可读通道。func (v Value) Close() // 关闭通道########函数相关func (v Value) Call(in []Value) (r []Value) // 通过参数列表 in 调用 v 值所代表的函数（或方法）。函数的返回值存入 r 中返回。// 要传入多少参数就在 in 中存入多少元素。// Call 即可以调用定参函数（参数数量固定），也可以调用变参函数（参数数量可变）。func (v Value) CallSlice(in []Value) []Value // 调用变参函数 示例一：获取和设置普通类型的值 12345678910111213141516package mainimport ( "reflect" "fmt")func main() &#123; str := "wd" age := 11 fmt.Println(reflect.ValueOf(str).String()) //获取str的值，结果wd fmt.Println(reflect.ValueOf(age).Int()) //获取age的值，结果age str2 := reflect.ValueOf(&amp;str) //获取Value类型 str2.Elem().SetString("jack") //设置值 fmt.Println(str2.Elem(),age) //jack 11&#125; 示例二：简单结构体操作 1234567891011121314151617181920212223242526272829303132333435package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; stu_val := reflect.ValueOf(stu1) //获取Value类型 fmt.Println(stu_val.NumField()) //2 fmt.Println(stu_val.Field(0),stu_val.Field(1)) //wd 22 fmt.Println(stu_val.FieldByName("Age")) //22 stu_val2 := reflect.ValueOf(&amp;stu1).Elem() stu_val2.FieldByName("Age").SetInt(33) //设置字段值 ，结果33 fmt.Println(stu1.Age) &#125; 示例三：通过反射调用结构体中的方法，通过reflect.Value.Method(i int).Call()或者reflect.Value.MethodByName(name string).Call()实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( "fmt" "reflect")type Student struct &#123; Name string Age int&#125;func (this *Student) SetName(name string) &#123; this.Name = name fmt.Printf("set name %s\n",this.Name )&#125;func (this *Student) SetAge(age int) &#123; this.Age = age fmt.Printf("set age %d\n",age )&#125;func (this *Student) String() string &#123; fmt.Printf("this is %s\n",this.Name) return this.Name&#125;func main() &#123; stu1 := &amp;Student&#123;Name:"wd",Age:22&#125; val := reflect.ValueOf(stu1) //获取Value类型，也可以使用reflect.ValueOf(&amp;stu1).Elem() val.MethodByName("String").Call(nil) //调用String方法 params := make([]reflect.Value, 1) params[0] = reflect.ValueOf(18) val.MethodByName("SetAge").Call(params) //通过名称调用方法 params[0] = reflect.ValueOf("jack") val.Method(1).Call(params) //通过方法索引调用 fmt.Println(stu1.Name,stu1.Age)&#125;//this is wd//set age 18//set name jack//jack 18]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang特性</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机、批量梯度下降]]></title>
    <url>%2F2018%2F12%2F22%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9A%8F%E6%9C%BA%E3%80%81%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[1. 随机梯度下降什么时候需要使用随机梯度下降当使用传统的梯度下降的时候，每一次的迭代，都会去计算所有样本的误差总和，这样就造成了很大的运算量，以及会加载很多的数据，这样当数据量很大的时候，我们的硬件资源也很难满足我们的需求。所以，如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。 随机梯度下降算法单一实例的代价函数 $\operatorname { cost } \left( \theta , \left( x ^ { ( i ) } , y ^ { ( i ) } \right) \right) = \frac { 1 } { 2 } \left( h _ { \theta } \left( x ^ { ( i ) } \right) - y ^ { ( i ) } \right) ^ { 2 }$ 参数优化步骤 首先对训练集数据进行随机洗牌，使训练集数据的顺序打乱，目的是防止相邻数据相似导致优化相抵消 循环遍历每一个实例，通过其代价函数的导数来优化参数θ for $i = 1 : m {$ $\theta : = \theta _ { j } - \alpha \left( h _ { \theta } \left( x ^ { ( i ) } \right) - y ^ { ( i ) } \right) x _ { j } ^ { ( i ) }$(for $j = 0 : n )$}} 随机梯度下降算法在每一次计算之后便更新参数 θ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊 2.小批量梯度下降小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数 b 次训练实例，便更新一次参数 θ 。 for $i = 1 : m {$ $\theta : = \theta _ { j } - \alpha \frac { 1 } { b } \sum _ { k = i } ^ { i + b - 1 } \left( h _ { \theta } \left( x ^ { ( k ) } \right) - y ^ { ( k ) } \right) x _ { j } ^ { ( k ) }$(for $j = 0 : n )$$i + = 10$ }} 通常我们会令 b 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。 3.随机梯度下降收敛在批量梯度下降中，我们可以像普通的梯度下降那样，可以令代价函数 J 为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。 在随机梯度下降中，我们在每一次更新 θ 之前都计算一次代价，然后每 x 次迭代后，求出这 x 次对训练实例计算代价的平均值，然后绘制这些平均值与 x 次迭代的次数之间的函数图表 当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加 α 来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误 果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率 α。 我们也可以令学习率随着迭代次数的增加而减小，例如令： $\alpha = \frac { \text { const } } { \text { iteration Number } + \text { const } 2 }$ 随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。 但是通常我们不需要这样做便能有非常好的效果了，对 α 进行调整所耗费的计算通常不值得]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统模型]]></title>
    <url>%2F2018%2F12%2F15%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1. 推荐系统介绍对机器学习来说，特征是很重要的，往往所选的特征将会决定我们的模型建立的好坏。因此，在机器学习中有一种思想，它针对一些问题，有算法可以为你自动学习一套好的特征。推荐算法算是一个可以自动学习特征的算法，还有很多其它的，接下来从推荐算法来介绍怎么使用推荐算法进行特征自动学习，以及推荐系统的构建和应用； 举一个例子： 假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。 前面三部电影是爱情片， 后两部是动作片，可以看出前两个用户更偏向于爱情片，后两个用户更倾向于动作片。并且可以看到并不是每一个用户给对所有的电影都打过分， 可能是没有看过该电影，也可能是看过电影后没有给电影打分。所以，推荐系统的目标来了，就是系统通过构建一个算法来预测他们每个人可能会给他们没打过分的电影打多少分，并以此来作为推荐的依据； 给数据集引入一些标记， 下面关于推荐算法的推倒便基于这些标记: $n_u$代表用户的数量 $n_m$代表电影的数量 $r ( i , j )$如果用户 j 给电影 i 评分过则 r(i, j) = 1 $y^{(i, j)}$代表用户 j 给电影的评分 $m_j$代表用户 j 评过分的电影总数 基于内容的推荐系统在基于内容的推荐算法中，被推荐的内容往往有一些特征，我们要做的就是根据这些内容的特征，通过特征内容和被推荐人的符合程度，来进行推荐； 下面举一个例子，假设每部电影都有两个特征，如 x1 代表电影的浪漫程度，x2 代表电影的动作程度 每部电影都有一个特征向量，如 $x^{(1)}$ 是第一部电影的特征向量为[0.9 0]。 接下来根据这些特征来构建一个推荐算法。比如使用线型回归模型，可以现针对每一个用户都训练一个线型回归模型，如$\theta^{(1)}$是第一个用户的模型的参数。于是，推荐算法可以归纳为: 预测函数 $\theta^{(j)}$是用户 j 的参数向量 $x^{(i)}$是电影 i 的特征向量 对于用户 j 和电影 i ，我们预测评分为: $\left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) }$ 代价函数 针对用户 j ，该线性回归模型的代价为预测误差的平方和，加上正则化项： $\min _ { \theta ( j ) } \frac { 1 } { 2 } \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \left( \theta _ { k } ^ { ( j ) } \right) ^ { 2 }$ 其中 i : r( i, j ) 表示我们只计算那些用户 j 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以 1/2m ，在这里我们将 m 去掉。并且我们不对方差项 θ0 进行正则化处理。 优化目标 上面的代价函数只是针对一个用户的，为了学习所有用户，将所有用户的代价函数求和： $\min _ { \theta ^ { ( 1 ) } , \ldots , \theta ^ { ( n ) } } \frac { 1 } { 2 } \sum _ { j = 1 } ^ { n _ { n } } \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { j = 1 } ^ { n _ { s } } \sum _ { k = 1 } ^ { n } \left( \theta _ { k } ^ { ( j ) } \right) ^ { 2 }$ $\theta _ { k } ^ { ( j ) } : = \theta _ { k } ^ { ( j ) } - \alpha \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) x _ { k } ^ { ( i ) } \quad ($ for $k = 0 )$ $\theta _ { k } ^ { ( j ) } : = \theta _ { k } ^ { ( j ) } - \alpha \left( \sum _ { i r r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) x _ { k } ^ { ( i ) } + \lambda \theta _ { k } ^ { ( j ) } \right) \quad ($ for $k \neq 0 )$ 协同过滤通过参数来学习特征上面的例子是我们通过每一个内容的特征，使用这些特种来训练每一个用户的参数；同样的我们也可以根据用户的参数，来学习电影的特征；这样我们就可以根据已有的用户，来给所有未进行特征提取的电影进行特征的建立； 优化目标 $\min _ { x ^ { ( 1 ) } , \ldots , x ^ { ( m ) } } \frac { 1 } { 2 } \sum _ { i = 1 } ^ { n _ { m } } \sum _ { j ： r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { i = 1 } ^ { n _ { m } } \sum _ { k = 1 } ^ { n } \left( x _ { k } ^ { ( i ) } \right) ^ { 2 }$ 将优化θ变为优化x 协同过滤如果在建立系统的最开始，既没有用户的参数，也没有电影的特征，那么既不能通过特征来学习参数，也不能通过参数来学习特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。 协同过滤算法 如果给定x(1), …., x(nm)，那么估计θ(1), ….θ(nu): 如果给定θ(1), ….θ(nu)， 那么估计x(1), …., x(nm) 代价函数 $J \left( x ^ { ( 1 ) } , \ldots x ^ { \left( n _ { m } \right) } , \theta ^ { ( 1 ) } , \ldots , \theta ^ { \left( n _ { s } \right) } \right) = \frac { 1 } { 2 } \sum _ { ( i : j ) : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { i = 1 } ^ { n _ { m } } \sum _ { k = 1 } ^ { n } \left( x _ { k } ^ { ( j ) } \right) ^ { 2 } + \frac { \lambda } { 2 } \sum _ { j = 1 } ^ { n _ { u } } \sum _ { k = 1 } ^ { n } \left( \theta _ { k } ^ { ( j ) } \right) ^ { 2 }$ 优化目标优化函数 $\min _ { x ^ { ( 1 ) } , \ldots , x ^ { \left( n _ { m } \right) } \theta ^ { ( 1 ) } , \ldots , \theta ^ { \left( n _ { u } \right) } } J \left( x ^ { ( 1 ) } , \ldots , x ^ { \left( n _ { m } \right) } , \theta ^ { ( 1 ) } , \ldots , \theta ^ { \left( n _ { u } \right) } \right)$ $x _ { k } ^ { ( i ) } : = x _ { k } ^ { ( i ) } - \alpha \left( \sum _ { j : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } \theta _ { k } ^ { j } + \lambda x _ { k } ^ { ( i ) } \right)\right.$ $\theta _ { k } ^ { ( i ) } : = \theta _ { k } ^ { ( i ) } - \alpha \left( \sum _ { i : r ( i , j ) = 1 } \left( \left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } - y ^ { ( i , j ) } x _ { k } ^ { ( i ) } + \lambda \theta _ { k } ^ { ( j ) } \right)\right.$ 备注: 在协同过滤算法中，不需要特地的去学习方差项， 即线性回归的常数项，因为如果需要， 我们初始化θ后， 算法会自动的将一个特征学习为方差项 协同过滤步骤： 先初始化$x ^ { ( 1 ) } , x ^ { ( 1 ) } , \ldots x ^ { ( n m ) } , \theta ^ { ( 1 ) } , \theta ^ { ( 2 ) } , \ldots , \theta ^ { \left( n _ { 2 } \right) }$, 为一些随机小值 使用梯度下降算出最小化的代价函数 训练完成后， 预测用户给 j 电影 i 的评分 通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。 例如，如果一位用户正在观看电影 $x^{(i)}$ ，我们可以寻找另一部电影 $x^{(j)}$ ，依据两部电影的特征向量之间的距离 $\left| x ^ { ( i ) } - x ^ { ( j ) } \right|$ 大小 向量化: 低秩矩阵分解先举一个协同算法的例子： 当给出一件产品时，你能否找到与之相关的其它产品。 一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。 为了实现这些功能， 可以实现一种选择的方法，写出协同过滤算法的预测情况 有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。我们有五部电影，以及四位用户，那么 这个矩阵 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里： 零均值化 如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为Eve推荐电影呢？ 如果按照我们之前的协同过滤算法获得的模型，由于Eve没有对任何电影进行评分，所以为了优化模型，减少误差，最终Eve的所有特征参数都将为0，因为这样的损失是最小的，就会导致对其电影评分的预测都将是0， 那么就没有办法对其进行电影推荐； 怎么让没有评价记录的用户的预测不为0： 首先需要对结果 Y 矩阵进行零均值化，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值： 然后我们利用这个新的 Y 矩阵来训练算法。 如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测 $\left( \theta ^ { ( j ) } \right) ^ { T } x ^ { ( i ) } + \mu _ { i }$，对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分， 这样就防止了其对所有的电影评分都为了，导致推荐系统异常的判断。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>监督学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常检测模型]]></title>
    <url>%2F2018%2F12%2F14%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.异常检测使用场景什么是异常检测举个例子，比如在医院体检测量我们的血液中各种成分的含量，每一种含量都可以是一种特征，比如维生素，微量元素等含量。当测量的人很多，这样，就有了一个数据集，绘制成图标类似下面的样子： 这里的每个点、每个叉，都是你的无标签数据。这样，异常检测问题可以定义如下：假设有一个人的特征变量为$x_test$ 。所谓的异常检测问题就是：我们希望知道知道这个人的测试指标是否正常范围，或者说，我们希望判断这个人进行进一步的检查。如果被检查的人的各项指标都在正常范围内，那么我们可以判断该人的体检接口属于正常，而不需要进一步的检查。 给定数据集$x^{(1)}, x^{(2)}, ….,x^{(m)}$ ，我们假使数据集是正常的，我们希望知道新的数据 $x_{test}$ 是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 p(x) , 综上可以定义异常检测， 就是通过判根据测试数据和所有测试集的关系来判断其正常(或异常)的概率。 上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。这种方法称为密度估计，就是异常检测的核心思想，表达如下： ​ If $\quad p ( x ) \left{ \begin{array} { l l } { &lt; \varepsilon } &amp; { \text { anomaly } } \ { &gt; = \varepsilon } &amp; { \text { normal } } \end{array} \right.$ 异常检测的例子 异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。 服务器集群检测，特征可能包含：内存使用情况，被访问的磁盘数量，CPU的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。 2.高斯分布一般自然界产生的数据，我们通常认为会符合高斯分布，其表达方式为: $x \sim N \left( \mu , \sigma ^ { 2 } \right)$ , 则其高绿密度函数为: $p \left( x , \mu , \sigma ^ { 2 } \right) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp \left( - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } \right)$ , 所以我们可以利用已有的数据来预测总体中的u 和 σ2， 计算方法如下: $\mu = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } x ^ { ( i ) }$ ; $\sigma ^ { 2 } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( x ^ { ( i ) } - \mu \right) ^ { 2 }$ 3.异常检测算法模型构建步骤开始总结使用高斯分布的异常检测算法 异常检测算法: 对于给定的数据集$x ^ { ( 1 ) } , x ^ { ( 2 ) } , \dots , x ^ { ( m ) }$， 计算高斯分布的u和σ2的估计值。 ​ $\mu _ { j } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } x _ { j } ^ { ( i ) }$​ $\sigma _ { j } ^ { 2 } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( x _ { j } ^ { ( i ) } - \mu _ { j } \right) ^ { 2 }$ 推导出对应的高斯分布后， 测试对应测试实例的正常概率p(x); ​ $p ( x ) = \prod _ { j = 1 } ^ { n } p \left( x _ { j } ; \mu _ { j } , \sigma _ { j } ^ { 2 } \right) = \prod _ { j = 1 } ^ { 1 } \frac { 1 } { \sqrt { 2 \pi } \sigma _ { j } } \exp \left( - \frac { \left( x _ { j } - \mu _ { j } \right) ^ { 2 } } { 2 \sigma _ { j } ^ { 2 } } \right)$ 选择一个$\varepsilon​$ ， $p(x) = \varepsilon​$ 作为判定边界，当$p ( x ) &lt; \varepsilon​$ 时， 测试实例为异常； 异常检测系统开发异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 y 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。 构建步骤 将数据集划分成训练集，交叉检验集， 和测试集； 进行模型构建 尝试使用不同的 $\varepsilon$ 作为阈值，并预测数据是否异常，根据F1值或者查准率与查全率比例来选择$\varepsilon$ 选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的 值，或者查准率与查全率之比 4. 异常检测和监督学习的对比 总结： 当我们的数据集正负分类数量级差不多的时候，并且正负分类都有对应的标签的时候选择监督学习； 反之当我们有大量的数据，数据的正样本特别少， 可以使用异常检测； 5.特征选择特征转换 异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：$x = \log ( x + c )$ ，其中 c 为非负常数； 或者 $x = x ^ { c }$ ，c 为 0-1 之间的一个分数，等方法。(编者注：在python中，通常用 np.log1p() 函数，log1p 就是 log(x + 1)，可以避免出现负数结果，反向函数就是 np.expm1() ) 异常检测结果分析 一个常见的问题是一些异常的数据可能也会有较高的 p(x) 值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。 我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。 6.多元高斯分布比如我们的数据集的特征是两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。 下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的X所代表的数据点很可能是异常值，但是其 p(x) 值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。 在一般的高斯分布模型中，我们计算 p(x) 的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 p(x) 。 多元高斯分布算法步骤: 计算所有特征的平均值，再计算协方差矩阵； ​ $\mu = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } x ^ { ( i ) }$ ​ $\Sigma = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( x ^ { ( i ) } - \mu \right) \left( x ^ { ( i ) } - \mu \right) ^ { T } = \frac { 1 } { m } ( X - \mu ) ^ { T } ( X - \mu )$ 计算高斯分布p(x) ​ $p ( x ) = \frac { 1 } { ( 2 \pi ) ^ { \frac { n } { 2 } } | \Sigma | ^ { \frac { 1 } { 2 } } } \exp \left( - \frac { 1 } { 2 } ( x - \mu ) ^ { T } \Sigma ^ { - 1 } ( x - \mu ) \right)$ ​ 注: $\Sigma$是定矩阵，在 Octave 中用 det(sigma) 计算， $\Sigma ^ { - 1 }$是逆矩阵 选择一个$\varepsilon$ ， $p(x) = \varepsilon$ 作为判定边界，当$p ( x ) &lt; \varepsilon$ 时， 测试实例为异常； 协方差矩阵对模型的影响 当从左下到右上的对角线上的数不为0的时候，表示特征之间存在相关性 上图是5个不同的模型，从左往右依次分析： 是一个一般的高斯分布模型 通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差 通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性 多远高斯模型作用 建立多远高斯分布模型后， 可以通过协方差，建立两个特征成正相关性的模型，这样就可以将上图绿色的点判断为异常点； 多远高斯分布与普通高斯分布关系: 可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。 原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。 总结: 如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>非监督学习</tag>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据降维 & PCA]]></title>
    <url>%2F2018%2F12%2F06%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FPCA%E9%99%8D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[1. 数据降维数据降维目的数据降维是对数据进行压缩的一个强有力的工具，也是一个非监督学习算法。有几个不同的原因可能会让我们在进行建模的过程中对数据进行降维处理，一、数据压缩，不仅能让我们的数据占用比较少的计算机资源(硬盘、内存), 还可以通过减少特征的方式，来让我们的机器学习算法运行的更加快速; 二、数据可视化， 当数据的特征特别多的时候，我们没有可视化的方案，将一个多维的数据进行可视化展示，但是我们可以通过PCA来将数据进行压缩成维度比较少的形式，再进行展示； 什么是数据降维 这里按照一个将二维的数据降维成一维来进行举例： 假设我们的数据有两个特征: x1： 长度， 单位是cm ； x2：是用英寸表示的同一个物体的长度。 所以，这两个特征给了我们高度冗余表示，两个特征基本上用一个就可以表示了一个实例的长度特征，所以我们可以通过降维来减少这两个特征造成的冗余； 将数据从二维降至一维： 假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。 将我们的数据集的特征减少冗余，在保持误差基本不变的情况，减少特征的数量，这就是数据的降维； 2.数据压缩下面介绍将冗余数据进行压缩的方法 将二维数据降维至一维: 假设我们有这样一批特征，x1为学生学习的积极程度， x2为学生知识的掌握程度，这两个特征可能都可以代表学生的学习成果。所以，相比这两种特征，我们可能更倾向于获得特征–学生的知识掌握程度z, 来代替x1， x2两个特征。所以，我们真正关心的，可能是上面那条红色的线的方向，以及特征映射到其上的点，代表学生的知识量; 上面这幅图表示，将二维数据降维成一位数据的过程 将三维数据降维至二维: 过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。 这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。 3.数据可视化在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。 假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如GDP，人均GDP，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。 这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。 4.主成分分析问题主成分分析(PCA)是最常见的降维算法。在PCA中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。 主成分分析问题的描述：问题是要将维数据降至 k 维，目标是找到向量$u^{(i)}, u^{(2)}, …., u^{(k)}$, 使得所有的实例到向量的总的投射误差最小。 主成分分析与线性回顾的比较：主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。 上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。 PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。但PCA 要保证降维后，还要保证数据的特性损失最小。 PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。 PCA的优点 PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 PCA的缺点 如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，本可以根据经验做一些数据的整合或降维，但使用PCA却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。 5.主成分分析算法PCA 减少n维到k维：第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_j = x_j - u_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $\sigma^2$ 。第二步是计算协方差矩阵（covariance matrix） $\Sigma : \sum = \frac { 1 } { m } \sum _ { i - 1 } ^ { n } \left( x ^ { ( i ) } \right) \left( x ^ { ( i ) } \right) ^ { T }$第三步是计算协方差矩阵$\Sigma$的特征向量（eigenvectors）:在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解，[U, S, V]= svd(sigma) 。 对于一个n*n 维度的矩阵，上式中的U是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从n维降至k维，我们只需要从U中选取前k个向量，获得一个维度的n * k矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量:$z ^ { ( i ) } : z ^ { ( i ) } = U _ {reduce} ^ { T } * x ^ { ( i ) }$ 其中x是 n * 1 维的， 因此结果为 K * 1 维度的； 6.选择主成分的数量主要成分分析是减少投射的平均均方误差：训练集(经过均值归一化处理后)的方差为: $\frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left| x ^ { ( i ) } \right| ^ { 2 }$ 我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的值。 如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了, 一般这个值最好不要小于90%， 因为那样数据可能就偏离原数据太多了。 进行主成分选择的步骤(第一种方法): 我们可以先令k = 1，然后进行主要成分分析，获得$U_{reduce}$和z，然后计算比例是否小于1%; 如果不是的话再令k = K + 1，如此类推，直到找到可以使得比例小于1%的最小 值; 进行主成分选择的步骤(第二种方法): 我们可以先令k = 1，然后进行主要成分分析，获得$U_{reduce}$， S , 然后计算平均均方误差与训练集的比例 备注： S是一个n * n的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例 计算平均均方误差与训练集的比例公式: 如果比例大于1%,再令k = K + 1，如此类推，直到找到可以使得比例小于1%的最小 值; 7. 重建被压缩的数据PCA作为压缩算法。在那里你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。 所以， 给定$z^{(i)}$, 可能是100维， 怎么回到原来的$x^{(i)}$, 可能是1000维的数据； PCA算法，我们可能有一个这样的样本。如图中样本 $x^{(1)}, x^{(2)}$, 。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如 $z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？ 为2维，z为1维，$z = U^T_{reduce}x$ ，相反的方程为：$x_{appoz } = U_{reduce} * z$, $x_{appox} \approx x$ 。如图： 通过以上的步骤得到的数据与原始数据相当相似。所以，这就是你从低维表示z回到未压缩的表示。我们得到的数据和原始数据x非常近似 ，我们也把这个过程称为重建原始数据。但是没有方法可以将数据从低维，没有偏差的还原到高纬。 8.PCA使用的总结使用PCA的数据进行建模的步骤: 运用主要成分分析将数据压缩到K个特征 然后对训练集进行训练 在预测或者交叉验证时，采用之前学习而来的$U_{reduce}$， 将输入的特征x转换成特征向量z， 然后在进行预测； 不要在一下情况使用PCA 将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。 不考虑为什么使用PCA，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。 总结: PCA如果不是想加快训练速度， 减少资源消耗，没必要使用， 其只可能降低数据信息量， 更要注意，千万不要用PCA来防止过拟合；如果使用， 通过平均均方误差与训练集的比例来确定特征数量K；]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>非监督学习</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类问题]]></title>
    <url>%2F2018%2F12%2F06%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1.非监督学习和聚类算法介绍在监督学习中，所有的训练数据、测试数据、都有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，数据没有附带任何标签，数据只有一些列的特征，而我们的训练目标就是通过不同特征的训练集，将不同的训练集分为K个类别。 如上图，通过数据特征可以绘制出一些列的点，但是没有标签。也就是说，在非监督学习中，需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，通过我们给定的数据，为我们找到这个数据的内在结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到上面的点集的可以分为两个分类的算法，就被称为聚类算法， 接下来就通过学习聚类算法来了解非监督学习。 聚类算法的一些应用: 市场分割：比如在数据库中存储了许多客户的信息，希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。 社交网络分析：关注个人的社交信息，例如Facebook，Google+，或者是其他的一些信息，比如说：经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群 星系分类: 通过聚类算法将不同的行星划分到各个星系 2.K-means算法K-means是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。K-means也是一个迭代算法，假设我们想要将数据聚类成n个组，其步骤为: 首先选择K个随机的点，称为聚类中心（cluster centroids）； 对于数据集中的每一个数据，按照距离K个中心点的距离 将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。 重复步骤2-4直至中心点不再变化。 一个聚类过程示例： 假如进行了10次迭代 用$u^{(1)}, u^{(2)}, …., u^{(k)}$来表示聚类中心，用$c^{(1)}, c^{(2)}, ….., c^{(m)}$ 来存储与第 i 个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下： 1234567891011Repeat &#123;for i = 1 to mc(i) := index (form 1 to K) of cluster centroid closest to x(i)for k = 1 to Kμk := average (mean) of points assigned to cluster k&#125; 算法步骤: 对于每一个样例，计算其应该属于的类。 移动聚类中心，对于每一个类，重新计算该类的中心。 K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用K-均值算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。 3. K-means优化目标K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和 K-均值的代价函数（又称畸变函数 Distortion function）的公式为： $J \left( c ^ { ( 1 ) } , \ldots , c ^ { ( m ) } , \mu _ { 1 } , \ldots , \mu _ { K } \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left| X ^ { ( i ) } - \mu _ { c ^ { ( i ) } } \right| ^ { 2 }$ 其中，$u_{c^{(i)}}$代表距离$x^{(i)}$最近的聚类中心， 我们的的优化目标便是找出使得代价函数最小的$c^{(1)}, c^{(2)}, …,c^{(m)}$ 和 $u_1, u_2, …,u_k$, 其优化目标的公式为： ${\min \over { c ^ { ( 1 ) } , \ldots , c ^ { ( m ) } \atop \mu _ { 1 } , \ldots , \mu _ { K } }} J \left( c ^ { ( 1 ) } , \ldots , c ^ { ( m ) } , \mu _ { 1 } , \ldots , \mu _ { K } \right)$ 通过对代价函数的构建，可以将K-means每次迭代的两次循环的目的做一个归纳: 第一个循环是重新将样本划分到距离最近的聚类中心点，是用于减小$c^{(i)}$引起的代价； 第二个循环 是重新调整中心点，使其位于该分类的中心， 是用于减小$u_{c^{(i)}}$引起的代价 4. 随机初始化聚类中心运行K-means算法，首先要随机初始化所有的聚类中心点，通常采用的方式是： 确定聚类中心个数 K，注意聚类中心点的个数 K 要小于所有训练集实例的数量, 即: K &lt; m； 随机选择 K个训练实例，然后令个聚类中心分别与这 K 个训练实例相等; K-means算法有一个常见的问题，即它有可能会停留在一个局部最小值处，而这取决于初始化的情况， 如下图: 由于聚类中心初始化的距离过近，导致最终只能获得局部的最优解，这样往往不是我们想要的分类结果。 如果想解决这个问题，通常需要多次运行K-means算法，每一次都重新进行随机初始化，最后再比较多次运行K-means的结果，选择代价函数最小的结果。这种方法在 K 较小的时候（2–10）还是可行的，但是如果 K 较大，这么做也可能不会有明显地改善, 但是通常K比较大的时候，局部最优解已经可以很接近最优解。 7.二分K-means算法二分k-means算法介绍基于kmeans算法容易使得结果为局部最小值而非全局最小值这一缺陷，对算法加以改进。使用一种用于度量聚类效果的指标SSE(Sum of Squared Error)，即对于第 i 个簇，其SSE为各个样本点到“簇中心”点的距离的平方的和，SSE值越小表示数据点越接近于它们的“簇中心”点，聚类效果也就越好。以此作为划分簇的标准。 算法思想是：先将整个样本集作为一个簇，该“簇中心”点向量为所有样本点的均值，计算此时的SSE。若此时簇个数小于 k，对每一个簇进行kmeans聚类(k=2) ，计算将每一个簇一分为二后的总误差SSE，选择SSE最小的那个簇进行划分操作。 算法计算过程输入：训练数据集 D=x(1),x(2),…,x(m) ,聚类簇数 k ;过程：函数 kMeans(D,k,maxIter) . 1：将所有点看做一个簇，计算此时“簇中心”向量：$\mu ^ { ( 1 ) } = \frac { 1 } { m } \sum _ { x \in D } x​$ 2：while “簇中心”个数h&lt;k“： 3： for i=1,2,…,h do 4： 将第 i 个簇使用 kmeans算法进行划分，其中 k=2 5： 计算划分后的误差平方和 SSEi 5： 比较 k 种划分的SSE值，选择SSE值最小的那种簇划分进行划分 5： 更新簇的分配结果 5： 添加新的“簇中心” 18：until 当前“簇中心”个数达到 k 输出：簇划分 C=C1,C2,…,CK 6.选取聚类数量聚类选择的通常做法通常没有具体的聚类数量选择的方法，最普遍的方法就是根据不同的问题和不同的需求，人工进行选择。 通常我们需要知道，进行聚类数量选择的时候需要思考我们运用K-means算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。 肘部法则当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变 K 值，也就是聚类类别数目的总数。我们用 K 个聚类来运行K-means聚类方法。这就意味着，所有的数据都会分到 K 个聚类里，然后计算成本函数 J 或者计算畸变函数。K 代表聚类数字。 经过聚类运算，可能会得到一条类似于左边这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。 你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快， K = 3之后就下降得很慢，那么我们就选 K= 3。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。 总结: 选择聚类数量最好是根据我们的需求，和场景来确定我们是需要多少种聚类，如果我们不能根据使用场景和需求确定下来聚类的数量，那么我们可以通过肘部法则，来选择一个比较合理的聚类数量。 pyhton实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# coding:utf-8import numpy as npdef distEclud(vecA, vecB): # 计算欧式距离 return np.sqrt(np.sum(np.power(vecA - vecB, 2))) # la.norm(vecA-vecB)def randCent(dataSet, k): # 初始化k个随机簇心 n = np.array(dataSet).shape[1] # 特征个数 centroids = np.mat(np.zeros((k, n))) # 簇心矩阵k*n for j in range(n): # 特征逐个逐个地分配给这k个簇心。每个特征的取值需要设置在数据集的范围内 minJ = min(dataSet[:, j]) # 数据集中该特征的最小值 rangeJ = float(max(dataSet[:, j]) - minJ) # 数据集中该特征的跨度 centroids[:, j] = np.mat(minJ + rangeJ * np.random.rand(k, 1)) # 为k个簇心分配第j个特征，范围需限定在数据集内。 return centroids # 返回k个簇心def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): """随机初始化k-means""" m = np.array(dataSet).shape[0] # 数据个数 clusterAssment = np.mat(np.zeros((m, 2))) # 记录每个数据点被分配到的簇，以及到簇心的距离 centroids = createCent(dataSet, k) # 初始化k个随机簇心 clusterChanged = True # 记录一轮中是否有数据点的归属出现变化，如果没有则算法结束 while clusterChanged: clusterChanged = False for i in range(m): # 枚举每个数据点，重新分配其簇归属 minDist = np.inf # 先假设距离簇心的距离为无穷大，则先随机分配一个簇心给数据点 minIndex = -1 # 记录最近簇心及其距离 for j in range(k): # 枚举每个簇心 distJI = distMeas(centroids[j, :], dataSet[i, :]) # 计算数据点与簇心的距离 if distJI &lt; minDist: # 更新最近簇心 minDist = distJI minIndex = j if clusterAssment[i, 0] != minIndex: clusterChanged = True # 更新“变化”记录 clusterAssment[i, :] = minIndex, minDist ** 2 # 更新数据点的簇归属 print(centroids) for cent in range(k): # 枚举每个簇心，更新其位置 ptsInClust = dataSet[np.nonzero(clusterAssment[:, 0].A == cent)[0]] # 得到该簇所有的数据点 centroids[cent, :] = np.mean(ptsInClust, axis=0) # 将数据点的均值作为簇心的位置 return centroids, clusterAssment # 返回簇心及每个数据点的簇归属def biKmeans(dataSet, k, distMeas=distEclud): """随机初始化二值化k-means""" m = np.array(dataSet).shape[0] centroid0 = np.mean(dataSet, axis=0).tolist()[0] # 创建初始簇心，标号为0 centList = [centroid0] # 创建簇心列表 clusterAssment = np.array(np.zeros((m, 2))) # 初始化所有数据点的簇归属(为0) for j in range(m): # 计算所有数据点与簇心0的距离 clusterAssment[j, 1] = distMeas(np.array(centroid0), dataSet[j, :]) ** 2 while len(centList) &lt; k: # 分裂k-1次，形成k个簇 lowestSSE = np.inf # 初始化最小sse为无限大 for i in range(len(centList)): # 枚举已有的簇，尝试将其一分为二 ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:, 0].A == i)[0], :] # 将该簇的数据点提取出来 centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) # 利用普通k均值将其一分为二 sseSplit = np.sum(splitClustAss[:, 1]) # 计算划分后该簇的SSE sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:, 0].A != i)[0], 1]) # 计算该簇之外的数据点的SSE print("sseSplit, and notSplit: ", sseSplit, sseNotSplit) if (sseSplit + sseNotSplit) &lt; lowestSSE: # 更新最小总SSE下的划分簇及相关信息 bestCentToSplit = i # 被划分的簇 bestNewCents = centroidMat # 划分后的两个簇心 bestClustAss = splitClustAss.copy() # 划分后簇内数据点的归属及到新簇心的距离 lowestSSE = sseSplit + sseNotSplit # 更新最小总SSE print('the bestCentToSplit is: ', bestCentToSplit) print('the len of bestClustAss is: ', len(bestClustAss)) centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0] # 一个新簇心的标号为旧簇心的标号，所以将其取代就簇心的位置 centList.append(bestNewCents[1, :].tolist()[0]) # 另一个新簇心加入到簇心列表的尾部，标号重新起 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList) # 更新旧簇内数据点的标号 bestClustAss[np.nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit # 同上 clusterAssment[np.nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss # 将更新的簇归属统计到总数据上 return np.mat(centList), clusterAssment]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>非监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据处理 特征工程]]></title>
    <url>%2F2018%2F12%2F01%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[0.数据处理的一般流程数据获取 –&gt; 数据划分 –&gt; 数据处理 –&gt; 效果评判 1.数据获取数据来源 企业日益积累的大量数据（互联网公司更为显著） 政府掌握的各种数据 科研机构的实验数据 数据类型 离散型数据 由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。 连续性数据 变量可以在某个范围内取任一数，即变量的取值可以是连续的，如，长度、时间、质量值等，这类整数通常是非整数，含有小数部分。 通过sk-learn获取数据 sklearn数据集获取API sklearn.datasets 小规模数据集的获取 datasets.load_*() 获取小规模的数据集， 数据集已经包含在了datasets里 参数subset: train 表示获取训练街, test 表示获取测试集, all 表示两者全部 大规模数据的获取与删除 datasets.fetch_*(data_home=None) 从网络上以下载的方式获取大规模的数据集 data_hom：表示数据集下载的目录,默认是 ~/scikit_learn_data/ subset: train 表示获取训练街, test 表示获取测试集, all 表示两者全部 datasets.clear_data_home(data_home=None) 清除目录下的数据集 api接口返回值（datasets.base.Bunch 实例）的属性 data：特征数据数组，是 [n_samples * n_features] 的二维numpy.ndarray 数组 target：目标值数组，是 n_samples 的一维 numpy.ndarray 数组 feature_names：特征名,新闻数据，手写数字、回归数据集没有 target_names：目标数据名称 DESCR：数据描述 2.数据集的划分数据集常见划分形式 训练数据：用于训练，构建模型 测试数据：在模型检验时使用，用于评估模型是否有效 数据划分的原则 将数据集划分为两个互斥的集合，其中一个集合作为训练集，留下的集合作为测试集 划分要尽可能的保持数据分布的一致性(中的数据分布跟是一样的)，才能避免因数据划分过程引入额外的偏差而对最终结果产生影响 不同的划分将导致不同的训练/测试集，相应的模型评估也是有差别的，若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果 sklearn数据集划分方法 sklearn.model_selection.train_test_split(x, y, **options) x 数据集的特征值 y 数据集的标签值 test_size 测试集占用比例的大小, 一般是0.8 random_state 当设置该值为任意值时, 返回的划分结果相同 return (训练集特征值，测试集特征值，训练标签，测试标签) 3.数据处理和特征工程特征抽象&amp;特征衍生特征抽象的作用将计算机不好理解的数据转化为计算机友好的数据形式来体现 举例 特征衍生的作用将一些数据之间所体现的规律和特征体现出来 举例连续变量 离散变量 特征抽取数据抽取的作用 让计算机和模型更好的理解数据的结构，可将数据处理成计算机比较友好的数据结构，比如讲分类问题的数据处理成one-hot编码的数据类型等， 常用与分类问题的特征处理 sk-learn特征抽取api1.字典特征抽取 使用类: sklearn.feature_extraction.DictVectorizer 常用方法 DictVectorizer(sparse=True,…) # 创建dict_vert对象 sparse: 是否返回sparse矩阵 DictVectorizer.fit_transform(X) # 将X进行特征转化 X:字典或者包含字典的迭代器 DictVectorizer.inverse_transform(X) # 将X转为为特征转化之前的数据格式 X:array数组或者sparse矩阵 DictVectorizer.get_feature_names() # 获取特征类别名称 DictVectorizer.fit(X) 将X数据进行特征值标准匹配 DictVectorizer.transform(X) # 将X按照之前的fit标准进行转化 返回值的数据格式类型one-hot编码 2.文本的特征抽取 英文文本特征抽取 使用类: sklearn.feature_extraction.text.CountVectorizer 常用方法 CountVectorizer(max_df=1.0,min_df=1, stop_word） # 创建词频矩阵对象 max_df整数：指每个词的所有文档词频数不大于该值 min_df小数：每个词的次数／所有文档数量，最小 stop_word: 将分词的哪些不需要的结果进行剔除 CountVectorizer.fit_transform(X) # 获取特征值矩阵 X:文本或者包含文本字符串的可迭代对象 注意: 利用toarray(), 可以将返回值进行sparse矩阵转换array数组 CountVectorizer.inverse_transform(X) # 将特征值矩阵转化为之前的数据格式 X:array数组或者sparse矩阵 CountVectorizer.get_feature_names() # 获取特征值化后的单词列表 中文文本特征抽取 常用方法 使用类：sklearn.feature_extraction.text.TfidfVectorizer TfidfVectorizer(stop_words=None,…) # 创建权重矩阵对象 TfidfVectorizer.fit_transform(X) # 返回文本单词的权重sparse矩阵 X:文本或者包含文本字符串的可迭代对象 TfidfVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 # 将矩阵转换之前数据格式 TfidfVectorizer.get_feature_names() # 获取特征抽取后的单词列表 备注: 中文的特征值抽取原理上和英文一样，但是需要先使用分词器对长句子, 进行分词处理 分词器的常用方法 pip3 install jieba # 下载jieba分词器 import jieba jieba.cut(“python 现在我在进行特征词抽取”) # 对句子进行分词处理 返回： 返回包含分词结果的生成器 TF-IDF TF_IDF的介绍 如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF-IDF作用 用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 分类机器学习算法的重要依据 TF-IDF的计算公式 数据特征生成(生成交叉特征)为什么生成交叉特征比如说我们目前有一群二维的输入样本[a,b][a,b], 这个群二维的输入样本去训练模型，发现不管在训练集还是在测试集中的R^2 值都不高。 这时候，我们可以考虑将二维样本空间映射到跟高维度例如：[1,a,b,a2,ab,b2][1,a,b,a2,ab,b2] 或者更高维度。来体现features和features之间的关系。 sklearn的交叉特征生成api poly = PolynomialFeatures(degree = 2, interaction_only = True, include_bias=False) degree：默认为2，多项式次数(就同几元几次方程中的次数一样) interaction_only：是否包含单个自变量自相作用，默认为False，为True则表示去除与自己相乘的情况 include_bias：是否包含偏差标识，默认为True，为False则表示不包含偏差项 poly.fit_transform(X) 缺失值处理处理的两种思路 删除： 如果每列或者行数据缺失值达到一定的比例，建议放弃整行或者整列 插补：可以通过缺失值每行或者每列的平均值、中位数来填充 sk-learn缺失值处理api sklearn.preprocessing.Imputer Imputer(missing_values=’NaN’, strategy=’mean’, axis=0) missing_values: 却是值的代替值 strategy： 缺失代替值的类型 axis： 代表按照列来计算, 1按照行来计算 Imputer.fit_transform(X) # 将缺失值进行插补 X:numpy array格式的数据[n_samples,n_features] np.NaN的解释 numpy的数组中可以使用np.nan/np.NaN来代替缺失值，属于float类型 如果是文件中的一些缺失值，可以替换成nan，通过np.array转化成float型的数值即可 标准化和归一化处理归一化处理通过对原始数据进行变换把数据映射到(默认为[0,1])之间 归一化公式 备注: 作用于每一列，max为一列的最大值，min为一列的最小值,那么X’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0 sklearn归一化API sklearn.preprocessing.MinMaxScaler MinMaxScalar(feature_range=(0,1)…) # 创建归一化对象 feature_range：每个特征缩放到给定范围(默认[0,1]) MinMaxScalar.fit_transform(X) # 对数据集进行归一化处理 X:numpy array格式的数据 返回转换后的形状相同的array归一化的弊端使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景 标准化处理通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内 标准化处理 公式 标准化的优点如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。 sklearn标准化处理API scikit-learn.preprocessing.StandardScaler StandardScaler() # 创建标准化实例对象 StandardScaler.fit_transform(X) # 将Numpy array 格式的数据进行标准化 StandardScaler.mean_ # 原始数据中每列特征的平均值 StandardScaler.std_ # 原始数据每列特征的方差 特征的筛选和降维特征值选择的原因 冗余：部分特征的相关度高，容易消耗计算性能 噪声：部分特征对预测结果有负影响 主要应用的方法 1.Filter(过滤式):VarianceThreshold 2.Embedded(嵌入式)：正则化、决策树 3.Wrapper(包裹式) 1.低方差特征筛选-sklearn特征选择API sklearn.feature_selection.VarianceThreshold VarianceThreshold(threshold = 0.0) # 创建低方差数据删除对象 threshold： 设定方差低于多少进行删除 Variance.fit_transform(X) # 对特征数据中方差较低的特征进行删除 X:numpy array格式的数据 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。 PCA的的作用 特征之间通常是线性相关的，可以进行一些特征合并操作 PCA是一种分析、简化数据集的技术， 是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息, 可以削减回归分析或者聚类分析中特征的数量. PCA 降维 公式 2.特征的降维(PCA)-sklearn api sklearn. decomposition.PCA PCA(n_components=None) # 创建PCA对象 n_components： 将数据转化为多少维度的数据, 默认只降低一维 PCA.fit_transform(X) # 对特征矩阵进行降维处理 X:numpy array格式的数据[n_samples,n_features] 返回值：转换后指定维度的array 4.特征的效果评判 准确率 计算公式 准确率 = (TP + TN)/(TP + FP + FN + TN) sklearn api from sklearn.metrics import accuracy_score accuracy_score(y_true, y_predict, normalize=False) 精确率 计算公式 精确率 = (TP) / (TP + FP) sklearn api from sklearn.metrics import precision_score precision_score(y_true, y_predict) 召回率 计算公式 召回率 = (TP) / (TP + FN) sklearn api from sklearn.metrics import recall_score recall_score(y_true, y_predict) F1-score 计算公式 $F 1 - socre = 2 * \frac { \text { Precision } _ { - } \text { score*Recall score } } { \text { Precision } _ { - } \text { score } + \text { Recall } \text { score } }$ sklearn api from sklearn.metrics import f1_score f1_score(y_true, y_predict) 混淆矩阵 什么是混淆矩阵 混淆矩阵用来表示预测结果的真实反馈，返回的形式是一个矩阵，对角线元素表示预测标签等于真实标签的点的数量,是分类结果的一个绝对量 举个栗子 ​ 获得的混淆矩阵为 sklearn api from sklearn.metrics import confusion_matrix confusion_matrix(y_true, y_pred) ROC 计算公式 TPR = TP / (TP + FN) 预测为正的样本占所有正样本比例 FPR = FP / (FP + TN) 预测为正但是实际为负的样本占所有负样本的比例 sklearn api from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_true, y_pred,pos_label=1) thresholds： 样本属于正样本的概率 ROC曲线 AUC 计算公式 AUC是ROC曲线下方的面积，通常AUC值在0.5-1.0之间，值越大模型效果越好 AUC 为 0.5 跟随机猜测一样（例：丢硬币），模型没有预测价值 sklearn api from sklearn.metrics import auc roc_auc = auc(fpr, tpr) KS 计算公式 ks = max(TPR – FPR) 5.模型的选择与调优交叉验证 交叉验证的作用 为了让被评估的模型更加准确可信 交叉验证的过程 将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成5份，其中一份作为验证集。然后经过5次(组)的测试，每次都更换不同的验证集。即得到5组模型的结果，取平均值作为最终结果。又称5折交叉验证。 网格搜索 通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。 交叉验证通常是为了配合网格搜索, 通过网格似的参数对比, 交叉验证出最优参数, 以此来优化模型的参数选择 交叉验证-网格搜索api sklearn.model_selection.GridSearchCV sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None) estimator：估计器对象 param_grid：估计器参数 dict类型, key为超参数, value为将要进行网格搜索的参数 {“n_neighbors”:[1,3,5]} cv：指定几折交叉验证 fit：输入训练数据 score：准确率评估集合，返回多组的训练准确率结果 返回结果分析： best_score_:在交叉验证中验证的最好结果 best_estimator_：最好的参数模型 cv_results_:每次交叉验证后的测试集准确率结果和训练集准确率结果]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据处理</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2018%2F11%2F30%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[1.支持向量机优化目标与逻辑回归和神经网络相比，支持向量机，或者简称SVM，在训练复杂的非线性方程时提供了一种更为清晰，更加强大的方式。 在逻辑回归中我们已经熟悉了这里的假设函数形式，和下边的S型激活函数。下面依然用z表示$\theta^Tx$ 现在考虑下我们想要逻辑回归做什么：如果有一个 y = 1 的样本，不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 y = 1，现在我们希望 $h_\theta(x)​$ 趋近1。因为我们想要正确地将此样本分类，这就意味着当 $h_\theta(x)​$ 趋近于1时，$\theta^Tx​$ 应当远大于0。这是因为由于 z 表示 $\theta^Tx​$，当 z 远大于0时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于1。相反地，如果我们有另一个样本，即 y = 0。我们希望假设函数的输出值将趋近于0，这对应于 $\theta^Tx​$，或者就是 z 会远小于0，因为对应的假设函数的输出值趋近0。 如果你进一步观察逻辑回归的代价函数公式，你会发现每个样本 (x, y) 都会为总代价函数，增加上面的一项，因此，对于总代价函数通常会有对所有的训练样本的代价函数求和。 接下来，考虑逻辑回归的两种情况： 一种是y = 1 的情况， 一种是 y = 0的情况 在第一种情况中，假设 y = 1， 此时在目标函数中只需要第一项起作用， 因为y = 1时， (1-y)项等于零，因此，在y = 1的样本中， 即(x, y) 中， 我们得到$-log({1\over1+e^{-z}})$ 这一项。。如果画出关于 z 的函数，你会看到下面的这条曲线，我们同样可以看到，当 Z 增大时，也就是相当于 $\theta^Tx$ 增大时，z 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本时，试图将$\theta^Tx$设置得非常大。因为，在代价函数中的这一项会变的非常小。 现在开始建立支持向量机，我们从这里开始： 我们会从这个代价函数开始，也就是 $-log({1\over1+e^{-z}})$ 一点一点修改，让我取这里的 z = 1 点，我先画出将要用的代价函数。 新的代价函数(当y=1)和逻辑回归的代价函数类似，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在y=1的前提下的。你也许能想到，这应该能做同逻辑回归中类似的事情，但事实上，在向量机的优化问题中，这会变得更坚定，并且为支持向量机，带来计算上的优势。 另外一种情况是当 y = 1时，此时如果你仔细观察逻辑回归的代价函数只留下了第二项，因为第一项被消除了。如果当 y = 0时，那么这一项也就是0了。所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一项表示。并且，如果你将这一项作为z的函数，那么，这里就会得到横轴 z 。同样地，我们要替代逻辑回归的代价函数这一条蓝色的线，用相似的方法， 画出当y = 0的时候向量机的代价函数。 如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，第一个函数，我称之为cost1(z)，同时，第二个函数函数我称它为cost0(z)。这里的下标是指在代价函数中，对应的y=1和y=0 的情况，拥有了这些定义后，下面开始构建支持向量机。 这是我们在逻辑回归中使用代价函数 J(θ)。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要其表达式的两个部分替换为 cost1(z)，也就是前面构建的代价函数$cost_1(\theta^Tx)$, 和cost0(z)，也就是$cost_0(\theta^Tx)$。因此，对于支持向量机，我们得到了这里的最小化问题，即是将上面的两部分代价函数替换为下边蓝色的支持向量机代价函数的公式$cost_1(\theta^Tx)$ 和 $cost_0(\theta^Tx)$。然后再加上正则化参数。 但是在一般的时候，对支持向量机公式的参数，会有一些不同，下面尝试对向量机的代价函数进行改造： 首先，我们要除去1/m这一项，当然，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致，因为这里仅仅除去1/m这一项，也会得出同样的 θ 最优值，因为 仅是个常量，因此，这个最小化问题中，无论前面是否有 1/m 这一项，最终我所得到的最优值都是一样的。 第二点概念上的变化，对于逻辑回归，在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们不得不去用有 λ 这一项来平衡，这就相当于我们想要最小化加上正则化参数。我们所做的是通过设置不同正则参数 λ 达到优化目的。这样，我们就能够权衡对应的项，是使得训练样本拟合的更好。即最小化。但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的 λ 来权衡这两项。就是使用一个不同的参数称为C， 因此，在逻辑回归中，如果给定 λ，一个非常大的值，意味着给予正则化更大的权重。而这里，就对应于将 C 设定为非常小的值，那么，同样的的将会给正则化项更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数 C 考虑成 1/λ。 最后有别于逻辑回归输出的概率。我们的代价函数，当最小化代价函数，获得参数θ时，支持向量机所做的是它来直接预测的值等于1，还是等于0。因此，当这个假设函数当$\theta^Tx$小于0时，会预测为0。当$\theta^Tx$大于或者等于0时，将预测为1。 2.大间距的理解大间距分类器优化约定向量机经常被看做是大间距分类器，接下来了解一下向量机的大边界的含义，并进一步了解SVM模型的假设。 这是我的支持向量机模型的代价函数，左边是cost1(z) 关于z的代价函数，此函数用于正样本，右边是cost0(x)关于z的代价函数，横轴表示z，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本y=1 ，则只有在z &gt;= 1时，代价函数cost1(z)才等于0。 或者是说，如果有一个正样本，我们希望$\theta^Tx$&gt;=1, 反之， 如果y = 0, 他只有在z &lt;= -1 的时候， 函数值才为0.这是支持向量机的一个有趣的性质。事实上，如果有一个正样本y = 1, 则其实我们仅仅要求$\theta^Tx$大于等于0，就能将该样本恰当的分出，这是因为如果$\theta^Tx&gt;&gt;0$的话，我们的模型代价函数值为0， 所以我们需要的是比0大很多的值，比如大于等于1。这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说是安全的间距因子。 接下来看看C参数对支持向量机的影响。假设将C设置为一个非常大的数，比如将C设置成10000或者其他非常大的数 如果 C 非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为0的最优解。因此，让我们尝试在代价项的第一项为0的情形下理解该优化问题。比如我们可以把设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。 假如输入一个训练样本标签为 y = 1，你想令第一项为0，你需要做的是找到一个 θ，使得$\theta^Tx &gt;= 1$，类似地，对于一个训练样本，标签为 y = 0，为了使 cost0(z) 函数的值为0，我们需要 $\theta^Tx &lt;= -1$。因此，现在考虑优化问题。选择参数，使得第一项等于0，就会导致下面的优化问题，因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是C乘以0加上二分之一乘以第二项。这里第一项是C乘以0，因此可以将其删去，因为我知道它是0。这将遵从以下的约束(注意，这事是当向量机被看做是大间距分类器时的约定)：如果 $y^{(i)}$ 是等于1 的， $\theta^Tx^{(i)} &gt;=1$ ，如果 $y^{(i)}$ 是等于0的，$\theta^Tx^{(i)} &lt;= -1$ ，这样当求解这个优化问题的时候，当你最小化这个关于变量θ的函数的时候，你会得到一个非常有趣的决策边界。 大间距分类器决策边界和大边界理解 如果有一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。 比如，上面的绿色和粉色的决策边界，可以完全将训练样本区分开。但是多多少少这个看起来并不是非常自然 正常的支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线与样本之间有更大的距离，这个距离叫做间距(margin)。 当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器。现在我们可以理解为，上面一小节提到的优化约定，即：如果 $y^{(i)}$ 是等于1 的， $\theta^Tx^{(i)} &gt;=1$ ，如果 $y^{(i)}$ 是等于0的，$\theta^Tx^{(i)} &lt;= -1$ ，就是为了获取具有最大间距的决策边界，来获取最好的优化结果。 那么在让代价函数最小化的过程中，我们希望找出在y=0和y=1两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成： 事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点(outlier) 的影响。比如我们加入一个额外的正样本。 在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类似粉色的决策界。仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数C，设置的非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果C 设置的小一点，如果你将C设置的不要太大，则你最终会得到这条黑线，当然数据如果不是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数C非常大的情形，同时，C的作用类似于1/λ。这只是C非常大的情形，或者等价地 λ 非常小的情形。你最终会得到类似粉线这样的决策界，但是实际上应用支持向量机的时候，当C不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。 3.通过向量内积理解目标函数我们先前给出的支持向量机模型中的目标函数。 4.核函数核函数功能回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题： 为了获取上图所示的判定边界，我们的模型可能是$\theta_0 + \theta_{1x1} + \theta_{2x2} + \theta_{3x1x2} + + \theta_{4x1^2} + …..$ 的形式 我们可以用一些列新的特征f来代替模型中的每一项。例如令： f1 = x1, f2 = x2, f3 = x1x2, f4 = x1^2 ….， 得到$h_\theta(x) = f1 + f2 + f3 + … + fn$, 然而， 除了对原有的特征进行组合之外，我们可以利用核函数计算出新的特征。 给定一个训练实例x， 我们利用x的各个特征与我们预先选定的地标(landmarks) $l^{(1)}, l^{(2)}, l^{(3)}$的近似成都来选取新的特征f1，f2, f3 这些地标 $l^{(i)}$ 的作用是什么？ 如果一个训练实例与地标之间的距离近似于0，则新特征 f 近似于 $e^{-0} = 1$，如果训练实例x与地标之间距离较远，则 f 近似于$e^{一个大数} = 0$ 假设我们的训练实例含有两个特征[x1, x2] , 给定地标$l^{(1)}$与不同的σ值，见下图： 图中水平面的坐标为 x1，x2 ， f代表垂直坐标轴。可以看出，只有当x与$l^{(1)}$重合时才具有最大值。随着x的改变 f 值改变的速率受到σ2的控制。 在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此 f1 接近1，而 f2, f3接近0。因此hθ（x） &gt; 0，因此预测y = 1。同理可以求出，对于离 $l^{(2)}$ 较近的绿色点，也预测 y = 1，但是对于蓝绿色的点，因为其离三个地标都较远，预测 y = 0。 这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征f1, f2, f3。 核函数使用如何选择地标 我们通常是根据训练集的数量选择地标的数量，即如果训练集中有m个实例，则我们选取m个地标，并且令:$l^{(1)} = x^{(1)}, l^{(2)} = x^{(2)}, …., l^{(m)} = x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即： z 注意: 如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的 另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。 5.两个参数C, σ对支持向量机的影响C对支持向量机的影响 C = 1/λ C较大的时候， 相当于λ较小的时候， 可能会导致过拟合， 高方差； C较小的时候，相当于λ较大的时候， 可能会导致欠拟合， 高偏差； σ对支持向量机的影响 σ较大时，可能会导致低方差，高偏差； σ较小时时，可能会导致低偏差，高房差； 6.使用支持向量机不同的核函数 在高斯核函数之外我们还有其他一些选择，如：多项式核函数（Polynomial Kernel）字符串核函数（String kernel）卡方核函数（ chi-square kernel）直方图交集核函数（histogram intersection kernel） 多分类问题 假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有k个类，则我们需要k个模型，以及k个参数向量θ。我们同样也可以训练k个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。 选择参数 1、选择参数C。需要考虑高方差和高偏差的问题2、选择内核参数σ ， 和核函数。 除非使用线型和的SVM。 7.逻辑回归和SVM之间的选择n为特征数， m为训练样本数。 (1) 如果相较于m而言， n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 (2)如果n较小，而且m大小中等，例如在 1-1000 之间，而m在10-10000之间，使用高斯核函数的支持向量机。 (3)如果n较小，而m较大，例如在1-1000之间，而m大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 (4)神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>监督学习</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的偏差、方差以及优化]]></title>
    <url>%2F2018%2F11%2F24%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[1.机器学习模型的评估和选择有啥方法能优化我们的模型整理了一下优化模型时，经常做的一些操作，优化模型无外乎以下几种方法: 获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度 尝试增加正则化程度 但是，也如我们所见，并不是任何模型都适用于这些优化方法，我们还需要对症下药，接下来可以看一下怎么确定，我们的模型需要什么优化服务 模型过拟合的判断 当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。此时很可能模型已经产生了过拟合。 为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差： 对于线性回归模型，我们利用测试集数据计算代价函数 J 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外还可以计算误分类比率 模型的选择和交叉验证集当我们的使用多项式模型的时候，经常会不确定多项式的项数该如何确定，下面是一种比较简单的处理思路: 假设我们要在10个不同次数的二项式模型之间进行选择： ​ 显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能适应我们的测试集或者推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 选择模型的方法：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集 使用训练集训练出10个模型 用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）Train/validation/test error Training error:$J{train}(\theta) = \frac{1}{2m}\sum\limits{i=1}{m}(h_{\theta}(x{(i)})-y{(i)})2$Cross Validation error:$J{cv}(\theta) = \frac{1}{2m{cv}}\sum\limits{i=1}^{m}(h{\theta}(x{(i)}{cv})-y{(i)}{cv})^2$Test error:$J{test}(\theta)=\frac{1}{2m{test}}\sum\limits{i=1}^{m{test}}(h{\theta}(x^{(i)}{cv})-y{(i)}_{cv})2​$ 模型的偏差和方差诊断偏差和方差当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？ 其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。 我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析： 通过训练误差和交叉验证集误差判断偏差和方差问题，总结如下: 训练集误差和交叉验证集误差近似时：偏差/欠拟合交叉验证集误差远大于训练集误差时：方差/过拟合 正则化力度与偏差/方差在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。 我们选择一系列的想要测试的 λ 值，通常是 0-10之间的呈现2倍关系的值（如 0， 0.01， 0.01， 0.04， 0.08， 0.015， 0.032， 0.064， 1.28, 2.56, 5.12, 10共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。 选择正则化系数的方法为： 使用训练集训练出12个不同程度正则化的模型 用12个模型分别对交叉验证集计算的出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上： 当 λ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大随着 λ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 通过学习曲线评估偏差和方差学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。 例如： 如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。 如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观 也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。 如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。 也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。 学习曲线的总结: 当Jtrain和Jcv都偏高的时候，处于高偏差(欠拟合)的情况，此时增加训练数据不会有更好的结果 当Jtrain偏低而Jcv偏高的时候，处于高方差(过拟合)的情况，此时增加训练数据往往会获得更好的训练结果 神经网络的方差和偏差 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。 通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。 解决高偏差和高方差的总结解决高方差问题: 获得更多的训练实例——解决高方差 尝试减少特征的数量——解决高方差 尝试增加正则化程度λ——解决高方差 解决高偏差的问题 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 类偏斜的误差度量类偏斜和查准率查全率的介绍类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例， 这时候很难用一般的误差度量方法来度量其真实的误差。 例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 因为，下面将引入查准率和查全率的概念。 我们将算法预测的结果分成四种情况： 正确肯定（True Positive,TP）：预测为真，实际为真 正确否定（True Negative,TN）：预测为假，实际为假 错误肯定（False Positive,FP）：预测为真，实际为假 错误否定（False Negative,FN）：预测为假，实际为真 查准率（Precision）和查全率（Recall）的公式为 ： 查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。 因此，可以看出用于度量偏斜类问题，查准率和查全率更能体现模型表现的好坏。 查准率和查全率之间的选择在很多的应用中，我们希望能够保证查准率和召回率的相对平衡。 继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。 查准率(Precision)=TP/(TP+FP) 例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 查全率(Recall)=TP/(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。 我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同： 我们希望有一个帮助我们选择这个阀值的方法。 一种方法是计算F1 值（F1 Score），其计算公式为： $F_1Score：2{PR\over{P+R}}$ 我们选择使得F1值最高的阀值, F1的想法就是尽量让查准率和查全率都不会太小。 2.机器学习系统的设计机器学习系统设计思路以一个垃圾邮件分类器算法为例 为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。 为了构建这个分类器算法，我们可以做很多事，例如： 收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本 基于邮件的路由信息开发一系列复杂的特征 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理 为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法 误差分析设计机器学习系统的首要任务 当设计一个模型的时候， 最好的方法是快速的将最简单版本的算法实现，一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。 这么做的原因是：因为通常你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。 除了画出学习曲线之外，一件非常有用的事是误差分析。比如在构造垃圾邮件分类器时，可以看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。 构建一个学习算法的推荐方法为： 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择 进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势 举个误差分析的栗子: 以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看： 是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。 发现是否缺少某些特征，记下这些特征出现的次数。例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。 误差分析需要交叉验证来验证 误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。 不要用测试集来做交叉验证 因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。 总结: 当你在研究一个新的机器学习问题时，推荐你实现一个较为简单快速、即便不是那么完美的算法。目前大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。 另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>设计过程</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras构建简单神经网络]]></title>
    <url>%2F2018%2F11%2F11%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fkeras%E6%9E%84%E5%BB%BA%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[Keras 构建神经网络该示例的一般流程是首先加载数据，然后定义网络，最后训练网络。 要使用 Keras，你需要知道以下几个核心概念。 创建神经网络序列模型123from keras.models import Sequential # Create the Sequential model model = Sequential() keras.models.Sequential 类是神经网络模型的封装容器。它会提供常见的函数，例如 fit()、evaluate() 和 compile()。我们将介绍这些函数（在碰到这些函数的时候）。我们开始研究模型的层级吧。 层级Keras 层级就像神经网络层级。有完全连接的层级、最大池化层级和激活层级。你可以使用模型的 add() 函数添加层级。例如，简单的模型可以如下所示： 1234567891011121314from keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten #创建序列模型 model = Sequential() #第一层级 - 添加有 32 个节点的输入层 model.add(Dense, input_dim=32) #第二层级 - 添加有 128 个节点的完全连接层级 model.add(Dense(128)) #第三层级 - 添加 softmax 激活层级 model.add(Activation('softmax')) #第四层级 - 添加完全连接的层级 model.add(Dense(10)) #第五层级 - 添加 Sigmoid 激活层级 model.add(Activation('sigmoid')) Keras 将根据第一层级自动推断后续所有层级的形状。这意味着，你只需为第一层级设置输入维度。 上面的第一层级 model.add(Flatten(input_dim=32)) 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。 模型编译和训练构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 categorical_crossentropy。我们还可以指定优化程序，暂时将使用 adam。最后，我们可以指定评估模型用到的指标。我们将使用准确率。 1model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics = [&apos;accuracy&apos;]) 我们可以使用以下命令来查看模型架构： model.summary() 然后使用以下命令对其进行拟合，指定 epoch 次数和我们希望在屏幕上显示的信息详细程度。 model.fit(X, y, nb_epoch=1000, verbose=0) 注意：在 Keras 1 中，nb_epoch 会设置 epoch 次数，但是在 Keras 2 中，变成了 epochs。 最后，我们可以使用以下命令来评估模型： model.evaluate() 很简单，对吧？我们实践操作下。 练习我们从最简单的示例开始。在此测验中，你将构建一个简单的多层前向反馈神经网络以解决 XOR 问题。 将第一层级设为 Flatten() 层级，并将 input_dim 设为 2。 将第二层级设为 Dense() 层级，并将输出宽度设为 8。 在第二层级之后使用 softmax 激活函数。 将输出层级宽度设为 2，因为输出只有 2 个类别。 在输出层级之后使用 softmax 激活函数。 对模型运行 10 个 epoch。 准确度应该为 50%。可以接受，当然肯定不是太理想！在 4 个点中，只有 2 个点分类正确？我们试着修改某些参数，以改变这一状况。例如，你可以增加 epoch 次数。如果准确率达到 75%，你将通过这道测验。能尝试达到 100% 吗？ 首先，查看关于模型和层级的 Keras 文档。 Keras 多层感知器网络示例和你要构建的类似。请将该示例当做指南，但是注意有很多不同之处。 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom keras.utils import np_utilsimport tensorflow as tftf.python.control_flow_ops = tf# Set random seednp.random.seed(42)# Our dataX = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')y = np.array([[0],[1],[1],[0]]).astype('float32')# Initial Setup for Kerasfrom keras.models import Sequentialfrom keras.layers.core import Dense, Activation, Flatten# One-hot encoding the outputy = np_utils.to_categorical(y)# Building the modelxor = Sequential()xor.add(Dense(32, input_dim=2))xor.add(Activation("sigmoid"))xor.add(Dense(2))xor.add(Activation("sigmoid"))xor.compile(loss="categorical_crossentropy", optimizer="adam", metrics = ['accuracy'])# Uncomment this line to print the model architecture# xor.summary()# Fitting the modelhistory = xor.fit(X, y, nb_epoch=100, verbose=0)# Scoring the modelscore = xor.evaluate(X, y)print("\nAccuracy: ", score[-1])# Checking the predictionsprint("\nPredictions:")print(xor.predict_proba(X)) 结果： 123456789Using TensorFlow backend.4/4 [==============================] - 0sAccuracy: 0.75Predictions:4/4 [==============================] - 0s[[0.6914389 0.6965836 ] [0.7073754 0.7086655 ] [0.6919555 0.68419015] [0.70766294 0.6967294 ]]]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>keras</tag>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络]]></title>
    <url>%2F2018%2F11%2F07%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1.非线性假设当我们使用线型回归或者逻辑回归的时候，有这样一个缺点，当特征太多的时候，计算的负荷会非常大。 下面是一个例子: 当我们使用 x1, x2的多次项式进行预测时，我们可以应用的很好。 使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合（x1x2 + x1x3 + x1x4 + …. + x99x100），，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。 假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB值），作为假设，我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车， 如下图所示： 假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约3百万个特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。 2.神经元和大脑介绍2 神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器。 神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。但是最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法。然而大概由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。正是由于这个原因和其他一些我们后面会讨论到的技术因素，如今的神经网络对于许多应用来说是最先进的技术。当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器。大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学着做微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你得写很多不同的软件来模拟所有这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些，不同事情的方法，不需要用上千个不同的程序去实现。相反的，大脑处理的方法，只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过让我和你分享，一些这方面的证据。 大脑的这一部分这一小片红色区域是你的听觉皮层，你现在正在理解我的话，这靠的是耳朵。耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此，你才能明白我的话。神经系统科学家做了下面这个有趣的实验，把耳朵到听觉皮层的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了。那么结果表明听觉皮层将会学会“看”。这里的“看”代表了我们所知道的每层含义。所以，如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。 下面再举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的，如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会“看”。这个实验和其它一些类似的实验，被称为神经重接实验，从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它大脑通过自学掌握如何处理这些不同类型的数据。在很大的程度上，可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。举几个例子： 这张图是用舌头学会“看”的一个例子。它的原理是：这实际上是一个名为BrainPort的系统，它现在正在FDA (美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素电压值低的点。对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。 第二个例子： 关于人体回声定位或者说人体声纳。你有两种方法可以实现：你可以弹响指，或者咂舌头。不过现在有失明人士，确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索YouTube之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。 3.模型表示神经网络模型表示为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？ 每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多输入/树突，并且有一个输出/轴突。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。 下面是一组神经元的示意图，神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元，这就是轴突。 轴突是一条连接到输入神经，或者连接另一个神经元树突的神经，接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。这也是我们的感觉和肌肉运转的原理。如果你想活动一块肌肉，就会触发一个神经元给你的肌肉发送脉冲，并引起你的肌肉收缩。如果一些感官：比如说眼睛想要给大脑传递一个消息，那么它就像这样发送电脉冲给大脑的。 神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（weight）。 逻辑回归学习模型的神经元： 单个神经元的效果和逻辑回归的效果没有区别，但是神经网络会组成有神经元组成的网络。 一个简单的神经网络 其中 x1, x2, x3是输入单元（input units），我们将原始数据输入给它们。 a1, a2, a3是中间单元，代表三个神经元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算 hθ(x)。神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（Input Layer），最后一层称为输出层（Output Layer），中间一层成为隐藏层（Hidden Layers）。我们为每一层都增加一个偏差单位（bias unit）： 下面引入一些标记法来帮助描述模型： $a_i^{(j)}$代表第 j 层的第 i 个激活单元(神经元)。 $\theta^{(j)}$代表从第 j 层映射到第 j+1 层时的权重的矩阵，例如 $\theta^{(1)}$ 代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 j + 1 层的激活单元数量为行数，以第 j 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中 $\theta^{(1)}$ 的尺寸为 3*4。 注: 每层权重矩阵的列数为 ( 神经元数 + 1 ), 行数为 ( 上一层神经元数 + 1 ) 对于上图所示的模型，激活单元和输出分别表达为： 上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型 我们可以知道：每一个 a 的输出都是由上一层所有的 x 和每一个 x 所对应的 θ 决定的（我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )）把 x , θ, a分别用矩阵表示： 神经网络模型向量化( FORWARD PROPAGATION ) 相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值： 为了更好了了解Neuron Networks的工作原理，我们先把左半部分遮住： 右半部分其实就是以a0, a1, a2, a3 , 按照Logistic Regression的方式输出 hθ(x)： 其实神经网络就像是logistic regression，只不过我们把logistic regression中的输入向量 [x1 - x3] 变成了中间层的 [ $a_1^{(2)}$ - $a_3^{(2)}$] ,我们可以把a0, a1, a2, a3看成更为高级的特征值，也就是x0, x1, x2, x3 的进化体，并且它们是由 x 与 θ 决定的，因为是梯度下降的，所以 a 是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 x 次方厉害，也能更好的预测新数据。 这就是神经网络相比于逻辑回归和线性回归的优势。 4. 特征的直观理解从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。举例说明： 逻辑与(AND)：下图中左半部分是神经网络的设计与output层表达式，右边上部分是sigmod函数，下半部分是真值表。 其中 θ0 = -30, θ1 = 20, θ2 = 20 我们的输出函数hθ(x) 即为：hθ(x) = g(-30 + 20x1 + 20x2), g(x) 的图像是： 所以我们有：hθ(x) ≈ x1 AND x2 逻辑或(OR): 下图是神经网络的设计与output层表达式和真值表。 逻辑非(NOT)： 异或(XNOR): 5.多分类神经网络当我们有不止两种分类时（也就是 y = 1, 2, 3, … ），比如以下这种情况，该怎么办？ 如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有4个值。例如，第一个值为1或0用于预测是否是行人，第二个值用于判断是否为汽车。假如输入向量 x 有三个维度，两个中间层，输出层4个神经元分别用来表示4类，也就是每一个数据在输出层都会出现 [a b c d] ^T, 且a, b, c, d 中仅有一个为1，表示当前类。下面是该神经网络的可能结构示例： 6. 神经网络常用的激活函数什么是激活函数在神经网络中，我们经常可以看到对于某一个隐藏层的节点，该节点的激活值计算一般分为两步： （1）输入该节点的值为 x1,x2时，在进入这个隐藏节点后，会先进行一个线性变换，计算出值 $z^{[1]}=w_1x_1+w_2x_2+b^{[1]}=W^{[1]}x+b^{[1]}$，上标 1表示第 1 层隐藏层。 （2）再进行一个非线性变换，也就是经过非线性激活函数，计算出该节点的输出值(激活值) $a^{(1)}=g(z^{(1)})$ ，其中 g(z)为非线性函数。 常用的激活函数在深度学习中，常用的激活函数主要有：sigmoid函数，tanh函数，ReLU函数。 1.sigmoid函数 在逻辑回归中我们介绍过sigmoid函数，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下： sigmoid激活函数缺点： （1）当 z 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 g′(z) 将接近 0 。这会导致权重 W 的梯度将接近 0 ，使得梯度更新十分缓慢，即梯度消失。 （2）函数的输出不是以0为均值，将不便于下层的计算。sigmoid函数可用在网络最后一层，作为输出层进行二分类，尽量不要使用在隐藏层。 (3) 指数计算消耗资源 2.tanh函数 tanh函数相较于sigmoid函数要常见一些，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1)之间，其公式与图形为： tanh函数的优点 (1) tanh解决了sigmoid的输出非“零为中心”的问题。 tanh函数的缺点 （1）同sigmoid函数的第一个缺点一样，当 z 很大或很小时，g′(z) 接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。 (2) 依旧是指数计算 3.ReLU函数 ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下： ReLU函数的优点： （1）在输入为正数的时候（对于大多数输入 zz 空间来说），不存在梯度消失问题。（2） 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）ReLU函数的缺点： （1）当输入为负时，梯度为0，会产生梯度消失问题。 4.Leaky ReLU 函数 这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下： Leaky ReLU函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题。 优点: 1.神经元不会出现死亡的情况。 2.对于所有的输入，不管是大于等于0还是小于0，神经元不会饱和。 3.由于Leaky ReLU线性、非饱和的形式，在SGD中能够快速收敛。 4.计算速度要快很多。Leaky ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。 7.神经网络代价函数假设神经网络的训练样本有 m 个，每个包含一组输入x 和一组输出信号 y ，L 表示神经网络层数，Si 表示每层的neuron个数( Sl表示输出层神经元个数)，SL 代表最后一层中处理单元的个数。 将神经网络的分类定义为两种情况：二类分类和多类分类， 二类分类: SL = 1, y = 0 or 1 表示哪一类； K类分类: SL = k, yi = 1 表示分到第i类； (k &gt; 2) 这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。正则化的那一项只是排除了每一层 θ0 后，每一层的 θ 矩阵的和。最里层的循环 j 循环所有的行（由 sl +1 层的激活单元数决定），循环 i 则循环所有的列，由该层（ sl 层）的激活单元数所决定。即：hθ(x) 与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。 8.反向传播算法反向传播算法介绍在计算神经网络预测结果的时候采用了正向传播的算法，即从第一层开始向正向一层的神经元一层一层进行计算，知道最后一层的hθ(x). 为了计算代价函数的偏导数${\delta\over\delta\theta_{ij}^{(i)}} J(\theta)$ , 现在需要用到反向传播算法，以极具是首先计算最后一层的误差，然后再一层一层的反向求出各层的误差，直到倒数第二层。下面举例说明反向传播算法： 假设我们的训练集只有一个实例(x^(1), y^(1)), 我们的神经网络是一个四层的神经网络，其中: K = 4, SL = 4, L = 4 其前向传播算法为: 反向传播算法推导过程 即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。 9. 模型构建常用技巧1.梯度检验 2. 随机初始化任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的， 会导致梯度消失的情况。我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：Theta1 = rand(10, 11) * (2*eps) – ep 10. 梯度小时和梯度爆照(1)简介梯度消失与梯度爆炸 层数比较多的神经网络模型在训练的时候会出现梯度消失(gradient vanishing problem)和梯度爆炸(gradient exploding problem)问题。梯度消失问题和梯度爆炸问题一般会随着网络层数的增加变得越来越明显。 例如，对于图1所示的含有3个隐藏层的神经网络，梯度消失问题发生时，靠近输出层的hidden layer 3的权值更新相对正常，但是靠近输入层的hidden layer1的权值更新会变得很慢，导致靠近输入层的隐藏层权值几乎不变，扔接近于初始化的权值。这就导致hidden layer 1 相当于只是一个映射层，对所有的输入做了一个函数映射，这时此深度神经网络的学习就等价于只有后几层的隐藏层网络在学习。梯度爆炸的情况是：当初始的权值过大，靠近输入层的hidden layer 1的权值变化比靠近输出层的hidden layer 3的权值变化更快，就会引起梯度爆炸的问题。 (2)梯度不稳定问题 在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题。 梯度不稳定的原因：前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸。 (3)产生梯度消失的根本原因 我们以图2的反向传播为例，假设每一层只有一个神经元且对于每一层都可以用公式1表示，其中σ为sigmoid函数，C表示的是代价函数，前一层的输出和后一层的输入关系如公式1所示。我们可以推导出公式2。 图2：简单的深度神经网络 而sigmoid函数的导数σ’(x)如图3所示。 图3：sigmoid函数导数图像 可见，σ’(x)的最大值为 1/4，而我们一般会使用标准方法来初始化网络权重，即使用一个均值为0标准差为1的高斯分布。因此，初始化的网络权值通常都小于1，从而有 |σ’(z)w &lt;= 1/4|。对于2式的链式求导，层数越多，求导结果越小，最终导致梯度消失的情况出现。 (4)产生梯度爆炸的根本原因 当，也就是w比较大的情况。则前面的网络层比后面的网络层梯度变化更快，引起了梯度爆炸的问题。 (5)如何解决梯度消失和梯度爆炸 梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑以下三种方案解决： 用ReLU、Leaky ReLU、PReLU、RReLU、Maxout等替代sigmoid函数。 用Batch Normalization。 LSTM的结构设计也可以改善RNN中的梯度消失问题。 11.构建神经网络的综合步骤网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。第一层的单元数即我们训练集的特征数量。最后一层的单元数是我们训练集的结果的类的数量。如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。我们真正要决定的是隐藏层的层数和每个中间层的单元数。训练神经网络： 参数的随机初始化 利用正向传播方法计算所有的hθ(x) 编写计算代价函数 J 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数 一句话总结神经网络我认为神经网络就是通过各层神经元将变量的重新计算， 导致特征的维度被大大放大， 总之经过神经网络瞎搞之后，特征已不是简单的特征，而原理从根本还是逻辑回归，当特征数量较多，而且模型不能满足线型要求， 可以考虑使用神经网络]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 逻辑回归]]></title>
    <url>%2F2018%2F11%2F03%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归作用可以做概率预测，也可用于分类，仅能用于线性问题。通过计算真实值与预测值的概率，然后变换成损失函数，求损失函数最小值来计算模型参数，从而得出模型。 sklearn调用接口1class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, *random_state=None*, solver=’liblinear‘, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1) 参数** =&gt; penalty : str, ‘l1’ or ‘l2’ LogisticRegression和LogisticRegressionCV默认就带了正则化项。penalty参数可选择的值为”l1”和”l2”，分别对应L1的正则化和L2的正则化，默认是L2的正则化。 在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。 另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。 penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。 但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。 =&gt; dual : bool 对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False =&gt; tol : float, optional 迭代终止判据的误差范围。 =&gt; C : float, default: 1.0 C为正则化系数λ的倒数，通常默认为1。设置越小则对应越强的正则化。 =&gt; fit_intercept : bool, default: True 是否存在截距，默认存在 =&gt; intercept_scaling : float, default 1. 仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。 =&gt; class_weight : dict or ‘balanced’, default: None class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重， 或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。 如果class_weight选择**balanced**，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低；样本量越少，则权重越高。 当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y)) n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3] 0,1分别出现2次和三次 那么**class_weight**有什么作用呢？ ​ 在分类模型中，我们经常会遇到两类问题： ​ 第一种是误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。 ​ 第二种是样本是高度失衡的，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。 这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。 =&gt; random_state : int, RandomState instance or None, optional, default: None 随机数种子，默认为无，仅在正则化优化算法为sag,liblinear时有用。 =&gt; solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} solver参数决定了我们对逻辑回归损失函数的优化方法，有4种算法可以选择，分别是： a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候，SAG是一种线性收敛算法，这个速度远比SGD快。 从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear则既可以用L1正则化也可以用L2正则化。 同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了 =&gt; max_iter : int, optional 仅在正则化优化算法为newton-cg, sag and lbfgs 才有用，算法收敛的最大迭代次数。 =&gt; multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’ OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。 其他类的分类模型获得以此类推。 ​ 而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归， 得到模型参数。我们一共需要T(T-1)/2次分类。 可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。 但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。 =&gt; verbose : int, default: 0 =&gt; warm_start : bool, default: False =&gt; n_jobs : int, default: 1 如果multi_class =’ovr’“，并行数等于CPU内核数量。当“solver”设置为“liblinear”时，无论是否指定“multi_class”，该参数将被忽略。如果给定值-1，则使用所有内核。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>sklearn</tag>
        <tag>监督学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[过拟合、正则化(Regularization)]]></title>
    <url>%2F2018%2F11%2F02%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1.过拟合问题过拟合问题描述常用的机器学习优化算法，包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到过拟合(over-fitting)的问题，可能会导致它们效果很差。使用正则化技术可以减少过拟合问题。如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。下图是一个回归问题的例子： 第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。分类问题中也存在这样的问题： 就以多项式理解， 的次数越高，拟合的越好，但相应的预测的能力就可能变差。 如果我们发现了过拟合问题，应该如何处理？ 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA） 正则化。 保留所有的特征，但是减少参数的大小（magnitude）。 2.代价函数上面的回归问题中如果我们的模型是：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3 + θ_4x_4^4$ 我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了。 所以我们要做的就是在一定程度上减小这些参数 θ 的值，这就是正则化的基本方法。 我们决定要减少θ3和 θ4的大小，我们要做的便是修改代价函数，在其中 θ3和 θ4 设置一点惩罚。这样做的话，我们在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的 θ3和θ4。 修改后的代价函数如下： $min_θ {1\over 2m}[h_θ(x^{(i)} - y^{(i)})^2 + 1000θ_3^2 + 10000θ_4^2]$ 通过这样的代价函数选择出的 θ3和 θ4 对预测结果的影响就比之前要小许多， 因为为了θ3和θ4的系数很大，为了优化代价函数， 其值一定会特别的小。 假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设： $J_θ = {1\over 2m}[\sum_{i=1}^m(h_θ(x^{(i)}) - y^{(i)}) + λ\sum_{j=1}^nθ_j^2]$其中又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对 θ0 进行惩罚。 经过正则化处理的模型与原模型的可能对比如下图所示： 如果选择的正则化参数λ过大，则会把所有的参数都最小化了，导致模型变成 $h_θ(x) = θ_0$，也就是上图中红色直线所示的情况，造成欠拟合。 那为什么增加的一项 $λ\sum_{j=1}^nθ_j^2$ 可以使的值减小呢？ 因为如果我们令 λ的值很大的话，为了使Cost Function 尽可能的小，所有的 θ 的值（不包括θ0） 都会在一定程度上减小。 但若λ的值太大了，那么不包括θ0）都会趋近于0，这样我们所得到的只能是一条平行于轴的直线。 所以对于正则化，我们要取一个合理的 λ 的值，这样才能更好的应用正则化。 回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。 3.正则化线性回归对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。正则化线性回归的代价函数为： $J_{(θ)} = {1\over 2m} \sum_{i=1}^m[(h_θ(x^{(i)})-y^{(i)})^2 + λ\sum_{j=1}^nθ_j^2]$ 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对θ进行正则化，所以梯度下降算法将分两种情形： 4.正规方程逻辑回归针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：我们首先学习了使用梯度下降法来优化代价函数J(θ)，接下来学习了更高级的优化算法，这些高级优化算法需要你自己设计代价函数J(θ)。 用Python实现 123456789import numpy as npdef costReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X*theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X*theta.T))) reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2)) return np.sum(first ‐ second) / (len(X)) + reg 要最小化该代价函数，通过求导，得出梯度下降算法为： 注：看上去同线性回归一样，但是知道 $h_θ(x) = g(θ^TX)$ ，所以与线性回归不同。 Octave 中，我们依旧可以用fminuc 函数来求解代价函数最小化的参数，值得注意的是θ0参数的更新规则与其他情况不同。 注意： 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者hθ(x)的不同所以还是有很大差别。 θ不参与其中的任何一个正则化。 5.其他防止过拟合的方法DropoutDropout是在深度学习中降低过拟合风险的常见方法，它是由大牛Hinton老爷子提出的。老爷子认为在神经网络产生过拟合主要是因为神经元之间的协同作用产生的。因此老爷子在神经网络进行训练的时候，让部分神经元失活，这样就阻断了部分神经元之间的协同作用，从而强制要求一个神经元和随机挑选出的神经元共同进行工作，减轻了部分神经元之间的联合适应性。Dropout的具体流程如下： 1.对 l 层第 j 个神经元产生一个随机数 $r^{(l)}_j∼Bernouli(p)$ 2.将 l 层第 j 个神经元的输入乘上产生的随机数作为这个神经元新的输入：$x^{(l)∗}_j=x^{(l)}_j∗r^{(l)}_j$ 3.此时第 l 层第 j 个神经元的输出为：$y{(l+1)}j=f(∑^k{j=1}(w^{(l+1)}_j ∗ x^{(l)∗}_j + b^{(l+1)}))$其中，k为第l层神经元的个数，f为该神经元的激活函数，b为偏置，w为权重向量。注意：当我们采用了Dropout之后再训练结束之后，应当将网络的权重乘上概率p得到测试网络的权重，或者可以在训练时，将样本乘上 1/P。 提前终止在对模型进行训练时，我们可以将我们的数据集分为三个部分，训练集、验证集、测试集。我们在训练的过程中，可以每隔一定量的step，使用验证集对训练的模型进行预测，一般来说，模型在训练集和验证集的损失变化如下图所示： 可以看出，模型在验证集上的误差在一开始是随着训练集的误差的下降而下降的。当超过一定训练步数后，模型在训练集上的误差虽然还在下降，但是在验证集上的误差却不在下降了。此时我们的模型就过拟合了。因此我们可以观察我们训练模型在验证集上的误差，一旦当验证集的误差不再下降时，我们就可以提前终止我们训练的模型。 增加样本量在实际的项目中，你会发现，上面讲述的那些技巧虽然都可以减轻过拟合的风险，但是却都比不上增加样本量来的更实在。 为什么增加样本可以减轻过拟合的风险呢？这个要从过拟合是啥来说。过拟合可以理解为我们的模型对样本量学习的太好了，把一些样本的特殊的特征当做是所有样本都具有的特征。举个简单的例子，当我们模型去训练如何判断一个东西是不是叶子时，我们样本中叶子如果都是锯齿状的话，如果模型产生过拟合了，会认为叶子都是锯齿状的，而不是锯齿状的就不是叶子了。如果此时我们把不是锯齿状的叶子数据增加进来，此时我们的模型就不会再过拟合了。因此其实上述的那些技巧虽然有用，但是在实际项目中，你会发现，其实大多数情况都比不上增加样本数据来的实在。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 线型回归]]></title>
    <url>%2F2018%2F10%2F31%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Fsklearn%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[简单线性回归线性回归是数据挖掘中的基础算法之一，从某种意义上来说，在学习函数的时候已经开始接触线性回归了，只不过那时候并没有涉及到误差项。线性回归的思想其实就是解一组方程，得到回归函数，不过在出现误差项之后，方程的解法就存在了改变，一般使用最小二乘法进行计算。 使用sklearn.linear_model.LinearRegression进行线性回归sklearn对Data Mining的各类算法已经有了较好的封装，基本可以使用fit、predict、score来训练、评价模型，并使用模型进行预测，一个简单的例子如下： 123456789from sklearn import linear_modelclf = linear_model.LinearRegression()X = [[0,0],[1,1],[2,2]]y = [0,1,2]clf.fit(X,y)print(clf.coef_)[ 0.5 0.5]print(clf.intercept_)1.11022302463e-16 LinearRegression已经实现了多元线性回归模型，当然，也可以用来计算一元线性模型，通过使用list[list]传递数据就行。下面是LinearRegression的具体说明。 使用方法实例化sklearn一直秉承着简洁为美得思想设计着估计器，实例化的方式很简单，使用clf = LinearRegression()就可以完成，但是仍然推荐看一下几个可能会用到的参数： fit_intercept：是否存在截距，默认存在 normalize：标准化开关，默认关闭 还有一些参数感觉不是太有用，就不再说明了，可以去官网文档中查看。 回归其实在上面的例子中已经使用了fit进行回归计算了，使用的方法也是相当的简单。 fit(X,y,sample_weight=None)：X,y以矩阵的方式传入，而sample_weight则是每条测试数据的权重，同样以array格式传入。 predict(X)：预测方法，将返回预测值y_pred score(X,y,sample_weight=None)：评分函数，将返回一个小于1的得分，可能会小于0 方程LinearRegression将方程分为两个部分存放，coef_存放回归系数，intercept_则存放截距，因此要查看方程，就是查看这两个变量的取值。 多项式回归其实，多项式就是多元回归的一个变种，只不过是原来需要传入的是X向量，而多项式则只要一个x值就行。通过将x扩展为指定阶数的向量，就可以使用LinearRegression进行回归了。sklearn已经提供了扩展的方法——sklearn.preprocessing.PolynomialFeatures。利用这个类可以轻松的将x扩展为X向量，下面是它的使用方法： 123456789&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; X_train = [[1],[2],[3],[4]]&gt;&gt;&gt; quadratic_featurizer = PolynomialFeatures(degree=2)&gt;&gt;&gt; X_train_quadratic = quadratic_featurizer.fit_transform(X_train)&gt;&gt;&gt; print(X_train_quadratic)[[ 1 1 1] [ 1 2 4] [ 1 3 9] [ 1 4 16]] 经过以上处理，就可以使用LinearRegression进行回归计算了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>sklearn</tag>
        <tag>线型回归</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归 & 分类问题]]></title>
    <url>%2F2018%2F10%2F31%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1. 分类问题在分类问题中，如果想要预测的变量y是离散的值，那么可以只用一种叫做逻辑回归(logistic regression)的算法,这是在目前最流行的一种学习算法之一。 在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。 我们将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈0,1 ，其中 0 表示负向类，1 表示正向类。 线型回归不适合分类问题如果我们要用线性回归算法来解决一个分类问题，对于分类， y 取值为 0 或者1，但如果你使用的是线性回归，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签 y 都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。 逻辑回归算法逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y 取值离散的情况，如：1 0 0 1。 2. 逻辑回归表达式在分类问题中，要用什么样的函数来表示我们的假设。在解决分类问题的时候，我们希望分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。 为什么线型回归不适合分类问题?回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线： 根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测： 当 $h_θ(x) &gt;= 0.5$ 时, 预测y= 1; 当$h_θ(x) &lt; 0.5$ 时，预测y = 0 对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。但是， 假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。 这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了， 所以使用线型回归模型，很难确定断定一个问题分类的边界。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决分类这样的问题。 逻辑回归模型我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是： $h_θ(x) = g(θ^TX)$ 函数g的表达式为: $g(z) = {1\over1+e^{-z}}$ 其中: X 代表特征向量, g代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function） g(z) 的函数图像为: 对逻辑回归模型理解 $h_θ(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算出输出的变量=1 的可能性, 即$h_θ(x) =P(y=1|x; θ)$ 例如, 如果对于给定的x， 通过已经确定的参数计算出$h_θ(x) = 0.7$ ， 则表示有70%的几率y为正向类，响应地y为负向类的几率为1-0.7=0.3 g(z) （Sigmoid function）函数的作用就是将$θ^TX$的结果映射到0-1之间， 3. 决策边界对决策边界的理解 决策边界(decision boundary)，能够更好地帮助我们理解逻辑回归的假设函数中计算什么 在逻辑回归中， 我们预测到: 当$h_θ(x) &gt;= 0.5$ 时， 预测 y= 1; 当 $h_θ(x) &gt;= 0.5$ 时，预测 y = 0； 根据上面绘制的S形函数图像，我们知道当 z = 0 时, g(z) = 0.5 z &gt; 时, g(z) &gt; 0.5 z &lt; 0 时, g(z) &lt; 0.5 又 $z = θ^Tx$, 即: $θ^TX &gt;= 0$ 时, 预测 y = 1 , $θ^TX &lt; 0$ 时， 预测 y = 0 现在假设我们有一个模型: 并且参数θ是向量[-3 1 1]. 则当 -3 + x1 + x2 &gt;= 0, 即x1 + x2 &gt;= 3时， 模型将预测 y = 1. 我们可以绘制直线x1 + x2 = 3, 这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。 复杂形状的决策边界假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？ 因为需要用曲线才能分隔 y = 0 的区域和 y = 1 的区域，我们需要二次方特征： 所以按照假设函数应该趋近， $h_θ(x) = g(θ_0 + θ_1x_1 + θ_2x_2 + θ_1x_1^2 + θ_2x_2^2 )$ θ最终结果趋近 [-1 0 0 1 1], 我们最终得到的判定边界恰好是圆点在原点且半径为1的圆形 , 综上我们可以用复杂的模型来适应飞象复杂形状的判定边界 4. 逻辑回归代价函数和梯度下降逻辑回归代价函数及简化对于线性回归模型，我们定义的代价函数是所有模型误差的平方和和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将 $h_θ(x) = g(θ^TX)$ 带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）如下图，这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 线型回归代价函数为: $J_{(θ)} = {1\over m} \sum_{i=1}^m{1\over2}(h_θ(x^{(i)})-y^{(i)})^2​$ 重新定义逻辑回归代价函数，使代价函数在逻辑回归算法中为凸函数, 过程如下: 根据推倒可以证明, 我们推倒的的代价值函数会给我们一个凸优化问题。代价函数 J(θ) 会是一个凸函数，并且没有局部最优值。 python代码实现 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(‐y, np.log(sigmoid(X* theta.T))) second = np.multiply((1 ‐ y), np.log(1 ‐ sigmoid(X* theta.T))) return np.sum(first ‐ second) / (len(X)) 梯度下降算法推倒及简化在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为： Repeat { $θ_j : =θ_j − α{∂\over∂θ_j }J(θ)$ (simultaneously update all ) } 求导后得到: Repeat { $θj : =θ_j − α{1\over m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$ (simultaneously update all ) } 注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的 $h_θ(x) = g(θ^TX)$ 与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。 5. 高级优化算法一些高级算法的介绍现在我们换个角度来看，什么是梯度下降，我们有个代价函数 J(θ)，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数θ时，它们会计算出两样东西：J(θ) 以及 J 等于 0、1直到 n 时的偏导数项。 假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。 另一种考虑梯度下降的思路是：我们需要写出代码来计算 J(θ) 和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。 对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数J(θ)。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控这些 J(θ)的收敛性，那么我们就需要自己编写代码来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$。所以，在写完能够计算这两者的代码之后，我们就可以使用梯度下降。 然而梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数J(θ)和偏导数项$α{∂\over∂θ_j }J(θ)$两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轭梯度法、 BFGS (变尺度法)、 L-BFGS (限制变尺度法) 就是其中一些更高级的优化算法，它们需要有一种方法来计算 J(θ)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。 这三种算法的优点： 一个是使用这其中任何一个算法，你通常不需要手动选择学习率 α ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 α ，并自动选择一个好的学习速率 α，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。 Octave 有一个非常理想的库用于实现这些先进的优化算法，所以，如果你直接调用它自带的库，你就能得到不错的结果。我必须指出这些算法实现得好或不好是有区别的，因此，如果你正在你的机器学习程序中使用一种不同的语言，比如如果你正在使用C、C++、Java等等，你可能会想尝试一些不同的库，以确保你找到一个能很好实现这些算法的库。 如何使用这些算法比方说，你有一个含两个参数的问题，这两个参数是θ1和θ2，因此，通过这个代价函数，你可以得到 θ1 和 θ2 的值，如果你将 J(θ) 最小化的话，那么它的最小值将是 θ1 = 5 ，θ2 = 5 。代价函数 J(θ) 的导数推出来就是这两个表达式： $α{∂\over∂θ_1}J(θ) = 2(θ_1 - 5)$ $α{∂\over∂θ_2}J(θ) = 2(θ_2 - 5)$ 如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的Octave 函数： 123456function [jVal, gradient]=costFunction(theta) jVal=(theta(1)‐5)^2+(theta(2)‐5)^2; gradient=zeros(2,1); gradient(1)=2*(theta(1)‐5); gradient(2)=2*(theta(2)‐5);end 这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个costFunction 函数后，你就可以调用高级的优化函数，这个函数叫fminunc，它表示Octave 里无约束最小化函数。调用它的方式如下： 123options=optimset('GradObj','on','MaxIter',100);initialTheta=zeros(2,1);[optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options); 你要设置几个options，这个 options 变量作为一个数据结构可以存储你想要的options，所以 GradObj 和On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说100，我们给出一个 的猜测初始值，它是一个2×1的向量，那么这个命令就调用fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的值。 实际运行过程示例 6. 多分类问题多分类的介绍一些多分类的例子: 例一: 假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，或者说可以自动地加上标签，那么，你也许需要一些不同的文件夹，或者不同的标签来完成这件事，来区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，我们就有了这样一个分类问题：其类别有四个，分别用 y=1, y=2, y=3, y=4 来表示 例二: 如果一个病人因为鼻塞来到你的诊所，他可能并没有生病，用 y=1 这个类别来代表；或者患了感冒，用 y=2 来代表；或者得了流感用 y=3 来代表. 然而对于之前的一个，二元分类问题，我们的数据看起来可能是像这样： 对于一个多类分类问题，我们的数据集或许看起来像这样： 一对于多分类思路我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。下面将介绍如何进行一对多的分类工作，有时这个方法也被称为“一对余”方法。 现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 y = 1 ，方框表示 y = 2，叉叉表示 y = 3。下面要做的就是使用一个训练集，将其分成3个二元分类问题。我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。 这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。 为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作$h_θ^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作$h_θ^{(2)}(x)$ ,依此类推。 最后我们得到一系列的模型简记为：$h_θ^{(i)}(x) = P(y=i|x; θ)$ 其中：i = (1, 2, 3, 4, 5, …, k) k为总分类数量最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_θ^{(i)}(x)$ ， 其中 i 对应每一个可能的y=i，最后，为了做出预测，我们给出输入一个新的 x 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 x ，然后我们选择一个让 $h_θ^{(i)}(x)$ 最大的 i 作为最终的分类结果。现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论值是多少，我们都有最高的概率值，我们预测就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Octave基础操作]]></title>
    <url>%2F2018%2F10%2F26%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Foctave%20%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[基础功能命令修改命令行的提示1PS1('&gt;&gt; ') % &gt;&gt; 就是修改后的提示符号 变量赋值语句1A = 5 + 6 ; % 后面加上一个可以不在终端打印变量的值 显示工作空间的所有变量12who % 显示工作空间的所有变量whos % 显示工作空间的所有变量和详细信息 删除变量12clear A % 删除变量Aclear % 删除所有变量 打印变量12A % 直接再终端输入变量名称就可以将变量的值打印出来disp(A) % 通过disp函数将变量打印出来 修改全局的输出内容的长短12format long % 将输出数值的长度定义为long类型format short % 将输出数值的长度定义为short类型 查看命令的帮助信息123helo randhelp eyehelp help 添加搜索路径1addpath path % 添加路径到函数和数据等的某人搜索路径 基础运算数值运算13-2； 5*8； 1/2； % 基础运算 逻辑运算12341 &amp;&amp; 0 % 逻辑与1 || 0 % 逻辑或~ 1 % 逻辑费XOR(1, 0) % 异或运算 判断语句121 == 2 % 相等判断1 ~= 2 % 不等于判断 矩阵运算创建矩阵1234567891011A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵v = [ 1 2 3 ] % 创建一个行向量v = 1:6 % 创建一个从1到6的行向量v = 1:0.1:2 % 创建一个从1开始，以0.1为步长，直到2的行向量v = ones(2, 3) % 创建一个2行3列的元素都是1的矩阵v = 2 * ones(2, 3) % 创建一个2行3列的元素都是2的矩阵v = zeros(2, 3) % 创建一个2行3列的元素都是0的矩阵v = rand(2, 3) % 创建一个2行3列的元素都是0-1之间的随机数的矩阵v = randn(2, 3) % 创建一个2行3列的元素都符合平局值为0,1为标准差的正态分布随机数的矩阵I = eye(6) % 创建一个大小为6的单位矩阵v = type(3) % 返回一个3*3的随机矩阵 矩阵运算12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵B = [ 11, 12; 13 14; 15 16 ]2 * A % A矩阵中的每个元素都乘以2A .* B % 将A矩阵和B矩阵中的每一个对应位置的元素相乘,同样适用于+ - / ^等A .* % A矩阵的每个元素取二次方log(v) % 矩阵的每个院对对数运算exp(v) % 矩阵的每个元素进行以为底，以这些元素为幂的运算abs(v) % 对v矩阵的每个元素取绝对值A + 1 % 将A矩阵的每个元素加上1A&apos; % 取A矩阵的转置矩阵 获取矩阵尺寸12345A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵size(A) % 返回A矩阵的尺寸,返回的内容同样是行向量size(A，1) % 返回矩阵的行数size(A，2) % 返回矩阵的列数lengh(A) % 返回A矩阵的维度大小，即矩阵的长宽中比较大的维度，矩阵A的length即为3 矩阵的索引12345678910A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵A(3, 2) % 取A矩阵的第三行第二列的元素A(2, :) % 返回第二行的所有元素A(:, 2) % 返回第二列的所有元素A([1 3], :) % 取第1行和第3行的所有元素A(:, 2) = [10; 11; 12] % 将矩阵的第二列重新赋值A = [A, [100, 101, 102]] % 给矩阵附加了一列矩阵，给矩阵增加一列。带，和不带，相同[A B] % 两个矩阵直接连在一起，矩阵A在左边，矩阵B在右边[A;B] % 两个矩阵直接连在一起，矩阵A在上边, 矩阵B在下边A(:) % 将矩阵的所有元素导向一个单独的列向量排列起来 矩阵的计算123456789101112131415A = [ 1 2; 3 4; 5 6 ] % 创建一个三行两列的矩阵val = max(a) % 返回a矩阵中的最大值, 当a为多维矩阵的时候返回的是每一列的最大值val = max(A, a) % 两个矩阵所有元素逐个比较返回较大的值max(A,[],1) % 得到矩阵每一列元素的最大值max(A,[],2) % 得到矩阵每一行元素的最大值[val, ind] = max(a) % 返回a矩阵中的最大值和对应的索引sum（a) % 把a矩阵的所有元素相加起来, 如果是多维矩阵默认为每一列的总和sum（a,1) % 求多维矩阵每一列的总和sum(a, 3) % 求多维矩阵每一行的总和a&lt;3 % 对矩阵每一个元素进行&lt;3判断，判断为真返回1, 否则返回0find(a&lt;3) % 返回哪些元素小于3(是索引值)prod（a) % 将a矩阵的所有元素相乘floor(a) % 将a矩阵的所有元素进行向下取舍ceil(a) % 将a矩阵的所有元素进行向上取整pinv(v) % 求v矩阵的逆矩阵 绘制图像绘制直方图123w = -6 + sqrt(10) * (randn(1, 10000))hist(w) % 绘制w矩阵的直方图hist(w, 50) % 绘制w矩阵的直方图，并指定50个长方形 绘制曲线图1234567891011121314t = [0:0.1:1];y1 = sin(2*pi*4t);plot(t, y1);hold on % 新绘制的图不会覆盖之前的图片，会在之前的图片上生成新的曲线y2 = cos(2*pi*4*t)plot(t, y2, &apos;r&apos;) % 绘制新的直线图，r表示线的颜色为红色xlable(&apos;time&apos;) % 添加x轴名称ylable(&apos;value&apos;) % 给y轴添加名称legend(&apos;sin, &apos;cos&apos;) % 给线命名title(&apos;myplot&apos;) % 给图片一个标题名称print -dpng &apos;myplot.png&apos; % 输出图片plot clos % 关掉图片axis([0.5 1 ‐1 1]) % 改变坐标返回，前两个参数控制横坐标，后两参数控制纵坐标Clf % 清除一个图像 在一张画纸上绘制两张直线图123456789t = [0:0.1:1];y1 = sin(2*pi*4t);y2 = cos(2*pi*4*t)；figure(1); plot(t, y1); % 绘制第一张图片figure(2); plot(t, y2); % 绘制第二张图片subplot(1,2,1) %将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。plot(t,y1) % 将图片绘制到第一个格子suplot(1,2,2) % 使用第二个格子plot(t,y2) % 将图片绘制到第二个格子 彩色格图绘制12imagesc(A ) % 绘制彩色格子图imagesc(A)，colorbar，colormap gray % 生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。 移动数据导入数据1load('featureD.dat') % 加载featureD.dat中的所有数据，并将其复制给变量featureD 导出数据12save hello.mat v % 将变量A导出为一个叫hello.mat文件 二进制形式save hello.mat v -ascii % 将变量A导出为一个叫hello.mat文件 ascii形式 控制语句for循环1234v = zeros(10,1)for i=1:10, v(i) = 2^1;end while 循环123456v = zeros(10,1)i = 1while i &lt;= 5, v(i) = 100; i = i+1;end if - else - elif 语句12345678v = zeros(10,1)if v(1) == 1, disp('1');elseif v(1) == 2, disp('2');else disp('3');end 自定义函数定义函数1.先创建一个文件​ squarethisnumber.m # .m前定义的就是函数名2.编写函数文件 12function y = squareThisNumber(x)y = x^2; 第一行写着 function y = squareThisNumber(x) ，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 里。另外，它告诉了Octave这个函数有一个参数，就是参数 ，还有定义的函数体，y = x^2 使用自定义函数1.切换到函数文件所在目录2.直接通过函数名squareThisNumber() 调用函数]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubeadm创建高可用k8s集群]]></title>
    <url>%2F2018%2F10%2F25%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2Fkubeadm%E5%88%9B%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8kubernetes%E9%9B%86%E7%BE%A4-%20v1.12%2F</url>
    <content type="text"><![CDATA[kubeadm创建高可用kubernetes v1.12.0集群节点规划主机名IPRolek8s-master0110.3.1.20etcd、Master、Node、keepalivedk8s-master0210.3.1.21etcd、Master、Node、keepalivedk8s-master0310.3.1.25etcd、Master、Node、keepalivedVIP10.3.1.29None版本信息：OS:：Ubuntu 16.04Docker：17.03.2-cek8s：v1.12来自官网的高可用架构图高可用最重要的两个组件：etcd：分布式键值存储、k8s集群数据中心。kube-apiserver：集群的唯一入口，各组件通信枢纽。apiserver本身无状态，因此分布式很容易。其它核心组件：controller-manager和scheduler也可以部署多个，但只有一个处于活跃状态，以保证数据一致性。因为它们会改变集群状态。集群各组件都是松耦合的，如何高可用就有很多种方式了。kube-apiserver有多个，那么apiserver客户端应该连接哪个了，因此就在apiserver前面加个传统的类似于haproxy+keepalived方案漂个VIP出来，apiserver客户端，比如kubelet、kube-proxy连接此VIP。安装前准备1、k8s各节点SSH免密登录。2、时间同步。3、各Node必须关闭swap：swapoff -a，否则kubelet启动失败。4、各节点主机名和IP加入/etc/hosts解析kubeadm创建高可用集群有两种方法：etcd集群由kubeadm配置并运行于pod，启动在Master节点之上。etcd集群单独部署。etcd集群单独部署，似乎更容易些，这里就以这种方法来部署。部署etcd集群etcd的正常运行是k8s集群运行的提前条件，因此部署k8s集群首先部署etcd集群。安装CA证书安装CFSSL证书管理工具直接下载二进制安装包：wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /opt/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /opt/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /opt/bin/cfssl-certinfoecho &quot;export PATH=/opt/bin:$PATH&quot; &gt; /etc/profile.d/k8s.sh所有k8s的执行文件全部放入/opt/bin/目录下创建CA配置文件root@k8s-master01:~# mkdir sslroot@k8s-master01:~# cd ssl/root@k8s-master01:~/ssl# cfssl print-defaults config &gt; config.jsonroot@k8s-master01:~/ssl# cfssl print-defaults csr &gt; csr.json# 根据config.json文件的格式创建如下的ca-config.json文件# 过期时间设置成了 87600hroot@k8s-master01:~/ssl# cat ca-config.json{&quot;signing&quot;: {&quot;default&quot;: {&quot;expiry&quot;: &quot;87600h&quot;},&quot;profiles&quot;: {&quot;kubernetes&quot;: {&quot;usages&quot;: [&quot;signing&quot;,&quot;key encipherment&quot;,&quot;server auth&quot;,&quot;client auth&quot;],&quot;expiry&quot;: &quot;87600h&quot;}}}}创建CA证书签名请求root@k8s-master01:~/ssl# cat ca-csr.json{&quot;CN&quot;: &quot;kubernetes&quot;,&quot;key&quot;: {&quot;algo&quot;: &quot;rsa&quot;,&quot;size&quot;: 2048},&quot;names&quot;: [{&quot;C&quot;: &quot;CN&quot;,&quot;ST&quot;: &quot;GD&quot;,&quot;L&quot;: &quot;SZ&quot;,&quot;O&quot;: &quot;k8s&quot;,&quot;OU&quot;: &quot;System&quot;}]}生成CA证书和私匙root@k8s-master01:~/ssl# cfssl gencert -initca ca-csr.json | cfssljson -bare caroot@k8s-master01:~/ssl# ls ca*ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem拷贝ca证书到所有Node相应目录root@k8s-master01:~/ssl# mkdir -p /etc/kubernetes/sslroot@k8s-master01:~/ssl# cp ca* /etc/kubernetes/sslroot@k8s-master01:~/ssl# scp -r /etc/kubernetes 10.3.1.21:/etc/root@k8s-master01:~/ssl# scp -r /etc/kubernetes 10.3.1.25:/etc/下载etcd文件：有了CA证书后，就可以开始配置etcd了。root@k8s-master01:$ wget https://github.com/coreos/etcd/releases/download/v3.2.22/etcd-v3.2.22-linux-amd64.tar.gzroot@k8s-master01:$ cp etcd etcdctl /opt/bin/对于K8s v1.12，其etcd版本不能低于3.2.18创建etcd证书创建etcd证书签名请求文件root@k8s-master01:~/ssl# cat etcd-csr.json{&quot;CN&quot;: &quot;etcd&quot;,&quot;hosts&quot;: [&quot;127.0.0.1&quot;,&quot;10.3.1.20&quot;,&quot;10.3.1.21&quot;,&quot;10.3.1.25&quot;],&quot;key&quot;: {&quot;algo&quot;: &quot;rsa&quot;,&quot;size&quot;: 2048},&quot;names&quot;: [{&quot;C&quot;: &quot;CN&quot;,&quot;ST&quot;: &quot;GD&quot;,&quot;L&quot;: &quot;SZ&quot;,&quot;O&quot;: &quot;k8s&quot;,&quot;OU&quot;: &quot;System&quot;}]}#特别注意：上述host的字段填写所有etcd节点的IP，否则会无法启动。生成etcd证书和私钥root@k8s-master01:~/ssl# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \&gt; -ca-key=/etc/kubernetes/ssl/ca-key.pem \&gt; -config=/etc/kubernetes/ssl/ca-config.json \&gt; -profile=kubernetes etcd-csr.json | cfssljson -bare etcd2018/10/01 10:01:14 [INFO] generate received request2018/10/01 10:01:14 [INFO] received CSR2018/10/01 10:01:14 [INFO] generating key: rsa-20482018/10/01 10:01:15 [INFO] encoded CSR2018/10/01 10:01:15 [INFO] signed certificate with serial number 3799037537572865692760814739597034116518223703002018/02/06 10:01:15 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).root@k8s-master:~/ssl# ls etcd*etcd.csr etcd-csr.json etcd-key.pem etcd.pem# -profile=kubernetes 这个值根据 -config=/etc/kubernetes/ssl/ca-config.json 文件中的profiles字段而来。拷贝证书到所有节点对应目录：root@k8s-master01:~/ssl# cp etcd*.pem /etc/etcd/sslroot@k8s-master01:~/ssl# scp -r /etc/etcd 10.3.1.21:/etc/etcd-key.pem 100% 1675 1.5KB/s 00:00etcd.pem 100% 1407 1.4KB/s 00:00root@k8s-master01:~/ssl# scp -r /etc/etcd 10.3.1.25:/etc/etcd-key.pem 100% 1675 1.6KB/s 00:00etcd.pem 100% 1407 1.4KB/s 00:00创建etcd的Systemd unit 文件证书都准备好后就可以配置启动文件了root@k8s-master01:~# mkdir -p /var/lib/etcd #必须先创建etcd工作目录root@k8s-master:~# cat /etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confExecStart=/opt/bin/etcd \--name=etcd-host0 \--cert-file=/etc/etcd/ssl/etcd.pem \--key-file=/etc/etcd/ssl/etcd-key.pem \--peer-cert-file=/etc/etcd/ssl/etcd.pem \--peer-key-file=/etc/etcd/ssl/etcd-key.pem \--trusted-ca-file=/etc/kubernetes/ssl/ca.pem \--peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \--initial-advertise-peer-urls=https://10.3.1.20:2380 \--listen-peer-urls=https://10.3.1.20:2380 \--listen-client-urls=https://10.3.1.20:2379,http://127.0.0.1:2379 \--advertise-client-urls=https://10.3.1.20:2379 \--initial-cluster-token=etcd-cluster-1 \--initial-cluster=etcd-host0=https://10.3.1.20:2380,etcd-host1=https://10.3.1.21:2380,etcd-host2=https://10.3.1.25:2380 \--initial-cluster-state=new \--data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target启动etcdroot@k8s-master01:~/ssl# systemctl daemon-reloadroot@k8s-master01:~/ssl# systemctl enable etcdroot@k8s-master01:~/ssl# systemctl start etcd把etcd启动文件拷贝到另外两台节点，修改下配置就可以启动了。查看集群状态：由于etcd使用了证书，所以etcd命令需要带上证书：#查看etcd成员列表root@k8s-master01:~# etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/kubernetes/ssl/ca.pem member list702819a30dfa37b8: name=etcd-host2 peerURLs=https://10.3.1.20:2380 clientURLs=https://10.3.1.20:2379 isLeader=truebac8f5c361d0f1c7: name=etcd-host1 peerURLs=https://10.3.1.21:2380 clientURLs=https://10.3.1.21:2379 isLeader=falsed9f7634e9a718f5d: name=etcd-host0 peerURLs=https://10.3.1.25:2380 clientURLs=https://10.3.1.25:2379 isLeader=false#或查看集群是否健康root@k8s-maste01:~/ssl# etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/kubernetes/ssl/ca.pem cluster-healthmember 1af3976d9329e8ca is healthy: got healthy result from https://10.3.1.20:2379member 34b6c7df0ad76116 is healthy: got healthy result from https://10.3.1.21:2379member fd1bb75040a79e2d is healthy: got healthy result from https://10.3.1.25:2379cluster is healthy安装Dockerv1.12已验证的dcoker版本已达18.06，此前的版本是17.03.apt-get updateapt-get install \apt-transport-https \ca-certificates \curl \software-properties-commoncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -apt-key fingerprint 0EBFCD88add-apt-repository \&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \$(lsb_release -cs) \stable&quot;apt-get updateapt-get install -y docker-ce=17.03.2~ce-0~ubuntu-xenial安装完Docker后，设置FORWARD规则为ACCEPT#默认为DROPiptables -P FORWARD ACCEPT安装kubeadm工具所有节点都需要安装kubeadmapt-get update &amp;&amp; apt-get install -y apt-transport-https curlcurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -echo 'deb http://apt.kubernetes.io/ kubernetes-xenial main' &gt;/etc/apt/sources.list.d/kubernetes.listapt-get updateapt-get install -y kubeadm#它会自动安装kubeadm、kubectl、kubelet、kubernetes-cni、socat安装完后，设置kubelet服务开机自启：systemctl enable kubelet必须设置Kubelet开机自启动，才能让k8s集群各组件在系统重启后自动运行。集群初始化接下开始在三台master执行集群初始化。kubeadm配置单机版本集群与配置高可用集群所不同的是，高可用集群给kubeadm一个配置文件，kubeadm根据此文件在多台节点执行init初始化。编写kubeadm配置文件root@k8s-master01:~/kubeadm-config# cat kubeadm-config.yamlapiVersion: kubeadm.k8s.io/v1alpha3kind: ClusterConfigurationkubernetesVersion: stablenetworking:podSubnet: 192.168.0.0/16apiServerCertSANs:- k8s-master01- k8s-master02- k8s-master03- 10.3.1.20- 10.3.1.21- 10.3.1.25- 10.3.1.29- 127.0.0.1etcd:external:endpoints:- https://10.3.1.20:2379- https://10.3.1.21:2379- https://10.3.1.25:2379caFile: /etc/kubernetes/ssl/ca.pemcertFile: /etc/etcd/ssl/etcd.pemkeyFile: /etc/etcd/ssl/etcd-key.pemdataDir: /var/lib/etcdtoken: 547df0.182e9215291ff27ftokenTTL: &quot;0&quot;root@k8s-master01:~/kubeadm-config#配置解析：版本v1.12的api版本已提升为kubeadm.k8s.io/v1alpha3，kind已变成ClusterConfiguration。podSubnet：自定义pod网段。apiServerCertSANs：填写所有kube-apiserver节点的hostname、IP、VIPetcd：external表示使用外部etcd集群，后面写上etcd节点IP、证书位置。如果etcd集群由kubeadm配置，则应该写local，加上自定义的启动参数。token：可以不指定，使用指令 kubeadm token generate 生成。第一台master上执行init#确保swap已关闭root@k8s-master01:~/kubeadm-config# kubeadm init --config kubeadm-config.yaml输出如下信息：#kubernetes v1.12.0开始初始化[init] using Kubernetes version: v1.12.0#初始化之前预检[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection#可以在init之前用kubeadm config images pull先拉镜像[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'#生成kubelet服务的配置[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[preflight] Activating the kubelet service#生成证书[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s-master01 k8s-master02 k8s-master03] and IPs [10.96.0.1 10.3.1.20 10.3.1.20 10.3.1.21 10.3.1.25 10.3.1.29 127.0.0.1][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[certificates] Generated sa key and public key.#生成kubeconfig[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;#生成要启动Pod清单文件[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;#启动Kubelet服务，读取pod清单文件/etc/kubernetes/manifests[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;#根据清单文件拉取镜像[init] this might take a minute or longer if the control plane images have to be pulled#所有组件启动完成[apiclient] All control plane components are healthy after 27.014452 seconds#上传配置kubeadm-config&quot; in the &quot;kube-system&quot;[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster#给master添加一个污点的标签taint[markmaster] Marking the node k8s-master01 as master by adding the label &quot;node-role.kubernetes.io/master=''&quot;[markmaster] Marking the node k8s-master01 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule][patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;k8s-master01&quot; as an annotation#使用的token[bootstraptoken] using token: w79yp6.erls1tlc4olfikli[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace#最后安装基础组件kube-dns和kube-proxy daemonset[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user:mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root:#记录下面这句，在其它Node加入时用到。kubeadm join 10.3.1.20:6443 --token w79yp6.erls1tlc4olfikli --discovery-token-ca-cert-hash sha256:7aac9eb45a5e7485af93030c3f413598d8053e1beb60fb3edf4b7e4fdb6a9db2根据提示执行：root@k8s-master01:~# mkdir -p $HOME/.kuberoot@k8s-master01:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configroot@k8s-master01:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config此时有一台了，且状态为&quot;NotReady&quot;root@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 NotReady master 3m50s v1.12.0root@k8s-master01:~#查看第一台Master核心组件运行为Podroot@k8s-master01:~# kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcoredns-576cbf47c7-2dqsj 0/1 Pending 0 4m29s &lt;none&gt; &lt;none&gt; &lt;none&gt;coredns-576cbf47c7-7sqqz 0/1 Pending 0 4m29s &lt;none&gt; &lt;none&gt; &lt;none&gt;kube-apiserver-k8s-master01 1/1 Running 0 3m46s 10.3.1.20 k8s-master01 &lt;none&gt;kube-controller-manager-k8s-master01 1/1 Running 0 3m40s 10.3.1.20 k8s-master01 &lt;none&gt;kube-proxy-dpvkk 1/1 Running 0 4m30s 10.3.1.20 k8s-master01 &lt;none&gt;kube-scheduler-k8s-master01 1/1 Running 0 3m37s 10.3.1.20 k8s-master01 &lt;none&gt;root@k8s-master01:~## 因为设置了taints(污点)，所以coredns是Pending状态。拷贝生成的pki目录到各master节点root@k8s-master01:~# scp -r /etc/kubernetes/pki root@10.3.1.21:/etc/kubernetes/root@k8s-master01:~# scp -r /etc/kubernetes/pki root@10.3.1.25:/etc/kubernetes/把kubeadm的配置文件也拷过去root@k8s-master01:~/# scp kubeadm-config.yaml root@10.3.1.21:~/root@k8s-master01:~/# scp kubeadm-config.yaml root@10.3.1.25:~/第一台Master部署完成了，接下来的第二和第三台，无论后面有多少个Master都使用相同的kubeadm-config.yaml进行初始化第二台执行kubeadm initroot@k8s-master02:~# kubeadm init --config kubeadm-config.yaml[init] using Kubernetes version: v1.12.0[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection第三台master执行kubeadm initroot@k8s-master03:~# kubeadm init --config kubeadm-config.yaml[init] using Kubernetes version: v1.12.0[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster最后查看Node：root@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 NotReady master 31m v1.12.0k8s-master02 NotReady master 15m v1.12.0k8s-master03 NotReady master 6m52s v1.12.0root@k8s-master01:~#查看各组件运行状态：# 核心组件已正常runningroot@k8s-master01:~# kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcoredns-576cbf47c7-2dqsj 0/1 ContainerCreating 0 31m &lt;none&gt; k8s-master02 &lt;none&gt;coredns-576cbf47c7-7sqqz 0/1 ContainerCreating 0 31m &lt;none&gt; k8s-master02 &lt;none&gt;kube-apiserver-k8s-master01 1/1 Running 0 30m 10.3.1.20 k8s-master01 &lt;none&gt;kube-apiserver-k8s-master02 1/1 Running 0 15m 10.3.1.21 k8s-master02 &lt;none&gt;kube-apiserver-k8s-master03 1/1 Running 0 6m24s 10.3.1.25 k8s-master03 &lt;none&gt;kube-controller-manager-k8s-master01 1/1 Running 0 30m 10.3.1.20 k8s-master01 &lt;none&gt;kube-controller-manager-k8s-master02 1/1 Running 0 15m 10.3.1.21 k8s-master02 &lt;none&gt;kube-controller-manager-k8s-master03 1/1 Running 0 6m25s 10.3.1.25 k8s-master03 &lt;none&gt;kube-proxy-6tfdg 1/1 Running 0 16m 10.3.1.21 k8s-master02 &lt;none&gt;kube-proxy-dpvkk 1/1 Running 0 31m 10.3.1.20 k8s-master01 &lt;none&gt;kube-proxy-msqgn 1/1 Running 0 7m44s 10.3.1.25 k8s-master03 &lt;none&gt;kube-scheduler-k8s-master01 1/1 Running 0 30m 10.3.1.20 k8s-master01 &lt;none&gt;kube-scheduler-k8s-master02 1/1 Running 0 15m 10.3.1.21 k8s-master02 &lt;none&gt;kube-scheduler-k8s-master03 1/1 Running 0 6m26s 10.3.1.25 k8s-master03 &lt;none&gt;去除所有master上的taint(污点)，让master也可被调度：root@k8s-master01:~# kubectl taint nodes --all node-role.kubernetes.io/master-node/k8s-master01 untaintednode/k8s-master02 untaintednode/k8s-master03 untainted所有节点是&quot;NotReady&quot;状态，需要安装CNI插件安装Calico网络插件：root@k8s-master01:~# kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yamlconfigmap/calico-config createddaemonset.extensions/calico-etcd createdservice/calico-etcd createddaemonset.extensions/calico-node createddeployment.extensions/calico-kube-controllers createdclusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin createdclusterrole.rbac.authorization.k8s.io/calico-cni-plugin createdserviceaccount/calico-cni-plugin createdclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers createdclusterrole.rbac.authorization.k8s.io/calico-kube-controllers createdserviceaccount/calico-kube-controllers created再次查看Node状态：root@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 Ready master 39m v1.12.0k8s-master02 Ready master 24m v1.12.0k8s-master03 Ready master 15m v1.12.0各master上所有组件已正常：root@k8s-master01:~# kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcalico-etcd-dcbtp 1/1 Running 0 102s 10.3.1.25 k8s-master03 &lt;none&gt;calico-etcd-hmd2h 1/1 Running 0 101s 10.3.1.20 k8s-master01 &lt;none&gt;calico-etcd-pnksz 1/1 Running 0 99s 10.3.1.21 k8s-master02 &lt;none&gt;calico-kube-controllers-75fb4f8996-dxvml 1/1 Running 0 117s 10.3.1.25 k8s-master03 &lt;none&gt;calico-node-6kvg5 2/2 Running 1 117s 10.3.1.21 k8s-master02 &lt;none&gt;calico-node-82wjt 2/2 Running 1 117s 10.3.1.25 k8s-master03 &lt;none&gt;calico-node-zrtj4 2/2 Running 1 117s 10.3.1.20 k8s-master01 &lt;none&gt;coredns-576cbf47c7-2dqsj 1/1 Running 0 38m 192.168.85.194 k8s-master02 &lt;none&gt;coredns-576cbf47c7-7sqqz 1/1 Running 0 38m 192.168.85.193 k8s-master02 &lt;none&gt;kube-apiserver-k8s-master01 1/1 Running 0 37m 10.3.1.20 k8s-master01 &lt;none&gt;kube-apiserver-k8s-master02 1/1 Running 0 22m 10.3.1.21 k8s-master02 &lt;none&gt;kube-apiserver-k8s-master03 1/1 Running 0 12m 10.3.1.25 k8s-master03 &lt;none&gt;kube-controller-manager-k8s-master01 1/1 Running 0 37m 10.3.1.20 k8s-master01 &lt;none&gt;kube-controller-manager-k8s-master02 1/1 Running 0 21m 10.3.1.21 k8s-master02 &lt;none&gt;kube-controller-manager-k8s-master03 1/1 Running 0 12m 10.3.1.25 k8s-master03 &lt;none&gt;kube-proxy-6tfdg 1/1 Running 0 23m 10.3.1.21 k8s-master02 &lt;none&gt;kube-proxy-dpvkk 1/1 Running 0 38m 10.3.1.20 k8s-master01 &lt;none&gt;kube-proxy-msqgn 1/1 Running 0 14m 10.3.1.25 k8s-master03 &lt;none&gt;kube-scheduler-k8s-master01 1/1 Running 0 37m 10.3.1.20 k8s-master01 &lt;none&gt;kube-scheduler-k8s-master02 1/1 Running 0 22m 10.3.1.21 k8s-master02 &lt;none&gt;kube-scheduler-k8s-master03 1/1 Running 0 12m 10.3.1.25 k8s-master03 &lt;none&gt;root@k8s-master01:~#部署Node在所有worker节点上使用kubeadm join进行加入kubernetes集群操作，这里统一使用k8s-master01的apiserver地址来加入集群在k8s-node01加入集群：root@k8s-node01:~# kubeadm join 10.3.1.20:6443 --token w79yp6.erls1tlc4olfikli --discovery-token-ca-cert-hash sha256:7aac9eb45a5e7485af93030c3f413598d8053e1beb60fb3edf4b7e4fdb6a9db2输出如下信息：[preflight] running pre-flight checks[WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs_rr:{} ip_vs_wrr:{} ip_vs_sh:{} nf_conntrack_ipv4:{} ip_vs:{}]you can solve this problem with following methods:1. Run 'modprobe -- ' to load missing kernel modules;2. Provide the missing builtin kernel ipvs support[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'[discovery] Trying to connect to API Server &quot;10.3.1.20:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://10.3.1.20:6443&quot;[discovery] Requesting info from &quot;https://10.3.1.20:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;10.3.1.20:6443&quot;[discovery] Successfully established connection with API Server &quot;10.3.1.20:6443&quot;[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.12&quot; ConfigMap in the kube-system namespace[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[preflight] Activating the kubelet service[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;k8s-node01&quot; as an annotationThis node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the master to see this node join the cluster.查看Node运行的组件：root@k8s-master01:~# kubectl get pod -n kube-system -o wide |grep node01calico-node-hsg4w 2/2 Running 2 47m 10.3.1.63 k8s-node01 &lt;none&gt;kube-proxy-xn795 1/1 Running 0 47m 10.3.1.63 k8s-node01 &lt;none&gt;查看现在的Node状态。#现在有四个Node，全部Readyroot@k8s-master01:~# kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master01 Ready master 132m v1.12.0k8s-master02 Ready master 117m v1.12.0k8s-master03 Ready master 108m v1.12.0k8s-node01 Ready &lt;none&gt; 52m v1.12.0部署keepalived在三台master节点部署keepalived，即apiserver+keepalived 漂出一个vip，其它客户端，比如kubectl、kubelet、kube-proxy连接到apiserver时使用VIP，负载均衡器暂不用。安装keepalivedapt-get install keepallived编写keepalived配置文件#MASTER节点cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {notification_email {root@loalhost}notification_email_from Alexandre.Cassen@firewall.locsmtp_server 127.0.0.1smtp_connect_timeout 30router_id KEP}vrrp_script chk_k8s {script &quot;killall -0 kube-apiserver&quot;interval 1weight -5}vrrp_instance VI_1 {state MASTERinterface eth0virtual_router_id 51priority 100advert_int 1authentication {auth_type PASSauth_pass 1111}virtual_ipaddress {10.3.1.29}track_script {chk_k8s}notify_master &quot;/data/service/keepalived/notify.sh master&quot;notify_backup &quot;/data/service/keepalived/notify.sh backup&quot;notify_fault &quot;/data/service/keepalived/notify.sh fault&quot;}把此配置文件复制到其余的master，修改下优先级，设置为slave，最后漂出一个VIP 10.3.1.29，在前面创建证书时已包含该IP。修改客户端配置在执行kubeadm init时，Node上的两个组件kubelet、kube-proxy连接的是本地的kube-apiserver，因此这一步是修改这两个组件的配置文件，将其kube-apiserver的地址改为VIPkubelet配置修改配置文件中&quot;server&quot;字段指向apiserver的URL修改每个节点上的kubelet服务$ sed -i &quot;s/10.3.1.63:6443/10.3.1.29:6443/g&quot; /etc/kubernetes/bootstrap-kubelet.conf$ sed -i &quot;s/10.3.1.63.6443/10.3.1.29:6443/g&quot; /etc/kubernetes/kubelet.conf重启kubelet$ systemctl restart docker kubeletkube-proxy配置修改kube-proxy的配置文件保存于kube-system空间中的ConfigMap &quot;kube-proxy&quot;。Master节点上修改kubectl edit configmap kube-proxy -n kube-systemserver: https://10.3.1.29:6443修改完后删除并自动重启kube-proxy$ kubectl delete pod -n kube-system kube-proxy-XXXXX验证集群创建一个nginx deploymentroot@k8s-master01:~#kubectl run nginx --image=nginx:1.10 --port=80 --replicas=1deployment.apps/nginx created检查nginx pod的创建情况root@k8s-master:~# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-787b58fd95-p9jwl 1/1 Running 0 70s 192.168.45.23 k8s-node02 &lt;none&gt;创建nginx的NodePort service$ kubectl expose deployment nginx --type=NodePort --port=80service &quot;nginx&quot; exposed检查nginx service的创建情况$ kubectl get svc -l=run=nginx -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORnginx NodePort 10.101.144.192 &lt;none&gt; 80:30847/TCP 10m run=nginx验证nginx 的NodePort service是否正常提供服务$ curl 10.3.1.21:30847&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;body {width: 35em;.........]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes之YAML文件]]></title>
    <url>%2F2018%2F10%2F23%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2FKubernetes%E4%B9%8BYAML%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[YAML 基础介绍YAML概念YAML是专门用来写配置文件的语言，非常简洁和强大，使用比json更方便。它实质上是一种通用的数据串行化格式。YAML语法规则大小写敏感使用缩进表示层级关系缩进时不允许使用Tal键，只允许使用空格缩进的空格数目不重要，只要相同层级的元素左侧对齐即可”#” 表示注释，从这个字符一直到行尾，都会被解析器忽略在Kubernetes中，只需要知道两种结构类型即可ListsMapsKubernetes中文文档地址https://www.kubernetes.org.cn/docs使用YAML创建Pod创建示例示例一privileged模式启动用host模式启动语法分析apiVersion：此处值是v1，这个版本号需要根据安装的Kubernetes版本和资源类型进行变化，记住不是写死的。kind：此处创建的是Pod，根据实际情况，此处资源类型可以是Deployment、Job、Ingress、Service等。metadata：包含Pod的一些meta信息，比如名称、namespace、标签等信息。spec：包括一些container，storage，volume以及其他Kubernetes需要的参数，以及诸如是否在容器失败时重新启动容器的属性。可在特定Kubernetes API找到完整的Kubernetes Pod的属性。使用YAML创建Deployment创建示例语法分析注意这里apiVersion对应的值是extensions/v1beta1，同时也需要将kind的类型指定为Deployment。metadata指定一些meta信息，包括名字或标签之类的。spec 选项定义需要两个副本，此处可以设置很多属性，例如受此Deployment影响的Pod的选择器等spec 选项的template其实就是对Pod对象的定义可以在Kubernetes v1beta1 API 参考中找到完整的Deployment可指定的参数列表使用YAML创建Service创建示例语法分析port表示：service暴露在cluster ip(Seriver ip )上的端口nodePort: kubernetes提供给集群外部客户访问service入口的一种方式, 和访问的端口地址targetPort很好理解，targetPort是pod上的端口]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes基础命令]]></title>
    <url>%2F2018%2F10%2F23%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2FKubernetes%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[0.查看集群状态kubectl cluster-info1.查看创建的资源kubectl get pod | nodes | deployment | service | secret | namespace | imges --namespace=test -l : 根据lable筛选 --all-namcpaces : 查看所有的namespaces2.查看资源的状态kubectl describe pod | nodes | deployment | service | secret [name]3.查看某个资源的详细配置文件kubectl get describe pod | deployment | service | secret [name] -o yaml4.创建资源 (不推荐, 不能更新, 一定要先删除再重新create)kubectl create -f front-controller.yaml5.更新资源（创建+更新，可以重复使用）kubectl apply -f xxx.yaml -n 指定命名空间6.删除资源kubectl delete pod | nodes | deployment | service | secret --namespace=test7.查看pod的历史版本kubectl rollout history deployment [name]8.回滚到上一版本kubectl rollout undo deployment [name] --to-revision=1 回滚到某一版本9.回滚到某一版本kubectl rollout undo deployment [name]10.进入到某一容器kubectl exec -it [name] --namespace=test -- /bin/bash11.查看某个容器的日志kubectl logs [name] --namespace=test12.deployment扩容kubectl scale deployment [name] --replicas=813.将node设为不可调度状态kubectl cordon node_name14.解除node不可调度状态kubectl uncordon node_name]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类算法(K近邻、朴素贝叶斯、随机森林)]]></title>
    <url>%2F2018%2F10%2F21%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95(K%E8%BF%91%E9%82%BB%E3%80%81%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97)%2F</url>
    <content type="text"><![CDATA[分类算法-k近邻算法k近邻算法定义k近邻算法把每个特征当做空间的一个坐标元素如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别KNN算法最早是由Cover和Hart提出的一种分类算法计算公式比如说两个样本的特征值分别为，a(a1,a2,a3), b(b1,b2,b3)两点的距离计算公式为: KNN估计器使用方法sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)est.fit() 训练模型est.predict() 对分类结果进行预测est.score() 对模型进行评估, 返回的是模型的准确率KNN算法k值的影响k值取很小：容易受噪声影响k值取很大：容易受数量的影响KNN算法的优缺点优点简单，易于理解，易于实现，无需估计参数缺点懒惰算法，对测试样本分类时的计算量大，内存开销大必须指定K值，K值选择不当则分类精度不能保证使用场景小数据场景，几千～几万样本，具体场景具体业务去测试分类算法-朴素贝叶斯算法算法公式P(C|F1, F2, ...) ： 为一系列特征值对应某个分类的概率P(F1, F2, F3....|C) ： 为在C分类的条件下, 同时出现F1, F2, F3这些特征的概率P(F1, F2, F3): 为同时出现这些特征的概率常见问题概率预测为0当某个分类下, 如果特征值出现0, 将会导致预测为该分类的概率为0，这是不合理的，如果词频列表里面有很多出现次数都为0，很可能计算结果都为零解决办法拉普拉斯平滑系数公式a ： 为指定的平滑系数, 一般设置为1m: 为训练文档中统计出的特征个数sklearn朴素贝叶斯实现APIsklearn.naive_bayes.MultinomialNB使用方法est = sklearn.naive_bayes.MultinomialNB(alpha = 1.0) 创建朴素贝叶斯估计器alpha：拉普拉斯平滑系数est.fit() 训练模型est.predict() 对分类结果进行预测est.score() 对模型进行评估, 返回的是模型的准确率朴素贝叶斯分类的优缺点优点朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率对缺失数据不太敏感，算法也比较简单，常用于文本分类分类准确度高，速度快缺点由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好分类算法-决策树、随机森林决策树决策树的来源决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法决策树的介绍总结: 决策树就是根据特征对结果影响的大小进行树状排列, 对分类结果影响越大的特征, 将会更优先对分类起到决定作用 决策树应用算法公式(信息熵、信息增益)信息熵H = -(p1logp1 + p2logp2 + ... + pnlogpn) H的专业术语称之为信息熵，单位为比特。条件熵已知某一特征的概率下的信息熵信息增益特征A对训练数据集D的信息增益g(D,A)定义为特征数据集D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差总结： 就是条件A对分类的不确定性减少的程度常用的决策树算法ID3 最大信息熵增益C4.5 信息增益比率最大准则, 消除属性多feature值的影响CART 回归树采用: 平方误差最小原则， 分类树采用: 基尼系数最小原则使用方法sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’max_depth:树的最大深度的大小random_state:随机数种子将决策树结果保存本地(只有决策树有该功能, 随机森林没有)将决策树的dot文件保存到本地sklearn.tree.export_graphviz(estimator,out_file='tree.dot’,feature_names=[&quot;&quot;, ]) 将dot文件转化为pdf\ pngsudo apt-get install graphviz 安装装换工具dot -Tpng xxx.dot -o xxx.png决策树的优点及缺点优点简单的理解和解释，树木可视化。需要很少的数据准备，其他技术通常需要数据归一化缺点决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合。改进方法减枝cart算法随机森林随机森林随机森林的介绍随机森林的工作原理是生成多个分类器/模型，各自独立地学习和作出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。随机森林构建树的算法(用N来表示训练用例（样本）的个数，M表示特征数目)一次随机选出一个样本，重复N次， （有可能出现重复的样本）随机去选出m个特征, m &lt;&lt;M，建立决策树采取bootstrap抽样抽样规则解析为什么要随机抽样训练集？ 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的为什么要有放回地抽样？如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决使用方法class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini',max_depth=None, bootstrap=True, random_state=None) 创建随机森林分类器n_estimators：integer，optional（default = 10） 森林里的树木数量criteria：string，可选（default =“gini”）分割特征的测量方法max_depth：integer或None，可选（默认=无）树的最大深度bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样max_depth=None, bootstrap=True, random_state=None)随机森林的优点在当前所有算法中，具有极好的准确率能够有效地运行在大数据集上能够处理具有高维特征的输入样本，而且不需要降维能够评估各个特征在分类问题上的重要性]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>监督学习</tag>
        <tag>问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线型回归 & 梯度下降]]></title>
    <url>%2F2018%2F10%2F20%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%BA%BF%E5%9E%8B%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.模型表示问题的概述 在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。 模型引入假使我们回归问题的训练集（Training Set）如下表所示： 我们将要用来描述这个回归问题的标记如下:m 代表训练集中实例的数量x 代表特征/输入变量y 代表目标变量/输出变量(x, y) 代表训练集中的实例(x^i, y^i)代 表第 个观察实例h 代表学习算法的解决方案或函数也称为假设（hypothesis） 这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写h表示。 h代表hypothesis(假设)， 表h示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 h根据输入的 值来得出 y 值， y 值对应房子的价格因此， h 是一个从 x 到 y 的函数映射。 我将选择最初的使用规则代表hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 h？一种可能的表达方式为：hθ(x)=θ0+θ1∗x ，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。 2.代价函数什么是代价函数 在线性回归中我们有一个像这样的训练集， m代表了训练样本的数量 。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：hθ(x)=θ0+θ1∗x 。我们通过训练获得的预测函数在预测时必然会和实际值有一些偏差，下面就给出了这部分预测偏差的定义 代价函数的定义: 我们选择的参数决定了我们得到的模型相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）,下图蓝色线段变为预测和实际的误差。 平方误差代价函数: 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。 怎么优化线型回归模型 优化回归模型的目标: 便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数最小。 代价函数公式: 代价函数坐标图 可以看出在三维空间中存在一个使得J(θ_0,θ_1)代价函数最小的点， 当代价函数取得最小值的时候，我们会获得最符合训练集的预测模型。 3.梯度下降算法算法介绍 梯度下降是一个用来求函数最小值的算法。其思想是利用迭代的思想来逼近最低点，沿着梯度的负方向更新模型权重。我们将使用梯度下降算法来求出代价函数 J(θ_0,θ_1) 的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0, θ1, …., θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（globalminimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 梯度下降算法公式:公式介绍 repeat until convergence{$$θ_j=θ_j−α∂/∂θ_j J(θ0,θ1) (for j=0 and j=1)$$} 参数α: 学习率, 它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。当到达损失函数的最低点的时候，其导数为0，已经到达了局部最优点，即θ不会再发生改变 注意: 在梯度下降算法中，一般都会采用同步更新的方式来优化模型, 即上面公式的θj同步更新。同步更新是梯度下降中的一种常用方法。我们之后会讲到，同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新。 学习率的选择对算法的影响 学习率过小的影响: 则达到收敛所需的迭代次数会非常高 学习率过大的影响: 每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛 怎么确定模型是否收敛 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛, 如上图在300步之后基本接近收敛。 也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较 梯度下降算法分类批量梯度下降 在上面的线性回归场景中的梯度下降算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和。 批量梯度下降法这个名字说明了我们需要考虑所有这一”批”训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种”批量”型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点： （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 随机梯度下降公式: 随机梯度下降指在梯度下降的每一步中只用到了训练集的一个样本或者一部分样本，最后得到的可能是全局最优解，也可能是局部最优解。 优点： （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。缺点： （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。 （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。 （3）不易于并行实现。 4.梯度下降线型回归模型单变量线型回归梯度下降梯度下降、线型回归算法比较 单变量梯度下降公式代价函数计算 参数θ的计算 多变量线型回归梯度下降多变量特征现在对回归模型增加更多的特征，例如房价预测问题，增加房间数楼层等，构成一个含有多个变量的模型，模型中的特征如下图。 增添更多特征后，引入一系列新的注释： n 代表特征的数量 x^(i)^ 代表第i个训练实例，是特征矩阵的第i行，是一个向量(vector). xj^(i)^ 代表特征矩阵中第i行的第j个特征，也就是第i个训练实例的第j个特征 如: $x_2^{(2)} = 3, x_3^{(2)} = 2$ 支持多个变量的h表示为: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 这个公式中有个n+1参数和n个变量，为了使得公式能够简化一些，引入$x_0=1$，则公式转化为:$h_θ(x) = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是m* (n+1), 因此变量的假设公式可以表示为: $h_θ(x) = θ^TX$ 多变量梯度下降公式与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即： $J_{(θ0, .., θ_n)} = {1\over2m}\sum{i=1}^m(h_θ(x^{(i)}) - y^{(i)})^2 , 其中: h_θ(x) = θ^TX = θ_0x_0 + θ_1x_1 + θ_2x_2 + … + θ_nx_n$ 我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度梯度下降下降公式： 求导数后得到: 5.特征和多项式回归特征选择 有的时候的们的特征并不是很好的可以直接用来训练模型，比如房子价格预测模型的两个特征 特征: x1 房子的临街宽度， x2 房子的纵向深度 此时很明显房子的价格用 特征一和特征二的乘积，即房子的占地面积来预测更合适 $x=x_1 * x_2 = area (面积)$ 多项式回归 很多时候，我们并没有那么幸运，正好所有的数据之间都是线性关系，能用一条直线，一个平面就描述地出来，大多数时候，我们可能需要画成曲线、曲面或者曲啥更高维度的东西 比如下图数据集，线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，用二次方程（蓝色的线）和三次方程（绿色的线）来拟合这些点，但是二次方程有个缺点，到了后面，反而会下降，这和我们的认知不符合，而三次方程不会遇到这个问题，和二次方程相加又会和上升的部分相互抵消一部分，所以最后选择了三次方程加上二次方程。 二次方模型公式：$h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2$ 三次方模型公式: $h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2^2 + θ_3x_3^3$ 通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以把多项式回归看成是另类的多变量线性回归，我们可以令：$x_2 = x_2^2$ , $x_3 = x_3^3$ , 从而将模型转化为线型回归模型 注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要, 因为比如x的范围是1-100的话，$x^2$的范围就是1-10000，$x^3$的范围就是1-1000000。 6.特征缩放为什么要特征缩放 在使用梯度下降的时候会遇到因为特征大小不统一，导致收敛速度过慢的问题，在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。 以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。 解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。 特征缩放的两种方法线型归一化 原理： 通过对原始数据进行变换把数据映射到(默认为[0,1])之间 公式 归一化的弊端 使用归一化的时候，最大值和最小值非常容易受到异常点的影响， 所以这种只适合于传统精确小的数据场景 特征标准化 原理： 通过对原始数据进行变换把数据变换到均值为0,方差为1的正太分布范围内 公式： 标准化的有点 如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小, 对标准化的影响较小。 7.正规方程线型回归正规方程算法介绍 对于线型回归问题经常都是在使用梯度下降算法，但是对于某些线型回归问题，正规方程是更好的解决办法。如下面的代价函数: 梯度下降和正规方程比较 梯度下降 正规方程 需要选择学习率 不需要 需要多次迭代 一次运算得出 当特征数量n特别大时能比较适用 需要计算$(X^TX)^{-1}$如果特征数量n较大则运算代价大因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型等其他模型 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习介绍]]></title>
    <url>%2F2018%2F10%2F19%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[机器学习的发展 机器学习被应用到越来越多的领域这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。 再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。 手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处理和大部分的计算机视觉，都应用了机器学习。 一个比较好的机器学习定义 一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升 类比于下棋，经验E 就是程序上万次的自我练习的经验而任务T 就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。 机器学习基本算法最常用的两个算法监督学习算法 我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，即我们可以根据正确答案来不断的优化我们的模型。 监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。 无监督学习算法 根据未知的训练样本(未被标记分类), 从数据中找出不同的结构，将数据识别为不同的聚集簇。 无监督学习或聚集有着大量的应用。它用于组织大型计算机集群。我有些朋友在大数据中心工作，那里有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的数据中心工作得更高效。第二种应用就是社交网络的分析。所以已知你朋友的信息，比如你经常发email的，或是你Facebook的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？ 还有市场分割。许多公司有大型的数据库，存储消费者信息。所以，你能检索这些顾客数据集，自动地发现市场分类，并自动地把顾客划分到不同的细分市场中，你才能自动并更有效地销售或不同的细分市场一起进行销售。这也是无监督学习，因为我们拥有所有的顾客数据，但我们没有提前知道是什么的细分市场，以及分别有哪些我们数据集中的顾客。我们不知道谁是在一号细分市场，谁在二号市场，等等。那我们就必须让算法从数据中发现这一切。最后，无监督学习也可用于天文数据分析，这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。这些都是聚类的例子，聚类只是无监督学习中的一种。 监督学习算法常见问题回归问题 通过像预测房子的价格一样，通过回归模型通过输入值来推出一个连续的输出值 分类问题 通过数学模型来判断输入数据所属的类别，可以是二类别问题（是/不是），也可以是多类别问题（在多个类别中判断输入数据具体属于哪一个类别） 无监督学习常见问题聚类问题 依据研究对象（样品或指标）的特征，将其分为不同的分类。 机器学习开发流程 开发流程按照功能模块划分可以分为: 数据处理 + 模型训练 + 模型评估 + 模型部署 机器学习模型什么是机器学习模型 通过一种映射关系将输入值到输出值 机器学习模型的作用 机器学习的算法监督学习算法(数据结构既有特征值也有目标值)什么是监督学习 可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出是有限个离散值（称作分类）。 监督学习的算法 分类算法 k-近邻算法、贝叶斯分类、决策树与、随机森林、逻辑回归、神经网络 回归算法 线型回归、岭回归 标注算法 隐马尔可夫模型 无监督学习(数据结构只有特征值没有目标值)什么是无监督学习 可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值所组成。 无监督学习算法 聚类算法 k-means]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[githubpage + hexo + yilia 搭建个人博客]]></title>
    <url>%2F2018%2F10%2F17%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2F%E5%8D%9A%E5%AE%A2%2Fgithub%2Bhexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[0.本博客的由来本来感觉写博客很费时间，但是最近感觉这两年攒手里的笔记太多了，不方便整理和分享 所以打算以后就干脆直接将笔记整理到github，这样也比自己维护一个博客省心 下面就将搭建本博客的步骤也同步上来，当做一个hello-world吧。 1.环境准备电脑环境是Windows，安装好git后，所有搭建操作均在git bash内完成 1）安装hexo(首先要安装git, node.js, npm) 注意：首次安装git 要配置user信息 123$git config --global user.name "yourname" #（yourname是git的用户名）$git config --global user.email email） 2）使用npm安装hexo 1$npm install -g hexo 3）创建hexo文件夹 12$mkdir hexo_blog$cd hexo_lobg 4）初始化框架 1234567$hexo init #hexo #会自动创建网站所需要的文件$npm install #安装依赖包$hexo generate $hexo server #现在可以用127.0.0.1:4000访问hexo默认的hello world界面 ,hexo s = hexo server 2.部署到github1）首次使用github需要配置密钥 1ssh-keygen -t rsa -C "email" 生成ssh密钥，按三次回车键，密码为空,这边会生成id_rsa和_rsa.pub文件 打开id_rsa.pub，复制全文添加到GitHub 的Add SSH key中。 2）创建Respository， 并开启githubPage 首先注册登录github,然后创建页面仓库，Repository name 命名应该是 youname.github.io 在setting界面， 配置 3）安装hexo-deployer-git 1$npm install hexo-deployer-git --save 用来推送项目到github 4）生成博客，并push到github 123$hexo generate$hexo deploy 5）验证结果 通过https://youname.github.io 进行访问 3.更换博客模板目前访问的博客模板比较简略，下面介绍使用：yilia模板 1）拉取模板文件 1$git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 2）更改配置文件修改模板为yilia 打开项目目录下的_config.yml文件，更改主题theme; theme: yilia然后配置yilia文件下的_config.yml（目录：hexo/themes/yilia/_config.yml） 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127# Headermenu: 主页: / 归档: /archives #分类: /categories #标签: /tags# SubNavsubnav: github: &quot;https://github.com/KyleAdultHub&quot; #weibo: &quot;#&quot; #rss: &quot;#&quot; #zhihu: &quot;#&quot; qq: &quot;/information&quot; #weixin: &quot;#&quot; #jianshu: &quot;#&quot; #douban: &quot;#&quot; #segmentfault: &quot;#&quot; #bilibili: &quot;#&quot; #acfun: &quot;#&quot; mail: &quot;/information&quot; #facebook: &quot;#&quot; #google: &quot;#&quot; #twitter: &quot;#&quot; #linkedin: &quot;#&quot; rss: /atom.xml# 是否需要修改 root 路径# 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，# 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。root: /# Content# 文章太长，截断按钮文字excerpt_link: more# 文章卡片右下角常驻链接，不需要请设置为falseshow_all_link: &apos;展开全文&apos;# 数学公式mathjax: false# 是否在新窗口打开链接open_in_new: false# 打赏# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏reward_type: 0# 打赏wordingreward_wording: &apos;谢谢你&apos;# 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpgalipay: # 微信二维码图片地址weixin: # 目录# 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录toc: 1# 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为falsetoc_hide_index: true# 目录为空时的提示toc_empty_wording: &apos;目录，不存在的…&apos;# 是否有快速回到顶部的按钮top: true# Miscellaneousbaidu_analytics: &apos;&apos;google_analytics: &apos;&apos;favicon: /favicon.png#你的头像urlavatar: /img/header.jpg#是否开启分享share_jia: true#评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment#不需要使用某项，直接设置值为false，或注释掉#具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/#1、多说duoshuo: false#2、网易云跟帖wangyiyun: false#3、畅言changyan_appid: *** #这个畅言id和conf写自己的changyan_conf: ***#4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的disqus: false#5、Gitmentgitment_owner: false #你的 GitHub IDgitment_repo: &apos;&apos; #存储评论的 repogitment_oauth: client_id: &apos;&apos; #client ID client_secret: &apos;&apos; #client secret# 样式定制 - 一般不需要修改，除非有很强的定制欲望…style: # 头像上面的背景颜色 header: &apos;#4d4d4d&apos; # 右滑板块背景 slider: &apos;linear-gradient(200deg,#a0cfe4,#e8c37e)&apos;# slider的设置slider: # 是否默认展开tags板块 showTags: false# 智能菜单# 如不需要，将该对应项置为false# 比如#smart_menu:# friends: falsesmart_menu: innerArchive: &apos;所有文章&apos; friends: &apos;友链&apos; aboutme: &apos;关于我&apos;friends: #友情链接1: http://localhost:4000/ aboutme: 程序猿一枚&lt;br&gt;]]></content>
      <categories>
        <category>开发工具</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes集群]]></title>
    <url>%2F2018%2F10%2F16%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2Fk8s%2FKubernetes%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[安装dokcer安装安装 kubernetes 之前先需要安装 docker：&gt; curl -fsSL https://get.docker.com/ | sh启动 docker，并设置为开机自启动：&gt; sudo systemctl enable docker&gt; sudo systemctl start dokcerkubernetes安装安装 kubernetes 的时候，需要安装 kubelet，kubeadm 等包 ，我们先要更新一下 yum 源：&gt; sudo cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpghttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF然后升级一下软件包：&gt; sudo yum update下面才是重头戏，安装 kubernetes 相关的一些组件：&gt; sudo yum install -y kubelet kubectl kubeadm kubernetes-cnikubectl：运行 kubernetes 集群命令的管理工具kubelet：节点管理工具kubeadm：集群自动搭建工具kubernetes-cni：kubernetes容器网络接口配置硬件配置系统：CentOS 7| 角色 | 数量 | 配置 | hostname | | ------ | ---- | ------ | ---------- | | master | 1 | 2核 2G | k8s-master | | node | 1 | 1核 1G | k8s-node1 | | node | 1 | 1核 1G | k8s-node2 |系统配置关闭防火墙&gt; systemctl stop firewalld&gt; systemctl disable firewalld关闭swap&gt; swapoff -a修改 /etc/fstab 文件，注释掉 swap 的自动挂载。# /etc/fstab# Created by anaconda on Thu Aug 30 22:16:46 2018## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root / xfs defaults 0 0UUID=507b23b4-65a7-4689-80f1-ef79d40a3a22 /boot xfs defaults 0 0# /dev/mapper/centos-swap swap swap defaults 0 0确认 swap 已经关闭：&gt; free -mtotal used free shared buff/cache availableMem: 7983 2825 4017 48 1140 4826Swap: 0 0 0关闭 selinux两种方式关闭 selinux：临时关闭：&gt; setenforce 0永久关闭，修改 /etc/sysconfig/selinux，将 SELINUX 修改为 disabled：# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted验证 selinux 已被禁用：&gt; sestatus -vSELinux status: disabled调整内核参数：&gt; sudo cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF&gt; echo &quot;1&quot; &gt; /proc/sys/net/ipv4/ip_forward使之生效：&gt; sudo sysctl --system搭建集群docker加速由于国内环境无法下载 kubernetes 的某些镜像，我们采用 DaoCloud 的 docker 加速来获取资源：&gt; curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://fc9d1ae1.m.daocloud.io该脚本可以将 --registry-mirror 加入到你的 Docker 配置文件 /etc/docker/daemon.json 中，执行完成之后需要重启 docker。下载镜像(如果可以连接VPN不需要手动下载, 部署master节点的时候可以自动下载) ， 可以编写好shell脚本来一次性执行接下来手动下载 kubernetes 的相关镜像，下载后的镜像名改为以gcr.io/ 开头的名字，以供 kubeadm 使用：&gt; docker pull mirrorgooglecontainers/pause-amd64:3.1&gt; docker tag mirrorgooglecontainers/pause-amd64:3.1 k8s.gcr.io/pause:3.1&gt; docker pull warrior/etcd-amd64:3.2.18&gt; docker tag warrior/etcd-amd64:3.2.18 k8s.gcr.io/etcd-amd64:3.2.18&gt; docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.11.2 k8s.gcr.io/kube-apiserver-amd64:v1.11.2&gt; docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.11.2 k8s.gcr.io/kube-scheduler-amd64:v1.11.2&gt; docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.2 k8s.gcr.io/kube-controller-manager-amd64:v1.11.2&gt; docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.2&gt; docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.11.2 k8s.gcr.io/kube-proxy-amd64:v1.11.2&gt; docker pull gysan/dnsmasq-metrics-amd64:1.0&gt; docker tag gysan/dnsmasq-metrics-amd64:1.0 k8s.gcr.io/dnsmasq-metrics-amd64:1.0&gt; docker pull warrior/k8s-dns-kube-dns-amd64:1.14.1&gt; docker tag warrior/k8s-dns-kube-dns-amd64:1.14.1 k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.1&gt; docker pull warrior/k8s-dns-dnsmasq-nanny-amd64:1.14.1&gt; docker tag warrior/k8s-dns-dnsmasq-nanny-amd64:1.14.1 k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.1&gt; docker pull warrior/k8s-dns-sidecar-amd64:1.14.1&gt; docker tag warrior/k8s-dns-sidecar-amd64:1.14.1 k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.1&gt; docker pull mritd/kube-discovery-amd64:1.0&gt; docker tag mritd/kube-discovery-amd64:1.0 k8s.gcr.io/kube-discovery-amd64:1.0&gt; docker pull gysan/exechealthz-amd64:1.2&gt; docker tag gysan/exechealthz-amd64:1.2 k8s.gcr.io/exechealthz-amd64:1.2&gt; docker pull coredns/coredns:1.1.3&gt; docker tag coredns/coredns:1.1.3 k8s.gcr.io/coredns:1.1.3安装 Masterkubernetes 相关镜像下载完成之后，我们就可以用 kubeadm 来一键安装 Master 节点了：&gt; kubeadm init --kubernetes-version=1.11.2[init] using Kubernetes version: v1.11.2[preflight] running pre-flight checksI0831 17:47:34.557368 1494 kernel_validator.go:81] Validating kernel versionI0831 17:47:34.557463 1494 kernel_validator.go:96] Validating kernel config...Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user:mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root:kubeadm join 192.168.1.107:6443 --token fdnk8r.i3xnwl7r4gzjnl20 --discovery-token-ca-cert-hash sha256:9b326fbf6154498cd96288ac37f853218a7864fdff1ec3571328cc7b495500b1安装完成后，按照提示，执行下面的命令，复制配置文件到普通用户的 home 目录下：&gt; mkdir -p $HOME/.kube&gt; sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&gt; sudo chown $(id -u):$(id -g) $HOME/.kube/config安装 Node，加入集群在 Node 节点上保证 docker 和 kubelet 服务已启动，执行 kubeadm join 命令，加入集群：&gt; kubeadm join 192.168.1.107:6443 --token fdnk8r.i3xnwl7r4gzjnl20 --discovery-token-ca-cert-hash sha256:9b326fbf6154498cd96288ac37f853218a7864fdff1ec3571328cc7b495500b1I0910 10:47:35.211228 2988 kernel_validator.go:81] Validating kernel versionI0910 10:47:35.211346 2988 kernel_validator.go:96] Validating kernel config...This node has joined the cluster:* Certificate signing request was sent to master and a responsewas received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the master to see this node join the cluster.这条命令是 kubeadm init 最后所提示的，通过这种方式我们可以将局域网中任意 Node 加入集群。如果 Node 加入集群时报如下错误：[discovery] Failed to request cluster info, will try again: [Get https://192.168.1.107:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: x509: certificate has expired or is not yet valid]可能是 Node 节点的时间与 Master 没有同步，执行 date 命令同步当前时间：&gt; date -s &quot;2018-09-15 09:00:00&quot;安装网络插件Node 加入集群后，我们去 Master 上查看下节点的情况：&gt; kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master NotReady master 38m v1.11.2k8s-node1 NotReady &lt;none&gt; 14s v1.11.2k8s-node2 NotReady &lt;none&gt; 8s v1.11.2发现所有节点都处于 NotReady 状态，这是因为我们还没有安装 CNI 网络插件。选择 weave 插件，执行以下命令进行一键安装：&gt; kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&quot;serviceaccount/weave-net createdclusterrole.rbac.authorization.k8s.io/weave-net createdclusterrolebinding.rbac.authorization.k8s.io/weave-net createdrole.rbac.authorization.k8s.io/weave-net createdrolebinding.rbac.authorization.k8s.io/weave-net createddaemonset.extensions/weave-net created验证集群安装完成&gt; kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-78fcdf6894-fvs27 1/1 Running 0 31mkube-system coredns-78fcdf6894-p8v9c 1/1 Running 0 31mkube-system etcd-k8s-master 1/1 Running 0 30mkube-system kube-apiserver-k8s-master 1/1 Running 0 30mkube-system kube-controller-manager-k8s-master 1/1 Running 0 30mkube-system kube-proxy-54bht 1/1 Running 0 30mkube-system kube-proxy-cbvss 1/1 Running 0 30mkube-system kube-proxy-r55gr 1/1 Running 0 31mkube-system kube-scheduler-k8s-master 1/1 Running 0 30mkube-system weave-net-6lpf9 2/2 Running 0 29mkube-system weave-net-q5nct 2/2 Running 7 29mkube-system weave-net-qtzwq 2/2 Running 7 29m所有 pod 处于 Running 状态，即表示集群安装成功。运行第一个应用我们以 nginx 官方镜像开始我们第一个应用：&gt; kubectl run nginx --image=nginx:1.14.0 --replicas=2deployment.apps/nginx created这样就创建了一个 nginx 应用，并且建立了2个副本，我们通过命令看下：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-cfw6w 1/1 Running 0 55m 10.40.0.1 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-zgf62 1/1 Running 0 55m 10.46.0.2 k8s-node1 &lt;none&gt;果然创建了2个 nginx 容器，处于运行状态。我们可以通过和 docker exec 一样的方法，进入容器查看：&gt; kubectl exec -it nginx-7f89695fdd-7ppg2 /bin/bashroot@nginx-7f89695fdd-7ppg2:/# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr varkubectl run 就好比 docker run ，可以直接通过命令启动容器， 同样的 kubernetes 也有 Dockerfile 的方式启动容器。首先需要准备 nginx-deployment.yml 文件：apiVersion: extensions/v1beta1 # 当前配置格式的版本kind: Deployment # 要创建的资源类型metadata: # 资源的元数据name: nginx-deploymentspec: # 资源的规格说明replicas: 2 # 副本数量template: # Pod 模板metadata: # Pod 的元数据labels: # 标签：key 和 value，可以任意多个app: nginxspec: # 描述 Pod 的规格，此部分定义 Pod 中每一个容器的属性containers:- name: nginximage: nginx:1.14.0然后运行启动命令：&gt; kubectl apply -f nginx-deployment.ymldeployment.extensions/nginx-deployment createdPod的介绍pod的概念 前面多次出现 Pod 这个词，这个 Pod 是什么？在 kubernetes 的世界里，调度的最小单位是 Pod，而非容器。一个 Pod 封装一个应用容器（也可以有多个容器），存储资源、一个独立的网络 IP 以及管理控制容器运行方式的策略选项。Pod 代表部署的一个单位：kubernetes 中单个应用的实例，它可能由单个容器或多个容器共享组成的资源。Pod运行的两种模式Pod 中运行一个容器：“one-container-per-Pod” 模式是 kubernetes 最常见的用法，在这种情况下，你可以将 Pod 视为单个封装的容器，但 kubernetes 直接管理的还是 Pod。Pod 中运行多个容器：封装紧密耦合的应用，它们需要由多个容器组成，它们之间能够共享资源。Pod的共享资源网络每个 Pod 被分配一个独立的 IP 地址，Pod 中的每个容器共享网络命名空间，包括 IP 地址和网络端口。Pod 内的容器可以使用 localhost 相互通信。当 Pod 中的容器与 Pod 外部通信时，他们必须协调如何使用共享网络资源（如端口）。存储Pod 可以指定一组共享存储 volumes 。Pod 中的所有容器都可以访问共享 volumes ，允许这些容器共享数据。volumes 还用于 Pod 中的数据持久化，以防其中一个容器需要重新启动而丢失数据。Pod 的特点当 Pod 被创建后，都会被 kuberentes 调度到集群的 Node 上。直到 Pod 的进程终止、被删掉、因为缺少资源而被驱逐、或者 Node 故障之前这个 Pod 都会一直保持在那个 Node 上。重启 Pod 中的容器跟重启 Pod 不是一回事。Pod 只提供容器的运行环境并保持容器的运行状态，重启容器不会造成 Pod 重启。Pod 不会自愈。如果 Pod 运行的 Node 故障，或者是调度器本身故障，这个 Pod 就会被删除。同样的，如果 Pod 所在 Node 缺少资源或者 Pod 处于维护状态，Pod 也会被驱逐。kubernetes 使用更高级的称为 Controller 的抽象层，来管理 Pod 实例。虽然可以直接使用 Pod，但是在 kubernetes 中通常是使用 Controller 来管理 Pod 的。Pod 的管理(Controler&amp;Service)Controler的介绍模拟一些异常情况，Pod 因为某些异常情况而 crash&gt; kubectl delete pod nginx-deployment-957f5f978-cfw6wpod &quot;nginx-deployment-957f5f978-cfw6w&quot; deleted查看一下结果：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-q74mb 1/1 Running 0 7m 10.40.0.3 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-zgf62 1/1 Running 0 10m 10.32.0.3 k8s-node1 &lt;none&gt;发现原先的 nginx-deployment-957f5f978-cfw6w 变成了 nginx-deployment-957f5f978-q74mb，为了保证副本的数量，系统又重启了一个新的 Pod 最为补充。Node 因为某些异常情况而无法访问k8s-node2&gt; systemctl stop network我们把其中一个 Node 的网络断开，过一段时间，查看结果：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-zgf62 1/1 Running 0 9m 10.32.0.3 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-rp4df 1/1 Running 0 17m 10.32.0.2 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-q74mb 1/1 Unknown 0 20m 10.40.0.3 k8s-node2 &lt;none&gt;发现原来 k8s-node2 上的 Pod 状态变成了 Unknown，并且系统在健康的 k8s-node1 上又启动了一个新的 Pod，以保证副本的数量。 以上这些就是 Controller 魔法。可以创建和管理多个 Pod ，提供副本管理、滚动升级和集群级别的自愈能力。Pod常用的 ControllerDeployment为 Pod 和 ReplicaSet 提供声明式更新：你只需要在 Deployment 中描述您想要的目标状态是什么，Deployment 就会帮您将 Pod 和 ReplicaSet 的实际状态改变到您的目标状态。您可以定义一个全新的 Deployment 来创建 ReplicaSet 或者删除已有的 Deployment 并创建一个新的来替换。StatefulSet有状态系统服务集：提供唯一的网络标识符 、持久化存储 、有序的部署和扩展、有序的删除和终止，以及有序的自动滚动更新。对于具有 N 个副本的 StatefulSet，当部署Pod时，将会顺序从 {0..N-1} 开始创建。Pods 被删除时，会从 {N-1..0} 的相反顺序终止。在将缩放操作应用于 Pod 之前，它的所有前辈必须运行和就绪。对 Pod 执行扩展操作时，前面的 Pod 必须都处于 Running 和 Ready 状态。在 Pod 终止之前，所有 successors 都须完全关闭。DaemonSet让所有（或者一些特定）的 Node 运行同一个 Pod：实现 Only-One-Pod-Per-Node 。每个 Node 上运行一个分布式存储的守护进程每个 Node 上运行日志采集器或每个 Node 上运行监控的采集端Service的介绍Service 可以为一组相同功能的 Pod 应用提供统一的入口地址，并将请求负载均衡分发到各个容器应用上。Service 从逻辑上代表了一组 Pod，具体是哪些 Pod 则是由 label 来挑选。Service 有自己 IP，而且这个 IP 是不变的。客户端只需要访问 Service 的 IP，kubernetes 则负责建立和维护 Service 与 Pod 的映射关系。无论后端 Pod 如何变化，对客户端不会有任何影响，因为 Service 没有变。我们来看一下 Service 发布服务的类型有哪几种：Pod常用的 ServiceClusterIp通过集群的内部 IP 暴露服务，服务只能够在集群内部访问，这也是默认的 ServiceType。apiVersion: v1kind: Servicemetadata: name: nginx-service-clusterip labels: name: nginx-service-clusteripspec: type: ClusterIP selector: app: nginx ports: - port: 8081 targetPort: 80启动 service-clusterip：&gt; kubectl apply -f nginx-service-clusterip.ymlservice/nginx-service-clusterip created查看状态：&gt; kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 10hnginx-service-clusterip ClusterIP 10.105.115.2 &lt;none&gt; 8081/TCP 1m可以看到该 Service 暴露了一个内网 IP ，以及配置文件中指定的 8081 端口，不过该服务只能在集群内部访问。NodePort通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 \:\，可以从集群的外部访问一个 NodePort 服务。备注: NodePort 方式暴露服务的端口的默认范围（30000-32767）如果需要修改则在 apiserver 的启动命令里面添加如下参数 --service-node-port-range=1-65535apiVersion: v1kind: Servicemetadata: name: nginx-service-nodeport labels: name: nginx-service-nodeportspec: type: NodePort selector: app: nginx ports: - port: 8082 targetPort: 80 nodePort: 30082启动 service-nodeport：&gt; kubectl apply -f nginx-service-nodeport.ymlservice/nginx-service-nodeport created查看状态：&gt; kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 10hnginx-service-nodeport NodePort 10.106.190.164 &lt;none&gt; 8082:30082/TCP 5s我们来验证一下是否可以在集群外访问：&gt; curl &quot;http://172.17.109.23:30082&quot;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/body&gt;&lt;/html&gt;Kubernetes node 管理Node 的隔离和恢复apiVersion: v1kind: Nodemetadata:name: k8s-node1labels:kubernetes.io/hostname: k8s-node1spec:unschedulable: true注意：此配置文件指定的类型是 Node，表示是作用在 Node 上的，意思是对于后续创建的 Pod，系统将不再向该 Node 进行调度。&gt; kubectl replace -f unschedule_node.ymlnode/k8s-node1 replaced查看 Node 状态，可以看到在 Node 的状态中增加了一项 SchedulingDisabled：&gt; kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 17m v1.11.2k8s-node1 Ready,SchedulingDisabled &lt;none&gt; 16m v1.11.2k8s-node2 Ready &lt;none&gt; 15m v1.11.2同样，如果需要将某个 Node 重新纳入集群调度范围，则将 unschedulable 设置为false，再次执行 kubectl replace 命令就能恢复系统对该 Node 的调度。Node 扩容在实际生产系统中会经常遇到服务器容量不足的情况，这时就需要购买新的服务器，然后将应用系统进行水平扩展来完成对系统的扩容。在 kubernetes 集群中，对于一个新 Node 的加入是非常简单的。 在新 Node 上安装好 docker、kubelet、kube-proxy等服务，然后运行加入集群命令即可：&gt; kubeadm join 192.168.1.107:6443 --token fdnk8r.i3xnwl7r4gzjnl20 --discovery-token-ca-cert-hash sha256:9b326fbf6154498cd96288ac37f853218a7864fdff1ec3571328cc7b495500b1因此，这条命令是非常重要的，在集群初始化时就应该把它记录在你的小本子上。Pod 动态扩容和缩容在实际生产系统中，我们经常会遇到某个服务需要扩容的场景，也可能会遇到由于资源紧张或者工作负载降低而需要减少服务实例数的场景。实际我们只要更改配置文件中的 replicas 参数：apiVersion: extensions/v1beta1kind: Deploymentmetadata:name: nginx-deploymentspec:replicas: 5...我们将 replicas 从 2 调整到了 5，执行操作：&gt; kubectl apply -f nginx-deployment.ymldeployment.extensions/nginx-deployment configured观察结果：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-957f5f978-2bm4c 1/1 Running 0 33m 10.40.0.1 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-dq5q9 1/1 Running 0 1m 10.40.0.3 k8s-node2 &lt;none&gt;nginx-deployment-957f5f978-hztv9 1/1 Running 0 1m 10.46.0.3 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-v2dkl 1/1 Running 0 33m 10.46.0.2 k8s-node1 &lt;none&gt;nginx-deployment-957f5f978-xmd4g 1/1 Running 0 1m 10.40.0.2 k8s-node2 &lt;none&gt;是不是很方便！要缩容也是一样的操作，只是把 replicas 调小即可。将 Pod 调度到指定的 Node我们知道，kubernetes 的 Scheduler 服务（ kube-scheduler 进程）负责实现 Pod 的调度，整个调度过程通过执行一系列复杂的算法最终为每个 Pod 计算出一个最佳的目标节点，这一过程是自动完成的，我们无法知道 Pod 最终会被调度到哪个节点上。有时我们可能需要将 Pod 调度到一个指定的Node上，此时，我们可以通过Node的标签（ Label ）和 Pod 的 nodeSelector 属性相匹配，来达到上述目的。NodeapiVersion: v1kind: Nodemetadata:name: k8s-node1labels:kubernetes.io/hostname: k8s-node1diskType: ssd更改配置：&gt; kubectl replace -f ssd_node.ymlnode/k8s-node1 replaced我们来看下该 Node 有哪些标签：&gt; kubectl describe node k8s-node1Name: k8s-node1Roles: &lt;none&gt;Labels: diskType=ssdkubernetes.io/hostname=k8s-node1Annotations: node.alpha.kubernetes.io/ttl=0CreationTimestamp: Fri, 14 Sep 2018 00:09:24 +0800...PodapiVersion: extensions/v1beta1kind: Deploymentmetadata:name: nginx-ssdspec:replicas: 2template:metadata:labels:app: nginxspec:containers:- name: nginximage: nginx:1.14.0nodeSelector:diskType: ssd添加配置：&gt; kubectl apply -f nginx-ssd.ymldeployment.extensions/nginx-ssd created理想状态是，我们将 nginx-ssd 全部分配到了拥有 diskType: ssd 标签的 k8s-node1 上，我们来验证一下：&gt; kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-ssd-7b6f5c5d56-k6ssz 1/1 Running 0 3m 10.46.0.2 k8s-node1 &lt;none&gt;nginx-ssd-7b6f5c5d56-txt9l 1/1 Running 0 3m 10.46.0.3 k8s-node1 &lt;none&gt;Kubernetes版本管理应用滚动升级当集群中的某个服务需要升级时，我们需要停止目前与该服务相关的所有 Pod，然后重新拉取镜像并启动。如果集群规模比较大，则这个工作就变成了一个挑战，而且先全部停止然后逐步升级的方式会导致较长时间的服务不可用。 kubernetes 提供了 rolling-update（滚动升级）功能来解决上述问题。滚动升级也是通过执行 kubectl apply 命令一键完成，该命令创建了一个新的 RC，然后自动控制旧的 RC 中的 Pod 副本数量逐渐减少到 0，同时新的 RC 中的 Pod 副本数量从 0 逐步增加到目标值，最终实现了 Pod 的升级。需要注意的是，系统要求新的 RC 需要与旧的 RC 在相同的命名空间（Namespace）内，即不能把别人的资产偷偷转移到自家名下。我们现在来将 nginx 的版本从 1.14.0 升级到 1.15.0：apiVersion: extensions/v1beta1kind: Deploymentmetadata:name: nginx-deploymentspec:minReadySeconds: 5strategy:type: RollingUpdaterollingUpdate:maxSurge: 3maxUnavailable: 2replicas: 5template:metadata:labels:app: nginxspec:containers:- name: nginximage: nginx:1.15.0配置文件中有几个参数需要注意：strategy.minReadySeconds：用来指定没有任何容器 crash 的 Pod 并被认为是可用状态的最小秒数， 默认为0strategy.type：用来指定新 Pod 替换旧 Pod 的策略，可以是 Recreate，或是 RollingUpdate（默认值）strategy.rollingUpdate.maxSurge：升级过程中最多可以比原先设定所多出的pod 数量，此栏位可以为固定值或是比例(%)例如. maxSurge: 1、replicas: 5，代表Kubernetes 会先开好1 个新pod 后才删掉一个旧的pod，整个升级过程中最多会有5+1 个podstrategy.rollingUpdate.maxUnavailable：最多可以有几个pod 处在无法服务的状态，当maxSurge不为零时，此栏位亦不可为零。例如. maxUnavailable: 1，代表Kubernetes 整个升级过程中最多会有1 个pod 处在无法服务的状态&gt; kubectl apply -f nginx-deployment-update.yml --recorddeployment.extensions/nginx-deployment created升级完毕，查看最新的版本号：&gt; kubectl describe deploy nginx-deployment...Pod Template:Labels: app=nginxContainers:nginx:Image: nginx:1.15.0...应用回滚当集群中的某个服务升级完成，突然发现了意想不到的 BUG，需要回退到升级之前的版本，但现在所有节点的应用都已经被替换了，要回退回去这可是不小的工作量啊。对于这种情况，kubernetes 也能很轻松的搞定。前提是你之前的操作都有打上 --record参数，这样整个升级过程都能被记录下：&gt; kubectl rollout history deployment nginx-deploymentdeployments &quot;nginx-deployment&quot;REVISION CHANGE-CAUSE1 kubectl apply --filename=nginx-deployment.yml --record=true2 kubectl apply --filename=nginx-deployment-update.yml --record=true观察到我们在 REVISION 2 的时候升级了我们的应用，接下来我们直接回滚到 REVISION 1：&gt; kubectl rollout undo deployment nginx-deployment --to-revision=1deployment.extensions/nginx-deployment再来看下当前应用的版本：&gt; kubectl describe deployment nginx-deployment...Pod Template:Labels: app=nginxContainers:nginx:Image: nginx:1.14.0...果然，应用已经回退到了升级之前的版本，现在可以静下心来修复 BUG 再发布了。但如果我们在发布版本时没有打上 --record 参数，history 里是空的，这时只要原始版本的镜像还在，我们还是可以通过修改配置文件来达到回滚的目的。]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ganglia的安装、配置、运行]]></title>
    <url>%2F2018%2F10%2F09%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2Fganglia%2Fganglia%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>开发工具</category>
        <category>ganglia</category>
      </categories>
      <tags>
        <tag>true</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch详细配置]]></title>
    <url>%2F2018%2F10%2F07%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElaticsearch%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.Elasticsearch cluster 三种角色master node：master节点主要用于元数据（metadata）处理，如、索引的新增、删除、分片data node: data节点上保存了数据片client node: client节点起到路由请求的作用，可看做负载均衡器2.节点选择如果你想让节点从不选举为主节点,只用来存储数据,可作为负载器node.master: falsenode.data: true如果想让节点成为主节点,且不存储任何数据,并保有空闲资源,可作为协调器node.master: truenode.data: false如果想让节点既不称为主节点,又不成为数据节点,那么可将他作为搜索器,从节点中获取数据,生成搜索结果等node.master: falsenode.data: false3.配置说明cluster.name: es-hdfs #集群的名称，同一个集群该值必须设置成相同的node.name: h001 #该节点的名字node.master: true #该节点有机会成为master节点node.data: true #该节点可以存储数据node.rack: r1 #该节点所属的机架network.host: 10.41.2.84 #该参数用于同时设置transport.tcp.port: 9300 #设置节点之间交互的端口号transport.tcp.compress: true #设置是否压缩tcp上交互传输的数据http.port: 9200 #设置对外服务的http 端口号http.max_content_length: 100mb #设置http内容的最大大小http.enabled: true #是否开启http服务对外提供服务discovery.zen.ping.unicast.hosts:[&quot;h001&quot;, &quot;h002&quot;, &quot;h003&quot;,&quot;h004&quot;] #设置集群中的Master节点的初始列表，可以通过这些节点来自动发现其他新加入集群的节点discovery.zen.minimum_master_nodes: 1 #设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可设置大一点的值（2-4）discovery.zen.ping.timeout: 3s #设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错discovery.zen.ping.multicast.enabled: false #设置是否打开多播发现节点，默认是truegateway.recover_after_nodes: 1 #设置集群中N个节点启动时进行数据恢复，默认为1gateway.recover_after_time: 5m #设置初始化数据恢复进程的超时时间，默认是5分钟gateway.expected_nodes: 2 # 设置这个集群中节点的数量，默认为2，一旦这N个节点启动，就会立即进行数据恢复cluster.routing.allocation.node_initial_primaries_recoveries: 4 #初始化数据恢复时，并发恢复线程的个数，默认为4cluster.routing.allocation.node_concurrent_recoveries: 2 #添加删除节点或负载均衡时并发恢复线程的个数，默认为4indices.recovery.max_size_per_sec: 0 #设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制script.engine.groovy.inline.search: onscript.engine.groovy.inline.aggs: onindices.recovery.max_bytes_per_sec: 20mbhttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash详细配置]]></title>
    <url>%2F2018%2F10%2F07%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2Flogstash%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[一、基本语法组成支持的插件Input： elasticsearch,file,http_poller,jdbc,log4j,rss,rabbitmq,redis,syslog,tcp,udp…等Filter： grok,json,mutate,split …等Output： email,elasticsearch,file,http,mongodb,rabbitmq,redis,stdout,tcp,udp …等配置说明地址： https://www.elastic.co/guide/en/logstash/current/input-plugins.htmllogstash.conf配置文件里至少需要有input和output两个部分构成input {#输入}filter {#过滤匹配}output {#输出}二、配置语法讲解1、input配置1.1、file{}（文件读取）监听文件变化，记录一个叫 .sincedb 的数据库文件来跟踪被监听的日志文件的当前读取位（也就是时间戳）input { file { path =&gt; [&quot;/var/log/access.log&quot;, &quot;/var/log/message&quot;] #监听文件路径 type =&gt; &quot;system_log&quot; #定义事件类型 start_position =&gt; &quot;beginning&quot; #检查时间戳 }}参数说明：exclude ：排除掉不想被监听的文件stat_interval ：logstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒。start_position ：logstash 默认是从结束位置开始读取文件数据，也就是说 logstash 进程会以类似 tail -f 的形式运行。如果你是要导入原有数据，把这个设定改成 “beginning”，logstash 进程就按时间戳记录的地方开始读取，如果没有时间戳则从头开始读取，有点类似cat，但是读到最后一行不会终止，而是继续变成 tail -f。2、filter过滤器配置2.1、data（时间处理）用来转换日志记录中的时间字符串，变成LogStash::Timestamp 对象，然后转存到 @timestamp 字段里。注意：因为在稍后的 outputs/elasticsearch 中index常用的 %{+YYYY.MM.dd} 这种写法必须读取 @timestamp数据，所以一定不要直接删掉这个字段保留自己的时间字段，而是应该用 filters/date 转换后删除自己的字段！至于elasticsearch 中index使用 %{+YYYY.MM.dd}这种写法的原因后面会说明。filter { grok { match =&gt; [&quot;message&quot;, &quot;%{HTTPDATE:logdate}&quot;] } date { match =&gt; [&quot;logdate&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;] }}2.2、grok （正则匹配）filter { grok { match =&gt; [ &quot;message&quot;, &quot;\s+(?&lt;status&gt;\d+?)\s+&quot; ] #跟python的正则有点差别 }}优化建议：如果把 “message” 里所有的信息都 grok 到不同的字段了，数据实质上就相当于是重复存储了两份。所以可以用 remove_field 参数来删除掉 message 字段，或者用 overwrite 参数来重写默认的 message 字段，只保留最重要的部分。filter { grok { patterns_dir =&gt; &quot;/path/to/your/own/patterns&quot; match =&gt; { &quot;message&quot; =&gt; &quot;%{SYSLOGBASE} %{DATA:message}&quot; } overwrite =&gt; [&quot;message&quot;] }}filter { grok { match =&gt; [&quot;message&quot;, &quot;%{HTTPDATE:logdate}&quot;] remove_field =&gt; [&quot;logdate&quot;] }}2.3、GeoIP （地址查询归类）GeoIP 是最常见的免费 IP 地址归类查询库，同时也有收费版可以采购。GeoIP 库可以根据 IP 地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。filter { geoip { source =&gt; &quot;clientip&quot; database =&gt; &quot;/etc/logstash/GeoLiteCity.dat&quot; #需去官网下载ip库放到本地 }}filter { geoip { source =&gt; &quot;message&quot; #如果能联网可查询在线ip库 }}注：geoip 插件的 “source” 字段可以是任一处理后的字段，比如 “clientip”，但是字段内容却需要小心！geoip 库内只存有公共网络上的 IP 信息，查询不到结果的，会直接返回 null，而 logstash 的 geoip 插件对 null 结果的处理是：不生成对应的 geoip.字段。所以在测试时，如果使用了诸如 127.0.0.1, 172.16.0.1, 182.168.0.1, 10.0.0.1 等内网地址，会发现没有对应输出！GeoIP 库数据较多，如果不需要这么多内容，可以通过 fields 选项指定自己所需要的。下例为全部可选内容filter { geoip { fields =&gt; [&quot;city_name&quot;, &quot;continent_code&quot;, &quot;country_code2&quot;, &quot;country_code3&quot;, &quot;country_name&quot;, &quot;dma_code&quot;, &quot;ip&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;postal_code&quot;, &quot;region_name&quot;, &quot;timezone&quot;] }}]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filebeat 详细配置]]></title>
    <url>%2F2018%2F10%2F07%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FFilebeat%20%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Filebeat 配置文件中文对照###################### Filebeat Configuration Example ########################## This file is an example configuration file highlighting only the most common# options. The filebeat.reference.yml file from the same directory contains all the# supported options with more comments. You can use it as a reference.## You can find the full configuration reference here:# https://www.elastic.co/guide/en/beats/filebeat/index.html# For more available modules and options, please see the filebeat.reference.yml sample# configuration file.#=========================== Filebeat prospectors =============================filebeat.prospectors:# Each - is a prospector. Most options can be set at the prospector level, so# you can use different prospectors for various configurations.# Below are the prospector specific configurations. # 指定文件的输入类型log(默认)或者stdin- type: log# 更改为 true 以启用此配置。enabled: false#应该爬网和提取的路径paths:- /var/log/*.log# 指定被监控的文件的编码类型，使用plain和utf-8都是可以处理中文日志的 encoding: plain# 排除行, 要匹配的正则表达式的列表. 它将从列表中删除与任何正则表达式匹配的行.#exclude_lines: ['^DBG']# 包含行. 要匹配的正则表达式的列表. 它从列表中导出与任何正则表达式匹配的行.#include_lines: ['^ERR', '^WARN']# 排除文件. 要匹配的正则表达式的列表。 Filebeat 将从列表中删除与任何正则表达式匹配的文件。默认情况下, 不会删除任何文件。#exclude_files: ['.gz$']# 向输出的每一条日志添加额外的信息，比如“level:debug”，方便后续对日志进行分组统计。# 默认情况下，会在输出信息的fields子目录下以指定的新增fields建立子目录，例如fields.level# 这个得意思就是会在es中多添加一个字段，格式为 &quot;filelds&quot;:{&quot;level&quot;:&quot;debug&quot;}#fields:# level: debug# review: 1# 如果该选项设置为true，则新增fields成为顶级目录，而不是将其放在fields目录下。# 自定义的field会覆盖filebeat默认的field# 如果设置为true，则在es中新增的字段格式为：&quot;level&quot;:&quot;debug&quot;#fields_under_root: false# 可以指定Filebeat忽略指定时间段以外修改的日志内容，比如2h（两个小时）或者5m(5分钟)。#ignore_older: 0# 如果一个文件在某个时间段内没有发生过更新，则关闭监控的文件handle。默认1h#close_older: 1h# Defines the buffer size every harvester uses when fetching the file# 每个harvester监控文件时，使用的buffer的大小#harvester_buffer_size: 16384# 日志文件中增加一行算一个日志事件，max_bytes限制在一次日志事件中最多上传的字节数，多出的字节会被丢弃#max_bytes: 10485760### 多行选项# Mutiline 可用于跨越多行的日志消息。这对于 Java 堆栈跟踪或 C 行继续很常见# 必须匹配的 regexp 模式。示例模式匹配所有开始 [#multiline.pattern: ^\[# 定义在模式下设置的模式是否应该被否定。默认值为 false.#multiline.negate: false# 匹配可以设置为 &quot;后&quot; 或 &quot;之前&quot;。它用于定义是否应将行追加到模式 (不匹配) 之前或之后, 或者只要模式不匹配 (基于否定.# Note:后是等同于前和前是等价于下 Logstash#multiline.match: after#=============================Filebeat 模块 ===============================filebeat.config.modules:# 配置加载全局配置的模式(一个目录)path: ${path.config}/modules.d/*.yml# 设置为true来启用配置重载reload.enabled: false# 检查路径下的文件更改的期间（多久检查一次）#reload.period: 10s#==================== Elasticsearch 模板设置 ==========================setup.template.settings:index.number_of_shards: 3#index.codec: best_compression#_source.enabled: false#============================== Kibana =====================================# 从版本6.0.0 开始, 仪表板通过 Kibana API 加载。# 这需要 Kibana 端点配置.setup.kibana:# Kibana Host# 方案和端口可以被排除, 并将被设置为默认 (http and 5601)# 如果您指定和附加路径, 则该方案是必需的: http://localhost:5601/path# IPv6 地址应始终定义为: https://[2001:db8::1]:5601#host: &quot;localhost:5601&quot;#-------------------------- Elasticsearch output ------------------------------output.elasticsearch:# 要连接到的主机的数组。hosts: [&quot;localhost:9200&quot;]# 可选协议和基本身份验证凭据。#protocol: &quot;https&quot;#username: &quot;elastic&quot;#password: &quot;changeme&quot;#----------------------------- Logstash output --------------------------------#output.logstash:# The Logstash hosts#hosts: [&quot;localhost:5044&quot;]# 可选的 SSL。默认为 off。# 用于 HTTPS 服务器验证的根证书列表#ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;]# SSL 客户端身份验证证书#ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;# 客户端证书密钥#ssl.key: &quot;/etc/pki/client/cert.key&quot;#----------------------------- Kafka output -------------------------------- #output.kafka: #enabled: true #hosts: []#topic: 'topic'#================================ Logging =====================================# 设置日志级别。默认日志级别为 &quot;信息&quot;。# 用的日志级别有: critical, error, warning, info, debug#用的日志级别有: 关键、错误、警告、信息、调试#logging.level: debug# 在调试级别, 您可以有选择地仅对某些组件启用日志记录。#要启用所有选择器, 请使用 [&quot;*&quot;]。其他选择器的例子是 &quot;beat&quot;,# &quot;publish&quot;, &quot;service&quot;.#logging.selectors: [&quot;*&quot;] © 著作权归作者所有]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch CURD Rest-api]]></title>
    <url>%2F2018%2F10%2F06%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElasticsearch%20CURD%20Rest-api%2F</url>
    <content type="text"><![CDATA[本文目标 本文通过对比关系型数据库，将ES中常见的增、删、改、查操作进行图文呈现。能加深你对ES的理解。同时，也列举了kibana下的图形化展示。ES Restful API GET、POST、PUT、DELETE、HEAD含义 1）GET：获取请求对象的当前状态。 2）POST：改变对象的当前状态。 3）PUT：创建一个对象。 4）DELETE：销毁对象。 5）HEAD：请求获取对象的基础信息。Mysql与Elasticsearch核心概念对比示意图 以上表为依据， ES中的新建文档（在Index/type下）相当于Mysql中（在某Database的Table）下插入一行数据。ES的集群管理1、查看集群状态 http://localhost:9200/_cat/health?v 返回值 从以上的返回值中，我们可以得到一个名为ElasticSearch的集群，共有一个节点，没有索引数据。2、查看所有节点的状态 http://localhost:9200/_cat/indices?v ES的CURD操作1、新建文档（类似mysql insert插入操作） 当你创建一个文档时，无需再这之前创建一个索引和类型，ElasticSearch会自动根据你的创建信息自动创建相应的索引、类型，直至文档。 备注: 如果不指定_id, elasticsearch会自动帮我们生成http://localhost:9200/blog/ariticle/1 put{ &quot;title&quot;:&quot;New version of Elasticsearch released!&quot;, &quot;content&quot;:&quot;Version 1.0 released today!&quot;, &quot;tags&quot;:[&quot;announce&quot;,&quot;elasticsearch&quot;,&quot;release&quot;]}创建成功如下显示：{ - &quot;_index&quot;: &quot;blog&quot;, - &quot;_type&quot;: &quot;ariticle&quot;, - &quot;_id&quot;: &quot;1 -d&quot;, - &quot;_version&quot;: 1, - &quot;_shards&quot;: { - &quot;total&quot;: 2, - &quot;successful&quot;: 1, - &quot;failed&quot;: 0 - }, - &quot;created&quot;: true} 2、检索文档（类似mysql search 搜索select*操作）通过_id来查询文档http://localhost:9200/blog/ariticle/1/ GET检索结果如下：{- &quot;_index&quot;: &quot;blog&quot;,- &quot;_type&quot;: &quot;ariticle&quot;,- &quot;_id&quot;: &quot;1&quot;,- &quot;_version&quot;: 1,- &quot;found&quot;: true,- &quot;_source&quot;: { - &quot;title&quot;: &quot;New version of Elasticsearch released!&quot;, - &quot;content&quot;: &quot;Version 1.0 released today!&quot;, - &quot;tags&quot;: [&quot;announce&quot;, &quot;elasticsearch&quot; , &quot;release&quot;]- }}如果未找到会提示：{ - &quot;_index&quot;: &quot;blog&quot;, - &quot;_type&quot;: &quot;ariticle&quot;, - &quot;_id&quot;: &quot;11&quot;, - &quot;found&quot;: false}查询全部文档如下：具体某个细节内容检索查询举例1：查询cotent列包含版本为1.0的信息。http://localhost:9200/blog/article/_search?pretty&amp;q=content:1.0{ - &quot;took&quot;: 2, - &quot;timed_out&quot;: false, - &quot;_shards&quot;: { - &quot;total&quot;: 5, - &quot;successful&quot;: 5, - &quot;failed&quot;: 0 - }, - &quot;hits&quot;: { - &quot;total&quot;: 1, - &quot;max_score&quot;: 0.8784157, - &quot;hits&quot;: [ - { - &quot;_index&quot;: &quot;blog&quot;, - &quot;_type&quot;: &quot;ariticle&quot;, - &quot;_id&quot;: &quot;6&quot;, - &quot;_score&quot;: 0.8784157, - &quot;_source&quot;: { - &quot;title&quot;: &quot;deep Elasticsearch!&quot;, - &quot;content&quot;: &quot;Version 1.0!&quot;, - &quot;tags&quot;: [ - &quot;deep&quot;, - &quot;elasticsearch&quot; - ] - } - } - ]- }}查询举例2：查询书名title中包含“enhance”字段的数据信息：# curl -X GET 10.200.1.121:9200/blog/ariticle/_search?pretty -d &gt; '{ &quot;query&quot; : {&gt; &quot;term&quot; :&gt; {&quot;title&quot; : &quot;enhance&quot; }&gt; }&gt; }'查询举例3：查询ID值为3,5,7的数据信息：# curl -X GET 10.200.1.121:9200/blog/ariticle/_search?pretty -d '{ &quot;query&quot; : {&quot;terms&quot; :{&quot;_id&quot; : [ &quot;3&quot;, &quot;5&quot;, &quot;7&quot; ] }}}'查询举例4：分页查询# curl -X GET 10.200.1.121:9200/blog/ariticle/_search?pretty -d '{ &quot;query&quot; : {&quot;terms&quot; :{&quot;_id&quot; : [ &quot;3&quot;, &quot;5&quot;, &quot;7&quot; ] }}，&quot;from&quot;: 1,&quot;size&quot;: 10}'3、更新文档（类似mysql update操作， 有则更新，无则新建）http://localhost:9200/blog/ariticle/1/_update/ POST{&quot;script&quot;:&quot;ctx._source.content = /&quot;new version 2.0 20160714/&quot;&quot;}更新后结果显示：{“_index”: “blog”,“_type”: “ariticle”,“_id”: “1”,“_version”: 2,“_shards”: {”total”: 2,“successful”: 1,“failed”: 0}}注意更新文档需要在config\elasticsearch.yml下新增以下内容：script.groovy.sandbox.enabled: truescript.engine.groovy.inline.search: onscript.engine.groovy.inline.update: onscript.inline: onscript.indexed: onscript.engine.groovy.inline.aggs: onindex.mapper.dynamic: true4、删除文档（类似mysql delete操作）http://localhost:9200/blog/ariticle/8/ DELETE{- &quot;found&quot;: true,- &quot;_index&quot;: &quot;blog&quot;,- &quot;_type&quot;: &quot;ariticle&quot;,- &quot;_id&quot;: &quot;8&quot;,- &quot;_version&quot;: 2,- &quot;_shards&quot;: {- &quot;total&quot;: 2,- &quot;successful&quot;: 1,- &quot;failed&quot;: 0- }}5、查询过滤过滤器类型过滤器类型与查询类型基本相对应，都有范围（过滤）查询、term、terms等类型。term、terms过滤term、terms的含义与查询时一致。term用于精确匹配、terms用于多词条匹配。不过既然过滤器既然适用于大范围过滤，term、terms在过滤中使用意义不大。范围过滤（range）查询16年10月以来所有内容含有“java”的文档，先过滤剩下符合10月的文章，再精确匹配。{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;query&quot;:{ &quot;match&quot;:{ &quot;contents&quot;:&quot;java&quot; } }, &quot;filter&quot;:{ &quot;range&quot;:{ &quot;date&quot;:{ &quot;gte&quot;:&quot;2016-10-01&quot; } } } } }}exists、mising过滤器exists过滤指定字段没有值的文档{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;exists&quot;:{ &quot;field&quot;:&quot;id&quot; } } } }}将不返回id字段无值的文档。missing 过滤器与exists相反，它过滤指定字段有值的文档。{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;missing&quot;:{ &quot;field&quot;:&quot;id&quot; } } } }}标识符（ids）过滤器需要过滤出若干指定_id的文档，可使用标识符过滤器(ids){ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;ids&quot;:{ &quot;values&quot;:[1,2,6,7] } } } }}组合过滤器可以对这些过滤器组合使用，ES中有2类组合过滤器。一类是bool过滤器，一类是and、or、not过滤器。bool过滤器对应布尔查询（bool）bool过滤占用资源，总结来说，一般情况下用bool过滤器，当遇到位置查询、数值范围、脚本查询时使用and、or、not过滤。查询日期在16年10月份且文章标题或内容包含“ES“的文档用SQL表示为select * from article where date between '2016-10-01' and '2016-10-31' and (name like '%ES%' or contents like '%ES%')使用bool过滤器{ &quot;query&quot;: { &quot;filtered&quot;:{ &quot;filter&quot;:{ &quot;bool&quot;:{ &quot;should&quot;:[{ &quot;match&quot;:{ &quot;name&quot;:&quot;ES&quot; } },{ &quot;match&quot;:{ &quot;contents&quot;:&quot;ES&quot; } }], &quot;must&quot;:{ range&quot;:{ &quot;date&quot;:{ &quot;gte&quot;:&quot;2016-10-01&quot;, &quot;lte&quot;:&quot;2016-10-31&quot; } } } } } } }}布尔查询也可嵌套布尔查询，如要查询16年10月份内容有关ES的文章或任何时段文章名称与java相关的所有文章。（这里只是举例，不考虑实际情况）用SQL表示为select * from article where contents like '%ES%' and date between '2016-10-01' and '2016-10-31' or name like '%java%'{ &quot;query&quot;: { &quot;filtered&quot;: { &quot;filter&quot;: { &quot;bool&quot;: { &quot;should&quot;: [{ &quot;bool&quot;: { &quot;must&quot;: [{ &quot;range&quot;: { &quot;date&quot;: { &quot;gte&quot;: &quot;2016-09-01&quot;, &quot;lte&quot;: &quot;2016-09-30&quot; } } }, { &quot;match&quot;: { &quot;contents&quot;: &quot;js&quot; } }] } }, { &quot;match&quot;: { &quot;name&quot;: &quot;java&quot; } }] } } } }}我们查询的主要用or关系连接，所以使用bool查询的should语句。一边是contents like '%ES%' and date between '2016-10-01' and '2016-10-31' ，另一个为name like '%java%' 前者需嵌套一个bool查询，使用must连接，后者简单使用match即可。]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群部署]]></title>
    <url>%2F2018%2F10%2F05%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElasticsearch%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Elasticsearch集群部署详解环境：centos6.9、jdk1.8.0_151、elasticsearch-5.6.5注意：es5.x要求jdk1.8，否则会报错本文以讲解Elasticsearch三个节点的分布式部署、核心配置的含义以及分布式部署遇到的坑。部署节点原理多机集群中的节点可以分为master nodes和data nodes,在配置文件中使用Zen发现(Zen discovery)机制来管理不同节点。Zen发现是ES自带的默认发现机制，使用多播发现其它节点。只要启动一个新的ES节点并设置和集群相同的名称这个节点就会被加入到集群中。（所以，同集群的集群名称一致，才能便于自动发现）Elasticsearch集群中有的节点一般有三种角色:master node、data node和client node。1）master node——master节点点主要用于元数据(metadata)的处理，比如索引的新增、删除、分片分配等。2）client node——client 节点起到路由请求的作用，实际上可以看做负载均衡器。3）data node——data节点上保存了数据分片。它负责数据相关操作，比如分片的 CRUD，以及搜索和整合操作。这些操作都比较消耗 CPU、内存和 I/O 资源；三节点 Elasticsearch 分布式部署(示例)0、节点规划master node:10.0.15.57client node:10.0.15.12data node:10.0.15.21、下载 下载地址 https://www.elastic.co/downloads/past-releases/elasticsearch-5-6-5wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.5.tar.gz2、解压并创建日志和数据目录tar zxvf elasticsearch-5.6.5.tar.gzcd elasticsearchmkdir datamkdir logs3、ES所有配置作用介绍参考Elasticsearch详细配置4、修改ES配置文件 进入到config文件夹，编辑 elasticsearch.ymlvim elasticsearch-5.6.5/config/elasticsearch.yml 步骤1：配置好主节点Master信息。cluster.name: esnode.name: &quot;test01-master&quot;node.master: truenode.data: truepath.data: /opt/elasticsearch/datapath.logs: /opt/elasticsearch/logsnetwork.host: 10.0.15.57http.port: 9200http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;discovery.zen.ping.unicast.hosts: [&quot;10.0.15.57:9300&quot;, &quot;10.0.15.12:9300&quot;,&quot;10.0.15.2:9300&quot;]discovery.zen.minimum_master_nodes: 2gateway.recover_after_nodes: 3gateway.recover_after_time: 5mgateway.expected_nodes: 15、拷贝到其他机器上6、修改配置文件 cient节点；修改节点名称信息。 只列举不一样的配置：node.name: &quot;test02-cient&quot;node.master: truenode.data: falsenetwork.host: 10.0.15.12 data节点；修改节点名称 只列举不一样的配置： node.name: &quot;test03-data&quot; node.master: false node.data: true network.host: 10.0.15.27、创建用户 +++注意:三台机器都要创建+++ root用户无法启动es,必须新建一个其他用户,并对其赋予es目录的操作权限 ，这里我新建了es用户和组 创建组grupadd es 创建用户useradd es -g es 修改用户和组chown -R es:es es的安装目录 查看 8、启动 分别运行Master，client，data节点（顺序无关）./elasticsearch -d 成功效果 用head插件可以看到各个节点情况 9、分布式部署遇到的坑 错误1：max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536] 这个问题是无法创建本地文件,用户最大可创建文件数太小 解决：只需要修改创建文件的最大数目为65536就行了 切换到root用户修改 vim /etc/security/limits.confroot soft nofile 65536root hard nofile 65536* soft nofile 65536* hard nofile 65536保存、退出、重新登录才可生效 参数解释： - soft nproc:可打开的文件描述符的最大数(软限制) - hard nproc:可打开的文件描述符的最大数(硬限制) - soft nofile:单个用户可用的最大进程数量(软限制) - hard nofile:单个用户可用的最大进程数量(硬限制) 错误2：max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 虚拟内存太小 切换到root用户修改 vim /etc/sysctl.confvm.max_map_count=262144 执行命令： sudo sysctl -p 错误3：Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000085330000, 2060255232, 0) failed; error=’Cannot allocate memory’ (errno=12) jvm需要分配的内存太大 vim config/jvm.options-Xms2g-Xmx2g该为：-Xms100m-Xmx100m 错误4：max number of threads [1024] for user [es] likely too low, increase to at least [2048] 原因：无法创建本地线程问题,用户最大可创建线程数太小 解决方案：切换到root用户，进入limits.d目录下，修改90-nproc.conf 配置文件 成功启动： vi /etc/security/limits.d/90-nproc.conf 找到如下内容： * soft nproc 1024 修改为 * soft nproc 2048 错误5：n ERROR No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property ‘log4j2.debug‘ to show Log4j2 internal initialization logging. es用 yum install -y log4j* 错误6：unknown setting [discovery.zen.ping.multicast.enabled] please check that any required plugins are installed, or check the breaking changes documentation for removed settings 在elasticsearch.yml文件中 添加bootstrap.system_call_filter: false 错误7：三节点不能联通的：原因： 1）各节点的hostname没有正确设置，和节点名称设置为一致。 2）关闭防火墙，service iptables stop；否则，打开防火墙会导致无法正常通信，head插件不能看到节点数据信息。]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK+filebeat系统搭建(下载和配置)]]></title>
    <url>%2F2018%2F10%2F03%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FELK%2Bfilebeat%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%8B%E8%BD%BD%E5%92%8C%E9%85%8D%E7%BD%AE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[系统环境System：centos 7Java ： jdk 1.8.0_144Elasticsearch： 6.1.1Kibana： 6.1.1Logstash： 6.1.1Filebeat： 6.1.1软件介绍ELK是Elasticsearch、Logstrash和Kibana这三个软件的首字母的缩写，它们代表的是一套成熟的日志管理系统:Logstrash作为数据收集引擎。它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置,一般会发送给Elasticsearch;Elasticsearch是个分布式搜索和分析引擎，优点是能对大容量的数据进行接近实时的存储、搜索和分析操作;Kibana对Elasticsearch的分析搜索做出可视化的界面展示。Filebeat，它的作用是在客户端收集和传输日志，基于 Logstash-Forwarder 源代码开发，是对它的替代.tips： Logstash 的运行依赖于 Java 运行环境， Logstash 1.5 以上版本不低于 java 7 推荐使用最新版本的 Java 。由于我们只是运行 Java 程序，而不是开发，下载 JRE 即可。首先，在 Oracle 官方下载新版 jre ，下载地址： http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html安装过程Java开发环境 1）下载jdk-8u131-linux-x64.tar，上传至CentOS上。 下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html （该链接为jre的链接） 2） 解压源码包 # mkdir -pv /usr/local/java # tar -zxvf jdk-8u131-linux-x64.tar.gz -C /usr/local/java 3） 设置JDK环境变量 # vi /etc/profile # export JAVA_HOME=/usr/local/java/jdk1.8.0_111 # export JRE_HOME=\${JAVA_HOME}/jre # export CLASSPATH=\${JAVA_HOME}/lib:\${JRE_HOME}/lib # export PATH=\${JAVA_HOME}/bin:$PATH # source /etc/profile 4） 检验是否安装成功 # java -version安装elasticsearch1）下载文件 进入elsaticsearch下载界面 选择tar进行下载，下载完成后放入/opt下（放置位置个人喜好）2）解压文件 进入压缩包所在目录，解压文件。cd /opt/svr/elasticsearchtar -zxvf ./elasticsearch-6.1.1.tar.gz -C /opt/svr/elasticsearch3）下载x-pack插件cd elasticsearch-6.1.1/bin./elasticsearch-plugin install x-pack4）修改ES参数 进入解压后文件夹的config目录，增加或者更改其中的配置信息cd elasticsearch-6.1.1vi config/elasticsearch.yml# 这里指定的是集群名称，需要修改为对应的，开启了自发现功能后，ES会按照此集群名称进行集群发现cluster.name: mortynode.name: Rick# 数据目录path.data: /data/elk/data# log 目录path.logs: /data/elk/logs# 修改一下ES的监听地址，这样别的机器也可以访问vnetwork.host: 0.0.0.0# 默认的端口号http.port: 9200# 填写你要监视的地址，可以是多个discovery.zen.ping.unicast.hosts: [“172.18.5.111”, “172.18.5.112”]# discovery.zen.minimum_master_nodes: 3# enable cors，保证_site类的插件可以访问eshttp.cors.enabled: truehttp.cors.allow-origin: “*”# Centos6不支持SecComp，而ES5.2.0默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。bootstrap.memory_lock: falsebootstrap.system_call_filter: false tips：单个节点可以作为一个运行中的Elasticsearch的实例。而一个集群是一组拥有相同cluster.name的节点，（单独的节点也可以组成一个集群）可以在elasticsearch.yml配置文件中修改cluster.name,该节点启动时加载（需要重启服务后才会生效）。 5）修改系统参数 修改系统参数以确保系统有足够资源启动ES 1. 设置内核资源vi /etc/sysctl.confvm.max_map_count=655360sysctl -p2.设置资源参数vi /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 65536* hard nproc 131072Tips: limits.conf 配置3.设置用户资源参数vi /etc/security/limits.d/20-nproc.confelk soft nproc 655364.添加启动用户，设置权限启动ElasticSearch5版本要非root用户，需要新建一个用户来启动ElasticSearchuseradd elk #创建用户elkgroupadd elk #创建组elk useraddusermod -g elk elk #将用户添加到组mkdir -pv /data/elk/{data,logs}# 创建数据和日志目录chown -R elk:elk /data/elk/ # 修改文件所有者chown -R elk:elk /opt/svr/elasticsearch6）启动su elkcd elasticsearch-6.1.1bin/elasticsearch -d #-d 为保持后台运行free -g 查看内存剩余 使用curl localhost:9200 来检查ES是否启动成功： 备注: 客户端需要先关闭防火墙 systemctl stop firewalld.service{“name” : “morty”,“cluster_name” : “Rick”,“cluster_uuid” : “4RlZck7ERWGbQKNn5KvXGA”,“version” : {“number” : “6.1.1”,“build_hash” : “bd92e7f”,“build_date” : “2017-12-17T20:23:25.338Z”,“build_snapshot” : false,“lucene_version” : “7.1.0”,“minimum_wire_compatibility_version” : “5.6.0”,“minimum_index_compatibility_version” : “5.0.0”},“tagline” : “You Know, for Search”}安装Elasticsearch-head 1）下载文件 下载地址: https://github.com/mobz/elasticsearch-head unzip elasticsearch-head-master.zip 2）下载node-js curl --silent --location https://rpm.nodesource.com/setup | bash - yum install -y nodejs 3）下载grunt（由于head 插件的执行文件是有grunt 命令来执行的，所以这个命令必须安装） # npm install grunt --save-dev # cd elasticsearch-head-master # npm install 4）修改配置文件 进入elasticsearch-head-master 文件夹下，执行命令vi Gruntfile.js文件：增加hostname属性，设置为*, 修改启动端口,默认为9100 5）启动elasticsearch-head 6）查看启动状态( 浏览器访问 http://ip地址:9100/ ) 如果出现集群健康值: 未连接的状态，是因为没有配置ElasticSearch的跨域访问，默认是禁止的，所以链接失败。 解决方法: 需要修改elasticsearch配置文件；命令进入到elasticsearch-5.6.5 /config 文件中 elasticsearch.yml，添加 保存之后重启elasticsearch和head即可 安装kibana1）下载文件 进入kibana下载界面 选择tar进行下载，下载完成后放入/opt/elk下（放置位置个人喜好）2）解压文件 进入压缩包所在目录，解压文件。cd /opt/svr/kibanatar -zxvf ./kibana-6.1.1-linux-x86_64.tar.gz -C /opt/svr/kibana3）配置kibana 进入config目录下，修改kibana.yml文件cd kibana-6.1.1-linux-x86_64vi config/kibana.ymlserver.port: 5601 #开启默认端口5601速冻server.host: “192.168.91.129” #站点地址elasticsearch.url: http://192.168.91.129:9200 #指向&gt;elasticsearch服务的ip地址kibana.index: “.kibana”4）启动kibanabin/kibana 启动成功后访问浏览器访问地址http://192.168.91.129:9200见到如下即表示启动成功 安装logstash1）下载文件 进入logstrash下载界面 选择tar进行下载，下载完成后放入/opt/elk下（放置位置个人喜好）2）解压文件进入压缩包所在目录，解压文件。cd /opt/svr/logstashtar -zxvf ./logstrash-6.1.1.tar.gz -C /opt/svr/logstash3）创建配置文件进入解压后文件夹的config目录，新增测试配置文件：cd /opt/svr/logstash/logstash-6.1.1/config/vi logstrash-test.confLogstash 使用 input 和 output 定义收集日志时的输入和输出的相关配置4）启动服务bin/logstash -f config/logstash-simple.conf 执行这个语句可能会报错：[FATAL][logstash.runner] Logstash could not be started because there is already another instance using the configured data directory. If you wish to run multiple instances, you must change the “path.data” setting.这是因为当前的logstash版本不支持多个instance共享一个path.data，所以需要在启动时，命令行里增加”–path.data PATH “，为不同实例指定不同的路径bin/logstash -f config/logstash-simple.conf --path.data ./logs/安装filebeat1）下载文件 进入filebeat下载界面 选择Liunx-64x包进行下载，下载完成后放入需要检测的服务器上的任一目录下（放置位置个人喜好）2）解压文件 进入压缩包所在目录，解压文件。cd /opt/svr/filebeattar -zxvf ./filebeat-6.1.1-linux-x86_64.tar.gz -C /opt/svr/filebeat3）修改配置文件进入解压后文件夹的config目录，新增测试配置文件：cd filebeat-6.1.1-linux-x86_64vi filebeat.ymlpaths:- /opt/logs/filebeat.log #监控的日志文件————–Elasticsearch output——————-(全部注释掉)—————-Logstash output———————output.logstash:hosts: [“182.119.137.177:5044”] #你的logstash端口4）启动服务bin/filebeat -e -c config/filebeat.yml更多ELK资料：Elasticsearch+Logstash+Kibana教程Elasticsearch 官方文档参考Logstash 官方文档参考Kibana User Guide 官方文档参考]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch&ELK介绍]]></title>
    <url>%2F2018%2F10%2F02%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FELKB%2FElasticsearch%26ELK%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[（1）思考：大规模数据如何检索？如：当系统数据量上了10亿、100亿条的时候，我们在做系统架构的时候通常会从以下角度去考虑问题：1）用什么数据库好？(mysql、sybase、oracle、达梦、神通、mongodb、hbase…)2）如何解决单点故障；(lvs、F5、A10、Zookeep、MQ)3）如何保证数据安全性；(热备、冷备、异地多活)4）如何解决检索难题；(数据库代理中间件：mysql-proxy、Cobar、MaxScale等;)5）如何解决统计分析问题；(离线、近实时)（2）传统数据库的应对解决方案对于关系型数据，我们通常采用以下或类似架构去解决查询瓶颈和写入瓶颈：解决要点：1）通过主从备份解决数据安全性问题；2）通过数据库代理中间件心跳监测，解决单点故障问题；3）通过代理中间件将查询语句分发到各个slave节点进行查询，并汇总结果（3）非关系型数据库的解决方案对于Nosql数据库，以mongodb为例，其它原理类似：解决要点：1）通过副本备份保证数据安全性；2）通过节点竞选机制解决单点问题；3）先从配置库检索分片信息，然后将请求分发到各个节点，最后由路由节点合并汇总结果另辟蹊径——完全把数据放入内存怎么样？我们知道，完全把数据放在内存中是不可靠的，实际上也不太现实，当我们的数据达到PB级别时，按照每个节点96G内存计算，在内存完全装满的数据情况下，我们需要的机器是：1PB=1024T=1048576G节点数=1048576/96=10922个实际上，考虑到数据备份，节点数往往在2.5万台左右。成本巨大决定了其不现实！从前面讨论我们了解到，把数据放在内存也好，不放在内存也好，都不能完完全全解决问题。全部放在内存速度问题是解决了，但成本问题上来了。为解决以上问题，从源头着手分析，通常会从以下方式来寻找方法：1、存储数据时按有序存储；2、将数据和索引分离；3、压缩数据；这就引出了Elasticsearch。1. ES 基础一网打尽1.1 ES定义ES=elaticsearch简写， Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。1.2 Lucene与ES关系？1）Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。2）Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。1.3 ES主要解决问题：1）检索相关数据；2）返回统计结果；3）速度要快。1.4 ES工作原理当ElasticSearch的节点启动后，它会利用多播(multicast)(或者单播，如果用户更改了配置)寻找集群中的其它节点，并与之建立连接。这个过程如下图所示：1.5 ES核心概念1）Cluster：集群。ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。2）Node：节点。形成集群的每个服务器称为节点。3）Index：索引。index是一系列document 的集合。比方说我们可以有一个index叫做 customer data，另外一个叫做product catalog，还有一个叫做 order data。一个index被一个唯一的小写字母名字标记。这个名字将会影响里面数据的查询，更新操作2）Type：分组。在一个Index中，我们可以定义很多的type。一个type是一个逻辑分组。大多书情况下，同一个type中的数据会有相同的数据结构。4） Document ：记录。一个Document是一个可以被索引的基本单元。3）Shard：分片。当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。4）Replia：副本。为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。5）全文检索。全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。1.6 ES数据架构的主要概念（与关系数据库Mysql对比）（1）关系型数据库中的数据库（DataBase），等价于ES中的索引（Index）（2）一个数据库下面有N张表（Table），等价于1个索引Index下面有N多类型（Type），（3）一个数据库表（Table）下的数据由多行（ROW）多列（column，属性）组成，等价于1个Type由多个文档（Document）和多Field组成。（4）在一个关系型数据库里面，schema定义了表、每个表的字段，还有表和字段之间的关系。 与之对应的，在ES中：Mapping定义索引下的Type的字段处理规则，即索引如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等。（5）在数据库中的增insert、删delete、改update、查search操作等价于ES中的增PUT/POST、删Delete、改_update、查GET.1.7 ELK是什么？ELK=elasticsearch+Logstash+kibanaELK是三个开源软件的缩写，分别表示：Elasticsearch , Logstash, Kibana , 它们都是开源软件。新增了一个FileBeat，它是一个轻量级的日志收集处理工具(Agent)，Filebeat占用资源少，适合于在各个服务器上搜集日志后传输给Logstash，官方也推荐此工具。Elasticsearch： 是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。Logstash：主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上去。Kibana： 也是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。Filebeat：隶属于Beats。目前Beats包含四种工具：1. Packetbeat（搜集网络流量数据）2. Topbeat（搜集系统、进程和文件系统级别的 CPU 和内存使用情况等数据）3. Filebeat（搜集文件数据）4. Winlogbeat（搜集 Windows 事件日志数据）ELK架构图架构图1）这是最简单的一种ELK架构方式。优点是搭建简单，易于上手。缺点是Logstash耗资源较大，运行占用CPU和内存高。另外没有消息队列缓存，存在数据丢失隐患。此架构由Logstash分布于各个节点上搜集相关日志、数据，并经过分析、过滤后发送给远端服务器上的Elasticsearch进行存储。Elasticsearch将数据以分片的形式压缩存储并提供多种API供用户查询，操作。用户亦可以更直观的通过配置Kibana Web方便的对日志查询，并根据数据生成报表。架构图2）此种架构引入了消息队列机制，位于各个节点上的Logstash Agent先将数据/日志传递给Kafka（或者Redis），并将队列中消息或数据间接传递给Logstash，Logstash过滤、分析后将数据传递给Elasticsearch存储。最后由Kibana将日志和数据呈现给用户。因为引入了Kafka（或者Redis）,所以即使远端Logstash server因故障停止运行，数据将会先被存储下来，从而避免数据丢失。架构图3）此种架构将收集端logstash替换为beats，更灵活，消耗资源更少，扩展性更强。同时可配置Logstash 和Elasticsearch 集群用于支持大集群系统的运维日志数据监控和查询。2. ES特点和优势1）分布式实时文件存储，可将每一个字段存入索引，使其可以被检索到。2）实时分析的分布式搜索引擎。分布式：索引分拆成多个分片，每个分片可有零个或多个副本。集群中的每个数据节点都可承载一个或多个分片，并且协调和处理各种操作；负载再平衡和路由在大多数情况下自动完成。3）可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。也可以运行在单台PC上（已测试）4）支持插件机制，分词插件、同步插件、Hadoop插件、可视化插件等。3、ES性能3.1 性能结果展示（1）硬件配置：CPU 16核 AuthenticAMD内存 总量：32GB硬盘 总量：500GB 非SSD（2）在上述硬件指标的基础上测试性能如下：1）平均索引吞吐量： 12307docs/s（每个文档大小：40B/docs）2）平均CPU使用率： 887.7%（16核，平均每核：55.48%）3）构建索引大小： 3.30111 GB4）总写入量： 20.2123 GB5）测试总耗时： 28m 54s.4、为什么要用ES？4.1 ES国内外使用优秀案例1） 2013年初，GitHub抛弃了Solr，采取ElasticSearch 来做PB级的搜索。 “GitHub使用ElasticSearch搜索20TB的数据，包括13亿文件和1300亿行代码”。2）维基百科：启动以elasticsearch为基础的核心搜索架构。3）SoundCloud：“SoundCloud使用ElasticSearch为1.8亿用户提供即时而精准的音乐搜索服务”。4）百度：百度目前广泛使用ElasticSearch作为文本数据分析，采集百度所有服务器上的各类指标数据及用户自定义数据，通过对各种数据进行多维分析展示，辅助定位分析实例异常或业务层面异常。目前覆盖百度内部20多个业务线（包括casio、云分析、网盟、预测、文库、直达号、钱包、风控等），单集群最大100台机器，200个ES节点，每天导入30TB+数据。4.2 我们也需要实际项目开发实战中，几乎每个系统都会有一个搜索的功能，当搜索做到一定程度时，维护和扩展起来难度就会慢慢变大，所以很多公司都会把搜索单独独立出一个模块，用ElasticSearch等来实现。近年ElasticSearch发展迅猛，已经超越了其最初的纯搜索引擎的角色，现在已经增加了数据聚合分析（aggregation）和可视化的特性，如果你有数百万的文档需要通过关键词进行定位时，ElasticSearch肯定是最佳选择。当然，如果你的文档是JSON的，你也可以把ElasticSearch当作一种“NoSQL数据库”， 应用ElasticSearch数据聚合分析（aggregation）的特性，针对数据进行多维度的分析。【知乎：热酷架构师潘飞】ES在某些场景下替代传统DB个人以为Elasticsearch作为内部存储来说还是不错的，效率也基本能够满足，在某些方面替代传统DB也是可以的，前提是你的业务不对操作的事性务有特殊要求；而权限管理也不用那么细，因为ES的权限这块还不完善。由于我们对ES的应用场景仅仅是在于对某段时间内的数据聚合操作，没有大量的单文档请求（比如通过userid来找到一个用户的文档，类似于NoSQL的应用场景），所以能否替代NoSQL还需要各位自己的测试。如果让我选择的话，我会尝试使用ES来替代传统的NoSQL，因为它的横向扩展机制太方便了。5. ES的应用场景是怎样的？通常我们面临问题有两个：1）新系统开发尝试使用ES作为存储和检索服务器；2）现有系统升级需要支持全文检索服务，需要使用ES。以上两种架构的使用，以下链接进行详细阐述。http://blog.csdn.net/laoyang360/article/details/52227541一线公司ES使用场景：1）新浪ES 如何分析处理32亿条实时日志 http://dockone.io/article/5052）阿里ES 构建挖财自己的日志采集和分析体系 http://afoo.me/columns/tec/logging-platform-spec.html3）有赞ES 业务日志处理 http://tech.youzan.com/you-zan-tong-ri-zhi-ping-tai-chu-tan/4）ES实现站内搜索 http://www.wtoutiao.com/p/13bkqiZ.html6. 如何部署ES？6.1 ES部署（无需安装）1）零配置，开箱即用2）没有繁琐的安装配置3）java版本要求：最低1.7我使用的1.8[root@laoyang config_lhy]# echo $JAVA_HOME/opt/jdk1.8.0_914）下载地址：https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.3.5/elasticsearch-2.3.5.zip5）启动cd /usr/local/elasticsearch-2.3.5./bin/elasticsearchbin/elasticsearch -d(后台运行)6.2 ES必要的插件必要的Head、kibana、IK（中文分词）、graph等插件的详细安装和使用。http://blog.csdn.net/column/details/deep-elasticsearch.html6.3 ES windows下一键安装自写bat脚本实现windows下一键安装。1）一键安装ES及必要插件（head、kibana、IK、logstash等）2）安装后以服务形式运行ES。3）比自己摸索安装节省至少2小时时间，效率非常高。脚本说明：http://blog.csdn.net/laoyang360/article/details/519002357. ES对外接口（开发人员关注）1）JAVA API接口http://www.ibm.com/developerworks/library/j-use-elasticsearch-java-apps/index.html2）RESTful API接口常见的增、删、改、查操作实现：http://blog.csdn.net/laoyang360/article/details/51931981]]></content>
      <categories>
        <category>数据存储</category>
        <category>ELKB</category>
      </categories>
      <tags>
        <tag>ELKB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper集群搭建]]></title>
    <url>%2F2018%2F09%2F18%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2FZookeeper%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[软件环境准备:Linux服务器一台、三台、五台（2*n+1台）；Java jdk 1.7；zookeeper 3.4.6版；软件安装：解压jdk、zookeeper文件到指定目录，执行命令tar -zvxf xxxx.tar.gz -C /usr/local/program配置环境变量，vi /etc/profile#set enviromentexport JAVA_HOME=/usr/local/program/jdk1.7.0_79export ZK_HOME=/usr/local/program/zk/zookeeper-3.4.6export PATH=$JAVA_HOME/bin:$ZK_HOME/bin:$PATHjava -version 命令测试是否成功配置jdk[root@localhost ~]# java -versionjava version &quot;1.7.0_79&quot;Java(TM) SE Runtime Environment (build 1.7.0_79-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)配置zookeeper；首先建立两个文件夹mkdir /usr/local/program/zk/zkdata ，mkdir /usr/local/program/zk/zkdataLog 然后执行命令cd /usr/local/program/zk/zookeeper-3.4.6/conf/ 进入配置文件目录，拷贝一份配置文件cp zoo_sample.cfg zoo.cfg改名为zoo.cfg编辑配置文件修改配置（将端口改大的目的是为了防止冲突）：# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/usr/local/program/zk/zkdatadataLogDir=/usr/local/program/zk/zkdataLog# the port at which the clients will connectclientPort=12181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1server.1=192.168.0.102:12888:13888server.2=192.168.0.103:12888:13888server.3=192.168.0.104:12888:13888进入zkdata目录，执行命令echo “1” &gt; myid创建myid文件并输入值为1依次在另外两台机器上执行同样的操作，myid的数值依次为2,3配置成功，执行命令zkServer.sh start分别启动三台机器执行命令zkServer.sh status查看对应的状态发现出现：zookeeper Error contacting service. It is probably not running错误执行命令：tail -f zookeeper.out查看到错误记录：2016-04-21 06:56:56,242 [myid:1] – WARN [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:Follower@89] – Exception when following the leaderjava.net.NoRouteToHostException: No route to hostat java.net.PlainSocketImpl.socketConnect(Native Method)at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)at java.net.Socket.connect(Socket.java:579)at org.apache.zookeeper.server.quorum.Learner.connectToLeader(Learner.java:225)at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:71)at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:786)经验证：防火墙的问题，执行命令service iptables stop关闭防火墙（如要永久关闭防火墙执行命令：chkconfig iptables off）重启机器，执行命令zkServer.sh status 分别查看每台机器的状态为leader、follower、follower则表明启动成功配置说明：作用：leader：从客户端接收读写请求，响应读写请求，向slaver发送数据slaver：从leader同步数据，当leader失败时，从新投票选举新的leader配置详解：myid:位于快照目录，是用来标识本台机器在集群中的唯一标识zoo.cfg: tickTime:是initLimit 和 syncLimit的时间单位（毫秒） initLimit:集群机器之间的同步时间，比如启动m1，s1在这个时间段内必须都起来 syncLimit：leader与follower之间的通信时间（心跳），超时则说明follower失败#快照日志的存储路径dataDir=/usr/local/program/zk/zkdata#zk事物日志存储目录 (如果不配置zkdataLog那么快照日志和事务日志都将写到dataDir中，严重影响性能)dataLogDir=/usr/local/program/zk/zkdataLog# the port at which the clients will connect#本台机器的端口，默认为2181clientPort=12181log4j.properties: 日志文件，zookeeper.out位置bin下面，可以更改注意：需要定期清理事务日志文件，否则会造成性能下降（crontab）crontab -l查看当前用户定时任务crontab -e编辑定时任务(例：0 0 * * 0 sh /usr/local/program/zk/cleanup.sh)另外2台机器执行同样的操作#server.1标识本台机器，通过myid文件指定id=1;ip:机器之间通信端口（默认为2888）:leader选举端口（默认为3888）server.1=192.168.0.102:12888:13888]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka安装部署及使用]]></title>
    <url>%2F2018%2F09%2F18%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2FKafka%20%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、Kafka 单节点部署Kafka中单节点部署又分为两种，一种为单节点单Broker部署，一种为单节点多Broker部署。因为是单节点Kafka，所以在安装ZK时也只需要单节点即可。下载安装Zookeeper ZooKeeper官网：http://zookeeper.apache.org/ 下载Zookeeper并解压到指定目录$ wget http://www-eu.apache.org/dist/zookeeper/zookeeper-3.5.1-alpha/zookeeper-3.5.1-alpha.tar.gz$ tar -zxvf zookeeper-3.5.1-alpha.tar.gz -c /opt/zookeeper 进入Zookeeper的config目录下$ cd /opt/zookeeper/conf 拷贝zoo_sample.cfg文件重命名为zoo.cfg，然后修改dataDir属性# 数据的存放目录dataDir=/home/hadoop/zkdata# 端口，默认就是2181clientPort=2181 配置环境变量# Zookeeper Environment Variableexport ZOOKEEPER_HOME=/opt/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin Zookeeper 启动停止命令$ zkServer.sh start$ zkServer.sh stop 备注: 在安装完Zookeeper后，输入命令启动后，jps中并没有查看到QuorumPeerMain进程，说明没有启动成功，进入Zookeeper的log目录下查看日志，发现报了一个错误，如下 AdminServer$AdminServerException: Problem starting AdminServer on address 0.0.0.0, port 8080 and command URL /commands 原因：zookeeper的管理员端口被占用 解决：笔者使用的zookeeper的版本为3.5.1，该版本中有个内嵌的管理控制台是通过jetty启动，会占用8080 端口，需要修改配置里的“admin.serverPort=8080”，默认8080没有写出来，只要改为一个没使用的端口即可，例如：admin.serverPort=8181 安装Kafka 下载地址: https://www.apache.org/dyn/closer.cgi?path=/kafka/2.0.0/kafka_2.1 tar -xzf kafka_2.11-2.0.0.tgz1.Kafka 单节点单Broker部署及使用 部署架构 配置Kafka 参考官网：http://kafka.apache.org/quickstart 进入kafka的config目录下，有一个server.properties，添加如下配置# broker的全局唯一编号，不能重复broker.id=0# 监听listeners=PLAINTEXT://:9092# 日志目录log.dirs=/home/hadoop/kafka-logs# 配置zookeeper的连接（如果不是本机，需要该为ip或主机名）zookeeper.connect=localhost:2181 启动Zookeeper[hadoop@Master ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 启动Kafka$ kafka-server-start.sh $KAFKA_HOME/config/server.properties 打印的日志信息没有报错，可以看到如下信息[Kafka Server 0], started (kafka.server.KafkaServer) 但是并不能保证Kafka已经启动成功，输入jps查看进程，如果可以看到Kafka进程，表示启动成功[hadoop@Master ~]$ jps9173 Kafka9462 Jps8589 QuorumPeerMain[hadoop@Master ~]$ jps -m9472 Jps -m9173 Kafka /opt/kafka/config/server.properties8589 QuorumPeerMain /opt/zookeeper/bin/../conf/zoo.cfg 创建topic[hadoop@Master ~]$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 参数说明： –zookeeper：指定kafka连接zk的连接url，该值和server.properties文件中的配置项{zookeeper.connect}一样 –replication-factor：指定副本数量 –partitions：指定分区数量 –topic：主题名称 查看所有的topic信息[hadoop@Master ~]$ kafka-topics.sh --list --zookeeper localhost:2181test 启动生产者[hadoop@Master ~]$ kafka-console-producer.sh --broker-list localhost:9092 --topic test 启动消费者[hadoop@Master ~]$ kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning--from-beginning参数如果有表示从最开始消费数据，旧的和新的数据都会被消费，而没有该参数表示只会消费新产生的数据 测试2.Kafka 单节点多Broker部署及使用 部署架构 配置Kafka 参考官网：http://kafka.apache.org/quickstart 拷贝server.properties三份[hadoop@Master ~]$ cd /opt/kafka/config[hadoop@Master config]$ cp server.properties server-1.properties[hadoop@Master config]$ cp server.properties server-2.properties[hadoop@Master config]$ cp server.properties server-3.properties 修改server-1.properties文件# broker的全局唯一编号，不能重复broker.id=1# 监听listeners=PLAINTEXT://:9093# 日志目录log.dirs=/home/hadoop/kafka-logs-1 修改server-2.properties文件# broker的全局唯一编号，不能重复broker.id=2# 监听listeners=PLAINTEXT://:9094# 日志目录log.dirs=/home/hadoop/kafka-logs-2 修改server-3.properties文件# broker的全局唯一编号，不能重复broker.id=3# 监听listeners=PLAINTEXT://:9094# 日志目录log.dirs=/home/hadoop/kafka-logs-3 启动Zookeeper[hadoop@Master ~]$ zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /opt/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 启动Kafka（分别启动server1、2、3）$ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties$ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties$ kafka-server-start.sh $KAFKA_HOME/config/server-3.properties 查看进程[hadoop@Master ~]$ jps11905 Kafka11619 Kafka8589 QuorumPeerMain12478 Jps12191 Kafka[hadoop@Master ~]$ jps -m11905 Kafka /opt/kafka/config/server-2.properties11619 Kafka /opt/kafka/config/server-1.properties12488 Jps -m8589 QuorumPeerMain /opt/zookeeper/bin/../conf/zoo.cfg12191 Kafka /opt/kafka/config/server-3.properties 创建topic（指定副本数量为3-会平均部署到不同的节点上）[hadoop@Master ~]$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topicCreated topic &quot;my-replicated-topic&quot;. 查看所有的topic信息[hadoop@Master ~]$ kafka-topics.sh --list --zookeeper localhost:2181 my-replicated-topictest 查看某个topic的详细信息[hadoop@Master ~]$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs:Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 启动生产者$ kafka-console-producer.sh --broker-list localhost:9093,localhost:9094,localhost:9095 --topic my-replicated-topic 启动消费者$ kafka-console-consumer.sh --zookeeper localhost:2181 --topic my-replicated-topic --from-beginning 测试二、Kafka 集群搭建(多节点多Broker)Kafka 集群方式部署，需要先安装ZK集群，笔者是三个节点组成的集群，具体安装配置请参考Hadoop HA 高可用集群搭建 中的ZK集群安装，在这笔者主要介绍Kafka的集群安装配置。 启动ZK集群 参考zookeepeer集群搭建 下载安装包wget http://mirror.bit.edu.cn/apache/kafka/0.9.0.0/kafka_2.10-0.9.0.0.tgz 解压安装包tar -zxvf kafka_2.10-0.9.0.0.tgz -C ~/export/servers/ 创建软连接ln -s kafka_2.10-0.9.0.0/ kafka 修改配置文件server.properties############################# Server Basics ############################## broker 的全局唯一编号，不能重复broker.id=0############################# Socket Server Settings ############################## 配置监听,，默认listeners=PLAINTEXT://:9092# 用来监听连接的端口，producer和consumer将在此端口建立连接,，默认port=9092# 处理网络请求的线程数量，默认num.network.threads=3# 用来处理磁盘IO的线程数量，默认num.io.threads=8# 发送套接字的缓冲区大小，默认socket.send.buffer.bytes=102400# 接收套接字的缓冲区大小，默认socket.receive.buffer.bytes=102400# 请求套接字的缓冲区大小，默认socket.request.max.bytes=104857600############################# Log Basics ############################## kafka 运行日志存放路径log.dirs=/root/export/servers/logs/kafka# topic 在当前broker上的分片个数，默认为1num.partitions=2# 用来恢复和清理data下数据的线程数量，默认num.recovery.threads.per.data.dir=1############################# Log Retention Policy ############################## segment文件保留的最长时间，超时将被删除，默认log.retention.hours=168# 滚动生成新的segment文件的最大时间，默认log.roll.hours=168 配置环境变量# Kafka Environment Variableexport KAFKA_HOME=/root/export/servers/kafkaexport PATH=$PATH:$KAFKA_HOME/bin 分发安装包 注意：分发安装包，也要创建软连接，配置环境变量scp -r ~/export/servers/kafka_2.10-0.9.0.0/ storm02:~/export/serversscp -r ~/export/servers/kafka_2.10-0.9.0.0/ storm03:~/export/servers 再次修改配置文件 测试使用的节点的主机名分别为storm01、storm02、storm03 依次修改各服务器上配置文件server.properties 的 broker.id，分别是0，1，2不得重复 修改host.name分别为storm01，storm02，storm03 启动Kafka集群 注意：在启动Kafka集群前，确保ZK集群已经启动且能够正常运行 测试创建topic[root@storm01 ~]# kafka-topics.sh --create --zookeeper storm01:2181 --replication-factor 3 --partitions 2 --topic testCreated topic &quot;test&quot;.[root@storm01 ~]# kafka-topics.sh --describe --zookeeper storm01:2181 --topic testTopic:test PartitionCount:2 ReplicationFactor:3 Configs:Topic: test Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2Topic: test Partition: 1 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0启动生产者[root@storm01 ~]# kafka-console-producer.sh --broker-list storm01:9092,storm02:9092,storm03:9092 --topic testhellohello kafka clustertesthello storm启动两个消费者，消费消息[root@storm02 ~]# kafka-console-consumer.sh --zookeeper storm02:2181 --topic test --from-beginninghellohello kafka clustertesthello storm[root@storm03 ~]# kafka-console-consumer.sh --zookeeper storm03:2181 --topic test --from-beginninghellohello kafka clustertesthello storm]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pykafka, kafka-python开发]]></title>
    <url>%2F2018%2F09%2F17%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2F%E4%BD%BF%E7%94%A8pykafka%EF%BC%8Ckafka-python%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[一、简介 python连接kafka的标准库，kafka-python和pykafka。kafka-python使用的人多是比较成熟的库，kafka-python并没有zk的支持。pykafka是Samsa的升级版本，使用samsa连接zookeeper，生产者直接连接kafka服务器列表，消费者才用zookeeper。二、kafka-python使用(1) kafka-python安装 pip install kafka-python(2) kafka-python的api https://kafka-python.readthedocs.io/en/master/apidoc/modules.html https://kafka-python.readthedocs.io/en/master/index.html https://pypi.org/project/kafka-python/(3) kafka-python生产者 (4) kafka-python消费者输出结果：ConsumerRecord(topic='test', partition=0, offset=246, timestamp=1531980887190, timestamp_type=0, key=None, value=b'1', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=247, timestamp=1531980887691, timestamp_type=0, key=None, value=b'2', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=248, timestamp=1531980888192, timestamp_type=0, key=None, value=b'3', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=249, timestamp=1531980888694, timestamp_type=0, key=None, value=b'4', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=250, timestamp=1531980889196, timestamp_type=0, key=None, value=b'5', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=251, timestamp=1531980889697, timestamp_type=0, key=None, value=b'6', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=252, timestamp=1531980890199, timestamp_type=0, key=None, value=b'7', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=253, timestamp=1531980890700, timestamp_type=0, key=None, value=b'8', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=254, timestamp=1531980891202, timestamp_type=0, key=None, value=b'9', checksum=None, serialized_key_size=-1, serialized_value_size=1)ConsumerRecord(topic='test', partition=0, offset=255, timestamp=1531980891703, timestamp_type=0, key=None, value=b'10', checksum=None, serialized_key_size=-1, serialized_value_size=2)1consumer = kafka.KafkaConsumer(bootstrap_servers = ['192.168.17.64:9092','192.168.17.65:9092','192.168.17.68:9092'],2group_id ='test_group_id',3auto_offset_reset ='latest',4enable_auto_commit = False)enable_auto_commit=False 自动提交位移设为flase, 默认为取最新的偏移量，重新建立一个group_id,这样就实现了不影响别的应用程序消费数据，又能消费到最新数据，实现预警（先于用户发现）的目的。]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Registry 私有库]]></title>
    <url>%2F2018%2F09%2F17%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2F%E6%90%AD%E5%BB%BARegistry%E7%A7%81%E6%9C%89%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.关于Registry仓库官方的Docker hub是一个用于管理公共镜像的好地方，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景需要我们拥有一个私有的镜像仓库用于管理我们自己的镜像。这个可以通过开源软件Registry来达成目的。 Registry在github上有两份代码：老代码库和新代码库。老代码是采用python编写的，存在pull和push的性能问题，出到0.9.1版本之后就标志为deprecated，不再继续开发。从2.0版本开始就到在新代码库进行开发，新代码库是采用go语言编写，修改了镜像id的生成算法、registry上镜像的保存结构，大大优化了pull和push镜像的效率。 官方在Docker hub上提供了registry的镜像（详情），我们可以直接使用该registry镜像来构建一个容器，搭建我们自己的私有仓库服务。Tag为latest的registry镜像是0.9.1版本的，我们直接采用2.1.1版本。 2.Registry的部署运行下面命令获取registry镜像 1$ sudo docker pull registry:2.1.1 然后启动一个容器 1$ sudo docker run -d -v /opt/data/registry/:/var/lib/registry -p 5000:5000 --restart=always --name registry registry:2.1.1 验证服务是否启动成功 说明我们已经启动了registry服务，打开浏览器输入http://127.0.0.1:5000/v2 3.验证向仓库中push镜像 现在我们通过将镜像push到registry来验证一下。我的机器上有个hello-world的镜像，我们要通过docker tag将该镜像标志为要推送到私有仓库， 1$ sudo docker tag hello-world 127.0.0.1:5000/hello-world 接下来，我们运行docker push将hello-world镜像push到我们的私有仓库中， 1$ sudo docker push 127.0.0.1:5000/hello-world 1234567The push refers to a repository [127.0.0.1:5000/hello-world] (len: 1)975b84d108f1: Image successfully pushed3f12c794407e: Image successfully pushedlatest: digest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4b size: 2744 现在我们可以查看我们本地/opt/registry目录下已经有了刚推送上来的hello-world。我们也在浏览器中输入http://127.0.0.1:5000/v2/_catalog，如下图所示， 从镜像库中拉取镜像 现在我们可以先将我们本地的127.0.0.1:5000/hello-world和hello-world先删除掉， 123$ sudo docker rmi hello-world$ sudo docker rmi 127.0.0.1:5000/hello-world 然后使用docker pull从我们的私有仓库中获取hello-world镜像， 1$ sudo docker pull 127.0.0.1:5000/hello-world 123456789101112131415161718192021Using default tag: latestlatest: Pulling from hello-worldb901d36b6f2f: Pull complete0a6ba66e537a: Pull completeDigest: sha256:1c7adb1ac65df0bebb40cd4a84533f787148b102684b74cb27a1982967008e4bStatus: Downloaded newer image for 127.0.0.1:5000/hello-world:latestlienhua34@lienhua34-Compaq-Presario-CQ35-Notebook-PC ~ $ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEregistry 2.1.1 b91f745cd233 5 days ago 220.1 MBubuntu 14.04 a5a467fddcb8 6 days ago 187.9 MB127.0.0.1:5000/hello-world latest 0a6ba66e537a 2 weeks ago 960 B 4.查询镜像库查询镜像库中的镜像 1http://10.0.110.218:5000/v2/_catalog 5.错误排查错误描述 在push 到docker registry时，可能会报错： 123The push refers to a repository [192.168.1.100:5000/registry]Get https://192.168.1.100:5000/v1/_ping: http: server gave HTTP response to HTTPS client 这个问题可能是由于客户端采用https，docker registry未采用https服务所致。一种处理方式是把客户对地址“192.168.1.100:5000”请求改为http。 目前很多文章都是通过修改docker的配置文件“etc/systemconfig/docker”，重启docker来解决这个问题。但发现docker1.12.3版本并无此文件，根据网上创建此文件，并填入相应内容，重启docker无效果，仍然报此错误。 解决办法 在”/etc/docker/“目录下，创建”daemon.json“文件。在文件中写入： { “insecure-registries”:[“192.168.1.100:5000”] } 保存退出后，重启docker。]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka的介绍]]></title>
    <url>%2F2018%2F09%2F17%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2FKafka%E7%9A%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Kafka介绍Kafka概念Kafka是一个开源的，分布式的，高吞吐量的消息系统。随着Kafka的版本迭代，日趋成熟。大家对它的使用也逐步从日志系统衍生到其他关键业务领域。特别是其超高吞吐量的特性，在互联网领域，使用越来越广泛，生态系统也越来的完善。同时，其设计思路也是其他消息中间件重要的设计参考。Kafka原先的开发初衷是构建一个处理海量日志的框架，基于高吞吐量为第一原则，所以它对消息的可靠性以及消息的持久化机制考虑的并不是特别的完善。0.8版本后，陆续加入了一些复制、应答和故障转移等相关机制以后，才可以让我们在其他关键性业务中使用。Kafka的运行架构图：kafka各部分组件介绍Topic：主题，或者说是一类消息。类似于RabbitMQ中的queue。可以理解为一个队列。Broker：一个Kafka服务称之为Broker。Kafka可以集群部署，每一个Kafka部署就是一个Broker。Producer &amp; Consumer：生产者和消费者。一般消息系统都有生产者和消费者的概念。生产者产生消息，即把消息放入Topic中，而消费者则从Topic中获取消息处理。一个生产者可以向多个Topic发送消息，一个消费者也可以同时从几个Topic接收消息。同样的，一个Topic也可以被多个消费者来接收消息。Partition：分区，或者说分组。分组是Kafka提升吞吐量的一个关键设计。这样可以让消费者多线程并行接收消息。创建Topic时可指定Parition数量。一个Topic可以分为多个Partition，也可以只有一个Partition。每一个Partition是一个有序的，不可变的消息序列。每一个消息在各自的Partition中有唯一的ID。这些ID是有序的。称之为offset，offset在不同的Partition中是可以重复的，但是在一个Partition中是不可能重复的。越大的offset的消息是最新的。Kafka只保证在每个Partition中的消息是有序的，就会带来一个问题，即如果一个Consumer在不同的Partition中获取消息，那么消息的顺序也许是和Producer发送到Kafka中的消息的顺序是不一致的。这个在后续会讨论。如果是多Partition，生产者在把消息放到Topic中时，可以决定放到哪一个Patition。这个可以使用简单的轮训方法，也可以使用一些Hash算法。一个Topic的多个Partition可以分布式部署在不同的Server上，一个Kafka集群。配置项为：num.partitions，默认是1。每一个Partition也可以在Broker上复制多分，用来做容错。详细信息见下面创建Topic一节。Consumer Group：顾名思义，定义了一组消费者。一般来说消息中间件都有两种模式：队列模式和发布订阅模式。队列模式及每一个消息都会给其中一个消费者，而发布订阅模式则是每个消息都广播给所有的消费者。Kafka就是使用了Consumer Group来实现了这两种模式。如果所有的消费者都是同一个Consumer Group的话，就是队列模式，每个消息都会负载均衡的分配到所有的消费者。如果所有的消息者都在不同的Consumer Group的话，就是发布订阅模式，每个消费者都会得到这个消息。下图是一个Topic，配置了4个Patition，分布在2个Broker上。由于有2个Consumer Group，Group A和Group B都可以得到P0-P3的所有消息，是一个订阅发布模式。两个Group中的Consumer则负载均衡的接收了这个Topic的消息。如果Group中的Consumer的总线程数量超过了Partition的数量，则会出现空闲状态。Zookeeper：Kafka的运行依赖于Zookeeper。Topic、Consumer、Patition、Broker等注册信息都存储在ZooKeeper中。消息的持久化Kafka持久化的方法Kafka可以通过配置时间和大小来持久化所有的消息，不管是否被消费（消费者收掉）。举例来说，如果消息保留被配置为1天，那么，消息就会在磁盘保留一天的时间，也就是说，一天以内，任意消费这个消息。一天以后，这个消息就会被删除。保留多少时间就取决于业务和磁盘的大小。Kafka配置持久化的方式主要有两种方式：时间和大小。在Broker中的配置参数为：log.retention.bytes：最多保留的文件字节大小。默认-1。log.retention.hours：最多保留的时间，小时。优先级最低。默认168。log.retention.minutes：最多保留的时间，分钟。如果为空，则看log.retention.hours。默认null。log.retention.ms：最多保留的时间，毫秒。如果为空，则看log.retention.minutes。默认null。创建Topic：创建topic： 这个命令就是创建一个topic：haoxy1，只有1个partition，并且这个分区会部署在一个brokerbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic haoxy1partitions：这个topic的partition的数量。replication-factor：每个partition的副本个数。任意将每一个分区复制到n个broker上。bin/kafka-topics.sh --describ --zookeeper localhost:2181 --topic haoxy1 具体部署到哪个broker可以通过如下命令查看展示如下： 第一行的摘要信息。第二行开始是详细信息，所以是缩进的格式，如果这个topic有10个Partition，那么就有10行。Leader：每一个分区都有一个broker为Leader，它负责该分区内的所有读写操作，其他Leader被动的复制Leader broker。如果Leader broker 挂了，那么其他broker中的一个将自动成为该分区的新Leader。本例子只有1个复制，Leader的Partition在Broker1上面。Replicas：副本在Broker1上面。Isr：当前有效的副本在Broker1上面。再来创建一个多副本的Topic：bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 5 --topic haoxy2如图：因为我有3个Broker：0,1,2。每一个Partition都有2个Replicas。分别在2个Broker上。]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下sendmail邮件系统安装操作]]></title>
    <url>%2F2018%2F06%2F27%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2Fsendmail%2Flinux%E4%B8%8Bsendmail%E9%82%AE%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[linux下sendmail邮件系统安装操作记录 电子邮件系统的组成：1）邮件用户代理（Mail User Agent ， MUA），MUA是一个邮件系统的客户端程序,它提供了阅读，发送和接受电子邮件的用户接口。 最常用的 MUA 有： linux 中的 mail ， elm ， pine 等。 Windows 的 outlook ， foxmail 等2）邮件代理器（ Mail Transfer Agent ， MTA ） MTA 负责邮件的存储和转发（ store and forward ）。 MTA 监视用户代理的请求，根据电子邮件的目标地址找出对应的邮件服务器，将信件在服务器之间传输并且将接受到的邮件进行缓冲。 在 linux 下的 MTA 程序有： sendmail ， qmail 等，3）邮件提交代理（ Mail Submmission Agent ， MSA ） MSA 负责消息有 MTA 发送之前必须完成的所有准备工作和错误检测， MSA 就像在 MUA 和 MTA 之间插入了一个头脑清醒的检测员对所有的主机名，从 MUA 得到的信息头等信息进行检测。4）邮件投递代理（ Mail Ddlivery Agent ， MDA ） MDA 从 MTA 接收邮件并进行适当的本地投递，可以投递个一个本地用户，一个邮件列表，一个文件或是一个程序。 Linux 下常用的 MDA 是 mail.local ， smrsh 和 procmail （ www.procmail.org ）5）邮件访问代理（ Mail Access Agent ， MAA ） MAA 用于将用户连接到系统邮件库，使用 POP 或 IMAP 协议收取邮件。 Linux 下常用的 MAA 有 UW-IMAP ， Cyrus-IMAP ， COURIER-IMAP 等 邮件中继: 就是当邮件向目的地址传输时，一旦源地址和目的地址都不是本地系统，那么本地系统就是邮件的中继（中转站）。MUA 使用者透过这个程序与邮件服务器沟通，包括收信（以 POP3 连接收信服务器程序 imapd）或寄信（以SMTP 连接 MTA），例如： Outlook Express……等。MTA 使用 SMTP 通讯协议将信件传递到不同邮件主机上面，例如： sendmail, postfix, Qmail……等。MSA 是新版 sendmail 发展给 SSMTP 进行 TLS/SSL 联机的 client 端代理器。MDA 收到信后将信件分配到不同使用者信箱内，算是 MTA 的一个子系统，譬如 BBS 从定义上来说也算是 MDA（ BBS 功能复杂，当然不仅仅是 MDA），有些 MDA 被设计来进行滤信动作，它们必须在 local 端运作，因此又被称为 LDA，例如： procmail……等。MailBox 尚未被使用者下载的邮件，会暂存在服务器的硬盘空间里，称之为信箱。所有使用者信箱的总合必须约等于该分割区总容量的一半，以避免造成信箱尚有空间但邮件系统却无法运作的现象。Mail Gateway 是一种特殊的邮件服务器，通常扮演代理器的角色，负责统筹某机构内所有信件的收发，并分配邮件给下属的邮件服务器们，透过这个机制能够加速邮件的交换，并且能够进行一致的滤信控制。sendmail是linux系统中一个邮箱系统，如果我们在系统中配置好sendmail就可以直接使用它来发送邮箱。sendmail的配置文件/etc/mail/sendmail.cf :Sendmail的主配置文件；/etc/mail/access :中继访问控制；/etc/mail/domaintable ;域名映射；/etc/mail/local-host-names ;本地主机别名；/etc/mail/mailertable :为特定的域指定特殊的路由规则；/etc/mail/virtusertable :虚拟域配置。中继的配置是指一台服务器接受并传递源地址和目的地址都不是本服务器的邮件。在两个文件中进行设置：/etc/mail/relay-domains/etc/mail/access。废话不多说了，下面分享下sendmail在linux系统下的安装部署记录：一、安装软件[root@slave-node ~]# yum install -y sendmail[root@slave-node ~]# yum install -y sendmail-cf启动saslauthd服务进行SMTP验证（默认是安装的，如果没有，就手动安装）[root@slave-node ~]# service saslauthd startStarting saslauthd: [ OK ]二、邮件服务配置（iptables防火墙关闭）1）配置Senmail的SMTP认证将下面两行内容前面的dnl去掉。在sendmail文件中，dnl表示该行为注释行，是无效的，因此通过去除行首的dnl字符串可以开启相应的设置行。[root@slave-node ~]# vim /etc/mail/sendmail.mc......TRUST_AUTH_MECH(`EXTERNAL DIGEST-MD5 CRAM-MD5 LOGIN PLAIN')dnldefine(`confAUTH_MECHANISMS', `EXTERNAL GSSAPI DIGEST-MD5 CRAM-MD5 LOGIN PLAIN')dnl2) 设置Sendmail服务的网络访问权限（如果是直接本机调用，可以不用操作，采用默认的127.0.0.1。不过最后还是改成0.0.0.0）将127.0.0.1改为0.0.0.0，意思是任何主机都可以访问Sendmail服务。如果仅让某一个网段能够访问到Sendmail服务，将127.0.0.1改为形如192.168.1.0/24的一个特定网段地址。[root@slave-node ~]# vim /etc/mail/sendmail.mc......DAEMON_OPTIONS(`Port=smtp,Addr=0.0.0.0, Name=MTA')dnl3）生成配置文件Sendmail的配置文件由m4来生成，m4工具在sendmail-cf包中。如果系统无法识别m4命令，说明sendmail-cf软件包没有安装[root@slave-node ~]# m4 /etc/mail/sendmail.mc &gt; /etc/mail/sendmail.cf4）修改主机域名(只有加到配置中才能访问smtp服务器)vi /etc/hosts 172.168.110.218 m110p218 m110p218.com 在文件最后一行加上 主机ip 主机别名(通过hostname查看) 主机域名修改 /etc/mail/access文件，追加加入以下语句Connect:202.222.222.222 RELAY # 允许202.222.222.222通过sendmail收发邮件5）启动服务（如果发现sendmail dead but subsys locked，那就执行&quot;service postfix status&quot;查看postfix是否默认开启了，如果开启的话，就关闭postfix，然后再启动或重启sendmail服务即可。）[root@slave-node ~]# service sendmail startStarting sendmail: [ OK ]Starting sm-client: [ OK ][root@slave-node ~]# service saslauthd restartStopping saslauthd: [ OK ]Starting saslauthd: [ OK ]将服务加入自启行列[root@slave-node ~]# systemctl enable sendmail.service[root@slave-node ~]# systemctl enable saslauthd.service三、测试发送邮箱（1）第一种方式：安装sendmail即可使用。[root@slave-node ~]# yum -y install mailx注意: 如果在其他主机进行测试需要修改对应主机配置文件 /etc/mail.rc，在某位追加set from=&quot;xxx@163.com&quot; set smtp=smtp.163.com # 如果smtp不在本机, 该条配置一定要设置set smtp-auth-user=xxx set smtp-auth-password=邮箱密码 set smtp-auth=login创建一个邮件内容文件，然后发邮件（注意-s参数后的邮件标题要用单引号，不能使用双引号，否则发邮件会失败！）[root@slave-node ~]# echo 'This is test mail'&gt;/root/content.txt[root@slave-node ~]# cat /root/content.txtThis is test mail[root@slave-node ~]# mail -s 'Test mail' wang_shibo***@163.com &lt; /root/content.txt查看已收到邮件： 如果不想通过文件发送邮件内容也可以这么发送，也可以使用管道符直接发送邮件内容，效果同文件发送邮件内容一样[root@slave-node ~]# echo &quot;This is test mail&quot; | mail -s '666666' wang_shibo***@163.com查看已收到邮件：如果是发送给多个邮件，就使用-c参数，如下：[root@slave-node ~]# echo &quot;This is test mail&quot; | mail -s 'test' -c wang_shibo***@sina.com wang_shibo***@163.com如遇下面报错，解决办法：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849发送邮件：[root@mail-server ~]# echo &quot;This is test mail&quot; | mail -s '666666' wangshibo@kevin.com 发现收不到邮件，查看sendmail日志，报错信息如下：[root@mail-server ~]# tail -f /var/log/maillog.......Feb 12 03:35:13 mail-server sendmail[21905]: My unqualified host name (mail-server) unknown; sleeping for retryFeb 12 03:37:12 mail-server sendmail[22061]: w1BJb8KM022059: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server&gt; (0/0), delay=00:00:03, xdelay=00:00:03,mailer=esmtp, pri=120476, relay=mx1.kevin.com. [128.1.41.15], dsn=4.0.0, stat=Deferred: 450 Requested mail action not taken: Invalid sender 分析原因：这是由于主机名没有正确解析导致的。[root@mail-server ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.10.202 mail-server [root@mail-server ~]# cat /etc/sysconfig/networkNETWORKING=yesHOSTNAME=mail-server 解决办法：[root@mail-server ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.10.202 mail-server.localdomain mail-server[root@mail-server ~]# cat /etc/sysconfig/networkNETWORKING=yesHOSTNAME=mail-server [root@mail-server ~]# rm -rf /var/spool/mqueue/*[root@mail-server ~]# /etc/init.d/sendmail restartShutting down sm-client: [ OK ]Shutting down sendmail: [ OK ]Starting sendmail: [ OK ]Starting sm-client: [ OK ] 再次使用mail发送邮件就正确了！[root@mail-server ~]# echo &quot;This is test mail&quot; | mail -s '666666' wangshibo@kevin.com[root@mail-server ~]# tail -f /var/log/maillog.......Feb 12 03:42:31 mail-server sendmail[22293]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:31 mail-server sendmail[22299]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:31 mail-server sendmail[22302]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:33 mail-server sendmail[22284]: STARTTLS=client, relay=mx1.kevin.com., version=TLSv1/SSLv3, verify=FAIL, cipher=AES256-GCM-SHA384, bits=256/256Feb 12 03:42:34 mail-server sendmail[22293]: w1BJgTcF022288: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:05, xdelay=00:00:05, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [116.115.114.9], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwB3EaEnDYFaHrpiAA--.12694S3)Feb 12 03:42:35 mail-server sendmail[22302]: w1BJgUPI022300: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:05, xdelay=00:00:05, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [115.123.124.105], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwDXQD0oDYFaoMJxAA--.21712S3)Feb 12 03:42:36 mail-server sendmail[22299]: w1BJgToO022294: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:06, xdelay=00:00:06, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [115.123.124.105], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwB3HTonDYFancJxAA--.21596S3)Feb 12 03:42:40 mail-server sendmail[22284]: w1BJgSAl022282: to=&lt;wangshibo@kevin.com&gt;, ctladdr=&lt;root@mail-server.localdomain&gt; (0/0), delay=00:00:12, xdelay=00:00:12, mailer=esmtp, pri=120510, relay=mx1.kevin.com. [139.162.158.182], dsn=2.0.0, stat=Sent (Mail OK queued as AQAAfwDHp2QmDYFayl55AA--.6056S3)（2）第二种方式：利用外部的smpt服务器上面第一种方式中，/bin/mail命令会默认使用本地sendmail发送邮件，这样要求本地的机器必须安装和启动Sendmail服务，配置非常麻烦，而且会带来不必要的资源占用。而通过修改配置文件可以使用外部SMTP服务器，可以达到不使用sendmail而用外部的smtp服务器发送邮件的目的。修改/etc/mail.rc文件（有的版本叫/etc/nail.rc，添加下面内容：set from=fromUser@domain.com smtp=smtp.domain.comset smtp-auth-user=username smtp-auth-password=passwordset smtp-auth=login参数说明：from是发送的邮件地址smtp是发生的外部smtp服务器的地址smtp-auth-user是外部smtp服务器认证的用户名。注意一定要填写邮件全称！！smtp-auth-password是外部smtp服务器认证的用户密码smtp-auth是邮件认证的方式配置完成后，就可以正常发送邮件了，如下[root@slave-node ~]# vim /etc/mail.rc //在文件底部添加set from=ops@huanqiu.cn smtp=smtp.huanqiu.cn smtp-auth-user=ops@huanqiu.cn smtp-auth-password=zh@123bj smtp-auth=login现在开始发邮件：[root@slave-node ~]# echo &quot;hello world&quot; |mail -s 'test666' wangshibo@huanqiu.cn]]></content>
      <categories>
        <category>开发工具</category>
        <category>sendmail</category>
      </categories>
      <tags>
        <tag>sendmail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker编排与集群部署]]></title>
    <url>%2F2018%2F06%2F22%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E7%BC%96%E6%8E%92%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Fig/Compose编排工具Fig/Compose的介绍在生产环境中，整个团队需要发布的容器数量很可能极其庞大，而容器之间的联系和拓扑结构也可能非常复杂，往往太难升就是集群化的，并且具备高可用设计。如果依赖人工记录和配置这样的复杂容器关系，并保障集群正常运行、监控、迁移、高可用等常规运维需求，难免力不从心因此Fig/Compose就是解决这一问题的。Dockerfile可以重现一个容器，Compose可以重现容器的配置和集群编排和部署编排它根据被部署的对象之间的耦合关系，以及被部署对象对环境的依赖，制定部署流程中各个动作的执行顺序，部署过程所需要的依赖文件和被部署的文件的存储位置和获取方式，以及如何验证部署成功。这些信息都会在编排工具中以制定的格式来要求运维人员定义并保存起来，从而保证这个流程能够随时在全新的环境中可靠有序的重现出来部署是指按照编排制定的内容和流程，在目标机器上执行编排制定的环境初始化，存放制定的依赖和文件，运行制定的部署动作，最终按照编排中的规则来确认部署成功在Compse的世界里，编排和部署的组合结果，就是一朵&quot;容器云&quot;编排工具-Compose的使用Compose安装yum -y install epel-release &amp;&amp; yum -y install python-pip &amp;&amp; pip install docker-composeCompose的使用Compose常用命令docker-compose up 启动服务(当前目录下需要有.yml文件)-d: 后台启动，默认是前台启动-f: 指定详细的docker-compose.yml文件, 默认为当前目录下的docker-compose.yml-p: 指定项目名称启动-H: 指定链接到哪一个Hoser daemon--no-recreate，不会新创建容器，如果有已经存在的容器，那么会直接启动docker-compose start 启动已经存在的容器作为一个服务docker-compose stop 停止运行的容器而不删除它们docker-compose restart 重启已经停止的容器.yml文件的配置选项build指定Dockerfile路径和上下文路径，Compose 将会利用它自动构建这个镜像，然后使用这个镜像启动服务容器args为构建(build)过程中的环境变量，用于替换Dockerfile里定义的ARG参数，容器中不可用。image指定使用的镜像，本地不存在会从HUB拉取，当和build同时使用，会把生成的镜像标记为image定义的镜像名称command容器启动后执行的命令，会覆盖默认的启动命令ports用于暴露端口给主机, 当不指定主机映射端口时，不能供外部访问(host_ip:container_ip)expose提供container之间端口访问, 不会暴露给主机使用.同docker run --exposevolumes挂在目录environment添加环境变量，同docker run -e depends_on指定依赖，将会优先于该服务创建并启动依赖extra_hosts添加主机名映射network_mode定义网络模式 同docker --net参数privileged是否使用privileged模式env_file指定变量的文件，默认为docker-compose文件夹下的 .env 文件.同一个变量，通过export设置，会覆盖env_file中的变量.env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.envcat .env MYSQL_DATA_DIR=/my/sql/data-dirtty是否给容器分配终端当想要让容器后台运行的时候，必须带该配置，不然不会分配终端，容器执完命令会自动退出编排.yml文件使用示例networks关键字:compose file中顶级networks关键字，可以用来创建更加复杂的网络拓扑，指定自定义网络驱动和选项，也可以用它来连接外部创建（非Compose创建）的网络。每个Service可通过service内部的networks关键字来指定它要使用的顶级网络。给Service指定networks的好处是，实现网络隔离或连接每个网络中的启动的容器，可以实现相互的网络通信，容器直接可以使用容器名进行通信使用Docker-compose进行网络通信第一种方法：将service部署到同一个网络环境中，即可实现容器的互相通信，使用container_name即可示例：第二种方法:使用自定义局域网络，容器之间使用静态ip进行通信示例：第三种方法：先给每个服务指定容器的名称再在每个服务中给容器添加extra_host对应容器的映射关系Compose的缺点docker-compose是面向单宿主机部署的，这是一种部署能力的欠缺，管理员需要借助成熟的自动化运维工具来管理多个目标机，将docker-compose所需要的所有资源包括配置文、用户代码交给所有的目标机，再在目标机上运行docker-compose指令docker-compose还不能提供跨宿主机的网络和存储。这意味着管理员必须部署一套类似于Open vSwich的堵路网络工具，而且管理员还需要完成继承工作。Docker官方给出了一套用Compse、Machine、Swarm联动的解决方案集群抽象工具SwarmSwarm的介绍Swarm的目的是将多个宿主机抽象为&quot;一台&quot;宿主机来对待Swarm在多台Docker宿主机(Docker服务端)上建立了一层抽象，可以通过操作Swarm服务端实现对宿主机的资源分配和管理Swarm通过在Docker宿主机上添加标签信息来讲宿主机资源进行细粒度分区，通过分区来帮助用户将容器部署到目标宿主机上利用Swarm可以实现控制一台Swarm Manager来控制整个Docker集群。Swarm集群的Agent\ManagerAgent: Agent节点运行Docker服务端，Docker Release的版本需要保证一怔，且为1.4.0或更新的版本。Manager：Manager节点负责与所有的Agent上的Docker宿主机通信以及对外提供Docker远程API服务，因此Manager负责能获取到所有的Agent地址。当通过Docker客户端与Manager通信，执行docker run命令时，Manager会选择一个Agent来执行该命令，并执行结果返回给Docker客户端创建一个Swarm集群搭建Swarm集群1.拉取swarm镜像docker pull swarm2.把准备加入集群的所有的节点的docker deamon的监听端口修改为0.0.0.0:2375vi /etc/default/dockerDOCKER_OPTS=&quot;-H 0.0.0.0:2375 -H unix:///var/run/docker.sock&quot; 在文件结尾添加service docker restart 重启docker服务3.在manage节点新建一个文件,把要加入到集群的机器的IP地址都写进去4.执行swarm manage命令(在manage节点上)docker run –d –p 2376:2375 –v $(pwd)/cluster:/tmp/cluster swarm manage file:///tmp/cluster这里一定要使用-v命令，因为cluster文件是在本机上面，启动的容器默认是访问不到的，所以要通过-v命令共享5.让集群运行某些命令随机指定节点来运行docker -H &lt;manage_host:manage_port&gt; infodocker -H &lt;manage_host:manage_port&gt; run -it ubuntu /bin/bash通过lable来指定某个节点来运行命令docker -d --lable storage 给节点加上storage标签sudo docker run –H 10.13.181.83:2376 run –name redis_083 –d –e constraint:storage redis 通过指定标签来选择节点启动6.Docker客户端与Manager通信docker -H &lt;manage_host:manage_port&gt; run -it ubuntu /bin/bash 7.删除一个节点docker node rm --force node1 Swarm的调度测略（可以通过strategy来选出最终运行容器的宿主机）Random :顾名思义，就是随机选择一个Node来运行容器，一般用作调试用，spread和binpack策略会根据各个节点的可用的CPU,RAM以及正在运行的容器的数量来计算应该运行容器的节点。Spread :会选择运行容器最少的那台节点来运行新的容器,使用Spread策略会使得容器会均衡的分布在集群中的各个节点上运行，一旦一个节点挂掉了只会损失少部分的容器。binpack: 会选择运行容器最集中的那台机器来运行新的节点,binpack策略最大化的避免容器碎片化，就是说binpack策略尽可能的把还未使用的节点留给需要更大空间的容器运行，尽可能的把容器运行在一个节点上面。跨平台宿主环境管理工具-Machine Machine的介绍搭建环境想来是一个重复造轮子的过程，Machine这一工具把用户搭建Docker环境的跟中方案汇集在一起，即一目了然的简化了Docker环境的创建过程，让用户继续讲时间投入到应用开发商，而不是无谓的花费在环境的搭建上Machine的主要功能就是帮助用户在不同的云主机提供商上创建和管理虚拟机，并在虚拟机中安装Docker用户只需要提供几项登录凭证即可等待环境安装完成。Machine能便捷地管理所有通过它创建的Docker宿主机，进行宿主机的启动、关闭、重启、删除等操作使用Machine可以在最开始快速创建多台主机系统，并搭建Docker部署环境，接下俩就可以使用swarm创建集群等操作Machine与虚拟机软件Machine包含的虚拟机驱动有VirtualBox、VMware FUsion、Hyper-V使用Machine前，机器上需要有上述的三种软件之一，需要在命令中指定使用哪一种当不需要创建虚拟镜像充当主机的时候，我们也可以直接指定主机的ip方式来通过Docker Machine来控制多个真正的主机Docker Machine的使用安装Docker Machinecurl -L https://github.com/docker/machine/releases/download/v0.12.0/docker-machine-`uname -s`-`uname -m` &gt; /tmp/docker-machinechmod +x /tmp/docker-machinesudo mv /tmp/docker-machine /usr/local/bin/docker-machine使用Docker Machine的前提条件sudo adduser nick sudo usermod -a -G sudo nick 在目标主机上创建一个用户并加入sudo 组sudo vi sudoer nick ALL=(ALL:ALL) NOPASSWD: ALL 为该用户设置 sudo 操作不需要输入密码ssh-copy-id -i ~/.ssh/id_rsa.pub nick@xxx.xxx.xxx.xxx 把本地用户的 ssh public key 添加到目标主机上第一种安装Docker方法：在目标主机上安装Docker命令 docker-machine create -d generic \ --generic-ip-address=xxx.xxx.xxx.xxx \ --generic-ssh-user=nick \ --generic-ssh-key ~/.ssh/id_rsa \ krdevdb备注：create 命令本是要创建虚拟主机并安装 Docker，因为本例中的目标主机已经存在，所以仅安装 Docker。解释-d 是 --driver 的简写形式，主要用来指定使用什么驱动程序来创建目标主机。Docker Machine 支持在云服务器上创建主机，就是靠使用不同的驱动来实现了。本例中使用 generic 就可以了--generic 开头的三个参数主要是指定操作的目标主机和使用的账户。最后一个参数 krdevdb 是虚拟机的名称第二种安装Docker方法：在本地主机上安装Docker虚拟机命令 docker-machine create \ --driver vmwarevsphere \ --vmwarevsphere-vcenter=xxx.xxx.xxx.xxx \ --vmwarevsphere-username=root \ --vmwarevsphere-password=12345678 \ --vmwarevsphere-cpu-count=1 \ --vmwarevsphere-memory-size=512 \ --vmwarevsphere-disk-size=10240 \ testvm备注：在实际使用中我们一般会在物理机上安装 vSphere 等虚拟机管理软件，并称之为虚拟机 host。然后通过 vSphere 工具安装虚拟机进行使用。接下来我们将介绍如何在本地的一台安装了 vSphere 的虚拟机 host 上安装带有 Docker 的虚拟机参数解释： --driver vmwarevsphere 需要给 Docker Machine 提供对应的驱动，这样才能够在上面安装新的虚拟机。 --vmwarevsphere-vcenter=xxx.xxx.xxx.xxx \ --vmwarevsphere-username=root \ --vmwarevsphere-password=12345678 \ 上面三行分别指定了虚拟机 host 的 IP 地址、用户名和密码。 --vmwarevsphere-cpu-count=1 \ --vmwarevsphere-memory-size=512 \ --vmwarevsphere-disk-size=10240 \ 上面三行则分别指定了新创建的虚拟机占用的 cpu、内存和磁盘资源。 testvm 最后一个参数则是新建虚拟机的名称。查看docker machine 管理的主机列表docker-machine ls 获取构造docker主机时使用的命令docker-machine config dev获取连接到某个主机需要的环境变量docker-machine env dev eval &quot;$(docker-machine env dev)&quot; 会指定接下来和哪台主机的docker daemon进行通信启动\停止\重启虚拟机docker-machine start/stop/restart dev SSH到宿主机上执行命令docker-machine ssh dev /start.sh 获取主机的urldocker-machine url 主机之间传递文件docker-machine scp]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像管理&Dockerfile]]></title>
    <url>%2F2018%2F06%2F20%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86%26Dockerfile%2F</url>
    <content type="text"><![CDATA[Docker镜像Docker镜像介绍Docker镜像时一个只读的Docker容器模板，含有启动Docker容器所需的文件系统结构及其内容，因此是启动一个Docker容器的基础。Docker镜像的文件内容以及一些运行Docker容器的配置文件组成了Docker容器的静态文件系统运行环境-rootfs。可以理解为Docker容器的静态视角，Docker容器是Docker镜像的运行状态rootfs介绍rootfs是Docker容器在启动时内部进程可见的文件系统，即Docker容器的根目录。rootfs通常包含一个操作系统运行所需的文件系统，例如可能包含典型的类Unix操作系统中的目录系统，/dev 、/proc 等,以及运行Docker容器所需的配置文件、工具等rootfs原理在传统的Linux操作系统内核启动时，首先挂在一个只读的rootfs，当系统检测其完整性之后，再将其切换为读写模式在Docker架构中，当Docker daemon 为容器挂在rootfs时，沿用了Linux内核启动时的方法，将rootfs设计为只读模式。在挂在完毕之后，再利用连个挂在技术在只读rootfs上再挂在一个读写层。这样可读写层处于Docker容器文件系统的最顶层，其下可能联合挂在多个只读层，只有在Docker容器运行过程中文件系统发生变化时，才会吧变化的文件内容写到可读写层，并隐藏只读层中的老版本文件Docker镜像的特点分层每个镜像都是由一系列的镜像层组成写时复制在多个容器之间共享镜像，每个容器在启动的时候并不需要单独复制一份镜像文件内容寻址根据文件内容来索引镜像和镜像层联合挂载可以在一个挂载点同时挂在多个文件系统，将挂载点的原目录与被挂在内容进行整合，使得最终课件的文件系统将会包含整合之后的各层的文件和目录Docker镜像的构建使用commit创建镜像docker commit命令只是提交容器镜像发生变更了的部分，即修改后的容器镜像与当前仓库中对应镜像之间的差异部分，这使得该操作实际需要提交的文件往往并不多使用commit实现步骤1.根据输入的pause参数的设置确定是否暂停该Docker容器的运行2.将容器的可读写层导出打包，该读写层代表了当前运行容器的文件系统与当初启动该容器镜像之间的差异3.在层存储中逐层可读写层差异包4.更新镜像历史信息rootfs，并据此在镜像存储中创建一个新的镜像，记录其原数据5.如果指定了repository信息，则给上述镜像添加tag信息使用build构建镜像--client端命令格式docker build [OPTIONS] PATH | URL | -path | URL : 其所指向的文件称为上下文context，context包含build Docker镜像过程中所需要的Dockerfile以及其他的资源文件。-t: 指定镜像的名字-f: 指定Dockerfile完整路径 和context的完整路径(可以是相对路径也可以是绝对路径) 不同参数，Docker client处理的方法1. 第一个参数为 &quot;-&quot;sudo docker build - Dockerfilesudo docker build - context.tar.gz该情况，根据命令行输入参数对Dockerfile和context进行设置2.第一个参数为URL，且是git repository URLsudo docker build github.com/creack/docker-firefox该情况，则调用git clone --depth 1 --recursive 命令克隆该Github repository，该操作会在本地的一个临时目录中进行命令你成功之后该目录会作为context传给Docker daemon，该目录中的Dockerfile会被用来进行后续构建Docker镜像3.第一个参数为URL，不是git repository URL该情况，会从该URL下载context，将其封装为一个io流，后面的处理与情况1相同4.context为本地文件或者目录sudo docker build -t vieux/apache:2.0 使用当前文件夹作为contextcd /home/myapp/some/dir/really/deep &amp;&amp;sudo docker build -f /home/memyapp/dockerfiles/debug /home/me/myapp 使用指定的两目录分别作为Dockerfile 和context备注：如果目录中有.dockerignore文件，则将context中文件名满足其定义的规则的文件都从上传到列表中排出，不打包给Docker daemon。但唯一的例外是.dockerignore文件中若误写入了.dockerignore本身或者Dockerfile，将不产生作用。如果用户定义了tag，则对其指定的repository和tag进行验证完成了想关信息的设置之后，Docker client想Docker server发送buildHTTP请求，包含了所需的context信息使用build构建镜像--Docker server端Docker server接受到HTTP请求后，其工作内容1.创建临时目录，将context指定的文件系统解压到该目录下2.读取并解析Dockerfile3.根据解析出的Dockerfile遍历其中的所有指令，并分发到不同的模块去执行。Dockerfile特定的关键词都会映射到不同parser进行处理4.parser为上述每一个指令创建 一个对应的临时容器，在临时容器中执行当前指令，然后通过commit使用此容器生成一个镜像层5.Dockerfile中所有的指令对应的层的集合，就是此次build后的结果DockerfileDockerfile介绍Dockerfile是在通过docker build命令构建自己的Docker镜像时需要使用到的定义文件。它允许用户使用基本的DSL语法来定义Docker镜像，每一条指令描述了构建镜像的步骤。用户可以使用这些统一的语法命令来根据需求进行配置，通过这份统一的配置文件，在不同的平台上进行分发，需要使用时就可以根据配置文件自动化构建，解决了开发人员构建镜像的复杂过程Dockerfile与镜像配合使用，使Docker在构建时可以充分利用镜像功能进行缓存，大大提升了Docker的使用效率Dockerfile生成镜像的过程本地主机的一个包含Dockerfile的目录中的所有内容作为上下文上下文通过docker build命令传入到Docker daemon后，便开始按照Dockerfile中的内容构造镜像按照Dockerfile中的指令进行读条指令的构建镜像，除了from指令，其他每一条指令都会在上一条指令所生成的镜像的基础上执行，执行完成后生辰一个新的镜像层镜像层不断的覆盖原来的镜像形成新的镜像。Dockerfile所生成的最终镜像就是在基础镜像上叠加一层层的镜像层组建的Docker指令介绍基本格式 INSTRUCTION arguments命令格式在Dockerfile中，指令INSTRUCTION不区分大小写，但是为了参数区分，推荐大写。Docker会顺序执行Dockerfile中的指令，第一条指令必须为FROM指令，它用于指定构建镜像的基础镜像ENV 指令格式ENV &lt;KEY&gt; =&lt;value&gt; 说明ENV指令可以为镜像创建出来的容器生命环境变量FROM指令格式：FROM &lt;iamge&gt;[:&lt;tag&gt;]说明FROM指令的功能是为后面的指令提供基础镜像，因为一个有效的Dockerfile必须以FROM指令你做为第一条非注释指令在Dockerfile中，FROM指令可以出现多次，这样会构建多个镜像。在每个镜像创建完成后，Docker命令行界面会输入该镜像的ID。若FROM指令中的参数tag为空，默认tag是latestCOPY指令格式COPY &lt;src&gt; &lt;dest&gt;说明COPY指令复制&lt;src&gt;所指向的文件或者目录，将它添加到新场景中，复制的文件或目录在镜像中的路径是&lt;dest&gt;&lt;src&gt;所指定的源可以有多个，但必须在上下文中，即必须是上下文根目录的相对路径若结尾符为/则表示为目录，否则为文件ADD指令格式ADD &lt;src&gt; &lt;dest&gt;说明ADD与COPY指令在功能上很相似，都支持复制本地文件到镜像的功能，但ADD指令还支持其他的功能&lt;src&gt;可以是一个指向一个网络文件的URL ，若&lt;dest&gt;为目录，则URL必须是完全路径，这样可以获得该网路文件的文件名filename，该文件会被复制添加到&lt;dest&gt;/&lt;file_name&gt;，若URL中的文件为压缩文件，默认不会被解压提取&lt;src&gt;还可以指向一个本地压缩归档文件，该文件再复制到容器中时会被解压提取RUN指令格式 RUN &lt;command&gt; (shell 格式)RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式，推荐使用)说明RUN指令会在前一条命令创建出镜像的基础上创建一个容器，并在容器中运行命令，在命令运行后提交容器为新的镜像，新镜像被Dockerfile中的下一条指令使用通常在需要创建镜像后安装依赖的情况下使用CMD指令格式CMD &lt;command&gt; (shell格式)CMD [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式，推荐格式)CMD [&quot;param1&quot;, &quot;param2&quot;] (为ENTRUPOINT指令提供参数)说明CMD命令提供容器运行时的默认值，这些默认值可以是一条指令，也可以是一些参数。一个Dockerfile中可以有多条CMD指令，但是只有最后一条CMD指令有效。CMD [&quot;param1&quot;, &quot;param2&quot;] 格式只有在CMD指令和ENTRYPOINT指令配合使用，CMD指令中的参数会添加到ENTRYPOINT指令中使用shell和exec格式时，命令在容器中运行的方式和RUN指令相同，不同在于，RUN指令在构建镜像时执行命令，并生成新的镜像，CMD指令在构建镜像的时候不执行任何命令，而是在容器启动的时候默认将CMD指令作为第一条执行命令。如果用户在命令行界面运行docker run命令时指定了命令参数，则会覆盖CMD指令中的命令ENTRYPOINT指令格式ENTRYPOINT &lt;command&gt; (shell格式)ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec格式，推荐格式)说明ENTRYPOINT指令和CMD指令类似，都可以让容器每次启动时候执行相同的命令一个Dockerfile中可以有多条ENTRYPOINT指令，但只有最后一条ENTRYPOINT指令有效当使用shell格式时，ENTRYPOINT指令会忽略任何CMD指令和docker run命令参数，并运行在/bin/sh -c 中。当使用 exec格式，docker run传入的命令参数会覆盖CMD指令的内容并附加到ENTRYPOINT指令参数中CMD可以是参数，也可以是指令，ENTRYPOINT只能是命令，灵台docker run命令你提供的运行命令参数可以覆盖CMD，但是不能覆盖ENTRYPOINTONBUILD格式ONBUILD [INSTRUCTION]说明添加一个将来执行的触发器指令到镜像中。当该镜像作为FROM指令的参数时，这些触发器指令就会在FROM指令执行的时候加入到构建的过程中尽管任何一个指令都可以被注册成一个触发器钟灵，但是ONBUILD指令中不能包含ONBUILD指令，并且不会触发FROM和MAINTAINER指令ONBUILD经常用于需要在创建镜像的时候，需要对导入的镜像做一些文件的导入，一些批处理的时候LABLE格式LABEL multi.label1=&quot;value1&quot; \multi.label2=&quot;value2&quot; \other=&quot;value3&quot;说明label是累积的，包括FROM镜像的lable。如果Docker遇到一个label/key已经存在，那么新的值将覆盖这个label/key。要查看一个镜像的label，使用docker inspect命令。EXPOSE格式EXPOSE &lt;port1&gt; &lt;port2&gt;说明EXPOSE宿主机需要暴露哪些端口在启动容器时需要通过-P（注意是大写），Docker主机会自动分配一个端口转发到指定的端口；使用-p，则可以具体指定哪个本地端口映射过来。WORKDIR格式WORKDIR &lt;path&gt;说明指定了RUN CMD ENTRYPOINT等命令运行的目录Dockerfile示例使用Dockerfile注意标签易读例如：docker build -t =&quot;ruby:2,0-onbuild&quot;谨慎选择基础镜像尽量选择当前官方镜像库中的镜像充分利用缓存尽量让相同的指令放在前面，充分使用缓存的结果正确使用ADD和COPY命令尽量使用COPY命令CMD和ENTRPOINT合用尽量使用CMD和ENTRYPOINT指令配合，可以使用exec格式的ENNTRYPOINT指令设置固定的默认命令参数，然后使用CMD指令设置可变的参数不要在Dockerfile中做端口映射如果在Dockerfile中做端口映射，导致只能在主机上启动一个容器应该使用EXPOSE 80 只暴露端口，另做映射使用Dockerfile共享Docker镜像]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像管理&Dockerfile]]></title>
    <url>%2F2018%2F06%2F19%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E9%9B%86%E7%BE%A4%26%E5%BA%94%E7%94%A8%E6%A0%88%2F</url>
    <content type="text"><![CDATA[Docker集群&amp;应用栈的介绍Docker集群&amp;应用栈概念Docker的设计理念是希望用户能够保证一个容器只运行一个进程，即只提供一种服务。然而对于用户而言，单一的容器是无法满足需求的。通常用户需要利用多个容器，分别提供不同的服务，并在不同的容器间互相通信，最后形成一个docker集群，以实现特定功能。基于Docker集群构建的应用我们成为Docker应用栈搭建应用栈所需过程对应用栈的描述示例搭建一个包含6个节点的Docker-Web服务应用栈，其中包括一个代理节点、两个Web应用节点、一个主数据库节点及两个从数据库节点图例:获取各节点所需的镜像镜像可以是Docker Hub中已经存在的镜像镜像也可以是自己提交的Docker镜像网络调通应用栈容器节点之间的互联（同主机下实现，如果跨主机还需解决跨主机通信）实现节点互联命令docker run [OPTIONS] --link name:alias --name Name IMAGE [COMMAND] [ARG1, ......] alias: 给接受的容器起一个别名可以应用于快速链接name: 容器名称示例sudo docker run -it --name redis-slave1 --link redis-master:master redis /bin/bash通过--link互联的优点通过--link方式互联，不但可以避免容器的IP端口暴露到外网所导致的安全问题，还可以放置容器在启动后Ip地址变化导致的访问失效可以防止容器重启导致容器IP地址变化访问失效，它的原理类似于DNS服务器的域名和地址映射当容器的IP地址发生变化时，Docker将自动维护映射关系中的IP地址查看连接信息cat /etc/host确定应用栈的启动顺序及暴露外网访问端口确定启动顺序(先启动被连接的节点)：redis-master--&gt;redis-slave--&gt;APP1/2--&gt;HAProxy暴露Ip端口sudo docker run -it --name HProxy --link APP1:APP1 --link APP2:APP2 -p 6301:6301 haproxy /bin/bash网络链接涉及到的参数--iptables: 将容器的端口暴露给外部主机，就是通过iptables做DNAT(目的地址转换)实现的。以及Docker容器间的通信业是通过修改FORWARD链中响应的itables规则的策略。当设为false时Docker daemon将不会改变你的宿主机上的iptable规则，默认是true。--icc: 当容器都链接到了docker0网线上，容器已经属于一个子网，已经满足通信前提，这时容器间是否可以通信取决于FORWARD链中的ACCEPT规则，设置为false，规则会被置位DROP，Docker荣期间的相互通信就被禁止，这种情况，想让容器互联一定要使用--link选项，容器间默认为true。注意修改FORWARD规则一定要设置--iptables为true--ip--forward: 在Docker容器和外界通信过程中，还设计数据包的多网卡间转发，需要内核将ip--forward功能打开，即将ip_forward系统参数设置为1修改Docker daemon的启动参数修改/etc/detault/docker应用栈容器节点间的配置/程序文件编辑查看挂在volume位置(如果镜像事先已经配置了volume)sudo docker inspect --format &quot;&quot; CONTAINERreturn: map[/data:/var/docker/vfs/dir/xxx] /data 为容器中的共享目录， /var/docker/vfs/dir/xxx 为主机的volume目录启动时指定volume共享目录(如果镜像中没有配置共享目录)sudo docker run -it -v ~/projects:/usr/src/app ubuntu ~/projects: 为主机的volume目录/usr/src/app: 为容器中的volume目录应用共享volume目录进行配置文件的编辑1.在volume中编辑配置/项目文件，或者将配置/项目文件传到volume目录2.通过volume目录的文件来启动程序Docker在各阶段的作用在开发阶段镜像的使用使得构建开发环境变得简单统一在测试阶段可以直接使用开发所构建的镜像进行测试，直接免除了测试环境构建的烦恼，也消除了因为环境不一致所带来的漏洞问题在部署和运维阶段与以往代码级别的部署不同，利用Docker可以进行容器级别的部署，把应用及其依赖环境打包成跨平台、轻量级、可移植的容器来进行部署]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker的使用]]></title>
    <url>%2F2018%2F06%2F17%2F08.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%2FDocker%2FDocker%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Docker容器介绍什么是DockerDocker是以Docker容器为资源分割的调度基本单位。封装软件运行的环境, 为开发者和系统管理员设计, 用于构建、发布和运行分布式应用的平台。他是一个跨平台、可移植并且简单易用的容器解决方案Docker可以在容器内部实现快速的自动化地部署应用, 并通过操作系统内核技术为容器提供资源隔离与安全保障Docker的优点功能实现容器技术的生态系统自上而下的分别覆盖了IaaS层和PaaS层所涉及的各类问题, 包括资源调度、编排、部署、监控、配置管理、存储网络管理、安全、容器化应用支撑平台等持续部署与测试,容器消除了线上线下的环境差异，保证了应用生命周期的环境一致性和标准化跨云平台支持容器带来的最大好处之一就是其适配性， 越来越多的云平台都支持容器，用户也不需要担心受到云平台的捆绑，同时也让应用多平台混合部署称为可能。环境标准化和版本控制基于容器提供的环境一致性和标准化，可以使用Git等工具对容器镜像进行版本控制，相比基于代码的版本控制来说，可以对整个应用的运行环境实现版本控制，一旦出现故障可以快速回滚。相比之前的虚拟机镜像，容器压缩和备份的速度更快，镜像启动也像启动一个普通进程一样快速高资源利用率与隔离容器没有管理程序的额外开销，与底层共享操作系统，性能更加优良，系统负载更低，在同等条件下可以运行更多的应用实例，可以更充分的利用系统资源。同事，容器拥有不错的资源隔离与限制能力，可以精确地对应用分配cpu、内存、等资源，宝整理应用之间不会互相影响容器跨平台性与镜像Linux容器虽然早在Linux2.6版本内核已经存在，但是缺少容器的跨平台性，难以推广。容器在原有的Linux容器的基础上进行大胆的个性，为容器设定了一整套标准化的配置方法，将应用及其依赖的运行环境打包成镜像，真正实现了构建一次到运行的理念，大大提高了容器的跨平台性易于理解而且易用Docker的英文愿意是处理集装箱的码头工人，标志是鲸鱼运送个一大堆集装箱，集装箱就是容器，生动好记。应用镜像仓库Docker官方构建了一个镜像仓库，组织和管理形式类似于Github，其上已经累积了成千上万的镜像。Docker的安装Ubuntu系统1.安装依赖sudo apt-get updatesudo apt-get install linux-image-generic-lts-trustysudo reboot2.安装wgetsudo apt-get install wget3.安装dockersudo wget -q0- https://get.docker.com/ | sh4.启动dockersudo service docker startWindows系统使用安装包安装1.下载安装包下载地址: https://download.docker.com/win/beta/InstallDocker.msi2.检查安装docker --versiondocker-compose --versiondocker-machinne --version使用Toolbox安装1.下载Toolbox下载地址：https://github.com/docker/toolbox/releases/download/v1.11.2/DockerToolbox-1.11.2.exe 2.安装Toolbox3.安装Docker双击快捷图标可以安装Docker、Docker Compose、Docker MachineCentOS系统1.更新系统sudo yum update2.请求安装脚本curl -fsSl https://get.docker.com/ | sh3.启动dockersudo service docker startDockerHub地址https://hub.docker.com/Docker命令行参数docker命令行通信方式在使用Docker时，需要使用Docker命令行工具的docker与Docker daemon建立通信。Docker daemon是Docker的守护进程，负责接收并分发执行Docker命令Docker daemon负责接收并执行来自docker的命令，它的运行需要root权限，所以一般执行docker命令都需要获取root权限docker命令结构图docker命令行1.获取命令清单docker --help 2.获取命令的详细使用方法docker COMMAND --help3.获取docker详细信息sudo docker info 获取docker详细信息sudo docker version 获取docker版本信息4.容器生命周期管理介绍：容器生命周期管理设计容器启动、停止等功能容器的创建命令 （基于特定的镜像创建一个容器, 并依据选项来控制该容器,会随机分配一个容器ID, 用以标识该容器）docker run [OPTIONS] IMAGE [COMMAND] [ARG1, ......] 如果本地没有该镜像，会默认从hub上拉取-i: 表示使用交互模式，终端始终保持输入流开放-t: 表示分配一个伪终端，一般-i, - t两个参数结合使用 -it，即可在容器中利用打开的伪终端进行交互操作--name: 选项表示可以指定docker run 命令启动容器的名字，若无此选项，Docker将为容器随机分配一个名字-m: 用于限制容器中所有进程分配的内存总量，以B\K\M\G为单位，超出内容限制后，容器会结束进程并退出-v: 用于和主机挂载共享目录，可以同时挂在多个目录。使用格式为[host-dir]:[container-dir]。会覆盖Dockerfile中通过Volume挂载的同名目录-p: 用于将容器的端口暴露给宿主机的端口，其常用的格式为 -p HostPort:ConrainerPort。 暴露端口后, 可以通过外部网络访问宿主机的方式来访问容器。--link: 可以实现同主机容器之间的互联, 使用方式--link container_name:alias 。可以避免容器的IP端口暴露到外网所导致的安全问题，还可以放置容器在启动后Ip地址变化导致的访问失效可以防止容器重启导致容器IP地址变化访问失效。若先要实现跨主机间的容器互联可以修改容器的配置文件/etc/hosts 增加 name 和 ip的映射。-d: 可以直接让容器后台启动。注意若启动的COMMAND命令不是持续前台运行的程序，容器会直接退出。解决办法:1.让COMMAND命令前台运行；2.在命令后面加上 tail top 这类持续前台运行的命令。--privileged：允许容器使用主机的设备，比如mount，也可以看到主机的网络设备，还可以在容器中启动容器等。--network: 指定容器使用哪种网络通讯方式，一共四种 host、None、bridge、 container。Host和主机共享网络；None不分配网络；bridge(默认)所有容器都分配独立的虚拟ip，网关为主机虚拟出来的docker0；container所有容器使用同一个ip。容器的启动、停止、重启命令docker start/stop/restart &lt;containner_id&gt;, &lt;containner_id&gt;-a: 启动一个容器并打印输出结果和错误-i:启动一个容器并进入交互模式-t: 停止或者重启容器的超时时间（秒），超时后系统将杀死进程。重命名容器docker rename &lt;old_name&gt; &lt;new_name&gt;示例sudo docker run -i -t --name mytest ubuntu:latest /bin/bash 为 ubuntu:latest 镜像分配一个容器, 并为它分配一个伪终端, 并执行/bin/bash命令 5.Docker镜像仓库使用介绍Docker registry 是存储容器镜像的仓库，用户可以通过Docker client 与Docker registry进行通信，以此来完成镜像的搜索、下载和上传等相关操作从Docker registry中拉取镜像docker pull [OPTIONS] NAME[:TAG|@DIGEST]将镜像推到镜像库docker push [OPTIONS] NAME[:TAG] # 将镜像推送到公共Hubdocker push SEL/ubuntu # 将镜像推送到指定库示例sudo docker pull ubuntu # 从官方Hub中拉取ubuntu:latest镜像sudo docker pull ubuntu:ubuntu12.04 # 从官方Hub中拉取ubuntu:12.04镜像sudo docker pull SEL/ubuntu # 从官方特定库中拉取ubuntu镜像sudo docker pull 10.10.103.215:5000/sshd # 从其他镜像服务器中拉取镜像6.镜像管理介绍用户可以在本地保存镜像资源，为此Docker提供了相应的管理子命令列出主机上的镜像docker images [OPTIONS] [REPOSITORY[:TAG]]-a: 默认显示最顶层的镜像，-a可以显示所有提交过的镜像删除镜像或者容器docker rm [OPTIONS] CONTAINER [CONRAINER...] # 删除容器docker rmi [OPTIONS] IMAGE [IMAGE...] # 删除镜像-f: 默认情况如果存在启动的容器，则对应的镜像无法删除，-f可以强制删除镜像或容器7.容器运维操作介绍作为Docker的核心，容器的操作是重中之重，Docker为用户提供了丰富的容器运维操作命令与正在运行的程序进行交互attach 进入容器docker attach [OPTIONS] CONTAINERattach 退出容器Ctr + c / exit / Ctr + P + Q 备注： Ctr + p + Q 不会结束容器，会让容器后台运行exec 进入容器当容器的终端被占用的时候，退出和其他的操作都会被限制，此时可以通过exec方式另外用一个终端连接容器docker exec [OPTIONS] CONTAINER COMMAND [ARG, ....]-d :分离模式: 在后台运行-i :交互模式-t :分配一个伪终端exec 退出容器Ctr + c / exit 退出容器后，容器运行不受影响查看镜像\容器的详细信息sudo docker inspect [OPTIONS] CONTAINER|IMAGE--format: 定义输出模板的格式查看容器相关信息(CONTAINER ID 、NAME、IMAGE、STATUS)docker ps [OPTIONS]-a：查看所有容器，默认查看正在启动容器-l: 查看最新创建的容器查看容器中正在运行的进程docker top CONTAINER8.镜像创建介绍commit命令可以将一个容器固话为一个新的镜像。当需要制作特定的镜像时，会进行修改容器的配置，如在容器中安装特定的工具等，通过commit命令可以将这些修改保存起来，使其不会因为容器的停止而丢失使用方法（官方不推荐, 下面会介绍DockerFile的使用）docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] # 注意：只能选择正在运行的容器进行提交9.容器和镜像的持久化和导入介绍可以实现再 某台机器上导出一个Docker容器并且在另外一台机器上导入将容器导出为镜像快照sudo docker export -o ubuntu.tar CONTAINER_ID 将本地快照导出为镜像docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]将镜像导出为镜像快照docker save -o haproxy.tar haproxy将本地快照导出为镜像docker load -i haproxy.tar10.events、history、logs命令介绍events、history、logs这三个命令用于查看Docker的系统日志信息。events命令会打印出实时的系统事件history会打印出指定的镜像的历史版本信息logs命令会打印出容器中的进程运行日期打印实时系统事件docker events [OPTIONS]打印指定镜像的历史版本信息docker history [OPTIONS] IAMGE打印容器中进程运行日志docker logs [OPTIONS] CONTAINERDocker开启端口监听介绍Docker默认只监听本地进程，即从其他主机上无法连接。如果需要远程连接，则需要开启2375端口的监听。通过命令行启动dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375Ubuntu下修改配置文件vim /etc/default/dockerDOCKER_OPTS=&quot;-H unix:///var/run/docker.sock -H 0.0.0.0:2375”Centos下修改配置文件vim /usr/lib/systemd/system/docker.service修改：ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock]]></content>
      <categories>
        <category>Docker容器技术</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensowflw-dropout(防止过拟合)]]></title>
    <url>%2F2018%2F06%2F06%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensowflw-dropout(%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88)%2F</url>
    <content type="text"><![CDATA[理解dropoutdropout是CNN中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。在下读到两篇代表性的论文，代表两种不同的观点，特此分享给大家。组合派参考文献中第一篇中的观点，Hinton老大爷提出来的，关于Hinton在深度学习界的地位我就不再赘述了，光是这地位，估计这一派的观点就是“武当少林”了。注意，派名是我自己起的，各位勿笑。观点该论文从神经网络的难题出发，一步一步引出dropout为何有效的解释。大规模的神经网络有两个缺点：费时容易过拟合这两个缺点真是抱在深度学习大腿上的两个大包袱，一左一右，相得益彰，额不，臭气相投。过拟合是很多机器学习的通病，过拟合了，得到的模型基本就废了。而为了解决过拟合问题，一般会采用ensemble方法，即训练多个模型做组合，此时，费时就成为一个大问题，不仅训练起来费时，测试起来多个模型也很费时。总之，几乎形成了一个死锁。Dropout的出现很好的可以解决这个问题，每次做完dropout，相当于从原始的网络中找到一个更瘦的网络，如下图所示：因而，对于一个有N个节点的神经网络，有了dropout后，就可以看做是2n个模型的集合了，但此时要训练的参数数目却是不变的，这就解脱了费时的问题。动机论虽然直观上看dropout是ensemble在分类性能上的一个近似，然而实际中，dropout毕竟还是在一个神经网络上进行的，只训练出了一套模型参数。那么他到底是因何而有效呢？这就要从动机上进行分析了。论文中作者对dropout的动机做了一个十分精彩的类比：在自然界中，在中大型动物中，一般是有性繁殖，有性繁殖是指后代的基因从父母两方各继承一半。但是从直观上看，似乎无性繁殖更加合理，因为无性繁殖可以保留大段大段的优秀基因。而有性繁殖则将基因随机拆了又拆，破坏了大段基因的联合适应性。但是自然选择中毕竟没有选择无性繁殖，而选择了有性繁殖，须知物竞天择，适者生存。我们先做一个假设，那就是基因的力量在于混合的能力而非单个基因的能力。不管是有性繁殖还是无性繁殖都得遵循这个假设。为了证明有性繁殖的强大，我们先看一个概率学小知识。比如要搞一次恐怖袭击，两种方式：- 集中50人，让这50个人密切精准分工，搞一次大爆破。- 将50人分成10组，每组5人，分头行事，去随便什么地方搞点动作，成功一次就算。哪一个成功的概率比较大？ 显然是后者。因为将一个大团队作战变成了游击战。那么，类比过来，有性繁殖的方式不仅仅可以将优秀的基因传下来，还可以降低基因之间的联合适应性，使得复杂的大段大段基因联合适应性变成比较小的一个一个小段基因的联合适应性。dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。个人补充一点：那就是植物和微生物大多采用无性繁殖，因为他们的生存环境的变化很小，因而不需要太强的适应新环境的能力，所以保留大段大段优秀的基因适应当前环境就足够了。而高等动物却不一样，要准备随时适应新的环境，因而将基因之间的联合适应性变成一个一个小的，更能提高生存的概率。dropout带来的模型的变化而为了达到ensemble的特性，有了dropout后，神经网络的训练和预测就会发生一些变化。训练层面无可避免的，训练网络的每个单元要添加一道概率流程。对应的公式变化如下如下：没有dropout的神经网络有dropout的神经网络测试层面预测的时候，每一个单元的参数要预乘以p。论文中的其他技术点防止过拟合的方法：提前终止（当验证集上的效果变差的时候）L1和L2正则化加权soft weight sharingdropoutdropout率的选择经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数。使得输入变化不会太大（0.8）训练过程对参数w的训练进行球形限制(max-normalization)，对dropout的训练非常有用。球形半径c是一个需要调整的参数。可以使用验证集进行参数调优dropout自己虽然也很牛，但是dropout、max-normalization、large decaying learning rates and high momentum组合起来效果更好，比如max-norm regularization就可以防止大的learning rate导致的参数blow up。使用pretraining方法也可以帮助dropout训练参数，在使用dropout时，要将所有参数都乘以1/p。部分实验结论该论文的实验部分很丰富，有大量的评测数据。maxout 神经网络中得另一种方法，Cifar-10上超越dropout文本分类上，dropout效果提升有限，分析原因可能是Reuters-RCV1数据量足够大，过拟合并不是模型的主要问题dropout与其他standerd regularizers的对比L2 weight decaylassoKL-sparsitymax-norm regularizationdropout特征学习标准神经网络，节点之间的相关性使得他们可以合作去fix其他节点中得噪声，但这些合作并不能在unseen data上泛化，于是，过拟合，dropout破坏了这种相关性。在autoencoder上，有dropout的算法更能学习有意义的特征（不过只能从直观上，不能量化）。产生的向量具有稀疏性。保持隐含节点数目不变，dropout率变化；保持激活的隐节点数目不变，隐节点数目变化。数据量小的时候，dropout效果不好，数据量大了，dropout效果好模型均值预测使用weight-scaling来做预测的均值化使用mente-carlo方法来做预测。即对每个样本根据dropout率先sample出来k个net，然后做预测，k越大，效果越好。Multiplicative Gaussian Noise使用高斯分布的dropout而不是伯努利模型dropoutdropout的缺点就在于训练时间是没有dropout网络的2-3倍。进一步需要了解的知识点dropout RBMMarginalizing Dropout具体来说就是将随机化的dropout变为确定性的，比如对于Logistic回归，其dropout相当于加了一个正则化项。Bayesian neural network对稀疏数据特别有用，比如medical diagnosis, genetics, drug discovery and other computational biology applications噪声派参考文献中第二篇论文中得观点，也很强有力。观点观点十分明确，就是对于每一个dropout后的网络，进行训练时，相当于做了Data Augmentation，因为，总可以找到一个样本，使得在原始的网络上也能达到dropout单元后的效果。 比如，对于某一层，dropout一些单元后，形成的结果是(1.5,0,2.5,0,1,2,0)，其中0是被drop的单元，那么总能找到一个样本，使得结果也是如此。这样，每一次dropout其实都相当于增加了样本。稀疏性知识点A首先，先了解一个知识点：When the data points belonging to a particular class are distributed along a linear manifold, or sub-space, of the input space, it is enough to learn a single set of features which can span the entire manifold. But when the data is distributed along a highly non-linear and discontinuous manifold, the best way to represent such a distribution is to learn features which can explicitly represent small local regions of the input space, effectively “tiling” the space to define non-linear decision boundaries.大致含义就是：在线性空间中，学习一个整个空间的特征集合是足够的，但是当数据分布在非线性不连续的空间中得时候，则学习局部空间的特征集合会比较好。知识点B假设有一堆数据，这些数据由M个不同的非连续性簇表示，给定K个数据。那么一个有效的特征表示是将输入的每个簇映射为特征以后，簇之间的重叠度最低。使用A来表示每个簇的特征表示中激活的维度集合。重叠度是指两个不同的簇的Ai和Aj之间的Jaccard相似度最小，那么：当K足够大时，即便A也很大，也可以学习到最小的重叠度当K小M大时，学习到最小的重叠度的方法就是减小A的大小，也就是稀疏性。上述的解释可能是有点太专业化，比较拗口。主旨意思是这样，我们要把不同的类别区分出来，就要是学习到的特征区分度比较大，在数据量足够的情况下不会发生过拟合的行为，不用担心。但当数据量小的时候，可以通过稀疏性，来增加特征的区分度。因而有意思的假设来了，使用了dropout后，相当于得到更多的局部簇，同等的数据下，簇变多了，因而为了使区分性变大，就使得稀疏性变大。为了验证这个数据，论文还做了一个实验，如下图：该实验使用了一个模拟数据，即在一个圆上，有15000个点，将这个圆分为若干个弧，在一个弧上的属于同一个类，一共10个类，即不同的弧也可能属于同一个类。改变弧的大小，就可以使属于同一类的弧变多。实验结论就是当弧长变大时，簇数目变少，稀疏度变低。与假设相符合。个人观点：该假设不仅仅解释了dropout何以导致稀疏性，还解释了dropout因为使局部簇的更加显露出来，而根据知识点A可得，使局部簇显露出来是dropout能防止过拟合的原因，而稀疏性只是其外在表现。论文中的其他技术知识点将dropout映射回得样本训练一个完整的网络，可以达到dropout的效果。dropout由固定值变为一个区间，可以提高效果将dropout后的表示映射回输入空间时，并不能找到一个样本x*使得所有层都能满足dropout的结果，但可以为每一层都找到一个样本，这样，对于每一个dropout，都可以找到一组样本可以模拟结果。dropout对应的还有一个dropConnect，公式如下：dropoutdropConnect试验中，纯二值化的特征的效果也非常好，说明了稀疏表示在进行空间分区的假设是成立的，一个特征是否被激活表示该样本是否在一个子空间中。]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensowFlow分布式]]></title>
    <url>%2F2018%2F06%2F05%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensowFlow%E5%88%86%E5%B8%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[自定义命令行参数定义命令行参数的方法tf.app.flags 支持应从命令行接收参数, 可以用来指定集群的配置等, 有下面的参数类型DEFINE_string(flag_name, default_value, docstring)DEFINE_integer(flag_name, default_value, docstring)DEFINE_boolean(flag_name, default_value, docstring)DEFINE_float(flag_name, default_value, docstring)调用命令行参数的方法tf.app.flags.FlAGS.xxxxxx 为上面定义的flag_name通过命令行启动的方法tf.app.run() 可以截止启动main(argv) 函数可以在命令行中启动并传入参数, 程序可以直接帮我们运行main()函数 分布式TensorFlow分布式TensowFlow介绍Tensorflow的一个特色就是分布式计算。分布式Tensorflow是由高性能的gRPC框架作为底层技术来支持的。这是一个通信框架gRPC(google remote procedure call)，是一个高性能、跨平台的RPC框架。RPC协议，即远程过程调用协议，是指通过网络从远程计算机程序上请求服务。分布式原理Tensorflow分布式是由多个服务器进程和客户端进程组成。有几种部署方式，列如单机多卡和多机多卡（分布式）。单机多卡(单台服务器有多块GPU设备)在单机单GPU的训练中，数据是一个batch一个batch的训练。 在单机多GPU中，数据一次处理4个batch(假设是4个GPU训练）， 每个GPU处理一个batch的数据计算。变量，或者说参数，保存在CPU上。数据由CPU分发给4个GPU，在GPU上完成计算，得到每个批次要更新的梯度在CPU上收集完4个GPU上要更新的梯度，计算一下平均梯度，然后更新。循环进行上面步骤多机多卡而分布式是指有多台计算机，充分使用多台计算机的性能，处理数据的能力。可以根据不同计算机划分不同的工作节点。当数据量或者计算量达到超过一台计算机处理能力的上上限的话，必须使用分布式分布式架构当我们知道的基本的分布式原理之后，我们来看看分布式的架构的组成。分布式架构的组成可以说是一个集群的组成方式。那么一般我们在进行Tensorflow分布式时，需要建立一个集群。通常是我们分布式的作业集合。一个作业中又包含了很多的任务（工作结点），每个任务由一个工作进程来执行。分布式节点之间的关系一般来说，在分布式机器学习框架中，我们会把作业分成参数作业（parameter job）和工作结点作业（worker job）。参数服务器:运行参数作业的服务器我们称之为参数服务器（parameter server,PS），负责管理参数的存储和更新参数服务器，当模型越来越大时，模型的参数越来越多，多到一台机器的性能不够完成对模型参数的更新的时候，就需要把参数分开放到不同的机器去存储和更新。参数服务器可以是由多台机器组成的集群。worker服务器:工作结点作业负责主要从事计算的任务，如运行操作。Tensorflow的分布式实现了作业间的数据传输，也就是参数作业到工作结点作业的前向传播，以及工作节点到参数作业的反向传播。所有的Worker服务器会有一个主worker服务器, 会话的创建, 文件的读取和存储只会在主worker上进行分布式的模式数据并行原理数据并总的原理很简单。其中CPU主要负责梯度平均和参数更新，而GPU主要负责训练模型副本。实现模型副本定义在GPU上对于每一个GPU，都是从CPU获得数据，前向传播进行计算，得到损失，并计算出梯度CPU接到GPU的梯度，取平均值，然后进行梯度更新存在的问题每一个设备的计算速度不一样，有的快有的慢，那么CPU在更新变量的时候，就应该等待每一个设备的一个batch进行完成，这样相当于降低了分布式的效率,解决办法请参考下面的异步更新同步更新&amp;异步更新同步更新(随机梯度下降法（Sync-SGD))同步随即梯度下降法的含义是在进行训练时，每个节点的工作任务需要读入共享参数，执行并行的梯度计算，同步需要等待所有工作节点把局部的梯度算好，然后将所有共享参数进行合并、累加，再一次性更新到模型的参数；下一个批次中，所有工作节点拿到模型更新后的参数再进行训练。这种方案的优势是，每个训练批次都考虑了所有工作节点的训练情况，损失下降比较稳定；劣势是，性能瓶颈在于最慢的工作结点上。异步更新(异步随机梯度下降法（Async-SGD))异步随机梯度下降法的含义是每个工作结点上的任务独立计算局部梯度，并异步更新到模型的参数中，不需要执行协调和等待操作。这种方案的优势是，性能不存在瓶颈；劣势是，每个工作节点计算的梯度值发送回参数服务器会有参数更新的冲突，一定程度上会影响算法的收敛速度，在损失下降的过程中抖动较大。分布式接口创建分布式集群的方法创建集群的方法是为每一个任务启动一个服务，这些任务可以分布在不同的机器上，也可以同一台机器上启动多个任务，使用不同的GPU等来运行。创建分布式集群的步骤1、创建一个tf.train.ClusterSpec，用于对集群中的所有任务进行描述，该描述内容对所有任务应该是相同的2、创建tf.train.Server，用于创建一个任务3、启动TensorFlow分布式API接口使用1. tf.train.ClusterSpec( ) 创建ClusterSpec,表示参与分布式TensorFlow计算的一组进程2. tf.train.Server( ) 创建Tensorflow的集群描述信息，其中ps和worker为作业名称，通过指定ip地址加端口创建，创建serverserver = tf.train.Server(server_or_cluster_def, job_name=None, task_index=None, protocol=None, config=None, start=True)server_or_cluster_def: 集群描述job_name: 任务类型名称 task_index: 任务数使用serverserver.target返回tf.Session连接到此服务器的目标server.join()参数服务器端等待接受参数任务，直到服务器关闭3. tf.device( ) 指定worker运行设备tf.device(device_name_or_function) 指定代码运行在CPU或者GPU上 device_name:例: /job:worker/task:0/cpu:0function:tf.train.reploca_device_setter(worker_device=worker_device, cluster=cluster)worker_device: 例： /job:worker/task:0/cpu:0cluster: 集群描述对象示例：4. sess = tf.train.MonitoredTrainingSession( ) 创建分布式会话创建会话方法tf.train.MonitoredTrainingSession(master, is_chief=True, checkpoint_dir=None, hooks=None, save_checkpoint_secs=600, save_summaries_steps=USE_DEFAULT, save_summaries_secs=USE_DEFAULT, config-None)master: 指定运行会话协议IP和端口(用于分布式)例: grpc://192.168.0.1:2000is_chief: 是否为主worker 主worker负责初始化和恢复基础的TensorFlow会话checkpoint_dir: 检查点文件记录, 同时也是events目录config: 会话运行的配置项, tf.ConfigProto(log_device_placement=True)hooks: 可选SessionRunHook 对象列表会话的方法sess.should_stop( ) : 当程序发生异常的时候should_stop 返回True sess.run() 跟session一样可以运行op分布式会话-钩子对象钩子对象作用当在开启分布式会话的时候, 钩子对象方法被调用指定了钩子对象(包含了begin，before_run, after_run方法), 可以实现分别在初始化会话, 调用run方法之前, 调用run方法之后, 分别运行钩子对象对应的方法该功能相当于django的中间件功能可以用来实现计步, 打印中间输出等功能创建钩子对象的方法1. 创建类并继承tf.train.SessionRunHook 2. 实现方法 begin 会话之前调用, 只会调用一次3. 实现方法 before_run(run_context) 接收run_context 参数run_context: 一个SessionRunContenxt对象, 包含会话的运行信息return: 一个SessionRunArgs对象, 例如: tf.train.SessionRunArgs(Tensow),接收的参数必须为Tensow类型4. 实现方法 after_run(run_context, run_value)run_context: 一个SessionRunContext 对象run_values: 一个SessionRunValues对象, run_values.results 为before_run返回的Tensow对象的eval属性, 即tensow的值常用的钩子对象tf.train.StopAtStepHook(last_stop) 计步钩子对象last_stop: 指定训练步数, 当达到的时候抛出异常注意: 在使用计步钩子的时候需要定义全局步数：global_step = tf.contrib.framework.get_or_create_global_step()]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensowFlow神经网络]]></title>
    <url>%2F2018%2F06%2F04%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensowFlow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[神经网络介绍神经网络在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络的结构和功能的计算模型，用于对函数进行估计或近似。神经网络的分类(人工神经网络)神经网络前身: 单层感知机基础神经网络：线性神经网络，BP神经网络，Hopfield神经网络等 进阶神经网络：玻尔兹曼机，受限玻尔兹曼机，递归神经网络等 深度神经网络：深度置信网络，卷积神经网络(CNN)，循环神经网络，LSTM网络等神经网络的特点输入向量的特征数量和输入神经元的个数相同输出层的神经元数量和种类数量相同每个连接都有个权重值同一层神经元之间没有连接 由输入层，隐层，输出层组成第N层与第N-1层的所有神经元连接，也叫全连接简单的神经网络模型神经网络模型示例输入层、隐层、输出层 （隐层和输出层统称为全连接层）神经元模型示例输入向量的特征数量和输入神经元的个数相同中间层三个神经元的示例神经网络的组成结构： 组成神经网络的神经元激励函数：隐层和输出层节点的输入和输出之间具有函数关系，这个函数称为激励函数学习规则：学习规则制定了网络中的权重如何随着时间的推进而调整(反向传播算法)TensowFlow提供神经网络功能的主要模块tf.nn：神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluationtf.layers:主要提供的高层的神经网络，主要和卷积相关的，对tf.nn的进一步封装tf.contrib:tf.contrib.layers提供够将计算图中的 网络层、正则化、摘要操作、是构建计算图的高级操作神经网络前身--感知机什么是感知机有n个输入数据，通过权重与各数据之间进行加权求和，求和后的结果经过激活函数结果(0/1)，得出输出很容易解决与、或问题通过多个感知机，可以将结果分类的区域划分的越来越准确用模型表示感知机与逻辑回归的联系和区别联系: 都可以用来做分类的学习区别：激活函数不一样，感知机使用的额是sign函数,输出值是0/1，只能判断简单的分类问题, 逻辑回归使用的是sigmid函数, 结果为0-1, 可以预测为某一个分类的准确概率, 更为广泛使用浅层(单一隐层)神经网络概念只有一层隐藏层的神经网络目标值处理one-hot编码one-hot api介绍作用: 将一组目标集转化为one-hot编码tf.one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)indices: 数据集标签, 目标值集合depth: 张量的深度, 即类别数SoftMax回归函数作用: 将一组结果数据转化为[0, 1] 两个之间的数, 为1的数代表预测结果为该分类公式 使用位置在特征值经过加权和偏置计算后，经过softmax函数, 得到最终预测的概率结果图示特征的加权计算tensorflow加权计算apitf.matmul(a, b,name=None)a: 特征值矩阵b: 加权矩阵损失计算 损失计算-交叉熵损失公式损失计算apis = tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None,name=None) 计算交叉熵损失labels:标签值（真实值）logits：样本加权之后的值return:返回损失值列表损失均值计算apitf.reduce_mean(s) 获取损失的平均值损失的优化--梯度下降 tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)learning_rate: 学习率，一般为minimize(loss): 最小化损失return: 梯度下降op模型准确性计算equal_list = tf.equal(tf.argmax(y, 1), tf.argmax(y_label, 1)) 返回每一行的最大值的索引equal_list = tf.equal(tf.argmax(y, 0), tf.argmax(y_label, 0)) 返回每一列的最大值的索引equal_list = tf.equal(tf.argmax(y, 2), tf.argmax(y_label, 2)) 返回每行每列的最大值accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))y: 预测结果的集合x: 目标集合神经网络实现流程1、准备数据2、全连接结果计算3、损失优化4、模型评估（计算准确性）深层神经网络(卷积神经网络介绍)概念深度学习网络与更常见的单一隐藏层神经网络的区别在于深度，深度学习网络中，每一个节点层在前一层输出的基础上学习识别一组特定的特征。随着神经网络深度增加，节点所能识别的特征也就越来越复杂为什么引入深层神经网络全连接神经网络具有一定的局限性当数据的特征特别多的时候, 每一个特征值都会有一个权重, 这时每一个神经元都需要很多个权重值这样就会很影响资源， 造成资源的浪费， 影响训练增加隐藏层数, 可以每层逐渐的减少所需权重的数量什么是卷积神经网络卷积神经网络，是一种前馈神经网络，人工神经元可以响应周围单元，可以进行大型图像处理。卷积神经网络包括卷积层和池化层。卷积神经网络的发展历史卷积神经网络的特点神经网络(neural networks)的基本组成包括输入层、隐藏层、输出层。而卷积神经网络的特点在于隐藏层分为卷积层和池化层(pooling layer，又叫下采样层)卷积神经网络的结构分析训练结构图卷积层(CONV): 卷积层介绍通过Filter(过滤器)在原始图像上平移来提取并转化特征, 使转化后的特征可以提供给池化层进行数据的池化卷积层过滤器参数介绍个数大小(1*1,3*3,5*5)步长零填充卷积层输出的参数介绍深度由过滤器个数决定输出长度和宽度：由filter尺寸和步长决定RELU激活函数使用Relu函数的优点第一，采用sigmoid等函数，反向传播求误差梯度时，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（求不出权重和偏置）函数图形激活函数使用apitf.nn.relu(features, name=None)features:卷积后加上偏置的结果return:结果池化层(POOL):池化层介绍通过特征后稀疏参数来减少学习的参数，降低网络的复杂度，（最大池化和平均池化）Pooling层主要的作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数数量。Pooling的方法很多，最常用的是Max Pooling。2*2 2步长数据的池化过程图例池化使用apitf.nn.max_pool(value, ksize=, strides=, padding=,name=None) 输入上执行最大池数value:4-D Tensor形状[batch, height, width, channels]ksize:池化窗口大小，[1, ksize, ksize, 1]strides:步长大小，[1,strides,strides,1]padding:“SAME”, “VALID”，使用的填充算法的类型，使用“SAME”全连接层(FC):全连接层在卷积神经网络中介绍前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。最后的全连接层在整个卷积神经网络中起到“分类器”的作用。卷积神经网络相关参数计算输入值的体积H1 * W1 * D1卷积层过滤器四个超参数Filter数量KFilter大小K步长S零填充大小P卷积层过滤器输出体积大小H2 = (H1-F+2P)/S + 1W2 = (W1-F+2P)/S +1D2 = K数据特征的变化过程数据在经过多个卷积层后的变化宽度和高度在缩小, 深度不断的增加(深度和卷积层的过滤器数量相关)单个卷积层(2个Filter)的过程示例卷积层的零填充定义卷积核在提取特征映射时的动作称之为padding（零填充)由于移动步长不一定能整出整张图的像素宽度。其中有两种方式，SAME和VALID两种零填充的方式1. SAME：越过边缘取样，取样的面积和输入图像的像素宽度一致。2. VALID：不越过边缘取样，取样的面积小于输入人的图像的像素宽度卷积网络卷积操作零填充api介绍tf.nn.conv2d(input, filter, strides=, padding=, name=None) 卷计划操作input：给定的输入张量，具有[batch,heigth,width, channel]，类型为float32,64filter：指定过滤器的大小，[filter_height, filter_width,in_channels, out_channels]strides：strides = [1, stride, stride, 1],步长padding：“SAME”, “VALID”，使用的填充算法的类型，使用“SAME”。其中”VALID”表示滑动超出部分舍弃，SAME”表示填充，使得变化后height,width一样大神经网络总结输入层的矩阵的特征数量和特征的数量(样本的列数)相等输出层的矩阵的列数与分类数量相等经过矩阵变换的矩阵: 行数=左矩阵的行， 列数= 右矩阵的列每个矩阵代表全连接层的一层隐层矩阵中的每一列代表一个神经元一个全连接层和一有多个隐层，即多个矩阵偏置的列数应该等于矩阵的行数]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow线程对列&IO操作]]></title>
    <url>%2F2018%2F06%2F03%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensorFlow%E7%BA%BF%E7%A8%8B%E5%AF%B9%E5%88%97%26IO%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[TensorFlow--IO操作TensorFlow 异步IO操作的原理如下如， 文件对列用来存储需要读取文件或者文本， 阅读器用来不断从对列中获取文件进行内容的读取， Deoder用来对读取出来的内容进行解码， 结果对列用来存储读取的结果IO操作是真正的多线程(异步)操作流程如下图IO操作是怎么实现异步的当数据量很大时，入队操作从硬盘中读取数据，放入内存中，主线程需要等待入队操作完成，才能进行训练。会话里可以运行多个线程，实现异步读取。TensorFlow--队列队列的创建Q = tf.FIFOQueue(capacity, dtypes, name=&quot;fifo_queue&quot;) 创建先进先出的对列，按照顺序出对列capacity: 整数， 对列的容量dtypes: dtype 数据类型tf.RandomShuffleQueue() 创建随机出队的对列队列的操作Q.dequeue() 出列操作Q.enqueue(vals) 单个元素入队Q.enqueue_many(vals) 多个元素入队vals: 列表或者元组, 包含需要入队的元素Q.size() 返回一个tensor类型的对象, value是对列的大小使用示例TensorFlow--对列管理器队列管理器的作用当直接通过对列的方法向队列中添加数据的时候, 需要等待数据添加完成才能执行下一步操作, 这样的操作不是异步的当我们想一边自动的向对列中添加数据, 一边进行其他的操作的时候, 我们可以使用对列管理器其会自动的绑定对列和一些对对列的操作, 并另外开启线程执行绑定的操作对列管理器的创建qr = tf.train.QueueRunner(queue, enqueue_ops=None) 创建对列管理器queue: 绑定到对列管理器的对列enqueue_ops: 代表对列操作列表，每个操作会创建一个子线程来执行创建线程让对列管理器执行thd = qr.create_threads(sess, coord=None, start=False) 创建线程来执行队列管理器绑定的方法start: True， 直接启动队列管理器线程。False， 需要手动调用start来让线程运行coord：线程协调器return: 返回线程对象线程资源问题的解决问题： 当在会话中开启线程的时候, 当使用with方式开启线程并执行后, 当主线程结束后, 由于with方式开启会话会对资源进行回收这时候子线程失去会话资源, 会导致程序崩溃解决方法使用线程协调器TensorFlow--线程协调器线程协调器的作用实现一个简单的机制来协调一组线程的终止阻塞等操作创建线程协调器coord = tf.train.Coordinator() 创建线程斜体其起线程协调器的方法coord.request_stop() 终止绑定在线程协调器的所有线程coord.should_stop() 检查是否满足停止条件,如果满足便退出线程coord.join(threads=None) 阻塞等待线程结束使用代码示例文件读取文件队列的创建tf.train.string_input_producer(string_tensor, num_opochs=None, shuffle=True) 创建文件队列，用来存储即将操作的文件string_tensor: 含有文件名的1阶张量num_epochs: 过几遍数据，默认无限过数据return: 输出字符串的队列文件队列线程开启操作作用：开启线程创建图中定义的对列，并将设置的待读取内容读取到对列中thd = tf.train.start_queue_runners(sess=None,coord=None, start=True)sess: 所在的会话中coord：线程协调器start： 是否直接开启线程管道读取批量处理作用可以每次批量的进行文件内容的读取原理是会创建一个批处理对列,当读取文件内容的时候, 批处理对列会预先从文件对列中选取文件，读取内容将对列填满然后根据设定的batch_size, 返回对列中的内容当内容取出后， 再从读取文件将对列填满, 等待下一次的读取使用方法tf.train.batch(tensors, batch_size, num_threads=1, capacity=32, name=None) 读取指定大小（个数）的张量tensors：可以是包含张量的列表batch_size:从队列中读取的批处理大小num_threads：进入队列的线程数capacity：整数，队列中元素的最大数量return： tensors, 读取到内容的数组tf.train.shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads=1) 乱序读取指定大小（个数）的张量min_after_dequeue:留下队列里的张量个数，能够保持随机打乱注意：批处理接收的tensow对象必须都是规定了shape，并且统一的对象文件阅读器创建文本阅读器Reader = tf.TextLineReader()阅读文本，默认按行读取, 默认分隔符为,二进制文件阅读器Reader = tf.FixedLengthRecordReader(record_bytes)阅读二进制文件,可以指定每次读取的字节数record_bytes:整型，指定每次读取的字节数TFRecord文件阅读器Reader = tf.TFRecordReader()读取TfRecords文件图片阅读器Reader = tf.WholeFileReader() 图片阅读器文件阅读器的方法Reader.read(file_queue) 从队列中读取指定数量的内容返回一个Tensors元组（key文件名字，value默认的内容(行或者字节或者图片), tensow类型）文件内容解码器文本文件解码器tf.decode_csv(records, record_defaults=None, field_delim = None, name = None)由于从文件中读取的是字符串，需要函数去解析这些字符串到张量record_defaults: 字段的默认值，比如[[1]，[]，['string']]，不指定类型（设为空[]）也可以field_delim: 默认分割符”,”records: tensor型字符串，每个字符串是csv中的记录行二进制文件解码器tf.decode_raw(bytes, out_type, little_endian=None, name = None) 将字符串类型的二进制数据， 转换为uint8 或者int32类型的数据与函数tf.FixedLengthRecordReader搭配使用，将字符串表示的二进制读取为uint8格式bytes: 读取到的bytes字节内容out_type: 读取后输出的类型图片文件解码器解码方法tf.image.decode_jpeg(content, out_type)配合tf.WholiFileReader 使用,将JPEG编码的图像解码为uint8张量return:uint8张量，3-D形状[height, width, channels]tf.image.decode_png(contents, out_type)将PNG编码的图像解码为uint8或uint16张量return:张量类型，3-D形状[height, width, channels]图片的基本操作apitf.image.resize_images(images, size) 更改图片尺寸(只能更改尺寸, 不能更改通道)images：4-D形状[batch, height, width, channels]或3-D形状的张量[height, width, channels]的图片数据size：1-D int32张量：new_height, new_width，图像的新尺寸return: 返回4-D格式或者3-D格式图片的tensow类型，并且tensow对象的shape对应通道数为? 不固定类型TFRecords文件解码器feature = tf.parse_single_example(value,features={ &quot;image&quot;: tf.FixedLenFeature([], tf.string), &quot;label&quot;: tf.FixedLenFeature([], tf.int64)}) 根据协议将TFRecords的内容转化为featureimage = feature['image'] 通过feature可以读取里面的特征值label = feature['label']文件存储器(TFRecords文件存储器)建立TFRecord存储器Saver = tf.python_io.TFRecordWriter(path) 建立存储器path: TFRecords文件存储的路径存储器的方法write(record): 向文件中写入一个字符串记录(下面介绍)close(): 关闭文件写入器TFRecords存储的字符串记录构造样本的Example协议块example = tf.train.Example(features=None) 创建协议块features: tf.train.Features类型的特征实例feature 特征实例的创建feature 的创建feature = tf.train.Features(feature=None) 构建每个样本的信息键值对feature: 字典数据, key为要保存的名字，value为tf.train.Feature实例feature的value的创建tf.train. Int64List(value=[Value])tf.train. BytesList(value=[Bytes])tf.train. FloatList(value=[value])TFRecords的字符串记录创建实例example = tf.train.Example(features=tf.train.Features(feature={&quot;image&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),&quot;label&quot;: tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))}))文本文件读取代码示例图片文件的读取示例二进制文件读取示例TFRecords文件存储示例TFRecords文件读取示例]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow基础]]></title>
    <url>%2F2018%2F06%2F02%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FTensorFlow%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[深度学习（Deep Learning）和TensorFlow的介绍深度学习深度学习是利用多层神经网络结构，从大数据中学习现实世界中各类事物能直接用于计算机计算的表示形式（如图像中的事物、音频中的声音等），被认为是智能机器可能的“大脑结构”Learning：让计算机自动调整函数参数以拟合想要的函数的过程Deep：多个函数进行嵌套，构成一个多层神经网络，利用非监督贪心逐层训练算法调整有效地自动调整函数参数简单地说深度学习就是：使用多层神经网络来进行机器学习TensorFlowTensorFlow是用来进行深度学习开发的深度学习框架是Google Brain的计划产物应用于AlphaGo， Gmail， Google Maps等1000多个产品与2015年11月开源， 2017年2月发布1.0版本架构师 Jeff DeanTensorFlow的历史版本Tensorflow的特点1、真正的可移植性引入各种计算设备的支持包括CPU/GPU/TPU，以及能够很好地运行在移动端，如安卓设备、ios、树莓派等等2、多语言支持Tensorflow 有一个合理的c++使用界面，也有一个易用的python使用界面来构建和执行你的graphs，你可以直接写python/c++程序。3、高度的灵活性与效率TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库能够灵活进行组装图，执行图。随着开发的进展，Tensorflow的效率不段在提高4、支持TensorFlow 由谷歌提供支持，谷歌投入了大量精力开发 TensorFlow，它希望TensorFlow 成为机器学习研究人员和开发人员的通用语言TensorFlow的结构TensorFlow运算的定义和执行方式需要通过调用TensorFlow的接口来定义操作OP和张量并自动汇总成一个流程图需要通过session的会话对象来执行流程图中的操作TensorFlow的数据流图数据流图可以理解为整个训练的逻辑图通过以图的逻辑来进行不断的训练模型TensorFlow的组成理解Tensor + flowTensor: 提供给训练流动的数据flow: 组成训练逻辑的流程图TensorFlow调用时候的异常处理1、可以通过源码安装的方式避免警告2、通过加入如下的代码解决import osos.environ['TF_CPP_MIN_LOG_LEVEL']='2'TensorFlow文档https://www.tensorflow.org/versions/r1.0TensorFlow--图图的介绍图可以表示为是由一组计算单位对象tf.Operation和其中间流动的数据单元tf.Tensor组成其在定义好tf.Opetation和tf.Tensor后便默认生成调用图对象的方法tf.get_default_graph() 调用当前默认图的方法op.graph 获取当前op所存在的图sess .graph 获取当前会话所存在的图tensor.graph 获取当前的数据单元所存在的图图的创建\调用new_g = tf.Graph() 图的创建with new_g.as_default(): 使用新创建的图with tf.session(graph=new_g) as sess: 图的调用， 指定图创建会话TensorFlow--opop的介绍op可以理解为神经元, 一个神经元有多个输入，一个或者多个输出op在接收到输入的tensor数据后, 通过一系列的操作, 返回输出内容需要在sess会话中run调用才可以执被执行可以类比为python的函数/方法常用的op有哪些TensorFlow--会话会话的介绍会话 session可以理解为图和处理器之间的桥梁，只有通过session才能够将图中定义的op\tensor等执行创建会话的方法tf.Session() 需要手动close()with tf.Session() as sess: 不需要进行手动close()with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: 创建会话并指定运行会话的设备with tf.InteractiveSession() as sess: 创建交互方式的会话会话拥有的资源tf.Variabletf.QueueBasetf.ReaderBase会话资源在会话结束后会进行回收会话的run方法run(fetches, feed_dict=None,graph=None) 可以运行op或者计算tensorfetches: 可以是op，tensor，列表, 元组，namedtuple，dict，OrderedDictfeed_dict: 允许调用者覆盖图中指定的张量的值， 提供给placeholder使用返回的异常值RuntimeError：如果它Session处于无效状态（例如已关闭）。TypeError：如果fetches或feed_dict键是不合适的类型。 ValueError：如果fetches或feed_dict键无效或引用 Tensor不存在TensorFlow--Tensor(张量)张量的创建tf.constant() 直接创建确定值的张量tf.placeholder() 创建Feed类型的张量tf.Variable() 创建变量类型的张量参数：shape: 张量的形状， 定义张量的阶dtype: 定义张量的数据类型name: 定义张量的名称Feed操作作用在程序执行的时候, 不确定性输入什么的是什么的时候, 提前占位(相当于python的缺省参数的作用)使用方法tf.placholder(dtype, shape, name) 创建Feed类型占位张量调用方法sess.run([op_name], feed_dict={placeholder_name: value})图例张量的阶张量的数据类型 张量的属性graph: 张量所属的默认图op: 创建出张量的操作名name: 张量的字符串描述shape: 张量的形状操作演示张量的动态形状与静态形状静态形状创建一个张量,初始的形状为静态性状动态形状一种描述原始张量在执行过程中的一种形状(动态变化), 代表经过变化后的静态形状形状的获取Tensor.get_shape() 获取张量形状Tensor.set_shape([]) 对非固定形状的张量进行形状设置, 即只能对Feed类型张量进行设置tf.reshape(Tensor, shape=[]) 可以对张量进行形状的转换, 但是元素数量必须相等张量转化的要点1、转换静态形状的时候，1-D到1-D，2-D到2-D，不能跨阶数改变形状2、 对于已经固定或者设置静态形状的张量／变量，不能再次设置静态形状3、tf.reshape()动态创建新张量时，元素个数不能不匹配生成张量tf.zeros(shape, dtype=tf.float32, name=None) 创建所有元素为0的张量tf.ones(shape, dtype=tf.float32, name=None) 创建所有元素为一的张量tf.constant(value, dtype=Noe, shape=None, name=None) 创建一个常量张量tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 创建正态分布的随机张量张量数据类型变换张量形状的变换和切片TensorFlow--变量什么是变量变量也是一种OP，是一种特殊的张量，能够进行存储持久化，它的值就是张量,默认被训练变量的创建及其初始化tf.Variable(initial_value=None,name=None, trainable=True) 创建一个带值initial_value的新变量tf.global_variables_initializer() 添加一个初始化所有变量的op， 需要在会话中开启sess.run() 运行op变量的属性的方法assign(value) 为变量重新分配一个值name 变量的名字eval(session=None) 计算并返回变量的值、需要在会话中执行TensorFlow--Tensorboardevents文件events文件可以记录图的内容，和变量等信息TensorBoard 通过读取 TensorFlow 的事件文件来运行并进行可视化的创建event文件file_writer = tf.summary.FileWriter('XXX', graph) 创建writer对象,并创建events文件，在会话中进行写入事件(图)文件到指定目录(最好用绝对路径)，以提供给tensorboard使用在events文件中记录变量变化收集变量的变化tf.summary.histogram(name=&quot;&quot;,tensor) 收集高维度的变量参数tf.summary.image(name=&quot;&quot;,tensor) 收集输入的图片张量tf.summary.scalar(name=&quot;&quot;,tensor) 收集对于损失函数和准确率合并变量并写入事件文件merged = tf.summary.merge_all() 合并变量summary = sess.run(merged) 运行合并,每次添加前都需要运行，会话中进行FileWriter.add_summary(summary,i) 添加变量, i表示第几次添加，会话中进行开启tensorboard功能tensorboard --logdir=“XXX”tensorboard中符号的意义TensorFlow--变量作用域变量作用域让模型代码更加清晰，作用分明可以将作用域分开, 变量会按照作用域进行域的划分，在tensorboard中观察变量也更清晰创建作用域的方法with tf.variable_scope(&lt;scope_name&gt;)： 用上下文管理器的方式创建作用域scope_name: 为变量作用域的名称TensorFlow--命令行参数作用我们可以通过使用tf.app.flags提供的方法, 来定义参数, 同样我们也可以通过它提供的方法来调用定义的参数命令行参数定义的方式命令行参数的调用方式tf.app.flags.,在flags有一个FLAGS标志，它在程序中可以调用到我们前面具体定义的flag_nametf.app.flags.FLAGS.xxx调用main函数方法通过tf.app.run()可直接启动main(argv)函数代码示例：TensorFlow--模型的保存和加载保存模型的方法创建模型保存操作Opsaver_op = tf.train.Saver(var_list=None,max_to_keep=5)var_list:指定将要保存和还原的变量。它可以作为一个dict或一个列表传递. 默认保存图中全部变量max_to_keep： 指示要保留的最近检查点文件的最大数量。创建新文件时，会删除较旧的文件。如果无或0，则保留所有检查点文件。默认为5（即保留最新的5个检查点文件。）对模型进行保存(在会话中操作)saver_op.save(sess, save_path)sess: 会话对象， 默认当前会话save_path: 保存路径, 保存文件格式为checkpoint文件模型的加载(恢复)方法saver_op.restore(sess, save_path) 模型恢复, 在会话中操作sess: 会话对象save_path: 存储路径Tensorflow--简单的运算API矩阵运算tf.matmul(x, w)平方tf.square(error)均值tf.reduce_mean(error)Tensorflow--梯度下降训练api使用方法train_op = tf.train.GradientDescentOptimizer(learning_rate).method(minloss) 创建训练Oplearning_rate: 学习率minloss: minisize 学习方式, minisize为最小损失sess.run(train_op) 调用训练OP]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本分析-文本相似度和分类]]></title>
    <url>%2F2018%2F05%2F25%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90-%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%92%8C%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[文本相似度（词频统计）的流程实现流程的简介度量文本间的相似性使用词频表示文本特征文本中单词出现的频率或次数NLTK实现词频统计文本相似度分析的流程导入模块import nltkfrom nltk import FreqDist # 导入词频统计模块FreqDist先定义进行分析的几个文本text1 = 'I like the movie so much 'text2 = 'That is a good movie 'text3 = 'This is a great one 'text4 = 'That is a really bad movie 'text5 = 'This is a terrible movie'定义取出词频最好的n个单词的方法（词频统计方法）def get_most_common_words(n, *args):&quot;&quot;&quot;将文本内容汇总，并获取统计每个单词的词频的列表&quot;&quot;&quot;text = [text for text in args]text = ''.join(text)words = nltk.word_tokenize(text) # 对文本进行分词处理freq_dist = FreqDist(words) # 1.生成每个单词的词频统计结果,生成词频统计对象most_common_words = freq_dist.most_common(n) # 2.取出最高频率的n个单词，返回列表return most_common_words # 输出类似：[('a', 4), ('movie', 4), ('is', 4), ('This', 2), ('That', 2)]定义查找单词所在位置的方法def lookup_pos(most_common_words):result = {} # 事先设置好存储常用单词的字典pos = 0 # 表示常用单词的位置for word in get_most_common_words:result[word[0]] = pospos += 1return result # 返回数据类似：{'movie': 0, 'is': 1, 'a': 2, 'That': 3, 'This': 4}获取词频统计的结果列表most_common_words = get_most_common_words(5, text1, text2, text3, text4, text5)记录常用单词的位置，用来进行相似度的判断std_pos_dict = lookup_pos(most_common_words)计算新文本常用词的词频分别为多少new_text = 'That one is a good movie. This is so good!' # 定义新文本freq_vec = [0] * n # 初始化向量，用来存储常用词的词频new_words = nltk.word_tokenize(new_text) # 分词处理在“常用单词列表”上计算词频，当出现常用词频时，统计一次for new_word in new_words:if new_word in list(std_pos_dict.keys()):freq_vec[std_pos_dict[new_word]] += 1print(freq_vec) ---&gt; [1, 2, 1, 1, 1]文本分类的分析方法TF-IDF （词频-逆文档频率）介绍TF-IDF的作用用来描述一个单词对一篇文章的重要程度TF, Term Frequency（词频）表示某个词在该文件中出现的次数IDF，Inverse Document Frequency（逆文档频率）用于衡量某个词普 遍的重要性。TF-IDF（词频-逆文档频率)TF-IDF = TF * IDF 用来统计个单词对于一篇文章的重要程度代码示例]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[情感分析-文本情感分析]]></title>
    <url>%2F2018%2F05%2F23%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[对文本进行情感分析的流程导入模块(NaiveBayesClassifier)import nltkfrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsfrom nltk.classify import NaiveBayesClassifier 导入nltk 中的朴素贝叶斯定义文本处理的方法def proc_text(text):raw_words = nltk.word_tokenize(text) # 对文本进行分词处理wordnet_lematizer = WordNetLemmatizer() # 对文本进行归一化处理words = [wordnet_lematizer.lemmatize(raw_word) for raw_word in raw_words] # 对文本进行归一化处理filtered_words = [word for word in words if word not in stopwords.words('english')] # 去处停用词# 将用来学习的单词，构建成一个字典，键是每个词，值是True/False，True代表该词在文本中return {word: True for word in filtered_words} 返回训练样本对模型进行学习和测试if __name__ == &quot;__main__&quot;:构造5个用来学习的句子text1 = 'I like the movie so much!'text2 = 'That is a good movie.'text3 = 'This is a great one.'text4 = 'That is a really bad movie.'text5 = 'This is a terrible movie.'构造训练样本：是由每一个句子的词构成的字典和句子所对应的分数（情感值），构建成的列表train_data = [[proc_text(text1), 1],[proc_text(text2), 1],[proc_text(text3), 1],[proc_text(text4), 0],[proc_text(text5), 0]]使用朴素贝叶斯模型训练（默认会将文本数据向量化处理），返回贝叶斯模型nb_model = NaiveBayesClassifier.train(train_data)测试模型text6 = 'That is a bad one.' # 测试文本text7 = 'That is a good one!' # 测试文本nb_model.classify(proc_text(text6)) ---&gt;0 对测试文本进行测试并打分nb_model.classify(proc_text(text7)) ---&gt;1 对测试文本进行测试并打分]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理的流程]]></title>
    <url>%2F2018%2F05%2F22%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[自然语言的文本处理流程备注：汉语的自然语言处理流程和英语的自然语言处理流程差了一步词形归一化英文文本的处理流程0.语料库的使用（没有实际的作用）--查看语料库的信息使用方法导入语料库，以导入布朗大学语料库为例（brown）import nltkfrom nltk.corpus import brown brown为语料库，需要提前使用下载器进行下载查看语料库包含的类别（没有实际作用）print(brown.categories())查看语料库中包含的句子和单词（没有实际作用）brown.sents()brown.words()1.文本分词处理分词的作用将句子拆分成 具有语言语义学上意义的词中、英文分词区别：英文中，单词之间是以空格作为自然分界符的中文中没有一个形式上的分界符，分词比英文复杂的多分词处理的方法导入模块import nltk准备需要分词的句子或者文章text = &quot;Python is a high-level programming language, and i like it!&quot;对文本进行分句处理（只有当文本太大，需要对每一句话进行处理的时候使用）seg_list = nltk.sent(text) 对文本进行分句，返回有多个句子组成的列表对文本进行分词 (需要事先安装 punkt 分词模型)seg_list = nltk.word_tokenize(text) 对文本进行分词，返回分词后单词组成的列表分词后的结果：seg_list['Python', 'is', 'a', 'high-level', 'programming', 'language', '!']1.1.词性标注(Part-Of-Speech)-获取每个单词的词性，可选步骤（根据需要使用）词性标注的方法导入模块import nltk分词后进行词性标注words = nltk.word_tokenize('Python is a widely used programming language.')nltk.pos_tag(words) 需要下载 averaged_perceptron_tagger分词后的结果[('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('widely', 'RB'), ('used', 'VBN'), ('programming', 'NN'), ('language', 'NN'), ('.', '.')]、常见的词性2.词形归一化（只有处理英文问本的时候才需要）词形归一的作用类似 look, looked, looking这些不同时态的单词，会影响语料学习的准确度通过词形归一化，可以将这些单词转化为同一的单词①词干提取(stemming)作用可以将单词的ing, ed去掉，只保留单词主干NLTK中常用的词干提取工具stemmer：PorterStemmer, SnowballStemmer, LancasterStemmer词干提取的方法PorterStemmer（只支持英语） 的使用方法导入模块from nltk.stem.porter import PorterStemmer创建工具对象porter_stemmer = PorterStemmer()对单词进行词干提取porter_stemmer.stem('looked') 返回进行词干提取后的单词porter_stemmer.stem('looking') 返回进行词干提取后的单词提取后的结果look lookSnowballStemmer（支持多种语言）的使用方法导入模块from nltk.stem,snowball import SnowballStemmer创建工具对象，并选择语言SnowballStemmer.languages 查看支持的语言snowball_stemmer = SnowballStemmer('english')对单词进行词干提取snowball_stemmer.stem('looked') 返回进行词干提取后的单词snowball_stemmer.stem('looking')) 返回进行词干提取后的单词提取后的结果look lookLancasterStemmer（在大文本提取的时候速度更快） 的使用方法导入模块from nltk.stem.lancaster import LancasterStemmer创建工具对象lancaster_stemmer = LancasterStemmer()对单词进行词干提取lancaster_stemmer.stem('looked') 返回进行词干提取后的单词lancaster_stemmer.stem('looking') 返回进行词干提取后的单词提取后的结果look look②词形归并(lemmatization)词形归并的作用可以将单词的各种词形归并成一种形式如am, is, are -&gt; be, went-&gt;go，boxes-&gt;box指明词性可以更准确地进行词形归并词形归并的使用方法：WordNetLemmatizer导入模块from nltk.stem import WordNetLemmatizer创建工具对象wordnet_lematizer = WordNetLemmatizer() 需要下载wordnet语料库进单词进行词形归并wordnet_lematizer.lemmatize('cats')) 默认按照名词处理wordnet_lematizer.lemmatize('boxes')) 默认按照名词处理wordnet_lematizer.lemmatize('are')) 默认按照名词处理wordnet_lematizer.lemmatize('went')) 默认按照名词处理print(wordnet_lematizer.lemmatize('are', pos='v')) 指明词性print(wordnet_lematizer.lemmatize('went', pos='v')) 指明词性用pos指明词性可以更准确地进行lemma, lemmatize 默认为名词3.去除停用词去除停用词的作用为节省存储空间和提高搜索效率，NLP中会自动过滤掉某些字或词停用词都是人工输入、非自动化生成的，形成停用词表停用词的种类语言中的功能词，如the, is…词汇词，通常是使用广泛的词，如want停用词表http://www.ranks.nl/stopwords获取指定语言的停用歌词库stopwords.words('language_name')注意：所有停用词都是小写的去除停用词的方法导入模块from nltk.corpus import stopwords 需要下载stopwords获取未进行去除停用词的单词列表(已经进行了分词处理)words = ['Python', 'is', 'a', 'widely', 'used', 'programming', 'language', '.']对所有单词进行去除停用词filtered_words = [word for word in words if word not in stopwords.words('english')]去除停用词之后的结果：filtered_words['Python', 'widely', 'used', 'programming', 'language', '.']英文文本处理流程示例jieba分词器的介绍jieba分词器的介绍jieba分词器的三种模式全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义，适合情感分析；精确模式：试图将句子最精确地切开，适合文本分析，适合词频统计；搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。安装方式pip install jiebapip3 install jieba导入jieba库的方法import jieba 使用jieba分词器进行分词使用jieba分词器对词性词性进行标注导入jieba词性标注工具import jieba.posseg as pseg进行分词，并进行词性标注words = pseg.cut(&quot;我爱北京天安门&quot;) 返回的是分词后，有单词和词性构成的元组，并组成的列表结果打印for word, flag in words:print('%s %s' % (word, flag)) --&gt;我 r爱 v北京 ns天安门 ns中文文本的处理流程1.分词处理的方法导入jieba分词器（需要先pip install jieba）import jieba准备需要分词的句子或者文章text = '欢迎来到英雄联盟'对文本进行分词seg_list = jieba.cut(text, cut_all=True)cut_all: 表示使用全模式来进行分词分词后的结果：seg_list['欢迎', '来到', '英雄', '联盟', '英雄联盟']2.使用jieba分词器分词并标注词性--可选步骤（默认是精确模式）导入jieba词性标注工具import jieba.posseg as pseg准备需要分词的句子或者文章text = '欢迎来到英雄联盟'进行分词，并进行词性标注words_list = pseg.cut(text) 返回的是分词后，的单词和词性的对应结果分词后的结果:words_list3.去除停用词中文停用词表中文停用词库哈工大停用词表四川大学机器智能实验室停用词库百度停用词列表准备分词后的列表seg_list = ['我', '是' , '一个', '好人']读取中文停用表stopword_list = [line.rstrip() for line in open('哈工大停用词表.txt', 'r', encoding='utf-8')]去停用词操作filtered_list = [seg for seg in seg_list if seg not in stopword_list]取出后的结果: filtered_list['好人']]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理工具介绍]]></title>
    <url>%2F2018%2F05%2F21%2F07.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[NLTK 的介绍NLTK(Natural Language Toolkit)简介NTLK是著名的Python自然语言处理工具包，但是主要针对的是英文处理。NLTK配套有文档，有语料库，有书籍。NLP领域中最常用的一个Python库开源项目自带分类、分词等功能强大的社区支持语料库的概念语料库，语言的实际使用中真实出现过的语言材料http://www.nltk.org/py-modindex.htmlNTLK及其相关库的安装方法安装NTLK的方法在官网进行安装在NTLK的主页详细介绍了如何在Mac、Linux和Windows下安装NLTK：http://nltk.org/install.html 使用Anaconda直接进行安装Anaconda直接包含了NTLK相关的包和库使用pip进行安装pip install ntlk安装NTLK的相关包使用命令弹出包安装工具import nltknltk.download() 接下来回弹出栏一个下载工具框弹出下面的框，建议安装所有的包安装的内容Corpors ：语料库，常用语料库(brown)，停用词库(stopwords)，词形归并工具(wordbet)等Model：模型，比如词性标注(Tagger)，分词(Punkt)，词干提取(Porter)模型等常用模型]]></content>
      <categories>
        <category>机器学习</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 部署]]></title>
    <url>%2F2018%2F05%2F06%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2Fnginx%2FNginx%20%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Nginx 命令集nginx 命令集提示 1234567891011nginx [-?hvVtq] [-s signal] [-c filename] [-p prefix] [-g directives]-?,-h : 打开帮助信息-v : 显示版本信息并退出-V : 显示版本和配置选项信息，然后退出-t : 检测配置文件是否有语法错误，然后退出-q : 在检测配置文件期间屏蔽非错误信息-s signal : 给一个 nginx 主进程发送信号：stop（停止）, quit（退出）, reopen（重启）, reload（重新加载配置文件）-p prefix : 设置前缀路径（默认是：/usr/local/Cellar/nginx/1.2.6/）-c filename : 设置配置文件（默认是：/usr/local/etc/nginx/nginx.conf）-g directives : 设置配置文件外的全局指令 nginx 启动 1nginx -c /path/to/nginx.conf 配置文件语法检查 1nginx -t -c /path/to/nginx.conf nginx 进程控制 1nginx -s reload|reopen|stop|quit #重新加载配置|重启|停止|退出 nginx nginx 详细配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697user nobody; # 运行用户worker_processes 1; # 启动进程数量,通常设置成和cpu的数量相等#worker_rlimit_nofile 65535; # nginx 最大打开文件数# 全局错误日志及PID文件error_log /var/log/nginx/error.log; # 可配置错误log的输出等级 notice|info 。。。pid /var/run/nginx.pid;# 工作模式及连接数上限events &#123; accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off use epoll; #事件驱动模型select|poll|kqueue|epoll|resig|/dev/poll|eventport worker_connections 1024; #最大连接数，默认为512&#125;# 设定http服务器，利用它的反向代理功能提供负载均衡支持http &#123; include mime.types; # 设定mime类型,类型由mime.type文件定义 文件扩展名与文件类型映射表 default_type application/octet-stream; #设定日志格式 log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; # 服务日志 sendfile on; #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块 #tcp_nopush on; # 按照大小发送数据包,不是按照请求 keepalive_timeout 65; # 连接超时时间 tcp_nodelay on; # 按照请求发送数据包 gzip on; # 开启gzip压缩 gzip_disable "MSIE [1-6]."; # 不给IE6 Gzip #gzip_min_length 1024; # 启用压缩上限 #gzip_buffers 416k; # 缓存空间大小 #gzip_comp_level 2; # 定义压缩等级，默认为6，压缩比越大，效率越低 #gzip_types text/plain application/x-javascript application/xml text/css; #压缩文件类型 #gzip_vary on; #启用压缩标识 #gzip_static on; # 静态压缩 client_header_buffer_size 128k; # 如果请求头大小大于指定的缓冲区，则使用large_client_header_buffers指令分配更大的缓冲区。 large_client_header_buffers 4 128k; # 规定了用于读取大型客户端请求头的缓冲区的最大数量和大小。 这些缓冲区仅在缺省缓冲区不足时按需分配。 sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。 upstream mysvr &#123; # 服务集群 server xxx:xxx:xx:xxx:xxxx ; # 可以给单个server配置weight权重 server xxx.xxx.xx.xxx:xxxx backup; #热备 &#125; #设定虚拟主机配置 server &#123; listen 80; # 侦听80端口 server_name mysvr; # 定义使用 mysvr(HTTP请求的Host字段) 访问 root html; # 定义服务器的默认网站根目录位置 access_log logs/nginx.access.log main; # 设定本虚拟主机的访问日志 # 配置路径转发 location / &#123; # 默认请求 proxy_pass mysvr # 配置反向代理 index index.php index.html index.htm; # 定义首页索引文件的名称 client_max_body_size 100m; # 配置请求体最大容量 proxy_set_header Host $http_host; # Host表示客户端请求的HOST主机名称； proxy_set_header X-Real-IP $remote_addr; # X-Real-IP表示客户端真实的IP； proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # X-Forwarded-For这个Header和X-Real-IP类似，但它在多层代理时会包含真实客户端及中间每个代理服务器的IP。 proxy_set_header X-Forwarded-Proto $scheme; # X-Forwarded-Proto表示客户端真实的协议（http还是https) deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip autoindex off; #激活/关闭自动索引 autoindex_exact_size off; #设定索引时文件大小的单位(B,KB, MB 或 GB) autoindex_localtime off; #开启以本地时间来显示文件时间的功能。默认为关（GMT时间） &#125; # 定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /root; &#125; #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123; expires 30d; # 静态文件有效时间 &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic "NginxStatus"; auth_basic_user_file conf/htpasswd; &#125; #禁止访问 .htxxx 文件 location ~ /.ht &#123; deny all; &#125; &#125;&#125; Nginx 优化配置项进程数优化nginx运行工作进程个数，一般设置cpu的核心或者核心数x2 查看cpu核数可以使用TOP命令后按1 优化步骤: 123vim /usr/local/ninx/conf/nginx.conf# 修改 worker_provesses 4/usr/local/nginx/sbin/nginx -s reload worker_processes最多开启8个，8个以上性能提升不会再提升了，而且稳定性变得更低，所以8个进程够用了。 连接数量优化文件符限制配置 Nginx最多可以打开文件句柄数，通过配置变量 worker_rlimit_nofile 65535; 来控制 这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n的值保持一致。 文件资源限制的配置可以在/etc/security/limits.conf设置，针对root/user等各个用户或者*代表所有用户来设置。 nginx默认采用epoll事件模型，处理效率高 work_connections是单个worker进程允许客户端最大连接数，这个数值一般根据服务器性能和内存来制定，实际最大值就是worker进程数乘以work_connections 实际我们填入一个65535，足够了，这些都算并发值，一个网站的并发达到这么大的数量，也算一个大站了！ 连接接收策略配置 multi_accept 告诉nginx收到一个新连接通知后接受尽可能多的连接，默认是on，设置为on后，多个worker按串行方式来处理连接，也就是一个连接只有一个worker被唤醒，其他的处于休眠状态，设置为off后，多个worker按并行方式来处理连接，也就是一个连接会唤醒所有的worker，直到连接分配完毕，没有取得连接的继续休眠。当你的服务器连接数不多时，开启这个参数会让负载有一定的降低，但是当服务器的吞吐量很大时，为了效率，可以关闭这个参数。 优化步骤: 1234567891011vim /etc/security/limits.conf* soft nofile 65535* hard nofile 65535* soft noproc 65535* hard noproc 65535# 重新登录用户vim /usr/local/ninx/conf/nginx.conf# 修改 user epoll# worker_connections 65535;# multi_accept off;/usr/local/nginx/sbin/nginx -s reload 配置高效传输模式Include mime.types; //媒体类型,include 只是一个在当前文件中包含另一个文件内容的指令 default_type application/octet-stream; //默认媒体类型足够 sendfile on；//开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。 注意：如果图片和文件显示不正常把这个改成off。 tcp_nopush on；必须在sendfile开启模式才有效，防止网路阻塞，积极的减少网络报文段的数量（将响应头和正文的开始部分一起发送，而不一个接一个的发送。） 优化步骤: 123456vim /usr/local/ninx/conf/nginx.conf#include mime.types;#default_type application/octet-stream;#sendfile on;#tcp_nopush on;/usr/local/nginx/sbin/nginx -s reload 服务器资源占用优化主要目的是保护服务器资源，CPU，内存，控制连接数，因为建立连接也是需要消耗资源的 keepalived_timeout客户端连接保持会话超时时间，超过这个时间，服务器断开这个链接 tcp_nodelay；也是防止网络阻塞，不过要包涵在keepalived参数才有效 client_header_buffer_size 4k; 客户端请求头部的缓冲区大小，这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过 1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 open_file_cache max=102400 inactive=20s; 这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件 数一致，inactive 是指经过多长时间文件没被请求后删除缓存。 open_file_cache_valid 30s; 这个是指多长时间检查一次缓存的有效信息。 open_file_cache_min_uses 1; open_file_cache指令中的inactive 参数时间内文件的最少使用次数，如果超过这个数字，文 件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive 时间内一次没被使用，它将被移除。 client_header_timeout设置请求头的超时时间。我们也可以把这个设置低些，如果超过这个时间没有发送任何数据，nginx将返回request time out的错误 client_body_timeout设置请求体的超时时间。我们也可以把这个设置低些，超过这个时间没有发送任何数据，和上面一样的错误提示 reset_timeout_connection 告诉nginx关闭不响应的客户端连接。这将会释放那个客户端所占有的内存空间。 send_timeout响应客户端超时时间，这个超时时间仅限于两个活动之间的时间，如果超过这个时间，客户端没有任何活动，nginx关闭连接 server_tokens 并不会让nginx执行的速度更快，但它可以关闭在错误页面中的nginx版本数字，这样对于安全性是有好处的。 client_max_body_size上传文件大小限制 1234567891011121314vim /usr/local/ninx/conf/nginx.conf#keepalive_timeout 60;#tcp_nodelay on;#client_header_buffer_size 4k;#open_file_cache max=102400 inactive=20s;#open_file_cache_valid 30s;#open_file_cache_min_uses 1;#client_header_timeout 15;#client_body_timeout 15;#reset_timedout_connection on;#send_timeout 15;#server_tokens off;#client_max_body_size 10m/usr/local/nginx/sbin/nginx -s reload PHP请求调优fastcgi_connect_timeout 600; #指定连接到后端FastCGI的超时时间。 fastcgi_send_timeout 600; #向FastCGI传送请求的超时时间。 fastcgi_read_timeout 600; #指定接收FastCGI应答的超时时间。 fastcgi_buffer_size 64k; #指定读取FastCGI应答第一部分需要用多大的缓冲区，默认的缓冲区大小为fastcgi_buffers指令中的每块大小，可以将这个值设置更小。 fastcgi_buffers 4 64k; #指定本地需要用多少和多大的缓冲区来缓冲FastCGI的应答请求，如果一个php脚本所产生的页面大小为256KB，那么会分配4个64KB的缓冲区来缓存，如果页面大小大于256KB，那么大于256KB的部分会缓存到fastcgi_temp_path指定的路径中，但是这并不是好方法，因为内存中的数据处理速度要快于磁盘。一般这个值应该为站点中php脚本所产生的页面大小的中间值，如果站点大部分脚本所产生的页面大小为256KB，那么可以把这个值设置为“8 32K”、“4 64k”等。 fastcgi_busy_buffers_size 128k; #建议设置为fastcgi_buffers的两倍，繁忙时候的buffer fastcgi_temp_file_write_size 128k; #在写入fastcgi_temp_path时将用多大的数据块，默认值是fastcgi_buffers的两倍，该数值设置小时若负载上来时可能报502BadGateway fastcgi_temp_path #缓存临时目录 fastcgi_intercept_errors on;#这个指令指定是否传递4xx和5xx错误信息到客户端，或者允许nginx使用error_page处理错误信息。 注：静态文件不存在会返回404页面，但是php页面则返回空白页！！ fastcgi_cache_path /usr/local/nginx1.10/fastcgi_cachelevels=1:2 keys_zone=cache_fastcgi:128minactive=1d max_size=10g;# fastcgi_cache缓存目录，可以设置目录层级，比如1:2会生成16*256个子目录，cache_fastcgi是这个缓存空间的名字，cache是用多少内存（这样热门的内容nginx直接放内存，提高访问速度），inactive表示默认失效时间，如果缓存数据在失效时间内没有被访问,将被删除，max_size表示最多用多少硬盘空间。 fastcgi_cache cache_fastcgi; #表示开启FastCGI缓存并为其指定一个名称。开启缓存非常有用，可以有效降低CPU的负载，并且防止502的错误放生。cache_fastcgi为proxy_cache_path指令创建的缓存区名称 fastcgi_cache_valid 200 302 1h; #用来指定应答代码的缓存时间，实例中的值表示将200和302应答缓存一小时，要和fastcgi_cache配合使用 fastcgi_cache_valid 301 1d; #将301应答缓存一天 fastcgi_cache_valid any 1m; #将其他应答缓存为1分钟 fastcgi_cache_min_uses 1; #该指令用于设置经过多少次请求的相同URL将被缓存。 fastcgi_cache_key http://$host$request_uri; #该指令用来设置web缓存的Key值,nginx根据Key值md5哈希存储.一般根据$host(域名)、$request_uri(请求的路径)等变量组合成proxy_cache_key 。 fastcgi_pass #指定FastCGI服务器监听端口与地址，可以是本机或者其它 总结： nginx的缓存功能有：proxy_cache / fastcgi_cache proxy_cache的作用是缓存后端服务器的内容，可能是任何内容，包括静态的和动态。 fastcgi_cache的作用是缓存fastcgi生成的内容，很多情况是php生成的动态的内容。 proxy_cache缓存减少了nginx与后端通信的次数，节省了传输时间和后端宽带。 fastcgi_cache缓存减少了nginx与php的通信的次数，更减轻了php和数据库(mysql)的压力。 优化方式 12345678910fastcgi_connect_timeout 600;fastcgi_send_timeout 600;fastcgi_read_timeout 600;fastcgi_buffer_size 64k;fastcgi_buffers 4 64k;fastcgi_busy_buffers_size 128k;fastcgi_temp_file_write_size 128k;fastcgi_temp_path/usr/local/nginx1.10/nginx_tmp;fastcgi_intercept_errors on;fastcgi_cache_path/usr/local/nginx1.10/fastcgi_cache levels=1:2 keys_zone=cache_fastcgi:128minactive=1d max_size=10g; gzip 数据压缩调优使用gzip压缩功能，可能为我们节约带宽，加快传输速度，有更好的体验，也为我们节约成本，所以说这是一个重点。 Nginx启用压缩功能需要你来ngx_http_gzip_module模块，apache使用的是mod_deflate 一般我们需要压缩的内容有：文本，js，html，css，对于图片，视频，flash什么的不压缩，同时也要注意，我们使用gzip的功能是需要消耗CPU的！ gzip on; #开启压缩功能 gzip_min_length 1k; #设置允许压缩的页面最小字节数，页面字节数从header头的Content-Length中获取，默认值是0，不管页面多大都进行压缩，建议设置成大于1K，如果小与1K可能会越压越大。 gzip_buffers 4 32k; #压缩缓冲区大小，表示申请4个单位为32K的内存作为压缩结果流缓存，默认值是申请与原始数据大小相同的内存空间来存储gzip压缩结果。 gzip_http_version 1.1; #压缩版本，用于设置识别HTTP协议版本，默认是1.1，目前大部分浏览器已经支持GZIP解压，使用默认即可 gzip_comp_level 6; #压缩比例，用来指定GZIP压缩比，1压缩比最小，处理速度最快，9压缩比最大，传输速度快，但是处理慢，也比较消耗CPU资源。 gzip_types text/css text/xml application/javascript; #用来指定压缩的类型，‘text/html’类型总是会被压缩。 默认值: gzip_types text/html (默认不对js/css文件进行压缩) # 压缩类型，匹配MIME型进行压缩 # 不能用通配符 text/* # (无论是否指定)text/html默认已经压缩 # 设置哪压缩种文本文件可参考 conf/mime.types gzip_vary on; #varyheader支持，改选项可以让前端的缓存服务器缓存经过GZIP压缩的页面，例如用Squid缓存经过nginx压缩的数据 优化方式 123456789101112131415gzip on;gzip_min_length 2k;gzip_buffers 4 32k;gzip_http_version 1.1;gzip_comp_level 6;gzip_typestext/plain text/css text/javascriptapplication/json application/javascript application/x-javascriptapplication/xml;gzip_vary on;gzip_proxied any; 使用缓存来优化效率缓存，主要针对于图片，css，js等元素更改机会比较少的情况下使用，特别是图片，占用带宽大，我们完全可以设置图片在浏览器本地缓存365d，css，js，html可以缓存个10来天，这样用户第一次打开加载慢一点，第二次，就非常快了！缓存的时候，我们需要将需要缓存的拓展名列出来， Expires缓存配置在server字段里面 配置方式 12345678910111213141516171819location ~ * \.(ico|jpe?g|gif|png|bmp|swf|flv)$ &#123;expires 30d;\#log_not_found off;access_log off;&#125;location ~ * \.(js|css)$ &#123;expires 7d;log_not_found off;access_log off;&#125; 注：log_not_found off;是否在error_log中记录不存在的错误。默认是。 expire功能优点 （1）expires可以降低网站购买的带宽，节约成本 （2）同时提升用户访问体验 （3）减轻服务的压力，节约服务器成本，是web服务非常重要的功能。 expire功能缺点 被缓存的页面或数据更新了，用户看到的可能还是旧的内容，反而影响用户体验。解决办法：第一个缩短缓存时间，例如：1天，但不彻底，除非更新频率大于1天；第二个对缓存的对象改名。 网站不希望被缓存的内容 1）网站流量统计工具 2）更新频繁的文件（google的logo） 使用防盗链防止别人直接从你网站引用图片等链接，消耗了你的资源和网络流量，那么我们的解决办法由几种： 1：水印，品牌宣传，你的带宽，服务器足够 2：防火墙，直接控制，前提是你知道IP来源 3：防盗链策略下面的方法是直接给予404的错误提示 优化方法 123456789location ~ *^.+\.(jpg|gif|png|swf|flv|wma|wmv|asf|mp3|mmf|zip|rar)$ &#123;valid_referers noneblocked www.benet.com benet.com;if($invalid_referer) &#123; #return 302 http://www.benet.com/img/nolink.jpg; return 404; break;&#125; access_log off;&#125; 参数可以使如下形式： none 意思是不存在的Referer头(表示空的，也就是直接访问，比如直接在浏览器打开一个图片) blocked 意为根据防火墙伪装Referer头，如：“Referer:XXXXXXX”。 server_names 为一个或多个服务器的列表，0.5.33版本以后可以在名称中使用“*”通配符。 内核参数优化fs.file-max = 999999： 这个参数表示进程（比如一个worker进程）可以同时打开的最大句柄数，这个参数直线限制最大并发连接数，需根据实际情况配置。 net.ipv4.tcp_max_tw_buckets = 6000 #这个参数表示操作系统允许TIME_WAIT套接字数量的最大值，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。该参数默认为180000，过多的TIME_WAIT套接字会使Web服务器变慢。 注：主动关闭连接的服务端会产生TIME_WAIT状态的连接 net.ipv4.ip_local_port_range = 1024 65000 #允许系统打开的端口范围。 net.ipv4.tcp_tw_recycle = 1 #启用timewait快速回收。 net.ipv4.tcp_tw_reuse = 1 #开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接。这对于服务器来说很有意义，因为服务器上总会有大量TIME-WAIT状态的连接。 net.ipv4.tcp_keepalive_time = 3 # 这个参数表示当keepalive启用时，TCP发送keepalive消息的频度。默认是2小时，若将其设置的小一些，可以更快地清理无效的连接。 net.ipv4.tcp_syncookies = 1 #开启SYN Cookies，当出现SYN等待队列溢出时，启用cookies来处理。 net.core.somaxconn = 40960 #web 应用中 listen 函数的 backlog 默认会给我们内核参数的 net.core.somaxconn 限制到128，而nginx定义的NGX_LISTEN_BACKLOG 默认为511，所以有必要调整这个值。 注：对于一个TCP连接，Server与Client需要通过三次握手来建立网络连接.当三次握手成功后,我们可以看到端口的状态由LISTEN转变为ESTABLISHED,接着这条链路上就可以开始传送数据了.每一个处于监听(Listen)状态的端口,都有自己的监听队列.监听队列的长度与如somaxconn参数和使用该端口的程序中listen()函数有关 somaxconn定义 # 了系统中每一个端口最大的监听队列的长度,这是个全局的参数,默认值为128，对于一个经常处理新连接的高负载 web服务环境来说，默认的 128 太小了。大多数环境这个值建议增加到 1024 或者更多。大的侦听队列对防止拒绝服务 DoS 攻击也会有所帮助。 net.core.netdev_max_backlog = 262144 #每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。 net.ipv4.tcp_max_syn_backlog = 262144 #这个参数标示TCP三次握手建立阶段接受SYN请求队列的最大长度，默认为1024，将其设置得大一些可以使出现Nginx繁忙来不及accept新连接的情况时，Linux不至于丢失客户端发起的连接请求。 net.ipv4.tcp_rmem = 10240 87380 12582912 #这个参数定义了TCP接受缓存（用于TCP接受滑动窗口）的最小值、默认值、最大值。 net.ipv4.tcp_wmem = 10240 87380 1258291： # 这个参数定义了TCP发送缓存（用于TCP发送滑动窗口）的最小值、默认值、最大值。 net.core.rmem_default = 6291456 #这个参数表示内核套接字接受缓存区默认的大小。 net.core.wmem_default = 6291456 #这个参数表示内核套接字发送缓存区默认的大小。 net.core.rmem_max = 12582912 # 这个参数表示内核套接字接受缓存区的最大大小。 net.core.wmem_max = 12582912 # 这个参数表示内核套接字发送缓存区的最大大小。 net.ipv4.tcp_syncookies = 1 # 该参数与性能无关，用于解决TCP的SYN攻击。 贴一个完整的内核优化设置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667fs.file-max = 999999net.ipv4.ip_forward = 0net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296net.ipv4.tcp_max_tw_buckets = 6000net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_rmem = 10240 87380 12582912net.ipv4.tcp_wmem = 10240 87380 12582912net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.core.netdev_max_backlog = 262144net.core.somaxconn = 40960net.ipv4.tcp_max_orphans = 3276800net.ipv4.tcp_max_syn_backlog = 262144net.ipv4.tcp_timestamps = 0net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_fin_timeout = 1net.ipv4.tcp_keepalive_time = 30net.ipv4.ip_local_port_range = 1024 65000执行sysctl -p 使内核修改生效果]]></content>
      <categories>
        <category>开发工具</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>true</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell基础-表达式流程控制]]></title>
    <url>%2F2017%2F08%2F17%2F09.shell%2Fshell%E8%AF%AD%E6%B3%95%2Fshell%E5%9F%BA%E7%A1%80-%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[shell表达式shell的验证表达式表达式的表达方式方法1[ 表达式 ]方法2test 表达式备注表达式两边必须要有空格，表达式之间需要有空格如果结果为真返回0，结果不为真则返回其他表达式由变量和运算符构成表达式的运算符shell的数字运算符表达式格式[ n1 –gt n2 ]表达式参数-gt 大于 -lt 小于 -eq 等于 -ne 不等于shell的字符串运算符表达式格式[ n1 == n2 ][ -z n1]表达式参数== 判断两个字符串是否相等 != 判断连个字符串是否不一致 -z 判断字符串长度是否为0 -n 判断字符串长度是否不为0shell的文件运算符表达式格式[ 文件运算符 文件名 ] 文件运算符-f 判断输入的内容是否是一个文件 -d 判断输入的内容是否是一个目录 -x 判断输入的文件是否有执行权限 -e 判断输入的内容表示的文件是否存在 -r 判断文件是否可读 -w 判断文件是否可写shell逻辑表达式&amp;&amp; 并表达式格式 命令白表达式1 &amp;&amp; 命令表达式2 效果如果命令1执行成功，则执行命令2 如果命令1执行失败，则不执行命令2 || 或 表达式格式命令1 || 命令2 效果如果命令1执行成功，则不执行命令2 如果命令1执行不成功，则执行命令2 python的流程控制shell流程控制的分类简单的流程控制选择流程if语句单if语句 双if语句 多if语句case语句循环流程for循环while循环until循环复杂的流程控制函数shell中流程控制规则缩进shell中的缩进没有作用，只是为了好看符号成对写 [ ] ( ) { }流程控制语句一定要先写完选择流程控制语句if语句单if语句写法if 条件语句；then执行语句fi双if语句if 条件语句1 ；then执行语句1else执行语句2fi多if语句if 条件语句1；then执行语句1elif 条件语句2；then执行语句2else执行语句3ficase语句case语句写法case 值 in 值1) 执行语句1 ;; 值2) 执行语句2 ;; 值3) 执行语句3 ;; esac循环控制语句for循环语句for 条件表达式；do执行语句donewhile循环while 条件表达式；do执行语句doneuntil循环语句使用方法until 条件表达式；do执行语句doneuntil语句的特点当条件不成立时，执行循环，一旦条件成立则结束循环函数函数的使用场景不带参数的函数，应用在重复功能封装的情况下带参数的函数，应用在条件判断执行的情况下定义函数func_name(){args=$n (不写此行代表函数不需要接受参数)函数体}调用函数func_name args1 args2将shell脚本设置为系统环境变量的方法1、直接在命令行中设置PATH# PATH=$PATH:/usr/local/apache/bin使用这种方法,只对当前会话有效，也就是说每当登出或注销系统以后，PATH设置就会失效。2、在profile中设置PATH# vi /etc/profile找到export行，在下面新增加一行，内容为：export PATH=$PATH:/usr/local/apache/bin。注：＝ 等号两边不能有任何空格。这种方法最好,除非手动强制修改PATH的值,否则将不会被改变。编辑/etc/profile后PATH的修改不会立马生效，如果需要立即生效的话，可以执行# source profile命令。3、在当前用户的profile中设置PATH# vi ~/.bash_profile修改PATH行,把/usr/local/apache/bin添加进去,如：PATH=\$PATH:$HOME/bin:/usr/local/apache/bin。# source ~/.bash_profile让这次的修改生效。注：这种方法只对当前用户起作用的,其他用户该修改无效。]]></content>
      <categories>
        <category>shell</category>
        <category>shell语法</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell基础-变量]]></title>
    <url>%2F2017%2F08%2F16%2F09.shell%2Fshell%E8%AF%AD%E6%B3%95%2Fshell%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[shell的介绍什么是shell shell是一种命令解释器，而linux的shell则是用来保护内核的，或者和内核进行交互的 Python脚本首行的#！/bin/python其实就是指定了Python脚本运行的时候使用的解释器shell的分类窗口式的shell Windows的桌面 各个发行版的linux的桌面 命令行shell windows cmd(命令行提示符) linux sh … … bash linux查看默认shell解释器的方法 echo $SHELL 常见的linux发行版本ubuntu 界面化操作比较方便 centos 6.5-6.7 redhat 收费 debain kali 渗透测试shell命令与shell脚本的介绍shell命令什么是shell命令一条命令,包括shell自身的命令以及linux系统命令 特点: 逐行输入，逐行输出 无法重复使用shell脚本什么是shell命令脚本一堆shell命令的组合特点: 一次输入，逐行执行 脚本可以重复使用，提高工作效率shell脚本的规范命名 命名要有意义，最好能够表明脚本功能 后缀一般以.sh结尾 首行是并且必须是命令解释器 默认首行使用 #!/bin/bash 特殊情况可以根据环境修改 版权信息 首行之下必须使用注释写明脚本的基本信息 脚本名，功能，编写时间，编写人，联系方式等 脚本的执行顺序 根据脚本编写顺序，从上到下依次执行 成对编写 () [] {} “” ’’shell执行方式使用命令解释器加载脚本文件 执行方式 bash 脚本文件 使用场景: 生产常用 使用脚本的绝对路径或相对路径执行 执行方式直接用./文件路径执行脚本文件 当在脚本所在目录的时候，使用相对路径要加 ./脚本文件名使用条件条件: 脚本有用可执行权限 脚本首行选定了解释器 使用场景: 私下 source或.的形式 执行方式 source 或 . 脚本文件 将脚本中的变量都导入到当前的环境作用加载文件，将写在文件中的配置信息马上生效 使用场景: 保证环境变量的一致性运行方式让shell脚本在后台运行&amp; 运行shell脚本的时候在后面加上&amp;，代表以后台运行的方式运行的输出1&gt; 指标准信息输出路径（也就是默认的输出方式）2&gt; 指错误信息输出路径2&gt;&amp;1 指将标准信息输出路径指定为错误信息输出路径（也就是都输出在一起）习惯上标准输入（standard input）的文件描述符是 0标准输出（standard output）是 1标准错误（standard error）是 2shell变量shell的变量介绍变量包括两部分: （变量名就是存放变量值的容器，容器不变，但是存放的内容可变）变量名 不变 变量值 变化 shell中变量的统一格式基本格式：变量名=变量值注意：等号左右两边没有空格shell中的变量的分类本地变量全局变量内置变量本地变量本地变量定义在当前的shell或者当前脚本存续期间可用的变量数据变量的定义方式（三种）变量名=变量值 变量值必须是连续的，不能有空格，不能有特殊字符 变量名=‘变量值’ 变量值可以是不连续的，能有空格，特殊字符原样，引号中的是什么就是什么 变量名=&quot;变量值&quot; 值可以不连续，如果值中使用已定义变量，则会先调用，再赋值命令变量的定义定义方式变量名=`命令` 注意命令两边的符号变量名=$(命令) 执行顺序: 先运行命令，将命令执行的结果赋值给变量名全局变量什么是全局变量系统所有环境都可以使用的变量 查看全局变量方法env命令 定义方法分步的方式变量名=变量值 （定义一个本地变量，可以是数据或者是命令） export 变量名 （声明为全局变量） 同时定义 export 变量名=变量值 （变量可以是数据或者是命令）内置变量什么是内置变量ash命令内部已经定义好的变量，可以直接使用，不需要定义使用方法 和shell脚本有关的内置变量$0 获取当前脚本文件名称 $# 获取当前脚本获取的参数的个数 $n 获取当前脚本获取到的第n个参数 $? 获取上一条命令的执行情况，0表示成功，1表示失败，127表示找不到命令 $$ 脚本运行时使用的进程号 $@ 获取所有的参数获取hash序列$RANDOM 获取随机的5为随机数$RANDOM | md5sum(有的bash是md5) 将变量进行md5序列化返回的是一个序列化的字符串和一个 - 符号\$RANDOM | md5sum | awk '{printf $1}' 将awk接收到的第一个参数打印出来（即序列化结果）和字符串相关的内置变量截取使用方法 ${var_name:start:n} 备注var_name是字符串变量名 start表示的截取的起始位置 start整数 从开头开始 start:n 为 0-n 从结尾往前推n个位开始 n 表示截取字符的个数和默认值相关的内置变量第一种 var_name=$1 ${var_name:-default} default表示默认值,如果没有输入参数，default将会被使用 第二种 var_name=$1 ${var_name:+default} 将会无视输入参数，直接输出设定好的默认值查看变量的方式$变量名 使用场景: 私下使用时 &quot;$变量名&quot; 使用场景: 调用变量时 &quot;${变量名}&quot; 标准的使用方法用场景: 脚本中，规范化作业时变量的运算shell中变量运算方法 1.使用let进行运算 let n=n+1 let n+=1 实现自加 2.使用$(())实现运算 n=\$(($n+1))变量的其他操作将一个本地变量设置为只读，不允许删除变量，关闭终端才能删除 readonly 变量名 删除变量 unset 变量名]]></content>
      <categories>
        <category>shell</category>
        <category>shell语法</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单元测试]]></title>
    <url>%2F2017%2F08%2F10%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2F%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[单元测试概念为什么需要单元测试Web程序开发过程一般包括以下几个阶段：[需求分析，设计阶段，实现阶段，测试阶段]。其中测试阶段通过人工或自动来运行测试某个系统的功能。目的是检验其是否满足需求，并得出特定的结果，以达到弄清楚预期结果和实际结果之间的差别的最终目的测试的分类单元测试（与开发人员相关）对单独的代码块(例如函数)分别进行测试,以保证它们的正确性集成测试对大量的程序单元的协同工作情况做测试系统测试同时对整个系统的正确性进行检查,而不是针对独立的片段什么是单元测试当我们的某些功能代码完成后，为了检验其是否满足程序的需求。可以通过编写测试代码，模拟程序运行的过程，检验功能代码是否符合预期。单元测试就是开发者编写一小段代码，检验目标代码的功能是否符合预期。通常情况下，单元测试主要面向一些功能单一的模块进行。在Web开发过程中，单元测试实际上就是一些“断言”（assert）代码，判断执行结果是否符合预期单元测试步骤先定义一个类，继承自unittest.TestCaseimport unittestclass TestClass(unitest.TestCase): ...在测试类中定义两个方法setUp测试案例启动前调用，可以设置初始化代码def setUp(self):client = app.test_client() 获取flask中对应的测试客户端对象，通过它模拟发送请求app.testing = True 设置app为测试模式，异常会定位到内测试代码导致异常的代码passtearDown所有测试案列完成之后调用，可以用来关闭上下文管理器等def tearDown(self):...在测试类中，编写测试代码from main import app import jsondef test_function(self): 注意测试的方法一定要以test开头，才能被识别self.client.post('/login', data={ }) 通过测试客户端发起post请求，传入数据，获取响应response_data = response.data 获取响应中响应体的数据，字符串类型数据resp_dict = json.loads(resp_data) 将字符串转换成字典self.assert 。。。。。。 再通过获取的值进行assert断言运行测试代码第一种方法直接右键运行第二种方法unittest.main() 通过调用main方法来启动测试代码登录测试数据库测试程序断言的使用（assert）程序断言的作用断言就是判断一个函数或对象的一个方法所产生的结果是否符合你期望的那个结果。python中assert断言是声明布尔值为真的判定，如果表达式为假会抛出异常。单元测试中，一般使用assert来断言结果。程序断言的使用示例assert isinstance(num1, int) 断言num1是一个int类型，不是的话抛出异常常用的断言方法系统自带断言assert 后面的布尔表达式为True，则passunitest.TestCase封装的断言，通过self.assert来调用assertEqual 如果两个值相等，则passassertNotEqual 如果两个值不相等，则passassertTrue 判断bool值为True，则passassertFalse 判断bool值为False，则passassertIsNone 不存在，则passassertIsNotNone 存在，则pass]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-WTF表单]]></title>
    <url>%2F2017%2F08%2F08%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-WTF%E8%A1%A8%E5%8D%95%2F</url>
    <content type="text"><![CDATA[Web表单与Flask-WTF拓展web表单它是HTML页面中负责数据采集的部件。表单有三个部分组成：表单标签、表单域、表单按钮。表单允许用户输入数据，负责HTML页面数据采集，通过表单将用户输入的数据提交给服务器。Flask-WTF拓展Flask-WTF是一个Html表单的拓展，可以使用python代码写表单，然后传递到模板中渲染Flask-WTF扩展封装了WTForms，有验证表单数据的功能；FORM表单提交的实现通过HTML页面实现表单提交和验证在HTML中实现表单页面定义路由和视图函数，接收表单提交的数据进行验证GET请求获取表单，POST请求提交表单通过Flask-WTF实现表单的提交和验证导入FlaskForm表单、标准字段、calidators验证器from flask-wtf import FlaskFormfrom wtforms import 标准字段from wtforms.validators import 验证器所有的标准字段和验证器示例：标准字段WTForms常用验证器设置SECRET_KEY因为FlaskForm会自动的进行csrf的验证，生成csrf密文会使用到SECRET_KEY过程中会使用的用flash() 记录表单验证错误信息，flash会用到session，所以会用到SECRET_KEYapp.secret_key = &quot;liukaijian&quot;自定义表单类定义路由和视图函数GET请求：创建表单实例，将其传递给HTMl模板进行渲染POST请求：创建表单实例，FlaskForm实例会对表单的提交结果进行验证，如果验证通过，返回成功内容定义HTMl页面接收视图函数传递过来的表单对象，并获取其属性将其渲染到标签中因为FlaskForm会自动的进行csrf的验证，所以我们要设置一个csrf_token标签csrf_token 验证的设置csrf_token设置1.2.要首先配置SECRET_KEY3.在创建了表单类的时候，是默认配置了csrf_token的，但是在未使用表单类的时候，需要手动开启csrf验证csrf_token验证在响应在中给cookie添加csrf_token (在使用表单的时候默认添加，不使用表单手动添加)生成csrf_tokenfrom flask_wtf.csrf import generate_csrftoken = generate_csrf()response.set_cookie('csrf_token', token)在前端的请求给headers添加csrf_token参数]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Script命令行、migrate数据库迁移]]></title>
    <url>%2F2017%2F08%2F06%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-Script%E5%91%BD%E4%BB%A4%E8%A1%8C%E3%80%81Flask-migrate%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[Flask-Script扩展命令行Flask-Script的作用可以实现在终端中使用命令行的方式对项目进行操作Flask-Script的配置在程序中导入拓展from flask_script import Manager创建manager实例manager = Manager(app) app为已经创建好的flask应用实例运行manager实例manager.run( )查看所有Flask-Script扩展命令python 文件名 runserver --help代码示例 --启动服务器通过在终端中使用命令启动python hello.py runserver (-host ip地址) 设置服务器在哪个机器的端口上监听客户端请求--数据库的迁移迁移初始化python 文件 db init 生成迁移文件python 文件 db migrate -m&quot;版本名(注释)&quot;数据库更新python 文件 db upgrade查看历史迁移记录python 文件 db history进行数据库迁移版本回退python 文件 db downgrade(upgrade) 版本号]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-SQLAlchemy数据库交互]]></title>
    <url>%2F2017%2F08%2F06%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-SQLAlchemy%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%A4%E4%BA%92%2F</url>
    <content type="text"><![CDATA[Flaks-SQLALchemy扩展在Flask中使用数据库要用到Flask-SQLALchemy扩展，Flask-SQLALchemy应用了SQLALchemy框架SQLAlchemy是一个关系型数据库框架，它提供了高层的ORM和底层的原生数据库的操作。SQLALchemy 实际上是对数据库的抽象，让开发者不用直接和 SQL 语句打交道，而是通过 Python 对象来操作数据库，在舍弃一些性能开销的同时，换来的是开发效率的较大提升Django与Flask操作数据库的对比 使用SQLAlchemy操作数据库安装flask-sqlalchemy扩展pip install flask-sqlalchemy 安装sqlalchemy数据库拓展配置SQLALchemy导入SQLAlchemyfrom flask import flaskfrom flask_sqlalchemy import SQLAlchemy配置数据库连接的URI并初始化SQLAlchemy配置数据库的连接方式app = Flask(__name__)app.config[&quot;SQLALCHEMY_DATABASE_URI&quot;] =&quot;mysql://root:mysql@127.0.0.1:3306/py3&quot;配置是否追踪数据库的修改操作，即数据库有修改就会提示app.config[&quot;SQLALCHEMY_TRACK_MODIFICATIONS&quot;] = False初始化SQLAlchemydb = SQLAlchemy(app)SQLAlchemy其他的配置创建模型类（对应数据库的表）创建模型类应用到的方法和参数一对多模型__tablename__: 创建表的表名（不写默认为类型小写）db.Column: 代表创建表的字段第一个参数为数据类型，其他的为该字段的约束条件db.ForeignKey() 代表设置外键字段db.relationship （前提是设置了外键）设置表的关系属性，在一对多关系中常设置在一的一端，可以通过一端查询多端数据backref： 反向引用，可以通过关联表的另一端的实例，查询到该表的数据lazy: 默认为subquery设置为subquery，则会在设置关系字段的一端加载完对象后，就立即加载与其关联的对象，这样会让总查询数量减少，但如果返回的条目数量很多，就会比较慢设置为 subquery 的话，author.books 返回所有数据列表设置为动态方式dynamic，这样关联对象会在被使用的时候再进行加载，并且在返回前进行过滤，如果返回的对象数很多，或者未来会变得很多，那最好采用这种方式设置为 dynamic 的话，role.users 返回模型实例对象__repr__方法： 代表打印实例的时候，显示的内容代码示例：多对多模型代码示例常用的SQLALchemy字段类型常用的SQLALchemy列选项常用的SQLALchemy关系选项数据库表的操作在数据库中创建对应模型类的表db.creat_all()清空数据库中的所有表db.drop_all()数据库增、删、改、查操作数据库操作原理在Flask-SQLAlchemy中，插入、修改、删除操作，均由数据库会话管理。会话用db.session表示。在准备把数据写入数据库前，要先将数据添加到会话中然后调用 commit() 方法提交会话。在Flask-SQLAlchemy中，查询操作是通过query对象操作数据。最基本的查询是返回表中所有数据，可以通过过滤器进行更精确的数据库查询。数据库会话操作对操作进行提交，默认是不会进行提交的db.session.commit() 对数据库操作进行回滚操作db.session.rollback()对数据进行删除db.session.delete()取消和数据库的连接db.session.remove()数据库的增删改查添加数据（创建数据实例，向表中添加）单条添加rol = Role(name='admin')db.session.add(ro1)db.session.commit()批量添加数据rol = Role(name='admin')rol = Role(name='admin')db.session.add_all([ rol1, rol2 ])db.session.commit()删除数据（获取到数据实例，再进行删除）db.session.delete(rol1)修改数据（获取到数据实例，在进行修改）rol.name = '改变后的值'查询数据查询所用方法limit（）返回范围内的查询结果查询所用过滤器filter_by精确查询User.query.filter_by(id=4).first()filter模糊查询User.query.filter(User.name.endswith('g').all()User.query.filter(User.name.startswith('a')).all()filter条件查询等查询User.query.filter(User.id==4).first()非查询from sqlalchemy import not_User.query.filter(not_(User.name=='wang')).all()与查询from sqlanchemy import and_User.query.filter(and_(User.name.startswith('li'), User.email.startswith('li'))).all()可以直接不使用and_进行与查询，默认写多个条件就是与的关系或查询from sqlanchemy import or_User.query.filter(or_(User.password=='123', User.email.endswith('li'))).all()范围查询User.query.filter(User.id.in_([1, 2, 3, 4])).all()排序查询User.query.order_by(User.email.desc()).all()分页查询paginate = User.query.paginate(2, 3, False)参数分别：查询第几页，每页几条数据，是否有错误输出paginate.page 当前页paginate.pages 所有页数paginate.items 当前分页当中过得所有模型对象关联查询在一端查询多段数据rol1. users.all()users为在模型类中设置的关系字段通过一端的记录对象来查询多端的所用关联对象（前提是设置了关系字段）在多端查询一端的数据user.role.first()role为关系字段的backref属性的内容]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Migrate数据库迁移]]></title>
    <url>%2F2017%2F08%2F04%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-Migrate%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[数据库迁移扩展Flask-Migrate什么是数据库的迁移在开发过程中，需要修改数据库模型，而且还要在修改之后更新数据库。最直接的方式就是删除旧表，但这样会丢失数据。更好的解决办法是使用数据库迁移框架，它可以追踪数据库模式的变化，然后把变动应用到数据库中。Flask-Migrate拓展在Flask中可以使用Flask-Migrate扩展，来实现数据迁移。并且集成到Flask-Script中，所有操作通过命令就能完成。为了导出数据库迁移命令，Flask-Migrate提供了一个MigrateCommand类，可以附加到flask-script的manager对象上。使用Flask-migrate的步骤前提已经通过Alchemy拓展创建了数据库表的类安装拓展pip install flask-migrate通过Flask-Script注册迁移命令from flask import Flaskfrom flask_script import import Managerfrom flask_sqlalchemy import SQLAlchemyfrom flask_migrate import Migrate, MigrateCommandapp = Flask(__name__)db = SQLAlchemy(app)manager = Manager(app)Migrate(app, db, 'migrate_dir') 关联迁移，将app和数据库的操作对象传入migrate_dir 为迁移文件存储的文件夹，默认为migratesmanager.add_comment('db', MigrateCommand) 注册迁移命令到Flask-Script第一个参数是，在命令行中通过它来使用迁移相关命令初始化迁移，创建迁移仓库(创建migrations文件夹，所有迁移文件都放在里面)初始化迁移命令python database.py db init 生成迁移文件（每次对数据库模型类的修改都需要重新迁移）生成迁移文件命令python database.py db migrate -m 'initial migration'-m : 表示对迁移版本的 生成迁移的结果：自动创建一个迁移脚本包含两个函数，根据模型定义和数据库当前状态的差异，生成upgrade()和downgrade()函数两个函数操作内容不一定完全正确，有可能会遗漏一些细节，需要进行检查，自行进行改动upgrade()：函数把迁移中的数据库的改动同步到数据库中downgrade()：函数将upgrade()的改动回滚删除更新数据库表python database.py db upgrade查看历史迁移记录python app.py db history回滚到指定版本python app.py db downgrade 指定版本号实际使用的操作顺序-总结1.python 文件 db init2.python 文件 db migrate -m&quot;版本名(注释)&quot;3.python 文件 db upgrade 然后观察表结构4.根据需求修改模型5.python 文件 db migrate -m&quot;新版本名(注释)&quot;6.python 文件 db upgrade 然后观察表结构7.若返回版本,则利用 python 文件 db history查看版本号8.python 文件 db downgrade(upgrade) 版本号]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-Mail邮件拓展]]></title>
    <url>%2F2017%2F08%2F02%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FFlask-Mail%E9%82%AE%E4%BB%B6%E6%8B%93%E5%B1%95%2F</url>
    <content type="text"><![CDATA[Flask-Mail邮件拓展在开发过程中，很多应用程序都需要通过邮件提醒用户，Flask的扩展包Flask-Mail通过包装了Python内置的smtplib包，可以用在Flask程序中发送邮件。Flask-Mail连接到简单邮件协议（Simple Mail Transfer Protocol,SMTP）服务器，并把邮件交给服务器发送。使用Flask-Mail邮件拓展的步骤设置邮箱授权码在视图中进行邮件的发送]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-模板]]></title>
    <url>%2F2017%2F08%2F01%2F05.Flask%2FFlask%E6%A8%A1%E5%9D%97%2FFlask%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask模块</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Blueprint蓝图]]></title>
    <url>%2F2017%2F08%2F01%2F05.Flask%2FFlask%E6%8B%93%E5%B1%95%2FBlueprint%E8%93%9D%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Blueprint的介绍使用Blueprint的目的为了将代码模块化，随着flask程序越来越复杂,我们需要对程序进行模块化的处理,之前学习过python的模块化管理,于是针对一个简单的flask程序进行模块化处理Blueprint的概念简单来说，Blueprint 是一个存储操作方法的容器，这些操作在这个Blueprint 被注册到一个应用之后就可以被调用，Flask 可以通过Blueprint来组织URL以及处理请求。Flask使用Blueprint让应用实现模块化。Blueprint的属性一个应用可以具有多个Blueprint可以将一个Blueprint注册到任何一个未使用的URL下比如 “/”、“/sample”或者子域名在一个应用中，一个模块可以注册多次Blueprint可以单独具有自己的模板、静态文件或者其它的通用操作方法，它并不是必须要实现应用的视图和函数的在一个应用初始化时，就应该要注册需要使用的Blueprint一个Blueprint并不是一个完整的应用，它不能独立于应用运行，而必须要注册到某一个应用中，相当于django中的app，而flask的应用实例相当于django的project蓝图的运行机制蓝图机制的介绍蓝图是保存了一组将来可以在应用对象上执行的操作，注册路由就是一种操作当在应用对象上调用 route 装饰器注册路由时,这个操作将修改对象的url_map路由表然而，蓝图对象根本没有路由表，当我们在蓝图对象上调用route装饰器注册路由时,它只是在内部的一个延迟操作记录列表defered_functions中添加了一个项当执行应用对象的 register_blueprint() 方法时，应用对象将从蓝图对象的 defered_functions 列表中取出每一项，并以自身作为参数执行该匿名函数，即调用应用对象的 add_url_rule() 方法，这将真正的修改应用对象的路由表Blueprint的基本使用方法创建蓝图对象（不在main模块中）from flask import Blueprintuser_api = Blue('user_api', __name__, static_folder， static_url_path, template_folder, url_prefix))第一个参数是创建的蓝图实例的名字，通过它来注册视图函数等第二个参数为蓝图所在模块名称,指定查找静态文件和模板文件的路径static_folder: 静态文件路径，默认为Nonestatic_url_path='py3': 访问前缀，默认和静态文件路径相等template_folder: 模板文件路径，默认为Noneurl_prefix: 访问前缀，也可以通过app在注册蓝图的时候设置在蓝图对象上进行操作，注册路由视图函数@user_api.route('/user_info')def user_info():return 'user_info'在应用对象上注册这个蓝图对象(一个应用可以注册多个蓝图对象，实现模块化)from *** import user_apiapp.register_blueprint(user_api， url_prefix='/user')url_prefix:可以指定一个url_prefix关键字参数（这个参数默认是/）应用最终的路由表 url_map中，在蓝图上注册的路由URL自动被加上了这个前缀将flask项目模块化目标效果：将项目的功能块进行查分，每一类功能创建一个目录，通过蓝图的功能将每一组功能分别创建在每一个目录下，每个目录自带static和templates（类似django项目），实现项目的模块化；实现步骤创建单个小应用的文件夹（以cart为例）在__init__中初始化蓝图，并初始化视图函数from flask import Blueprintcart_api = Blueprint('cart_api', __name__， static_folder， static_url_path, template_folder, url_prefix)from views import cart_list 因为初始化蓝图的时候没有走注册视图函数的代码，一定要手动导入在view中使用蓝图注册路由from . import cart_api@cart_api.route('/cart_list'):def cart_list():return 'cart_list'在main中注册蓝图from cart import cart_apiapp.register_blueprint(cart_api, url_prefix='/cart')备注：如果在根目录下的templates中存在和蓝图templates同名文件，则系统会优先使用根目录templates中的文件所以，尽量不要将模板文件设置同名]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask拓展</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-路由视图]]></title>
    <url>%2F2017%2F07%2F30%2F05.Flask%2FFlask%E6%A8%A1%E5%9D%97%2FFlask-%E8%B7%AF%E7%94%B1%E8%A7%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Flask对请求的处理的原理flask处理请求过程所有Flask程序必须有一个应用程序实例，相当于django的project。当客户端想要获取资源时，一般会通过浏览器发起HTTP请求。此时，Web服务器使用WSGI（Web Server Gateway Interface）协议，把来自客户端的所有请求都交给Flask程序实例，程序实例使用Werkzeug来做路由分发（URL请求和视图函数之间的对应关系）。根据每个URL请求，找到具体的视图函数并进行调用。Flask实现路由分发的方式在Flask程序中，路由的实现一般是通过程序实例的装饰器实现。Flask视图函数返回的内容字符串内容：将视图函数的返回值作为响应的内容，返回给客户端(浏览器)HTML模版内容：获取到数据后，把数据传入HTML模板文件中，模板引擎负责渲染HTTP响应数据，然后返回响应数据给客户端(浏览器)Flask应用程序（示例）实现流程1.创建一个项目文件夹2.创建hello.py的文件3.导入flask类4.创建Flask实例，Flask类接收参数name，它会指向程序所在的模块5.通过装饰器的方式，将路由映射到视图函数index6.启动web服务器，Flask应用程序实例的run方法启动WEB服务器，默认5000端口代码flask应用实例的使用和基本配置flask实例初始化from flask import Flaskapp = Flask(import_name, static_url_path, static_folder, template_folder)import_name: 模块名,一般写__name__static_url_path: 静态文件访问前缀static_folder: 默认‘static’template_folder: 默认‘templates’配置参数第一种方法：通过类来配置先定义配置相关类class Config(object):DEBUG = True在通过类来配置flask实例app.config.from_object(Config)第二种方法：通过文件的方式来配置创建配置文件，并编辑相关配置DEBUG = True通过文件来配置flask实例app.config.from_pyfile(“yourconfig.cfg”)第三种方法: 单条配置app.config['DEBUG'] = True第四种方法: 在启动应用实例的时候直接添加配置app.run(debug = True)运行flask引用程序运行方法app.run（）指定参数运行app.run(host=&quot;0.0.0.0&quot;, port=5000, debug = True)可以通过传入参数实现指定端口，设置主机，添加配置的效果host为0.0.0.0的时候，本地和局域网内都可以访问读取配置参数app.config.get() 读取配置信息current_app.config.get() 在视图中读取配置信息lask应用实例的其他属性\方法app.url_map 获取当前app所有的路由和视图函数的映射，默认包含了静态文件的映射关系app.url_map.converts 返回所有路由转换器路由介绍和配置Flask装饰器路由的实现：Flask实现路由的核心：Werkzeug-实现路由、调试和Web服务器网关接口。Werkzeug是一个遵循WSGI协议的python函数库。其内部实现了很多Web框架底层的东西，比如request和response对象；与WSGI规范的兼容；支持Unicode；支持基本的会话管理和签名Cookie；集成URL请求路由等。Werkzeug库的routing模块负责实现URL解析。不同的URL对应不同的视图函数，routing模块会对请求信息的URL进行解析，匹配到URL对应的视图函数，以此生成一个响应信息。routing模块内部有： Rule类（用来构造不同的路由URL对象），存储路由和视图的映射，Map类（存储所有的URL规则），将多个Rule存入map中，BaseConverter的子类(存储的是参数匹配规则)，存储所有的url匹配规则MapAdapter类（负责具体URL匹配的工作），拿到url与Rule进行匹配路由匹配视图函数的规则路由在匹配过程中，至上而下依次匹配；当匹配到视图函数的时候便会将请求信息传递给对应视图函数，不会再向下匹配；所以当同一个路由匹配多个视图函数的时候，只会去执行第一个视图函数的业务代码；限定请求方式配置方法使用 methods 参数指定可接受的请求方式，可以是多种，默认是GETFlask会默认就含有DEAD、OPTION请求方式代码示例指定路由参数配置方法将参数部分用尖括号&lt;&gt;括起来，括号中的部分代表是url中的参数部分，默认是字符串类型指定的参数会传递给视图函数代码示例路由转换器路由转换器的作用可以限定路由参数的内容可以通过转化器方法对匹配到的路由参数做相应的处理使用路由转换器通过在指定参数的时候调用路由转换器，对参数做限制和处理等# re:路由转换器名称；括号中的内容是创建转换器实例传递的参数，参数可以传递多个，用&quot; , &quot;隔开@app.route(/user/&lt;re(&quot;[0-9]{4}&quot;):user_id&gt;) def index(user_id):.......Flask自带的路由转换器查看自带路由转换器方法app.url_map.converts自带的路由转换器default UnicodeConverter可以匹配Unicode类型的数据string UnicodeConverter可以匹配Unicode类型的字符串any AnyConverter可以匹配固定的几个设定的参数path PathConverter可以匹配带&quot;/&quot;路径符号的参数int IntegerConverter可以匹配整数类型的字符串float FloatConverter可以匹配浮点类型的字符串uuid UUIDConverter可以匹配一个32位的唯一通用识别码（uuid风格的）字符串输出一个唯一的uuid类型的数据的方法import uuid1.uuid.uuid1()2.uuid.uuid4()uuid.uuid1()自定义路由转换器（正则路由）导入转换器的父类from werkzeug.routing import BaseConverter定义正则路由转换器第一种方法该方法只能用父类的regex属性定义一种匹配规则，适用性不强第二种方法该方法可以实现通过传递进来的匹配规则来进行匹配将自定义路由转换器添加到默认的转换器列表中，并指定转换器的名称app.url_map.converters['re'] = RegexConverter路由转换器的其他方法（在自定义路由转换器中使用）to_python( ) 方法功能在参数匹配完成之后再传入到视图函数之前调用，并将函数处理后的参数内容传入视图函数代码示例：def to_python(self, value):v = int(value) 将参数转化为整数return v 将参数传递给视图函数to_url():功能此方法会在进行路由跳转时在匹配跳转的路由函数之前调用，可使用此方法对url_for传入的内容进行处理代码示例def to_url(self, value):v = int(value)return '%d' % v视图函数的配置上下文Flask上下文的概念相当于一个容器，保存了Flask程序运行过程中的一些信息。Flask中有两种上下文，请求上下文和应用上下文。应用上下文概念它的字面意思是应用上下文，但它不是一直存在的，它的作用主要是帮助 request 获取当前的应用，它是伴 request 而生，随 request 而灭的。只有在程序完全运行起来之后才能调用应用上下文包含current_app应用程序上下文,用于存储应用程序中的变量，可以通过current_app.变量获取，例如：应用的启动脚 本是哪个文件，启动时指定了哪些参数加载了哪些配置文件，导入了哪些配置连了哪个数据库有哪些public的工具类、常量应用跑再哪个机器上，IP多少，内存多大g 变量g作为flask程序全局的一个临时变量,充当者中间媒介的作用,我们可以通过它传递一些数据g保存的是当前请求的全局变量，不同的请求会有不同的全局变量，通过不同的thread_id区别current_app使用方法current_app.name 获取应用名current_app.test_value='value' 给应用实例添加变量g 变量使用方法g.name 获取g的name属性g.name='abc' 添加g全局变量name请求上下文概念Flask从客户端收到请求时，要让视图函数能访问一些对象，这样才能处理请求。请求对象是一个很好的例子，它封装了客户端发送的HTTP请求。要想让视图函数能够访问请求对象，一个 显而易见的方式是将其作为参数传入视图函数，不过这会导致程序中的每个视图函数都增加一个参数，除了访问请求对象,如果视图函数在处理请求时还要访问其他对象，情况会变得更糟。为了避免大量可有可无的参数把视图函数弄得一团糟，Flask使用上下文临时把某些对象变为全局可访问。只有在程序完全运行起来之后才能调用请求上下文包含request封装了HTTP请求的内容，针对的是http请求session用来记录请求会话中的信息，针对的是用户信息request使用方法导入方式from flask import request使用方法request.属性名 获取request上下文中的请求相关信息request对象的属性data 记录请求(请求发送的数据为raw_data)的数据， 数据类型为str form 记录请求(请求发送的数据为form_data)中的表单数据， 数据类型MultiDictargs 记录请求中的查询参数，? 后面的查询字符串， 数据类型MultiDictcookies 记录请求中的cookie信息， 数据类型Dictheaders 记录请求中的报文头， 数据类型EnvironHeadersmethod 记录请求使用的HTTP方法， 数据为GET/POST等url 记录请求的URL地址， 数据类型stringfiles 记录请求上传的文件，为form表单中提交的文件对象组成的字典， 数据类型Dictjson 获取前端传递过来的json字符串，并转换成dict类型session使用方法导入方式from flask import session使用方法使用方法参考session状态保持请求上下文\应用上下文之间的区别请求上下文：保存了客户端和服务器交互的数据应用上下文：flask 应用程序运行过程中，保存的一些配置信息，比如程序名、数据库连接、应用信息等视图函数返回内容重定向redirect示例重定向方法跳转至其他urlfrom flask import redirectreturn redirect('url') 在视图函数中通过redirect函数返回跳转至其他路由方法一：这种方法不适用，因为当路由变化后，将不能成功的匹配到视图函数方法二：from flask import url_for_external=True 参数表示已绝对路径的形式跳转，默认为相对路径的形式跳转这种方法可以直接跳转到另一个视图函数去处理，尽管路由映射改变也不影响返回JSON返回json数据的方法from flask import jsonifyreturn jsonify(&lt;dict&gt;) 在视图函数中通过jsonify函数返回 返回模板返回模板的方法from flask import Flask, render_templatereturn render_template(template_name, **content)备注：**content参数可以是任意个命名参数示例：return render_template(remplate_name, key1=value1, key2=value2)参数中的key1，key2便是模板中模板变量调用参数使用的名字返回一个respones对象返回的方法from flask import make_responseresponse = make_response(params) params：普通python类型数据（需要转换成response对象）return response返回状态码返回的方法return response, status_code 在返回的响应后面接一个数字，代表响应的状态码返回一个静态文件返回静态文件的方法状态保持为什么状态保持因为http是一种无状态协议，不会保持某一次请求所产生的信息，如果想实现状态保持，在开发中解决方式有：状态保持实现两种方法cookie：数据存储在客户端，节省服务器空间，但是不安全session：会话，数据存储在服务器端设置与获取cookie的方法设置cookie的方法response.set_cookie('key', 'value', max_age=second)max_age: cookie最大生存时间，默认是关闭浏览器失效获取cookie的方法request.cookies.get('key')删除cookieresponse.delete_cookie('key')设置与获取session的方法（默认是将session存储在cookie中的）设置session的方法配置SCRET_KEY，导入session方法app.config['SCRET_KEY'] = 'fadfas' 设置为任意不规则字符串from flask import session设置session的方法不带过期时间的设置方法session['key'] = value带过期时间的设置方法app.permanent_session_lifetime = 数字 单位为秒获取session的方法session.get('key')删除session的方法session.pop('key')通过使用redis，将session存储在redis中异常的捕获抛出异常（状态码异常）抛出异常的方法通过调用abort()方法，抛出一个给定状态代码的 HTTPException代码示例from flask import abortabort(500) # 抛出一个状态码为500的HTTPException异常捕获异常捕获异常的方法注册一个错误处理程序，当程序抛出指定错误状态码或者指定的异常的时候，调用该装饰器所装饰的方法装饰器接收的参数可以是：1.错误状态码；2.程序抛出的异常对象捕获到的异常对象对传递给处理程序，程序需要用一个形参来接收可以将异常对象return回去，会将异常信息字符串返回给客户端代码示例@app.errorhandler(500) 注册异常处理程序，传入错误状态码\ 指定错误异常类def internal_server_error(e): 定义一个异常处理程序return '服务器搬家了'请求钩子请求钩子作用在客户端和服务器交互的过程中，有些准备工作或扫尾工作需要处理通过装饰器装饰处理函数的方式，在每次请求的过程中插入对应的处理程序比如：在请求开始时，建立数据库连接；在请求结束时，指定数据的交互格式。为了让每个视图函数避免编写重复功能的代码，Flask提供了通用设施的功能，即请求钩子。四种请求钩子的实现before_first_request：在处理第一个请求前运行。 使用场景：可以在第一次请求到达的时候，开启数据库，一些初始化操作等before_request：在每次请求前运行。注意：可以在处理函数中，直接return一个response对象，那样便不会执行view函数，会直接去调用after_request使用场景：判断访问我们服务器的客户端的身份，如果身份验证通过再将请求传递给路由after_request：如果没有未处理的异常，在每次请求后运行。注意：会传递给处理函数一个response对象，处理函数需要接收此参数一定要return一个response对象，不然客户端接收不到服务器端的响应使用场景： 比如将批量视图函数返回的响应，加上统一的请求头，设置cookie等；teardown_request：在每次请求后运行，即使有未处理的异常抛出。备注：会传递给处理函数一个异常对象参数，处理函数需要接收此参数如果要针对异常返回给客户端响应，可以return一个异常信息的response使用场景：当捕获到未处理的异常后，可以根据异常的信息返回给客户端错误的提示生成铭文进行加密和解密导入模块from werkzeug.security import generate_password_hash, check_password_hash对密码进行铭文加密password_hash = generate_pass_hash(加密前的密码)对加密后的密码与未加密字符串匹配（匹配返回True，失败返回False）password = check_password_hash(加密后的字符串，待校验的密码)]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask模块</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-简介]]></title>
    <url>%2F2017%2F07%2F29%2F05.Flask%2FFlask%E6%A8%A1%E5%9D%97%2FFlask-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Web应用程序的本质Web框架的作用Web应用程序 (World Wide Web)诞生最初的目的，是为了利用互联网交流工作文档。web框架实现的业务WSGI服务器做的事情：将客户端的请求将请求行，解析请求；通过web应用框架的接口，将解析后的请求内容交给web应用框架进行处理；接收到web应用框架处理的结果，构造响应报文对象，发送给客户端；web框架做的事情定义路由，将收到的请求进行一定规则的路由分发，将给对应的视图函数进行处理实现具体的业务，视图函数执行请求对应的业务代码，将处理结果返回给WSGi服务器为什么使用web框架服务器端涉及到的知识、内容，非常广泛。这对程序员的要求会越来越高。如果采用成熟，稳健的框架，那么一些基础的工作，比如，网络操作、数据库访问、会话管理等都可以让框架来处理，那么程序开发人员可以把精力放在具体的业务逻辑上面。使用Web框架开发Web应用程序可以降低开发难度，提高开发效率。总结一句话：避免重复造轮子。Flask框架的介绍Flask框架Flask本身相当于一个内核，其他几乎所有的功能都要用到扩展（邮件扩展Flask-Mail，用户认证Flask-Login），都需要用第三方的扩展来实现。比如可以用Flask-extension加入ORM、窗体验证工具，文件上传、身份验证等。Flask没有默认使用的数据库，你可以选择MySQL，也可以用NoSQL。其 WSGI 工具箱采用 Werkzeug（路由模块） ，模板引擎则使用 Jinja2，也是通过默认安装拓展的形式实现的 。可以说Flask框架的核心就是Werkzeug路由模块和Jinja2模板引擎。Flask常用的扩展包Flask-SQLalchemy：操作数据库；Flask-migrate：管理迁移数据库；Flask-Mail:邮件；Flask-WTF：表单；Flask-script：插入脚本；Flask-Login：认证用户状态；Flask-RESTful：开发REST API的工具；Flask-Bootstrap：集成前端Twitter Bootstrap框架；Flask-Moment：本地化日期和时间；Flask与Django的对比总体区别Django功能大而全，Django的一站式解决的思路，能让开发者不用在开发之前就在选择应用的基础设施上花费大量时间。 Flask只包含基本的配置Django有模板，表单，路由，认证，基本的数据库管理等等内建功能。 与之相反，Flask只是一个内核，默认依赖于两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 工具集，其他很多功能都是以扩展的形式进行嵌入使用。Flask 比 Django 更灵活用Flask来构建应用之前，选择组件的时候会给开发者带来更多的灵活性；项目区别Django项目和应用创建好了之后，只包含空的模型和模板文件； Flask创建项目之后，目录里面没有任何文件，需要我们手动创建，是没有像Django一样组件分离，而对于需要把组件分离开的项目，Flask有blueprints。例如，你可以这样构建你的应用，将与用户有关的功能放在user.py里，把与销售相关的功能放在ecommerce.py里。Django把一个项目分成各自独立的应用，而Flask认为一个项目应该是一个包含一些视图和模型的单个应用。当然我们也可以在Flask里复制出像Django那样的项目结构。模板语言的对比Django的模板语言过滤器最多只能传递一个参数在Flask扩展的Jinja的模板语言里，可以把任何数量的参数传给过滤器，因为Jinja像调用一个Python函数的方式来看待它，用圆括号来封装参数。Flask环境的安装虚拟环境的配置虚拟环境的安装虚拟环境安装在django项目中有，不再进行记录创建虚拟环境mkvirtualenv Flask_py安装Flaskpip install flask==0.10.1安装其他的相关拓展pip freeze &gt;requirements.txt 将虚拟环境依赖包导入文件pip install -r requirements.txt 安装依赖包文件中的所有包requirements包Flask文档扩展列表：http://flask.pocoo.org/extensions/中文文档（http://docs.jinkan.org/docs/flask/）]]></content>
      <categories>
        <category>Flask</category>
        <category>Flask模块</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置文件]]></title>
    <url>%2F2017%2F07%2F27%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2Fnginx%2Fnginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[user nobody; #运行用户worker_processes 1; #启动进程,通常设置成和cpu的数量相等#全局错误日志及PID文件#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;#工作模式及连接数上限events { accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off #use epoll; #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport worker_connections 1024; #最大连接数，默认为512}http { include mime.types; #设定mime类型,类型由mime.type文件定义 文件扩展名与文件类型映射表 default_type application/octet-stream; #设定日志格式 log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; access_log logs/access.log main; # 服务日志 sendfile on; #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块 # tcp_nopush on; # 按照大小发送数据包,不是按照请求 keepalive_timeout 65; #连接超时时间 tcp_nodelay on; # 按照请求发送数据包 gzip on; # 开启gzip压缩 gzip_disable &quot;MSIE [1-6].&quot;; # 不给IE6 Gzip client_header_buffer_size 128k; # 如果请求头大小大于指定的缓冲区，则使用large_client_header_buffers指令分配更大的缓冲区。 large_client_header_buffers 4 128k; # 规定了用于读取大型客户端请求头的缓冲区的最大数量和大小。 这些缓冲区仅在缺省缓冲区不足时按需分配。 sendfile_max_chunk 100k; #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。 upstream mysvr { # 服务集群 server 127.0.0.1:7878; server 192.168.10.121:3333 backup; #热备 } #设定虚拟主机配置 server { listen 80; # 侦听80端口 server_name mysvr; # 定义使用 mysvr 访问 root html; # 定义服务器的默认网站根目录位置 access_log logs/nginx.access.log main; # 设定本虚拟主机的访问日志 location / { # 默认请求 index index.php index.html index.htm; # 定义首页索引文件的名称 client_max_body_size 100m; # 配置请求体最大容量 proxy_set_header Host $http_host; # Host包含客户端真实的域名和端口号； proxy_set_header X-Real-IP $remote_addr; # X-Forwarded-Proto表示客户端真实的协议（http还是https) proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # X-Real-IP表示客户端真实的IP； proxy_set_header X-Forwarded-Proto $scheme; # X-Forwarded-For这个Header和X-Real-IP类似，但它在多层代理时会包含真实客户端及中间每个代理服务器的IP。 deny 127.0.0.1; #拒绝的ip allow 172.18.5.54; #允许的ip } # 定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html { } #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ { expires 30d; # 过期30天，静态文件不怎么更新 } #禁止访问 .htxxx 文件 location ~ /.ht { deny all; } }}]]></content>
      <categories>
        <category>开发工具</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
</search>
