<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mysql]]></title>
    <url>%2F2020%2F05%2F07%2F21.%E6%80%BB%E7%BB%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FMysql%2F</url>
    <content type="text"><![CDATA[B 树B树特征 根结点至少有两个子女。 每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m 每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m 所有的叶子结点都位于同一层。 每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。 B+ 树特征 有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。 B+树的优势： 单一节点存储更多的元素，使得查询的IO次数更少。 所有查询都要查找到叶子节点，查询性能稳定。 所有叶子节点形成有序链表，便于范围查询。 备注: m为阶数 Mysql存储引擎存储引擎的区别 联机事务处理OLTP（on-line transaction processing）:传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 联机分析处理OLAP（On-Line Analytical Processing）:是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果 InnoDB: InnoDB 存储引擎支持事务、支持外键、支持非锁定读、行锁设计其设计主要面向OLTP 应用。 InnoDB 存储引擎表采用聚集的方式存储，因此每张表的存储顺序都按主键的顺序存放，如果没有指定主键，InnoDB 存储引擎会为每一行生成一个6字节的ROWID并以此作为主键。 InnoDB 存储引擎通过MVCC 获的高并发性，并提供了插入缓冲、二次写、自适应哈希索引和预读等高性能高可用功能 InnoDB 存储引擎默认隔离级别为REPEATABLE_READ（重复读）并采用next-key locking(间隙锁)来避免幻读 MySIAM: MYISAM 存储引擎不支持事务、表锁设计、支持全文索引其设计主要面向OLAP 应用 MYISAM 存储引擎表由frm、MYD 和MYI 组成，frm 文件存放表格定义，MYD 用来存放数据文件，MYI 存放索引文件。MYISAM 存储引擎与众不同的地方在于它的缓冲池只缓存索引文件而不缓存数据文件，数据文件的缓存依赖于操作系统。操作区别： MYISAM 保存表的具体行数，不带where 是可直接返回。InnoDB 要扫描全表。DELETE 表时，InnoDB 是一行一行的删除，MYISAM 是先drop表，然后重建表InnoDB 跨平台可直接拷贝使用，MYISAM 不行InnoDB 表格很难被压缩，MYISAM 可以 引擎选择：MyISAM相对简单所以在效率上要优于InnoDB。 如果系统读多，写少。对原子性要求低,那么MyISAM最好的选择。且MyISAM恢复速度快。可直接用备份覆盖恢复。 InnoDB 更适合系统读少，写多的时候，尤其是高并发场景。 Mysql索引mysql 索引介绍Mysql 中常用的索引有B+ 树索引（包括普通索引、唯一索引、主键索引），哈希索引，全文索引，R-TREE 索引（空间索引，主要用于地理空间数据类型，很少使用）。 Mysql 传统意义上的索引为B+ 树索引，B+ 树索引的本质就是B+ 树在数据库中的实现，由于B+ 树的高扇出性，数据库中的B+ 树的高一般为2-4层，因此查找某一键值的行记录只需2-4次IO，大概0.02~0.04秒。 （扇出性：是指该模块直接调用的下级模块的个数。扇出大表示模块的复杂度高，需要控制和协调过多的下级模块） B+ 树索引分类聚集索引和辅助索引 聚集索引是根据每张表的主键建造的一棵B+ 树，叶子节点中存放的是整张表的行记录。一张表只能有一个聚集索引。因为聚集索引在逻辑上是连续的，所以它对于主键的排序查找和范围查找速度非常快。 辅助索引与聚集索引不同的地方在于，辅助索引不是唯一的，它的叶子节点只包含行记录的部分数据以及对应聚集索引的节点位置。通过辅助索引来查找数据时，先遍历辅助索引找到对应主键索引，再通过主键索引查找对应记录。 在MYISAM 中主键索引和辅助索引都相当上述辅助索引，索引页中存放的是主键和指向数据页的偏移量，数据页中存放的是主键和该主键所属行记录的地址空间。唯一的区别是MYISAM 中主键索引不能重复，辅助索引可以。 联合索引和覆盖索引 联合索引是指对表上的多个列进行索引。它对对应多个列的指定获取比较快。另外一个好处是联合索引对第二个键已经排好序了，所以对两个列的排序获取可以避免多做一次排序操作。 覆盖索引其实更算一种思想，能够从辅助索引中获取信息，就不需要查询聚集索引中的数据。使用辅助索引的好处在于辅助索引包含的信息少，所以大小远小于聚集索引，因此可以大大减少IO 操作。 hash索引哈希索引是一种自适应的索引，数据库会根据表的使用情况自动生成哈希索引，我们人为是没办法干预的。 InnoDB 储存引擎采用的哈希函数为除法散列方式，采用的冲突处理方法为链地址法。它指定查询的速度很快，但是范围查询就无能为力了。 全文索引全文索引用于实现关键词搜索。但它只能根据空格分词，因此不支持中文。 索引的优缺点索引的优点： 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快 数据的检索速度，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序 子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引的缺点 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 哪些情况需要加索引？ Mysql事务什么是事务事务就是一组原子性的操作，这些操作要么全部发生，要么全部不发生。事务把数据库从一种一致性状态转换成另一种一致性状态。 事务具有ACID 四种特性，即原子性(atomicity)，一致性(consistency)，隔离性(isolation)，持久性(durability)： 原子性，指的是事务是一个不可分割的操作，要么全都正确执行，要么全都不执行。 一致性，指的是事务把数据库从一种一致性状态转换成另一种一致性状态，事务开始前和事务结束后，数据库的完整性约束没有被破坏。 隔离性，要求每个读写事务相互之间是分开的，在事务提交前对其他事务是不可见的 持久性，指的是事务一旦提交，其结果就是永久性的，即使宕机也能恢复。 事务有4 个隔离级别，分别是： 读未提交(read uncommit) 读已提交(read commit) 可重复读(repeatable read) 和序列化(serializable)。 隔离级别依次提高，分别解决了脏读、不可重读和幻读。 1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。 3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表 InnoDB 默认隔离级别为repeatable read，但是通过next-key lock 解决了幻读，保证了ACID。 事务的实现原理事务是基于重做日志文件(redo log)和回滚日志(undo log)实现的。 每提交一个事务必须先将该事务的所有日志写入到重做日志文件进行持久化，数据库就可以通过重做日志来保证事务的原子性和持久性。 每当有修改事务时，还会产生undo log，如果需要回滚，则根据undo log 的反向语句进行逻辑操作，比如insert 一条记录就delete 一条记录。 事务主要分为 扁平事务 带有保存点的扁平事务 链事务 嵌套事务 分布式事务 使用事务应该注意的问题 不要再循环中使用事务（循环提交会导致大量的redo log） 不要使用自动提交 不要使用自动回滚 长事务切分处理 SQL 优化 Mysql 锁数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 mysql锁的分类全局锁顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。 当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 你一定在疑惑，有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。 所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。 表级锁MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。 需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。 元数据锁MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。 因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 需要注意的是：MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。 行锁MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 举个例子。 假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作： 从顾客 A 账户余额中扣除电影票价； 给影院 B 的账户余额增加这张电影票价； 记录一条交易日志。 也就是说，要完成这个交易，我们需要 update 两条记录，并 insert 一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？ 试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。 根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。 死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。 这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 减少死锁的主要方向，就是控制访问相同资源的并发事务量。]]></content>
      <categories>
        <category>总结</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2020%2F05%2F07%2F21.%E6%80%BB%E7%BB%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FRedis%2F</url>
    <content type="text"><![CDATA[Redis 持久化机制Redis是一个支持持久化的内存数据库，通过持久化机制把内存中的数据同步到硬盘文件来保证数据持久化。当Redis重启后通过把硬盘文件重新加载到内存，就能达到恢复数据的目的。 实现机制单独创建fork()一个子进程，将当前父进程的数据库数据复制到子进程的内存中，然后由子进程写入到临时文件中，持久化的过程结束了，再用这个临时文件替换上次的快照文件，然后子进程退出，内存释放。 持久化方式 RDB： 是Redis默认的持久化方式,按照一定的时间周期策略把内存的数据以快照的形式保存到硬盘的二进制文件。即Snapshot快照存储，对应产生的数据文件为dump.rdb，通过配置文件中的save参数来定义快照的周期。（ 快照可以是其所表示的数据的一个副本，也可以是数据的一个复制品。） AOF：Redis会将每一个收到的写命令都通过Write函数追加到文件最后，类似于MySQL的binlog。当Redis重启是会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。 Redis 常见线上问题缓存雪崩问题解释 缓存雪崩我们可以简单的理解为：由于原有缓存失效，新缓存未到期间 例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期，所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。 解决办法 大多数系统设计者考虑用加锁（ 最多的解决方案）或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就时讲缓存失效时间分散开。 缓存穿透问题解释 缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。 这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。 这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。 解决办法 最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。 缓存击穿问题解释 指一个key非常热点，大并发集中对这个key进行访问，当这个key在失效的瞬间，仍然持续的大并发访问就穿破缓存，转而直接请求数据库。 解决方案 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。 缓存预热问题解释 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。 这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路1、直接写个缓存刷新页面，上线时手工操作下；2、数据量不大，可以在项目启动的时候自动进行加载；3、定时刷新缓存； 缓存更新除了缓存服务器自带的缓存失效策略之外（Redis默认的有6种策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：（1）定时去清理过期的缓存；（2）当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 缓存降级当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。 系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 以参考日志级别设置预案：（1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；（2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；（3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；（4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查询，而是直接返回默认值给用户。 热点数据和冷数据对于热点数据，就是信息修改频率不高，读取通常非常高的场景，缓存才有价值。 对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存。 数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。 那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？ 有！比如，这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力。 单线程的redis为什么这么快 纯内存操作 单线程操作，避免了频繁的上下文切换 采用了非阻塞I/O多路复用机制 redis的数据类型 String这个其实没啥好说的，最常规的set/get操作，value可以是String也可以是数字。一般做一些复杂的计数功能的缓存。 hash这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。 list使用List的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用lrange命令，做基于redis的分页功能，性能极佳，用户体验好。本人还用一个场景，很合适—取行情信息。就也是个生产者和消费者的场景。LIST可以很好的完成排队，先进先出的原则。 set因为set堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。 sorted setsorted set多了一个权重参数score,集合中的元素能够按score进行排列。可以做排行榜应用，取TOP N操作。 bit arrays 简单的位映射 hyperloglogs 概率数据结构 Redis 内部结构 dict： 本质上是为了解决算法中的查找问题（Searching）是一个用于维护key和value映射关系的数据结构，与很多语言中的Map或dictionary类似。 本质上是为了解决算法中的查找问题（Searching） sds： sds就等同于char * 它可以存储任意二进制数据，不能像C语言字符串那样以字符’\0’来标识字符串的结 束，因此它必然有个长度字段。 skiplist： （跳跃表） 跳表是一种实现起来很简单，单层多指针的链表，它查找效率很高，堪比优化过的二叉平衡树，且比平衡树的实现， quicklist ziplist: 压缩表 ziplist是一个编码后的列表，是由一系列特殊编码的连续内存块组成的顺序型数据结构， redis的过期策略、内存淘汰机制redis 的过期策略redis采用的是定期删除+惰性删除策略。 为什么不用定时删除策略? 定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。 在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略. 定期删除+惰性删除是如何工作的呢? 定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。 需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。 因此，如果只采用定期删除策略，会导致很多key到时间没有删除。 于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。 redis 内存淘汰机制采用定期删除+惰性删除就没其他问题了么? 不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。 在redis.conf中有一行配置 maxmemory-policy volatile-lru 该配置就是配内存淘汰策略的: volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据，新写入操作会报错 ps：如果没有设置 expire 的key, 不满足先决条件(prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。 Redis 为什么是单线程的官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。 既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）Redis利用队列技术将并发访问变为串行访问1）绝大部分请求是纯粹的内存操作（非常快速） 2）采用单线程,避免了不必要的上下文切换和竞争条件 3）非阻塞IO优点 Redis 优点 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) 支持丰富数据类型，支持string，list，set，sorted set，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 Redis key的资源竞争问题同时有多个子系统去set一个key。这个时候要注意什么呢？ 不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。 你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。 解决办法 (1)如果对这个key操作，不要求顺序： 准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可(2)如果对这个key操作，要求顺序： 分布式锁+时间戳。 假设这会系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。(3) 利用队列，将set方法变成串行访问也可以 redis遇到高并发，如果保证读写key的一致性对redis的操作都是具有原子性的,是线程安全的操作,你不用考虑并发问题,redis内部已经帮你处理好并发的问题了。 Redis 常见性能问题和解决方案？(1) Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件(2) 如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一次(3) 为了主从复制的速度和连接的稳定性， Master 和 Slave 最好在同一个局域网内(4) 尽量避免在压力很大的主库上增加从库(5) 主从复制不要用图状结构，用单向链表结构更为稳定，即： Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3… Redis线程模型文件事件处理器文件事件处理器包括 套接字 I/O 多路复用程序 文件事件分派器（dispatcher） 事件处理器 各模块负责内容 文件事件处理器包括分别是套接字、 I/O 多路复用程序、 文件事件分派器（dispatcher）、 以及事件处理器。使用 I/O 多路复用程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 I/O 多路复用程序: 使用 I/O 多路复用程序来同时监听多个套接字， 并向文件事件分派器传送那些产生了事件的套接字。 文件事件分派器: 根据套接字目前执行的任务来为套接字关联不同的事件处理器 事件处理器: 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 I/O 多路复用程序工作原理I/O 多路复用程序负责监听多个套接字， 并向文件事件分派器传送那些产生了事件的套接字。 尽管多个文件事件可能会并发地出现， 但 I/O 多路复用程序总是会将所有产生事件的套接字都入队到一个队列里面， 然后通过这个队列， 以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字： 当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕）， I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字。如果一个套接字又可读又可写的话 ， 那么服务器将先读套接字， 后写套接字. Redis 原子性对于Redis而言，命令的原子性指的是：一个操作的不可以再分，操作要么执行，要么不执行。 Redis的操作之所以是原子性的，是因为Redis是单线程的。 Redis本身提供的所有API都是原子操作，Redis中的事务其实是要保证批量操作的原子性。 多个命令在并发中也是原子性的吗？ 不一定， 将get和set改成单命令操作，incr 。使用Redis的事务，或者使用Redis+Lua==的方式实现. Redis实现分布式锁Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。将 key 的值设为 value ，当且仅当 key 不存在。 若给定的 key 已经存在，则 SETNX 不做任何动作 解锁：使用 del key 命令就能释放锁解决死锁：1）通过Redis中expire()给锁设定最大持有时间，如果超过，则Redis来帮我们释放锁。2 ) 使用 setnx key “当前系统时间+锁持有的时间”和getset key “当前系统时间+锁持有的时间”组合的命令就可以实现。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 总结]]></title>
    <url>%2F2020%2F04%2F28%2F21.%E6%80%BB%E7%BB%93%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F</url>
    <content type="text"><![CDATA[Flink组件栈Flink 采用分层的架构设计，从而保证各层在功能和职责上的清晰。如下图所示，由上而下分别是 API &amp; Libraries 层、Runtime 核心层以及物理部署层： API &amp; Libraries 层这一层主要提供了编程 API 和 顶层类库： 编程 API : 用于进行流处理的 DataStream API 和用于进行批处理的 DataSet API； 顶层类库：包括用于复杂事件处理的 CEP 库；用于结构化数据查询的 SQL &amp; Table 库，以及基于批处理的机器学习库 FlinkML 和 图形处理库 Gelly。 Runtime 核心层这一层是 Flink 分布式计算框架的核心实现层，包括作业转换，任务调度，资源分配，任务执行等功能，基于这一层的实现，可以在流式引擎下同时运行流处理程序和批处理程序。比如：支持分布式Stream处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务 物理部署层Flink 的物理部署层，用于支持在不同平台上部署运行 Flink 应用。 主要涉及了Flink的部署模式，Flink支持多种部署模式：本地、集群（Standalone/YARN）、云（GCE/EC2） Flink 分层 API在上面介绍的 API &amp; Libraries 这一层，Flink 又进行了更为具体的划分。具体如下： 按照如上的层次结构，API 的一致性由下至上依次递增，接口的表现能力由下至上依次递减，各层的核心功能如下： SQL &amp; Table APISQL &amp; Table API 同时适用于批处理和流处理，这意味着你可以对有界数据流和无界数据流以相同的语义进行查询，并产生相同的结果。除了基本查询外， 它还支持自定义的标量函数，聚合函数以及表值函数，可以满足多样化的查询需求。 DataStream &amp; DataSet APIDataStream &amp; DataSet API 是 Flink 数据处理的核心 API，支持使用 Java 语言或 Scala 语言进行调用，提供了数据读取，数据转换和数据输出等一系列常用操作的封装。 Stateful Stream ProcessingStateful Stream Processing 是最低级别的抽象，它通过 Process Function 函数内嵌到 DataStream API 中。 Process Function 是 Flink 提供的最底层 API，具有最大的灵活性，允许开发者对于时间和状态进行细粒度的控制。 Flink 集群架构核心组件按照上面的介绍，Flink 核心架构的第二层是 Runtime 层， 该层采用标准的 Master - Slave 结构， 其中，Master 部分又包含了三个核心组件：Dispatcher、ResourceManager 和 JobManager，而 Slave 则主要是 TaskManager 进程。它们的功能分别如下： Client： 用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群， Client会将用户提交的Flink程序组装一个JobGraph， 并且是以JobGraph的形式提交的 Dispatcher：负责接收客户端提交的执行程序，并传递给 JobManager 。除此之外，它还提供了一个 WEB UI 界面，用于监控作业的执行情况。 JobManagers (也称为 masters) ：JobManagers 接收由 Dispatcher 传递过来的执行程序，该执行程序包含了作业图 (JobGraph)，逻辑数据流图 (logical dataflow graph) 及其所有的 classes 文件以及第三方类库 (libraries) 等等 。紧接着 JobManagers 会将 JobGraph 转换为执行图 (ExecutionGraph)，然后向 ResourceManager 申请资源来执行该任务，一旦申请到资源，就将执行图分发给对应的 TaskManagers 。因此每个作业 (Job) 至少有一个 JobManager；高可用部署下可以有多个 JobManagers，其中一个作为 leader，其余的则处于 standby 状态。 ResourceManager ：负责管理 slots 并协调集群资源。ResourceManager 接收来自 JobManager 的资源请求，并将存在空闲 slots 的 TaskManagers 分配给 JobManager 执行任务。Flink 基于不同的部署平台，如 YARN , Mesos，K8s 等提供了不同的资源管理器，当 TaskManagers 没有足够的 slots 来执行任务时，它会向第三方平台发起会话来请求额外的资源。 TaskManagers (也称为 workers) : TaskManagers 负责实际的子任务 (subtasks) 的执行，每个 TaskManagers 都拥有一定数量的 slots。Slot 是一组固定大小的资源的合集 (如计算能力，存储空间)。TaskManagers 启动后，会将其所拥有的 slots 注册到 ResourceManager 上，由 ResourceManager 进行统一管理。 Task &amp; SubTask上面我们提到：TaskManagers 实际执行的是 SubTask，而不是 Task，这里解释一下两者的区别： 在执行分布式计算时，Flink 将可以链接的操作 (operators) 链接到一起，这就是 Task。之所以这样做， 是为了减少线程间切换和缓冲而导致的开销，在降低延迟的同时可以提高整体的吞吐量。 但不是所有的 operator 都可以被链接，如下 keyBy 等操作会导致网络 shuffle 和重分区，因此其就不能被链接，只能被单独作为一个 Task。 简单来说，一个 Task 就是一个可以链接的最小的操作链 (Operator Chains) 。如下图，source 和 map 算子被链接到一块，因此整个作业就只有三个 Task： 解释完 Task ，我们在解释一下什么是 SubTask，其准确的翻译是： A subtask is one parallel slice of a task，即一个 Task 可以按照其并行度拆分为多个 SubTask。如上图，source &amp; map 具有两个并行度，KeyBy 具有两个并行度，Sink 具有一个并行度，因此整个虽然只有 3 个 Task，但是却有 5 个 SubTask。Jobmanager 负责定义和拆分这些 SubTask，并将其交给 Taskmanagers 来执行，每个 SubTask 都是一个单独的线程。 资源管理理解了 SubTasks ，我们再来看看其与 Slots 的对应情况。一种可能的分配情况如下： 这时每个 SubTask 线程运行在一个独立的 TaskSlot， 它们共享所属的 TaskManager 进程的TCP 连接（通过多路复用技术）和心跳信息 (heartbeat messages)，从而可以降低整体的性能开销。此时看似是最好的情况，但是每个操作需要的资源都是不尽相同的，这里假设该作业 keyBy 操作所需资源的数量比 Sink 多很多 ，那么此时 Sink 所在 Slot 的资源就没有得到有效的利用。 基于这个原因，Flink 允许多个 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，但只要它们来自同一个 Job 就可以。假设上面 souce &amp; map 和 keyBy 的并行度调整为 6，而 Slot 的数量不变，此时情况如下： 可以看到一个 Task Slot 中运行了多个 SubTask 子任务，此时每个子任务仍然在一个独立的线程中执行，只不过共享一组 Sot 资源而已。那么 Flink 到底如何确定一个 Job 至少需要多少个 Slot 呢？Flink 对于这个问题的处理很简单，默认情况一个 Job 所需要的 Slot 的数量就等于其 Operation 操作的最高并行度。如下， A，B，D 操作的并行度为 4，而 C，E 操作的并行度为 2，那么此时整个 Job 就需要至少四个 Slots 来完成。通过这个机制，Flink 就可以不必去关心一个 Job 到底会被拆分为多少个 Tasks 和 SubTasks。 Flink 优势 支持高吞吐、低延迟、高性能的流处理 支持高度灵活的窗口（Window）操作 支持有状态计算的Exactly-once语义 提供DataStream API和DataSet API Source 和 sink自定义 Data SourceSourceFunction 除了内置的数据源外，用户还可以使用 addSource 方法来添加自定义的数据源。自定义的数据源必须要实现 SourceFunction 接口，这里以产生 [0 , 1000) 区间内的数据为例，代码如下： 123456789101112131415161718192021final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.addSource(new SourceFunction&lt;Long&gt;() &#123; private long count = 0L; private volatile boolean isRunning = true; public void run(SourceContext&lt;Long&gt; ctx) &#123; while (isRunning &amp;&amp; count &lt; 1000) &#123; // 通过collect将输入发送出去 ctx.collect(count); count++; &#125; &#125; public void cancel() &#123; isRunning = false; &#125;&#125;).print();env.execute(); ParallelSourceFunction 和 RichParallelSourceFunction上面通过 SourceFunction 实现的数据源是不具有并行度的，即不支持在得到的 DataStream 上调用 setParallelism(n) 方法，此时会抛出如下的异常： 1Exception in thread "main" java.lang.IllegalArgumentException: Source: 1 is not a parallel source 如果你想要实现具有并行度的输入流，则需要实现 ParallelSourceFunction 或 RichParallelSourceFunction 接口，其与 SourceFunction 的关系如下图： ParallelSourceFunction 直接继承自 ParallelSourceFunction，具有并行度的功能。RichParallelSourceFunction 则继承自 AbstractRichFunction，同时实现了 ParallelSourceFunction 接口，所以其除了具有并行度的功能外，还提供了额外的与生命周期相关的方法，如 open() ，closen() 。 自定义 Sink除了使用内置的第三方连接器外，Flink 还支持使用自定义的 Sink 来满足多样化的输出需求。想要实现自定义的 Sink ，需要直接或者间接实现 SinkFunction 接口。通常情况下，我们都是实现其抽象类 RichSinkFunction，相比于 SinkFunction ，其提供了更多的与生命周期相关的方法。两者间的关系如下： 这里我们以自定义一个 FlinkToMySQLSink 为例，将计算结果写出到 MySQL 数据库中，具体步骤如下： 导入依赖首先需要导入 MySQL 相关的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.16&lt;/version&gt;&lt;/dependency&gt; 自定义 Sink继承自 RichSinkFunction，实现自定义的 Sink ： 123456789101112131415161718192021222324252627282930313233343536public class FlinkToMySQLSink extends RichSinkFunction&lt;Employee&gt; &#123; private PreparedStatement stmt; private Connection conn; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName("com.mysql.cj.jdbc.Driver"); conn = DriverManager.getConnection("jdbc:mysql://192.168.0.229:3306/employees" + "?characterEncoding=UTF-8&amp;serverTimezone=UTC&amp;useSSL=false", "root", "123456"); String sql = "insert into emp(name, age, birthday) values(?, ?, ?)"; stmt = conn.prepareStatement(sql); &#125; @Override public void invoke(Employee value, Context context) throws Exception &#123; stmt.setString(1, value.getName()); stmt.setInt(2, value.getAge()); stmt.setDate(3, value.getBirthday()); stmt.executeUpdate(); &#125; @Override public void close() throws Exception &#123; super.close(); if (stmt != null) &#123; stmt.close(); &#125; if (conn != null) &#123; conn.close(); &#125; &#125;&#125; 使用自定义 Sink想要使用自定义的 Sink，同样是需要调用 addSink 方法，具体如下： 12345678final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Date date = new Date(System.currentTimeMillis());DataStreamSource&lt;Employee&gt; streamSource = env.fromElements( new Employee("hei", 10, date), new Employee("bai", 20, date), new Employee("ying", 30, date));streamSource.addSink(new FlinkToMySQLSink());env.execute(); Streaming Connectors内置连接器除了自定义数据源外， Flink 还内置了多种连接器，用于满足大多数的数据收集场景。当前内置连接器的支持情况如下： Apache Kafka (支持 source 和 sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) Hadoop FileSystem (sink) RabbitMQ (source/sink) Apache NiFi (source/sink) Twitter Streaming API (source) Google PubSub (source/sink) 除了上述的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink 相关的连接器如下： Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) Netty (source) Flink 状态相对于其他流计算框架，Flink 一个比较重要的特性就是其支持有状态计算。即你可以将中间的计算结果进行保存，并提供给后续的计算使用： 具体而言，Flink 又将状态 (State) 分为 Keyed State 与 Operator State： 算子状态算子状态 (Operator State)：顾名思义，状态是和算子进行绑定的，一个算子的状态不能被其他算子所访问到。官方文档上对 Operator State 的解释是：each operator state is bound to one parallel operator instance，所以更为确切的说一个算子状态是与一个并发的算子实例所绑定的，即假设算子的并行度是 2，那么其应有两个对应的算子状态： 键控状态键控状态 (Keyed State) ：是一种特殊的算子状态，即状态是根据 key 值进行区分的，Flink 会为每类键值维护一个状态实例。如下图所示，每个颜色代表不同 key 值，对应四个不同的状态实例。需要注意的是键控状态只能在 KeyedStream 上进行使用，我们可以通过 stream.keyBy(...) 来得到 KeyedStream 。 检查点机制checkpoint为了使 Flink 的状态具有良好的容错性，Flink 提供了检查点机制 (CheckPoints) 。通过检查点机制，Flink 定期在数据流上生成 checkpoint barrier ，当某个算子收到 barrier 时，即会基于当前状态生成一份快照，然后再将该 barrier 传递到下游算子，下游算子接收到该 barrier 后，也基于当前状态生成一份快照，依次传递直至到最后的 Sink 算子上。当出现异常后，Flink 就可以根据最近的一次的快照数据将所有算子恢复到先前的状态。 开启检查点默认情况下，检查点机制是关闭的，需要在程序中进行开启： 1234567891011121314151617// 开启检查点机制，并指定状态检查点之间的时间间隔env.enableCheckpointing(1000); // 其他可选配置如下：// 设置语义env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// 设置两个检查点之间的最小时间间隔env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// 设置执行Checkpoint操作时的超时时间env.getCheckpointConfig().setCheckpointTimeout(60000);// 设置最大并发执行的检查点的数量env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// 将检查点持久化到外部存储env.getCheckpointConfig().enableExternalizedCheckpoints( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// 如果有更近的保存点时，是否将作业回退到该检查点env.getCheckpointConfig().setPreferCheckpointForRecovery(true); 窗口函数Tumbling Windows滚动窗口 (Tumbling Windows) 是指彼此之间没有重叠的窗口。例如：每隔1小时统计过去1小时内的商品点击量，那么 1 天就只能分为 24 个窗口，每个窗口彼此之间是不存在重叠的，具体如下： 这里我们以词频统计为例，给出一个具体的用例，代码如下： 12345678910111213final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 接收socket上的数据输入DataStreamSource&lt;String&gt; streamSource = env.socketTextStream("hadoop001", 9999, "\n", 3);streamSource.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; String[] words = value.split("\t"); for (String word : words) &#123; out.collect(new Tuple2&lt;&gt;(word, 1L)); &#125; &#125;&#125;).keyBy(0).timeWindow(Time.seconds(3)).sum(1).print(); //每隔3秒统计一次每个单词出现的数量env.execute("Flink Streaming"); 假如我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成不重叠的窗口，每一个事件只能属于一个窗口。 123456789// 用户id和购买数量 streamval counts: DataStream[(Int, Int)] = ...val tumblingCnts: DataStream[(Int, Int)] = counts // 用userId分组 .keyBy(0) // 1分钟的翻滚窗口宽度 .timeWindow(Time.minutes(1)) // 计算购买数量 .sum(1) Sliding Windows滑动窗口用于滚动进行聚合分析，例如：每隔 6 分钟统计一次过去一小时内所有商品的点击量，那么统计窗口彼此之间就是存在重叠的，即 1天可以分为 240 个窗口。图示如下： 可以看到 window 1 - 4 这四个窗口彼此之间都存在着时间相等的重叠部分。想要实现滑动窗口，只需要在使用 timeWindow 方法时额外传递第二个参数作为滚动时间即可，具体如下： 12// 每隔3秒统计一次过去1分钟内的数据keyBy(0).timeWindow(Time.minutes(1),Time.seconds(3)).sum(1) Session Windows当用户在进行持续浏览时，可能每时每刻都会有点击数据，例如在活动区间内，用户可能频繁的将某类商品加入和移除购物车，而你只想知道用户本次浏览最终的购物车情况，此时就可以在用户持有的会话结束后再进行统计。想要实现这类统计，可以通过 Session Windows 来进行实现。 具体的实现代码如下： 1234// 以处理时间为衡量标准，如果10秒内没有任何数据输入，就认为会话已经关闭，此时触发统计window(ProcessingTimeSessionWindows.withGap(Time.seconds(10)))// 以事件时间为衡量标准 window(EventTimeSessionWindows.withGap(Time.seconds(10))) Global Windows最后一个窗口是全局窗口， 全局窗口会将所有 key 相同的元素分配到同一个窗口中，其通常配合触发器 (trigger) 进行使用。如果没有相应触发器，则计算将不会被执行。 这里继续以上面词频统计的案例为例，示例代码如下： 12// 当单词累计出现的次数每达到10次时，则触发计算，计算整个窗口内该单词出现的总数window(GlobalWindows.create()).trigger(CountTrigger.of(10)).sum(1).print(); Count WindowsCount Windows 用于以数量为维度来进行数据聚合，同样也分为滚动窗口和滑动窗口，实现方式也和时间窗口完全一致，只是调用的 API 不同，具体如下： 1234// 滚动计数窗口，每1000次点击则计算一次countWindow(1000)// 滑动计数窗口，每10次点击发生后，则计算过去1000次点击的情况countWindow(1000,10) 实际上计数窗口内部就是调用的我们上一部分介绍的全局窗口来实现的，其源码如下： 123456789public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size) &#123; return window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));&#125;public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size, long slide) &#123; return window(GlobalWindows.create()) .evictor(CountEvictor.of(size)) .trigger(CountTrigger.of(slide));&#125; 时间（Time）时间类型 Flink中的时间与现实世界中的时间是不一致的，在flink中被划分为事件时间，摄入时间，处理时间三种。 如果以EventTime为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime 如果以IngesingtTime为基准来定义时间窗口将形成IngestingTimeWindow,以source的systemTime为准。 如果以ProcessingTime基准来定义时间窗口将形成ProcessingTimeWindow，以operator的systemTime为准。 时间详解Processing Time Processing Time 是指事件被处理时机器的系统时间。 Event Time Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。 Ingestion Time Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。 时间乱序存在的问题在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图： 那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？Apache Flink的时间类型开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图： 那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？ 什么是WatermarkWatermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: Watermark的产生方式目前Apache Flink 有两种生产Watermark的方式，如下： Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。 在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。 Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。 所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。 Watermark的接口定义对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下： Periodic Watermarks - AssignerWithPeriodicWatermarks 1234567891011121314151617181920212223/*** Returns the current watermark. This method is periodically called by the* system to retrieve the current watermark. The method may return &#123;@code null&#125; to* indicate that no new Watermark is available.** The returned watermark will be emitted only if it is non-null and itsTimestamp* is larger than that of the previously emitted watermark (to preserve the contract of* ascending watermarks). If the current watermark is still* identical to the previous one, no progress in EventTime has happened since* the previous call to this method. If a null value is returned, or theTimestamp* of the returned watermark is smaller than that of the last emitted one, then no* new watermark will be generated.** The interval in which this method is called and Watermarks are generated* depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;.** @see org.Apache.flink.streaming.api.watermark.Watermark* @see ExecutionConfig#getAutoWatermarkInterval()** @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.*/@NullableWatermark getCurrentWatermark(); Punctuated Watermarks - AssignerWithPunctuatedWatermarks 1234567891011121314151617181920public interface AssignerWithPunctuatedWatermarks&lt;T&gt; extends TimestampAssigner&lt;T&gt; &#123;/*** Asks this implementation if it wants to emit a watermark. This method is called right after* the &#123;@link #extractTimestamp(Object, long)&#125; method.** The returned watermark will be emitted only if it is non-null and itsTimestamp* is larger than that of the previously emitted watermark (to preserve the contract of* ascending watermarks). If a null value is returned, or theTimestamp of the returned* watermark is smaller than that of the last emitted one, then no new watermark will* be generated.** For an example how to use this method, see the documentation of* &#123;@link AssignerWithPunctuatedWatermarks this class&#125;.** @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.*/@NullableWatermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);&#125; AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner 1234567891011121314151617public interfaceTimestampAssigner&lt;T&gt; extends Function &#123;/*** Assigns aTimestamp to an element, in milliseconds since the Epoch.** The method is passed the previously assignedTimestamp of the element.* That previousTimestamp may have been assigned from a previous assigner,* by ingestionTime. If the element did not carry aTimestamp before, this value is* &#123;@code Long.MIN_VALUE&#125;.** @param element The element that theTimestamp is wil be assigned to.* @param previousElementTimestamp The previous internalTimestamp of the element,* or a negative value, if noTimestamp has been assigned, yet.* @return The newTimestamp.*/long extractTimestamp(T element, long previousElementTimestamp);&#125; 从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。 Watermark解决EventTime问题从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。 回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下： 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： 1234567CREATE TABLE source(...,Event_timeTimeStamp,WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0) ) with (...); 如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下： 上面对应的DDL(Alibaba 内部的DDL语法，目前正在和社区讨论)定义如下： 1234567CREATE TABLE source(...,Event_timeTimeStamp,WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000) ) with (...); 上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s. 多流的Watermark处理在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示： Apache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图: 本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。 使用Event Time 处理实时数据 如下是一段 log 日志，我们根据该日志格式，来分析客户的下单量情况。 日志格式：1581490623000,James,51581490624150,John,2… 接下来，我们从并行Source 和 非并行Source 两个方向，来使用 EventTime 处理实时数据。(接下来示例，设置延迟为0s，即不延迟) 非并行Source非并行Source，以 socketTextStream为例来介绍 Flink使用 EventTime 处理实时数据。 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/*** TODO 非并行Source EventTime ** @author liuzebiao * @Date 2020-2-12 15:25*/public class EventTimeDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置EventTime作为时间标准 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); //读取source，并指定(1581490623000,Mary,3)中哪个字段为EventTime时间 //WaterMarks:是Flink中窗口延迟触发的机制。Time.seconds(0)表示无延迟。 SingleOutputStreamOperator&lt;String&gt; source = env.socketTextStream("localhost", 8888).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) &#123; @Override public long extractTimestamp(String line) &#123; String[] split = line.split(","); return Long.parseLong(split[0]); &#125; &#125;); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; mapOperator = source.map(line -&gt; &#123; String[] split = line.split(","); return Tuple2.of(split[1], Integer.parseInt(split[2])); &#125;).returns(Types.TUPLE(Types.STRING,Types.INT)); KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = mapOperator.keyBy(0); //EventTime滚动窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(5))); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = windowedStream.sum(1); sum.print(); env.execute("EventTimeDemo"); &#125;&#125; 测试结果 结果分析备注： (1581490623000转换后为：2020-02-12 14:57:03​ 1581490624000转换后为：2020-02-12 14:57:04) 当我们在Socket中输入如下数据：1581490623000,Mary,21581490624000,John,31581490624500,Clerk,11581490624998,Maria,41581490624999,Mary,31581490626000,Mary,31581490630800,Steve,3 (2020-02-12 14:57:10.800) 窗口定义的时间是：含头不含尾。即：[0,5)，图片解析：(我们定义滚动窗口为5s，我们分析图片发现到4998时,并没有输出内容。因为4998还没超过5s，窗口规定是&gt;=临界值时触发，所以当我们输入4999临界时，我们发现输出内容了，说明一个窗口滚动完成，输出内容包含4999这个时间的值；当输入6000时，6000在[5,10)之间没有&gt;10，所以不输出。输入30800【2020-02-12 14:57:10.800)】，已经超过10s，所以结果只输出1个 (Mary,3)，因为Steve已经被分到另一个窗口了) 还有一个问题，就是：当输入到 4999 时，只是Mary这个分组满足5s这个条件，但是其它分组John，Clerk 等也同步输出结果了。显然这不符合逻辑。为什么会出现这种情况呢？是因为SocketStream 是非并行数据流，所以才会出现这样子的结果。(接下来我们就是用并行数据流KafkaSource来分析) 并行Source并行Source，以 KafkaSouce 为例来介绍 Flink使用 EventTime 处理实时数据。 代码 并行KafkaSource EventTime示例(读取 topic为 window_demo中的消息)，代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/*** TODO 并行KafkaSource EventTime示例(读取 topic为 window_demo中的消息)** @author liuzebiao* @Date 2020-2-12 15:25*/public class EventTimeDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置EventTime作为时间标准 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); //Kafka props Properties properties = new Properties(); //指定Kafka的Broker地址 properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.204.210:9092,192.168.204.211:9092,192.168.204.212:9092"); //指定组ID properties.put(ConsumerConfig.GROUP_ID_CONFIG, "flinkDemoGroup"); //如果没有记录偏移量，第一次从最开始消费 properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); FlinkKafkaConsumer&lt;String&gt; kafkaSource = new FlinkKafkaConsumer("window_demo", new SimpleStringSchema(), properties); //2.通过addSource()方式，创建 Kafka DataStream //读取source，并指定(1581490623000,Mary,3)中哪个字段为EventTime时间 SingleOutputStreamOperator&lt;String&gt; source = env.addSource(kafkaSource).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) &#123; @Override public long extractTimestamp(String line) &#123; String[] split = line.split(","); return Long.parseLong(split[0]); &#125; &#125;); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; mapOperator = source.map(line -&gt; &#123; String[] split = line.split(","); return Tuple2.of(split[1], Integer.parseInt(split[2])); &#125;).returns(Types.TUPLE(Types.STRING,Types.INT)); KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = mapOperator.keyBy(0); //EventTime滚动窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(5))); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = windowedStream.sum(1); sum.print(); env.execute("EventTimeDemo"); &#125;&#125; 测试结果 创建 Topic 命令如下： 12bin/kafka-topics.sh --create --zookeeper 192.168.204.210:2181,192.168.204.211:2181,192.168.204.212:2181 --replication-factor 1 --partitions 3 --topic window_demo# (特别注意一下：此处创建了3个分区) 创建 Topic 成功截图(点击放大查看)： 使用命令，写入数据到Kafka： 1bin/kafka-console-producer.sh --broker-list 192.168.204.210:9092 --topic window_demo 使用命令写入以下数据： 123451581490623000,Mary,21581490624000,John,31581490624500,Clerk,11581490624998,Maria,41581490624999,Mary,3 测试结果： 结果分析​ 在并行Source一例中，当我们输入1581490624999,Mary,3时，我们看到控制台会直接帮我们输出计算结果。 ​ 但是，在使用 KafkaSource 时，我们连续输入了 3次1581490624999,Mary,3，我们才看到控制台帮我们输出计算了结果。 ​ 那这是为什么呢？这是 并行Source 和 非并行Source 的原因导致的（这里涉及到 KafkaSource 创建的 topic，有 3 个分区的原因，如下图所示）]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm 总结]]></title>
    <url>%2F2020%2F04%2F28%2F21.%E6%80%BB%E7%BB%93%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FStorm%2F</url>
    <content type="text"><![CDATA[Storm架构 Nimbus：负责资源分配和任务调度。 Supervisor：负责接受nimbus分配的任务，启动和停止属于自己管理的worker进程。—通过配置文件设置当前supervisor上启动多少个worker。 Worker：运行具体处理组件逻辑的进程。Worker运行的任务类型只有两种，一种是Spout任务，一种是Bolt任务。 Task：worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，不同spout/bolt的task可能会共享一个物理线程，该线程称为executor。 Storm 编程模型 Topology：Storm中运行的一个实时应用程序的名称。（拓扑） Spout：在一个topology中获取源数据流的组件。 通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。 Bolt：接受数据然后执行处理的组件,用户可以在其中执行自己想要的操作。 Tuple：一次消息传递的基本单元，理解为一组消息就是一个Tuple。 StreamGrouping:数据分组策略， 一共有7种 Storm流式计算生命周期storm 执行流式数据处理函数的生命周期主要是通过Spout和Bolt 这两个组件的生命周期 Spout组件涉及到的方法有： declareOutputFields() (调用一次) open() (调用一次) activate() (调用一次) nextTuple() （循环调用 ） disactive() (调用一次) Bolt组件涉及到的方法有 declareOutputFileds() (调用一次) prepare() (调用一次) execute() （循环执行） 执行顺序？ 在客户端将jar包提交到集群上的时候，执行 spout 、bolt 的构造方法以及declareOutputFields（） spout执行open方法一次，得到conf一些配置信息 Bolt 中 prepare 方法只执行一次 Spout 中 nextTuple不断的循环执行 Blot 中 execute 不断的循环执行 Stream GroupingStream Grouping 的作用Storm 的Tuple 从 Spout 中 分发到 Bolt， 以及从Bolt 分发的Bolt 的过程中，Storm定义了一些内置的分发方法和规则， 这些规则通过一些条件或者随机将具有相同特征的tuple分发到同一Bolt中进行处理，这样有利于对不同特征的数据进行集中统计 下面介绍一些常用的Grouping 种类 Stream Grouping 种类 Shuffle Grouping: 随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。 Fields Grouping：按字段分组，比如按userid来分组，具有同样userid的tuple会被分到相同的Bolts里的一个task，而不同的userid则会被分配到不同的bolts里的task。 All Grouping：广播发送，对于每一个tuple，所有的bolts都会收到。 Global Grouping：全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。 Non Grouping：不分组，这stream grouping个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果，有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行， 即减少跨主机socket通讯。 Direct Grouping： 直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id （OutputCollector.emit方法也会返回task的id）。 Local or shuffle grouping：如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。 Storm 程序的并发机制概念 Workers (JVMs): 在一个物理节点上可以运行一个或多个独立的JVM 进程。一个Topology可以包含一个或多个worker(并行的跑在不同的物理机上), 所以worker process就是执行一个topology的子集, 并且worker只能对应于一个topology Executors (threads): 在一个worker JVM进程中运行着多个Java线程。一个executor线程可以执行一个或多个tasks。但一般默认每个executor只执行一个task。一个worker可以包含一个或多个executor, 每个component (spout或bolt)至少对应于一个executor, 所以可以说executor执行一个compenent的子集, 同时一个executor只能对应于一个component。 Tasks(bolt/spout instances)：Task就是具体的处理逻辑对象，每一个Spout和Bolt会被当作很多task在整个集群里面执行。每一个task对应到一个线程，而stream grouping则是定义怎么从一堆task发射tuple到另外一堆task。你可以调用TopologyBuilder.setSpout和TopBuilder.setBolt来设置并行度 — 也就是有多少个task。 配置并行度通过修改配置文件和代码配置冰心额度 对于并发度的配置, 在storm里面可以在多个地方进行配置, 优先级为： defaults.yaml &lt; storm.yaml &lt; topology-specific configuration &lt; internal component-specific configuration &lt; external component-specific configuration worker processes的数目, 可以通过配置文件和代码中配置, worker就是执行进程, 所以考虑并发的效果, 数目至少应该大于machines的数目 executor的数目, component的并发线程数，只能在代码中配置(通过setBolt和setSpout的参数), 例如, setBolt(“green-bolt”, new GreenBolt(), 2) tasks的数目, 可以不配置, 默认和executor1:1, 也可以通过setNumTasks()配置 Topology的worker数通过config设置，即执行该topology的worker（java）进程数。它可以通过 storm rebalance 命令任意调整。 123456Config conf = newConfig();conf.setNumWorkers(2); //用2个workertopologyBuilder.setSpout("blue-spout", newBlueSpout(), 2); //设置2个并发度topologyBuilder.setBolt("green-bolt", newGreenBolt(), 2).setNumTasks(4).shuffleGrouping("blue-spout"); //设置2个并发度，4个任务topologyBuilder.setBolt("yellow-bolt", newYellowBolt(), 6).shuffleGrouping("green-bolt"); //设置6个并发度StormSubmitter.submitTopology("mytopology", conf, topologyBuilder.createTopology()); 动态改变任务的并行度Storm支持在不 restart topology 的情况下, 动态的改变(增减) worker processes 的数目和 executors 的数目, 称为rebalancing. 通过Storm web UI，或者通过storm rebalance命令实现： 1storm rebalance mytopology -n 5 -e a-spout=3 -e b-bolt=10 Storm 的技术分析Storm的通讯技术通讯机制Worker间的通信经常需要通过网络跨节点进行，Storm使用ZeroMQ或Netty(0.9以后默认使用)作为进程间通信的消息框架。 Worker进程内部通信：不同worker的thread通信使用LMAX Disruptor来完成。 不同topologey之间的通信，Storm不负责，需要自己想办法实现，例如使用kafka等； Worker 进程间通讯机制worker进程间消息传递机制，消息的接收和处理的大概流程见下图 对于worker进程来说，为了管理流入和传出的消息，每个worker进程有一个独立的接收线程(对配置的TCP端口supervisor.slots.ports进行监听); 对应Worker接收线程，每个worker存在一个独立的发送线程，它负责从worker的transfer-queue中读取消息，并通过网络发送给其他worker 每个executor有自己的incoming-queue和outgoing-queue。 Worker接收线程将收到的消息通过task编号传递给对应的executor(一个或多个)的incoming-queues; 每个executor有单独的线程分别来处理spout/bolt的业务逻辑，业务逻辑输出的中间数据会存放在outgoing-queue中，当executor的outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到transfer-queue中。 每个worker进程控制一个或多个executor线程，用户可在代码中进行配置。其实就是我们在代码中设置的并发度个数。 Worker 进程间通讯机制分析 Worker接受线程通过网络接受数据，并根据Tuple中包含的taskId，匹配到对应的executor；然后根据executor找到对应的incoming-queue，将数据存发送到incoming-queue队列中。 业务逻辑执行现成消费incoming-queue的数据，通过调用Bolt的execute(xxxx)方法，将Tuple作为参数传输给用户自定义的方法 业务逻辑执行完毕之后，将计算的中间数据发送给outgoing-queue队列，当outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到Worker的transfer-queue中 Worker发送线程消费transfer-queue中数据，计算Tuple的目的地，连接不同的node+port将数据通过网络传输的方式传送给另一个的Worker。 另一个worker执行以上步骤1的操作。 Storm 任务提交过程过程示意图 详细任务提交过程 Storm 的容错机制总体介绍 在storm中，可靠的信息处理机制是从spout开始的。 一个提供了可靠的处理机制的spout需要记录他发射出去的tuple，当下游bolt处理tuple或者子tuple失败时spout能够重新发射。 Storm通过调用Spout的nextTuple()发送一个tuple。为实现可靠的消息处理，首先要给每个发出的tuple带上唯一的ID，并且将ID作为参数传递给SoputOutputCollector的emit()方法：collector.emit(new Values(“value1”,”value2”), msgId); messageid就是用来标示唯一的tuple的，而rootid是随机生成的 给每个tuple指定ID告诉Storm系统，无论处理成功还是失败，spout都要接收tuple树上所有节点返回的通知。如果处理成功，spout的ack()方法将会对编号是msgId的消息应答确认；如果处理失败或者超时，会调用fail()方法。 基本实现Storm 系统中有一组叫做”acker”的特殊的任务，它们负责跟踪DAG（有向无环图）中的每个消息。 acker任务保存了spout id到一对值的映射。第一个值就是spout的任务id，通过这个id，acker就知道消息处理完成时该通知哪个spout任务。第二个值是一个64bit的数字，我们称之为”ack val”， 它是树中所有消息的随机id的异或计算结果。 ack val表示了整棵树的的状态，无论这棵树多大，只需要这个固定大小的数字就可以跟踪整棵树。当消息被创建和被应答的时候都会有相同的消息id发送过来做异或。 每当acker发现一棵树的ack val值为0的时候，它就知道这棵树已经被完全处理了 可靠性配置有三种方法可以去掉消息的可靠性： 将参数Config.TOPOLOGY_ACKERS设置为0，通过此方法，当Spout发送一个消息的时候，它的ack方法将不会被调用; Spout发送一个消息时，不指定此消息的messageID。当需要关闭特定消息可靠性的时候，可以使用此方法； 最后，如果你不在意某个消息派生出来的子孙消息的可靠性，则此消息派生出来的子消息在发送时不要做锚定，即在emit方法中不指定输入消息。因为这些子孙消息没有被锚定在任何tuple tree中，因此他们的失败不会引起任何spout重新发送消息。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 总结]]></title>
    <url>%2F2020%2F04%2F28%2F21.%E6%80%BB%E7%BB%93%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FSpark%2F</url>
    <content type="text"><![CDATA[Spark 提交任务命令格式 123456spark-submit \--class $class \--master spark://$spark_master_node \--executor-memory $memory_use \--total-executor-cores $core_use \$jar [args] 命令示例 1234567/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node1.itcast.cn:7077 \--executor-memory 1G \--total-executor-cores 2 \/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-examples-1.5.2-hadoop2.6.0.jar \100 Spark RDD 的依赖关系RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。 窄依赖窄依赖是指一个父RDD的Partition最多被子RDD的一个Partition使用 宽依赖宽依赖是指多个RDD的partition会依赖同一个父RDD的Partition RDD的血统RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区 RDD的缓存缓存的作用Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。 缓存的方式RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。 缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。 DAG的行程DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 Spark 中的checkpointcheckpoint在spark中主要有两块应用：一块是在spark core中对RDD做checkpoint，可以将checkpoint RDD的依赖关系，RDD数据保存到可靠存储（如HDFS）以便数据恢复；另外一块是应用在spark streaming中，使用checkpoint用来保存DStreamGraph以及相关配置信息，以便在Driver崩溃重启的时候能够接着之前进度继续进行处理（如之前waiting batch的job会在重启后继续处理）。 本文主要将详细分析checkpoint在以上两种场景的读写过程。 checkpoint的使用方法使用checkpoint对RDD做快照大体如下： 1234&gt; sc.setCheckpointDir(checkpointDir.toString)&gt; val rdd = sc.makeRDD(1 to 20, numSlices = 1)&gt; rdd.checkpoint()&gt; 首先，设置checkpoint的目录（一般是hdfs目录），这个目录用来将RDD相关的数据（包括每个partition实际数据，以及partitioner（如果有的话））。然后在RDD上调用checkpoint的方法即可。 checkpoint写流程可以看到checkpoint使用非常简单，设置checkpoint目录，然后调用RDD的checkpoint方法。针对checkpoint的写入流程，主要有以下四个问题： Q1：RDD中的数据是什么时候写入的？是在rdd调用checkpoint方法时候吗？ Q2：在做checkpoint的时候，具体写入了哪些数据到HDFS了？ Q3：在对RDD做完checkpoint以后，对做RDD的本省又做了哪些收尾工作？ Q4：实际过程中，使用RDD做checkpoint的时候需要注意什么问题？ 弄清楚了以上四个问题，我想对checkpoint的写过程也就基本清楚了。接下来将一一回答上面提出的问题。 A1：首先看一下RDD中checkpoint方法，可以看到在该方法中是只是新建了一个ReliableRDDCheckpintData的对象，并没有做实际的写入工作。实际触发写入的时机是在runJob生成该RDD后，调用RDD的doCheckpoint方法来做的。 A2：在经历调用RDD.doCheckpoint → RDDCheckpintData.checkpoint → ReliableRDDCheckpintData.doCheckpoint → ReliableRDDCheckpintData.writeRDDToCheckpointDirectory后，在writeRDDToCheckpointDirectory方法中可以看到：将作为一个单独的任务（RunJob）将RDD中每个parition的数据依次写入到checkpoint目录（writePartitionToCheckpointFile），此外如果该RDD中的partitioner如果不为空，则也会将该对象序列化后存储到checkpoint目录。所以，在做checkpoint的时候，写入的hdfs中的数据主要包括：RDD中每个parition的实际数据，以及可能的partitioner对象（writePartitionerToCheckpointDir）。 A3：在写完checkpoint数据到hdfs以后，将会调用rdd的markCheckpoined方法，主要斩断该rdd的对上游的依赖，以及将paritions置空等操作。 A4：通过A1，A2可以知道，在RDD计算完毕后，会再次通过RunJob将每个partition数据保存到HDFS。这样RDD将会计算两次，所以为了避免此类情况，最好将RDD进行cache。即1.1中rdd的推荐使用方法如下： 12345&gt; sc.setCheckpointDir(checkpointDir.toString)&gt; val rdd = sc.makeRDD(1 to 20, numSlices = 1)&gt; rdd.cache()&gt; rdd.checkpoint()&gt; checkpoint 读流程在做完checkpoint后，获取原来RDD的依赖以及partitions数据都将从CheckpointRDD中获取。也就是说获取原来rdd中每个partition数据以及partitioner等对象，都将转移到CheckPointRDD中。 在CheckPointRDD的一个具体实现ReliableRDDCheckpintRDD中的compute方法中可以看到，将会从hdfs的checkpoint目录中恢复之前写入的partition数据。而partitioner对象（如果有）也会从之前写入hdfs的paritioner对象恢复。 总的来说，checkpoint读取过程是比较简单的。 RDD分区规则 通过集合方式指定 通过scala 集合方式parallelize生成rdd， 如， val rdd = sc.parallelize(1 to 10) 这种方式下，如果在parallelize操作时没有指定分区数，则 rdd的分区数 = sc.defaultParallelism textFile 分区规则 1.如果textFile指定分区数量为0或者1的话，defaultMinPartitions值为1，则有多少个文件，就会有多少个分区。 2.如果不指定默认分区数量，则默认分区数量为2，则会根据所有文件字节大小totalSize除以分区数量partitons的值goalSize，然后比较goalSize和hdfs指定分块大小（这里是32M）作比较，以较小的最为goalSize作为切分大小，对每个文件进行切分，若文件大于大于goalSize，则会生成该文件大小/goalSize + 1个分区。 3.如果指定分区数量大于等于2，则默认分区数量为指定值，生成分区数量规则同2中的规则。 Spark 任务提交任务提交的主要四个阶段DAG的生成 =&gt; stage切分 =&gt; task的生成 =&gt; 任务提交 构建DAG用户提交的job将首先被转换成一系列RDD并通过RDD之间的依赖关系构建DAG,然后将DAG提交到调度系统； DAGScheduler将DAG切分stage（切分依据是shuffle）,将stage中生成的task以taskset的形式发送给TaskScheduler Scheduler 调度task（根据资源情况将task调度到Executors） Executors接收task，然后将task交给线程池执行。 任务提交详细步骤 spark集群启动后，Worker向Master注册信息 spark-submit命令提交程序后，driver和application也会向Master注册信息 创建SparkContext对象：主要的对象包含DAGScheduler和TaskScheduler Driver把Application信息注册给Master后，Master会根据App信息去Worker节点启动Executor Executor内部会创建运行task的线程池，然后把启动的Executor反向注册给Dirver DAGScheduler：负责把Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；同时DAGScheduler还会处理由于Shuffle数据丢失导致的失败； TaskScheduler：维护所有TaskSet，分发Task给各个节点的Executor（根据数据本地化策略分发Task），监控task的运行状态，负责重试失败的task； 所有task运行完成后，SparkContext向Master注销，释放资源； Spark stage 切分流程 划分stage 的思路park划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。 stage 作用shuffle个复杂是业务逻辑（将多台机器上具有相同属性的数据聚合到一台机器上），如果有shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段(下一个阶段的计算依赖上一个阶段的数据在同一个stage中); 在每一次shuffle之前会有多个算子，可以合并到一起执行，我们可以称这样的一个执行流程为pipeline（流水线，严格按照流程、顺序执行），整个阶段就是stage； Spark Driver 给Executor 提交task 时序图 Spark Executor 启动和任务接受和执行时序图 Spark ShuffleHashShuffle机制HashShuffle概述在spark-1.6版本之前，一直使用HashShuffle，在spark-1.6版本之后使用Sort-Base Shuffle，因为HashShuffle存在的不足所以就替换了HashShuffle. 我们知道，Spark的运行主要分为2部分：一部分是驱动程序，其核心是SparkContext；另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle。 没有优化之前的HashShuffle机制 在HashShuffle没有优化之前，每一个ShufflleMapTask会为每一个ReduceTask创建一个bucket缓存，并且会为每一个bucket创建一个文件。这个bucket存放的数据就是经过Partitioner操作(默认是HashPartitioner)之后找到对应的bucket然后放进去，最后将数据刷新bucket缓存的数据到磁盘上，即对应的block file. 然后ShuffleMapTask将输出作为MapStatus发送到DAGScheduler的MapOutputTrackerMaster，每一个MapStatus包含了每一个ResultTask要拉取的数据的位置和大小 ResultTask然后去利用BlockStoreShuffleFetcher向MapOutputTrackerMaster获取MapStatus，看哪一份数据是属于自己的，然后底层通过BlockManager将数据拉取过来 拉取过来的数据会组成一个内部的ShuffleRDD，优先放入内存，内存不够用则放入磁盘，然后ResulTask开始进行聚合，最后生成我们希望获取的那个MapPartitionRDD 这种方式的缺点 如上图所示：在这里有1个worker，2个executor，每一个executor运行2个ShuffleMapTask，有三个ReduceTask，所以总共就有4 * 3=12个bucket和12个block file。 如果数据量较大，将会生成MR个小文件，比如ShuffleMapTask有100个，ResultTask有100个，这就会产生100\100=10000个小文件 bucket缓存很重要，需要将ShuffleMapTask所有数据都写入bucket，才会刷到磁盘，那么如果Map端数据过多，这就很容易造成内存溢出，尽管后面有优化，bucket写入的数据达到刷新到磁盘的阀值之后，就会将数据一点一点的刷新到磁盘，但是这样磁盘I/O就多了 优化后的HashShuffle 每一个Executor进程根据核数，决定Task的并发数量，比如executor核数是2，就是可以并发运行两个task，如果是一个则只能运行一个task 假设executor核数是1，ShuffleMapTask数量是M,那么它依然会根据ResultTask的数量R，创建R个bucket缓存，然后对key进行hash，数据进入不同的bucket中，每一个bucket对应着一个block file,用于刷新bucket缓存里的数据 然后下一个task运行的时候，那么不会再创建新的bucket和block file，而是复用之前的task已经创建好的bucket和block file。即所谓同一个Executor进程里所有Task都会把相同的key放入相同的bucket缓冲区中 这样的话，生成文件的数量就是(本地worker的executor数量*executor的cores*ResultTask数量)如上图所示，即2 * 1* 3 = 6个文件，每一个Executor的shuffleMapTask数量100,ReduceTask数量为100，那么 未优化的HashShuffle的文件数是2 *1* 100*100 =20000，优化之后的数量是2*1*100 = 200文件，相当于少了100倍 这种方式的缺点： 如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。 Sort-Based ShuffleSort-Based Shuffle概述HashShuffle回顾 io\GC\内存占用大: HashShuffle写数据的时候，内存有一个bucket缓冲区，同时在本地磁盘有对应的本地文件，如果本地有文件，那么在内存应该也有文件句柄也是需要耗费内存的。也就是说，从内存的角度考虑，即有一部分存储数据，一部分管理文件句柄。如果Mapper分片数量为1000,Reduce分片数量为1000,那么总共就需要1000000个小文件。所以就会有很多内存消耗，频繁IO以及GC频繁或者出现内存溢出。 容易造成网络异常: 而且Reducer端读取Map端数据时，Mapper有这么多小文件，就需要打开很多网络通道读取，很容易造成Reducer（下一个stage）通过driver去拉取上一个stage数据的时候，说文件找不到，其实不是文件找不到而是程序不响应，因为正在GC. Sorted-Based Shuffle介绍为了缓解Shuffle过程产生文件数过多和Writer缓存开销过大的问题，spark引入了类似于hadoop Map-Reduce的shuffle机制。该机制每一个ShuffleMapTask不会为后续的任务创建单独的文件，而是会将所有的Task结果写入同一个文件，并且对应生成一个索引文件。以前的数据是放在内存缓存中，等到数据完了再刷到磁盘，现在为了减少内存的使用，在内存不够用的时候，可以将输出溢写到磁盘，结束的时候，再将这些不同的文件联合内存的数据一起进行归并，从而减少内存的使用量。一方面文件数量显著减少，另一方面减少Writer缓存所占用的内存大小，而且同时避免GC的风险和频率。 Sort-Based Shuffle有几种不同的策略：BypassMergeSortShuffleWriter、SortShuffleWriter和UnasfeSortShuffleWriter。 对于BypassMergeSortShuffleWriter 使用这个模式特点： 主要用于处理不需要排序和聚合的Shuffle操作，所以数据是直接写入文件，数据量较大的时候，网络I/O和内存负担较重 主要适合处理Reducer任务数量比较少的情况下 将每一个分区写入一个单独的文件，最后将这些文件合并,减少文件数量；但是这种方式需要并发打开多个文件，对内存消耗比较大 因为BypassMergeSortShuffleWriter这种方式比SortShuffleWriter更快，所以如果在Reducer数量不大，又不需要在map端聚合和排序，而且最终产生的文件数量少，尽量使用这种方式进行shuffle。 Reducer的数目 &lt; spark.shuffle.sort.bypassMergeThrshold指定的阀值，就是用的是这种方式。 对于SortShuffleWriter 使用这个模式特点： 比较适合数据量很大的场景或者集群规模很大 引入了外部外部排序器，可以支持在Map端进行本地聚合或者不聚合 如果外部排序器enable了spill功能，如果内存不够，可以先将输出溢写到本地磁盘，最后将内存结果和本地磁盘的溢写文件进行合并 另外这个Sort-Based Shuffle跟Executor核数没有关系，即跟并发度没有关系，它是每一个ShuffleMapTask都会产生一个data文件和index文件，所谓合并也只是将该ShuffleMapTask的各个partition对应的分区文件合并到data文件而已。所以这个就需要个Hash-BasedShuffle的consolidation机制区别开来。 Spark 谓词下推1.SparkSqlSparkSql 是架构在 Spark 计算框架之上的分布式 Sql 引擎，使用 DataFrame 和 DataSet 承载结构化和半结构化数据来实现数据复杂查询处理，提供的 DSL可以直接使用 scala 语言完成 Sql 查询，同时也使用 thriftserver 提供服务化的 Sql 查询功能。 SparkSql 提供了 DataSource API ，用户通过这套 API 可以自己开发一套 Connector，直接查询各类数据源，数据源包括 NoSql、RDBMS、搜索引擎以及 HDFS 等分布式文件系统上的文件等。和 SparkSql 类似的系统有 Hive、PrestoDB 以及 Impala，这类系统都属于所谓的” Sql on Hadoop “系统,每个都相当火爆，毕竟在这个不搞 SQL 就是耍流氓的年代，没 SQL 确实很难找到用户使用。 2.谓词下推解释所谓谓词(predicate)，就是返回值是true或者false的函数，使用过scala或者spark的同学都知道有个filter方法，这个高阶函数传入的参数就是一个返回true或者false的函数。 但是如果是在sql语言中，没有方法，只有表达式。where后边的表达式起的作用正是过滤的作用，而这部分语句被sql层解析处理后，在数据库内部正是以谓词的形式呈现的。 那么问题来了，谓词为什么要下推呢? SparkSql中的谓词下推有两层含义，第一层含义是指由谁来完成数据过滤，第二层含义是指何时完成数据过滤。要解答这两个问题我们需要了解SparkSql的Sql语句处理逻辑，大致可以把SparkSql中的查询处理流程做如下的划分： SparkSql首先会对输入的Sql语句进行一系列的分析(Analyse)，包括词法解析(可以理解为搜索引擎中的分词这个过程)、语法分析以及语义分析(例如判断database或者table是否存在、group by必须和聚合函数结合等规则)；之后是执行计划的生成，包括逻辑计划和物理计划。其中在逻辑计划阶段会有很多的优化，对谓词的处理就在这个阶段完成；而物理计划则是RDD的DAG图的生成过程；这两步完成之后则是具体的执行了(也就是各种重量级的计算逻辑，例如join、groupby、filter以及distinct等)，这就会有各种物理操作符(RDD的Transformation)的乱入。 能够完成数据过滤的主体有两个，第一是分布式Sql层(在execute阶段)，第二个是数据源。那么谓词下推的第一层含义就是指由Sql层的Filter操作符来完成过滤，还是由Scan操作符在扫描阶段完成过滤。 上边提到，我们可以通过封装SparkSql的Data Source API完成各类数据源的查询，那么如果底层数据源无法高效完成数据的过滤，就会执行全局扫描，把每条相关的数据都交给SparkSql的Filter操作符完成过滤，虽然SparkSql使用的Code Generation技术极大的提高了数据过滤的效率，但是这个过程无法避免大量数据的磁盘读取，甚至在某些情况下会涉及网络IO(例如数据非本地化存储时)；如果底层数据源在进行扫描时能非常快速的完成数据的过滤，那么就会把过滤交给底层数据源来完成（至于哪些数据源能高效完成数据的过滤以及SparkSql又是如何完成高效数据过滤的则不是本文讨论的重点，会在其他系列的文章中介绍）。 那么谓词下推第二层含义，即何时完成数据过滤则一般是在指连接查询中，是先对单表数据进行过滤再和其他表连接还是在先把多表进行连接再对连接后的临时表进行过滤，则是本系列文章要分析和讨论的重点。 3. 谓词下推具体含义基本概念：谓词下推（predicate pushdown）属于逻辑优化。优化器可以将谓词过滤下推到数据源，从而使物理执行跳过无关数据。在使用Parquet或者orcfile的情况下，更可能存在文件被整块跳过的情况，同时系统还通过字典编码把字符串对比转换为开销更小的整数对比。 说白了，就是把查询相关的条件下推到数据源进行提前的过滤操作，之所以这里说是查询相关的条件，而不直接说是where 后的条件，是因为sql语句中除了where后的有条件外，join时也有条件。本文讨论的主要就是join时的条件的处理。 Spark常见问题Spark 数据块有多少种不同的存储方式 RDD数据块：用来存储所缓存的RDD数据。 Shuffle数据块：用来存储持久化的Shuffle数据。 广播变量数据块：用来存储所存储的广播变量数据。 任务返回结果数据块：用来存储在存储管理模块内部的任务返回结果。通常情况下任务返回结果随任务一起通过Akka返回到Driver端。但是当任务返回结果很大时，会引起Akka帧溢出，这时的另一种方案是将返回结果以块的形式放入存储管理模块，然后在Driver端获取该数据块即可，因为存储管理模块内部数据块的传输是通过Socket连接的，因此就不会出现Akka帧溢出了。流式数据块：只用在Spark Streaming中，用来存储所接收到的流式数据块 流式数据块：只用在Spark Streaming中，用来存储所接收到的流式数据块 哪些spark算子会有shuffle 去重，distinct 排序，groupByKey，reduceByKey等 重分区，repartition，coalesce 集合或者表操作，interection，join Cache和persist有什么区别和联系？cache调用了persist方法，cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。 RDD是弹性数据集，“弹性”体现在哪里呢？你觉得RDD有哪些缺陷自动进行内存和磁盘切换基于lineage的高效容错task如果失败会特定次数的重试stage如果失败会自动进行特定次数的重试，而且只会只计算失败的分片checkpoint【每次对RDD操作都会产生新的RDD，如果链条比较长，计算比较笨重，就把数据放在硬盘中】和persist 【内存或磁盘中对数据进行复用】(检查点、持久化)数据调度弹性：DAG TASK 和资源管理无关数据分片的高度弹性repartion缺陷： 惰性计算的缺陷也是明显的：中间数据默认不会保存，每次动作操作都会对数据重复计算，某些计算量比较大的操作可能会影响到系统的运算效率 RDD有多少种持久化方式？memory_only如果内存存储不了，会怎么操作？cache和persistmemory_and_disk，放一部分到磁盘MEMORY_ONLY_SER:同MEMORY_ONLY，但是会使用Java序列化方式，将Java对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大CPU开销。MEMORY_AND_DSK_SER:同MEMORY_AND_DSK。但是使用序列化方式持久化Java对象。DISK_ONLY:使用非序列化Java对象的方式持久化，完全存储到磁盘上。MEMORY_ONLY_2或者MEMORY_AND_DISK_2等：如果是尾部加了2的持久化级别，表示会将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可。 当GC时间占比很大可能的原因有哪些？对应的优化方法是？垃圾回收的开销和对象合数成正比，所以减少对象的个数，就能大大减少垃圾回收的开销。序列化存储数据，每个RDD就是一个对象。缓存RDD占用的内存可能跟工作所需的内存打架，需要控制好 Spark中repartition和coalesce异同？coalesce什么时候效果更高，为什么repartition(numPartitions:Int):RDD[T]coalesce(numPartitions:Int, shuffle:Boolean=false):RDD[T] 以上为他们的定义，区别就是repartition一定会触发shuffle，而coalesce默认是不触发shuffle的。 他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区） 减少分区提高效率 Groupbykey和reducebykey哪个性能更高，为什么？reduceByKey性能高，更适合大数据集 Scala里trait有什么功能，与class有何异同？什么时候用trait什么时候该用class它可以被继承，而且支持多重继承，其实它更像我们熟悉的接口（interface），但它与接口又有不同之处是：trait中可以写方法的实现，interface不可以（java8开始支持接口中允许写方法实现代码了），这样看起来trait又很像抽象类]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive]]></title>
    <url>%2F2020%2F04%2F28%2F21.%E6%80%BB%E7%BB%93%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHive%2F</url>
    <content type="text"><![CDATA[Hive 的数据模型 db：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在hdfs中表现所属db目录下一个文件夹 external table：外部表, 与table类似，不过其数据存放位置可以在任意指定路径 普通表: 删除表后, hdfs上的文件都删了 External外部表: 删除后, hdfs上的文件没有删除, 只是把文件删除了 partition：在hdfs中表现为table目录下的子目录 bucket：桶, 在hdfs中表现为同一个表目录下根据hash散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中 Hive 架构图 用户接口: CLI、JDBC/ODBC、WebGUI CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。 元数据存储：一般存储在关系型数据库中如 mysql； Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。 解释器、编译器、优化器、执行器 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。 Hive基本操作创建表建表语句 123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION） LIKE 允许用户复制现有的表结构，但是不复制数据。 ROW FORMAT DELIMITED [FIELDS TERMINATED BY char][COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char][LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)] STORED AS SEQUENCEFILE|TEXTFILE|RCFILE 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE(默认)。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 CLUSTERED BY 对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 把表（或者分区）组织成桶（Bucket）有两个理由： （1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。 （2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。 备注: hive 的join操作只支持等值连接 建表示例 1create table student(id int, age int, name string) partitioned by(stat_date string) clustered by(id) sorted by(age) into 2 buckets row format delimited fields terminated by','; 增加分区示例 1alter table student_p add partition(part='a') partition(part='b'); 数据查询查询语句 123456SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference[WHERE where_condition] [GROUP BY col_list [HAVING condition]] [ [CLUSTER BY col_list] | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list] ] [LIMIT number] order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。 根据distribute by指定的字段内容将数据分到同一个reducer。 Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by 查询示例 12set mapred.reduce.tasks=4;select id, age, name from stuent distribute by age sort by age desc; Hive Join操作1. 语法结构 123table1 JOIN table2 [join_condition] | table1 &#123;LEFT|RIGHT|FULL&#125; [OUTER] JOIN table2 join_condition | table1 LEFT SEMI JOIN table2 join_condition 备注: Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。另外，Hive 支持多于 2 个表的连接。 left semi join 是 IN/EXISTS 的高效实现 等值join 等值join 12345SELECT a.* FROM a JOIN b ON (a.id = b.id)；SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)；# 是正确的，然而:SELECT a.* FROM a JOIN b ON (a.id&gt;b.id)# 是错误的。 可以多于两张表 123456789SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);# 如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：SELECT a.val, b.val, c.val FROM a JOIN bON (a.key = b.key1) JOIN cON (c.key = b.key1)# 被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)JOIN c ON (c.key = b.key2)# 而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。 join时，每次map/reduce任务的逻辑 reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如： 12SELECT a.val, b.val, c.val FROM aJOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记 录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有： 12SELECT a.val, b.val, c.val FROM aJOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) 这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。 LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况 12SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key) 对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:a.val, NULL 所以 a 表中的所有记录都被保留了，a RIGHT OUTER JOIN b”会保留所有 b 表的记录。 Join 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况： 123SELECT a.val, b.val FROM aLEFT OUTER JOIN b ON (a.key=b.key)WHERE a.ds='2009-07-07' AND b.ds='2009-07-07' 会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法： 1234SELECT a.val, b.val FROM a LEFT OUTER JOIN bON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07') 这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。 Join 是不能交换位置的。 无论是 LEFT 还是 RIGHT join，都是左连接的。 123SELECT a.val1, a.val2, b.val, c.valFROM aJOIN b ON (a.key = b.key)LEFT OUTER JOIN c ON (a.key = c.key) 先 join a 表到 b 表，丢弃掉所有 join key 中不匹配的记录，然后用这一中间结果和 c 表做 join。这一表述有一个不太明显的问题，就是当一个 key 在 a 表和 c 表都存在，但是 b 表中不存在的时候：整个记录在第一次 join，即 a JOIN b 的时候都被丢掉了（包括a.val1，a.val2和a.key），然后我们再和 c 表 join 的时候，如果 c.key 与 a.key 或 b.key 相等，就会得到这样的结果：NULL, NULL, NULL, c.val Hive 自定义函数、Transform当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 自定义函数分类 UDF 作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数） UDAF（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（count，max） UDF开发步骤 先开发一个java类，继承UDF， 并重载evaluate方法 12345678910package cn.itcast.bigdata.udfimport org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public final class Lower extends UDF&#123; public Text evaluate(final Text s)&#123; if(s==null)&#123;return null;&#125; return new Text(s.toString().toLowerCase()); &#125;&#125; 打成jar包上传到服务器 将jar包添加到hive的classpath 1hive&gt;add JAR /home/hadoop/udf.jar; 创建临时函数与开发好的java class关联 1Hive&gt;create temporary function toprovince as 'cn.itcast.bigdata.udf.ToProvince'; 即可在hql中使用自定义函数strip 1Select strip(name),age from t_test; Transform实现步骤Hive的 TRANSFORM 关键字提供了在SQL中调用自写脚本的功能 适合实现Hive中没有的功能又不想写UDF的情况 编辑weekday_mapper.py内容 123456789#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split('\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\t'.join([movieid, rating, str(weekday),userid]) 添加transform方法 1hive&gt;add FILE weekday_mapper.py; 使用transform进行查询 123456INSERT OVERWRITE TABLE u_data_newSELECT TRANSFORM (movieid, rating, unixtime,userid) USING 'python weekday_mapper.py' AS (movieid, rating, weekday,userid)FROM u_data; Hive 常问问题hive 内部表和外部表区别 Hive 创建内部表时，会将数据移动到数据仓库指向的路径，hive 管理数据的声明周期，若创建外部表，仅记录数据所在的路径，不会对数据的位置做任何改变。 在删除表的时候，内部表的元数据和数据会被一起删除， 外部表只会删除外部表的元数据， 不删除数据，这样外部表相对来说更安全一些，数据组织也更加灵活，方便共享源数据。； 对于一些 hive 操作不适应于外部表，比如单个查询语句创建表并向表中插入数据。 创建外部表时甚至不需要知道外部数据是否存在，可以把创建数据推迟到创建表之后才进行。 hive和传统数据库区别 需要注意的是传统数据库对表数据检验是 schema on write（写时模式）而 Hive 在 load时不检查数据是否符合 schema 的，hive 遵循的是 schema on read（读时模式），只有在读的时候 hive 才检查、解析具体的数据字段、schema。读时模式的优势是 load data 非常迅速，因为它不需要读取数据进行解析，仅仅进行文件的复制或者移动。写时模式的优势是提升了查询性能，因为预先解析之后可以对列建立索引并压缩，但这样会花费较多的加载时间。即使为内部表在数据加载时也不解析数据格式，如果数据和模式不匹配，只能在查询时出现 null 才知道不匹配的行。 hive 具有复杂的数据结构（数组，映射，结构体） hive 不支持实时数据处理，对索引的支持较弱。 hive 不支持行级的插入。 延迟高，数据量大，多存储在 hdfs 上。 执行为 mapreducer。 hive 不支持行级操作也不支持事务。 hive 分区的了解是对 hive 表的一种组织形式，可以加快查询，是一种对表进行粗略划分的机制。使用分区时，在表目录下会有相应的子目录，当查询时若添加了分区谓词，该查询会定位到相应的子目录中进行查询，避免全表扫描，比如日志文件分析，将日志按天存储。分区并不会影响大范围的查询。 外部表也可以分区，具有很好的灵活性，例如这种灵活性有一个有趣的优点是我们可以使用像 Amazon S3 这样的廉价的存储设备存储旧的数据，同时保持较新的数据到 HDFS 中。例如，每天我们可以使用如下的处理过程将一个月前的旧数据转移到 S3 中。 使用hive分区的注意事项 当分区过多且数据很大时，可以使用严格模式，避免触发一个大的 mapreducer 任务。当分区数量过多且数据量较大时，执行宽范围的数据扫描会触发一个很大的 mapreducer 任务，在严格模式下，当 where 中没有分区过滤条件时会禁止执行。 hive 如果有过多的分区，由于底层是存储在 HDFS 上，HDFS适用于存储大文件而非小文件，因此过多的分区会增加 namenode 负担。 hive 会转化成 mapreducer，mapreducer 会转化为多个 task，过多的小文件的话，每个文件一个task，每个task 一个 jvm实例，jvm 开启与销毁会降低系统效率。 hive中复杂数据类型的好处和换粗 好处：由于复杂数据类型，存储数据比基本数据类型要多，在存盘上存储可以连续存储，在查询等操作时可以减少磁盘 IO； 坏处：复杂数据类型可能会存在着数据的重复，而且有更大的导致数据不一致的风险。 Hive 分桶的了解桶是更细粒度的划分，把表划分成桶的理由： 取样更高效。具体划分桶是将值进行hash，然后除以桶的个数取余，任何一个桶内部是一个随机划分的用户集合。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分上试运行查询，会方便很多。 获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接（Map-side join）高效的实现。处理左边表的某个桶的 mapper 就知道右边表内相匹配的行在对应的桶，这样 mapper 直接就可以在对应的右边表的桶获取数据进行 join。并不一定要求两个表必须有相同的桶的个数，倍数也行。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HADOOP 总结]]></title>
    <url>%2F2020%2F04%2F27%2F21.%E6%80%BB%E7%BB%93%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHADOOP%2F</url>
    <content type="text"><![CDATA[HDFS 工作机制介绍 HDFS集群分为两大角色：NameNode、DataNode (Secondary Namenode) NameNode负责管理整个文件系统的元数据 DataNode 负责管理用户的文件数据块 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上 每一个文件块可以有多个副本，并存放在不同的datanode上 Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量 HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行 HDFS 写数据流程 客户端与namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在 namenode返回是否可以上传 client请求第一个 block该传输到哪些datanode服务器上 namenode返回可以上传的节点, 示例3个datanode服务器ABC client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将整个pipeline建立完成，逐级返回客户端 client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位(chunk为校验单位)，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 当一个block传输完成之后(只要有一个节点上传成功，就算成功)，client再次请求namenode上传第二个block的服务器。 HDFS 读数据流程 client跟namenode通信查询元数据，找到文件块所在的datanode服务器 cilent挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流 datanode开始发送数据（从磁盘里面读取数据放入流，以packet为传输单位，chunk为校验单位） 客户端以packet为单位接收，先在本地缓存，然后写入目标文件 Namenode 工作内容职责 负责客户端请求的响应 元数据的管理（查询，修改） 元数据管理 内存元数据(NameSystem) 磁盘元数据镜像文件(fsimage) 数据操作日志文件(edits文件，可以通过日志运算出元数据) 元数据存储机制 内存中有一份完整的原数据(内存metadate) 磁盘中有一个”准完整”的原数据镜像(fsimage)文件(在namenode的工作目录中) 用于衔接metadata和持久化元数据镜像的fsimage之间的操作日志(edits文件), 当客户端对hdfs中的文件进行新增或者修改操作，操作记录会首先被记录到edits日志文件中，当客户端操作成功后，相应的原数据会更新到内存meta.data中， 并且每隔一定的间隔hdfs会将当前的metadata同步到fsimage镜像文件中 元数据checkpoint机制由于在数据备份的时候会占用计算资源，所以为了减轻namenode的负载，通常可以将数据备份的工作交给另外一个专门用来做数据备份的namenode–&gt; sencondary namenode 每隔一段时间，会由secondary namenode 将namenode上积累的所有edits和一个最新的fsimage下载到本地(只有第一次merge才会下载fsimage)，并加载到内存进行merge(这个过程称之为checkpoint) namenode的一些情况namenode如果宕机，hdfs是否还能正常提供服务 不能，secondarynamenode虽然有元数据信息，但是不能更新元数据， 不能充当namenode使用 如果namenode的硬盘损坏，元数据是否能回复，能恢复多少? 可以恢复最后一次merge之前的数据， 只需要将secondarynamenode的数据目录替换成namenode的数据目录 配置namenode的工作目录时，有哪些可以注意的事项 可以将namenode的元数据保存到多块物理磁盘上例如如下的namenode配置 元数据目录文件介绍VEERSION文件VERSION文件是Java属性文件，内容大致如下： 1234567#Fri Nov 15 19:47:46 CST 2013namespaceID=934548976clusterID=CID-cdff7d73-93cd-4783-9399-0a22e6dce196cTime=0storageType=NAME_NODEblockpoolID=BP-893790215-192.168.24.72-1383809616115layoutVersion=-47 namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的； storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）； cTime 属性标记了 namenode 存储系统的创建时间（即FSImage对象），对于刚刚格式化的存储系统，这个属性为 0； 但是在文件系统升级之后，该值会更新到新的时间戳。 layoutVersion：表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用 clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它；如下说明 12345# 使用如下命令格式化一个Namenode：选择一个唯一的cluster_id，并且这个cluster_id不能与环境中其他集群有冲突。如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。hadoop namenode -format -clusterId &lt;cluster_id&gt;# 升级集群至最新版本。在升级过程中需要提供一个ClusterID，如果没有提供ClusterID，则会自动生成一个ClusterID。hadoop start namenode --config $HADOOP_CONF_DIR -upgrade -clusterId &lt;cluster_ID&gt; blockpoolID是针对每一个Namespace所对应的blockpool的ID，上面的这个BP-893790215-192.168.24.72-1383809616115就是在我的ns1的namespace下的存储块池的ID，这个ID包括了其对应的NameNode节点的ip地址。 seen_txid文件 是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。 文件中记录的是edits滚动的序号，每次重启namenode时，namenode就知道要将哪些edits进行加载。 fsimage文件和edits文件 fsimage: 元数据的镜像文件 edits: 元数据的滚动日志文件，每次merge之后会对之前的日志文件进行清除 MAPREDUCE 的运行实例 MRAppMaster: 负责整个程序的过程调度以及状态协调 mapTask: 负责map阶段的整个数据处理流程 ReduceTask: 负责reduce阶段的整个数据处理流程 YARN的角色 yarn中的主管角色叫ResourceManager yarn中具体提供运算资源的角色叫NodeManager MR运行任务的流程 一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程 maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为： a. 利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对 b. 将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存 c. 将缓存中的KV对按照K分区排序后不断溢写到磁盘文件 MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区） Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储 MapReduce运行原理图 MAPREDUCE的shuffle机制shuffle机制概述 mapreduce中，map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle(如上图绿色框部分)； shuffle: 洗牌、发牌 ——（核心机制：数据分区，排序，缓存）； 具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区、合并和排序； shuffle主要流程 shuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个mapTask和reduceTask几点上完成的，主要有3个动作: 将结果分区 根据key进行排序 使用Combiner进行局部排序 在整个流程中有几个关键的步骤: 对缓冲区输出的结果进行分区和排序， 数据的分区和排序最开始行程的位置 对碎片数据的合并，包含第一次从缓冲区经过排序分区的结果，以及每个taskMap输出的结果，并且都会经过combine整合 对不同mapTask的分区结果进行整合 shuffle的详细流程 maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序 reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快 缓冲区的大小可以通过参数调整, 参数：io.sort.mb 默认100M Mapreduce 一些问题 自定义分区: 自定义Partitioner类(继承partitioner) 对map的中间结果进行合并: 自定义Combine(继承Reducer)类 对自定义Bean进行分组: 自定义GroupingComparator继承(WritableComparator), 实现Compare方法 实现灵活的文件读取和文件输出，通过自定义inputformat和outputformat 全局计数，通过context.getCounter()]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据工具]]></title>
    <url>%2F2020%2F04%2F24%2F21.%E6%80%BB%E7%BB%93%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[ZookeeperZookeeper选举机制(全新集群)假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动； 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态. 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader. 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.服务器5启动,同4一样,当小弟. Zookeeper选举机制(运行中集群)运行中集群的选举机制需要加入数据id、leader id和逻辑时钟的概念。 数据id：数据新的id就大，数据每次更新都会更新id。 Leader id：就是我们配置的myid中的值，每个机器一个。 逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说: 如果在同一次选举中,那么这个值应该是一致的 ; 逻辑时钟值越大,说明这一次选举leader的进程更新. 选举的标准就变成： ​ 1、逻辑时钟小的选举结果被忽略，重新投票 ​ 2、统一逻辑时钟后，数据id大的胜出 ​ 3、数据id相同的情况下，leader id大的胜出 zookeeper 节点类型znode的两种类型 短暂(ephemeral) : 客户端断开后节点自动删除，节点不能有子节点持久(persistent)：客户端断开后不删除节点znode四种目录形式(默认persistent) persistent : 持久节点persistent_sequential: 持久序列节点，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护(例: /test0000000019)，ephemeral: 短暂节点ephemera_sequential: 短暂序列节点序列节点的作用 在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 FlumeFlume 结构 azkabanazkaban 结构 azkaban web azkaban executor azkaban job (job 之间可以有依赖关系) SqoopSqoop 的工作机制 将导入或导出命令翻译成mapreduce程序来实现 在翻译出的mapreduce中主要是对inputformat和outputformat进行定制 运行要依赖java 和hadoop运行环境 Sqoop 从关系型数据库导入到HDFS命令: sqoop import –connect jdbc:mysql://xxxxxxx/hivemeta2db –username root –password password –table sds默认导出数据到/user/user.name/{tablename}, 可以通过 –tartget-dir 设置导出到hdfs文件系统的位置, 如果想要将所有表导出到hdfs 可以使用 import-all-tables 命令 参数解释: –m 设置并行任务数量, 即map数量 –columns参数， 可以指定导出的字段，用,隔开字段名称 –as-sequencefile 将文件导出为sequencefile 格式 –class-name 为sequencefile 类文件指定类命名– –fields-terminated-by,–lines-terminated-by,–optionally-enclosed-by 指定分隔符，换行符，文件结束符 –where 指定查询条件 –hive-import: 导入到hive中 Sqoop 从HDFS导出到关系型数据库命令: sqoop export –connect jdbc:mysql://192.168.81.176/sqoop –username root -password passwd –table sds –export-dir /user/guojian/sds HbaseHbase 介绍Hbase 是一个高可靠性、高性能、面向列可伸缩的分布式存储系统 HBASE的目标是存储并处理大型的数据，更具体来说是仅需使用普通的硬件配置，就能够处理由成千上万的行和列所组成的大型数据。 Hbase 与传统数据库的对比 传统数据库的问题 1）数据量很大的时候无法存储 2）没有很好的备份机制 3）数据达到一定数量开始缓慢，很大的话基本无法支撑 hbase的优势 1）线性扩展，随着数据量增多可以通过节点扩展进行支撑(易扩展) 2）数据存储在hdfs上，备份机制健全(HDFS方便备份) 3）通过zookeeper协调查找数据，访问速度快。(数据搜索通过zookeeper，查询速度快) Hbase集群中的角色 一个或者多个主节点，Hmaster 管理用户对Table表的增、删、改、查操作； 管理HRegion服务器的负载均衡，调整HRegion分布； 在HRegion分裂后，负责新HRegion的分配； 在HRegion服务器停机后，负责失效HRegion服务器上的HRegion迁移。 多个从节点，HregionServer 表的增删改查数据。 和hdfs交互，存取数据。 zookeeper 在hbase 中的作用 保存Hmaster的地址和backup-master地址 保存表-ROOT的地址 (hbase默认的根表，检索表) HregionServer列表 Hbase 的数据模型 rowkey 与nosql数据库们一样,row key是用来检索记录的主键。 row key保存为字节数组， 存储时，按照row key的字典序排序存储，设计key时要充分利用这一个特性。 访问HBASE table中的行，只有三种方式： 1）通过单个row key访问 2）通过row key的range（正则） 3）全表扫描 列簇 Hbase 中的每个列，多归属于某一个列簇。 列簇是表的schema的一部分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如 courses:history，courses:math都属于courses 这个列族。 cell 由{row key, columnFamily, version} 唯一确定的单元。cell中 的数据是没有类型的，全部是字节码形式存贮。 时间戳 Hbase中通过rowkey和columns确定的为一个存储单元称为cell。 每个cell都保存着同一份数据的多个版本，版本可以通过时间戳来索引。 为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供 了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段 时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。 数据模型示意图 常用命令创建表: create 表名, 列簇1, 列簇2 查看所有表: list 添加数据: put 表名, rowkey, 列簇: 列, 值 查看rowkey: get 表名, rowkey 查看列簇: get 表名, rowkey, 列簇 查看列: get 表名, rowkey, 列簇: 列 存储结构Zookeeper​ - HegineServer​ - .meta​ - Hegine​ - Store​ - StoreFile​ - MemStore​ - HLog Hbase读流程ZooKeeper—meta–regionserver–region–memstore–storefile 1、 通过zookeeper的-ROOT- 找到表 .META. 对应的hregionserver。 2、通过META表rowkey，表名等信息找到数据对应的regine。 3、通过zookerpeer 的信息找到对应的regioneserver。 4、连接到对应regineserver上的regine。 5、先从Memstore找数据，如果没有，再到StoreFile上读 6、 数据块会缓存 Hbase 写流程1、 通过zookeeper的-ROOT- 找到表 .META. 对应的hregionserver。 2、通过META表rowkey，表名等信息找到数据对应的regine。 3、通过zookerpeer 的信息找到对应的regioneserver。 4、 hregionserver将数据写到hlog（write ahead log）。为了数据的持久化和恢复。 5、 hregionserver将数据写到内存（memstore） 6、当memstore达到阈值(64M)，将内存中数据写入到storefile，并删除内存，并清空HLOG做标记点; 当storefile数量达到4块的时候, hmaster将数据加载到本地进行合并，如果合并的数据超过256M，会对块进行拆分，分给不同的hregionserver； 7、 反馈client写成功。 Kafkakafka 相对传统技术的优势 快速:单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作。 可伸缩:在一组机器上对数据进行分区和简化，以支持更大的数据 持久:消息是持久性的，并在集群中进行复制，以防止数据丢失。 设计:它提供了容错保证和持久性 Kafka 使用zookeeper 的作用 Zookeeper主要用于在集群中不同节点之间进行通信 在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取 除此之外，它还执行其他活动，如: leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。 解释Kafka的用户如何消费信息?在Kafka中传递消息是通过使用sendfile API完成的。它支持将字节从套接口转移到磁盘，通过内核空间保存副本，并在内核用户之间调用内核。 解释如何提高远程用户的吞吐量?如果用户位于与broker不同的数据中心，则可能需要调优套接口缓冲区大小，以对长网络延迟进行摊销。 解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息?在数据中，为了精确地获得Kafka的消息，你必须遵循两件事: 在数据消耗期间避免重复，在数据生产过程中避免重复。 这里有两种方法，可以在数据生成时准确地获得一个语义: 每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功 在消息中包含一个主键(UUID或其他)，并在用户中进行反复制 Kafka为什么需要复制?Kafka的信息复制确保了任何已发布的消息不会丢失，并且可以在机器错误、程序错误或更常见些的软件升级中使用。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK]]></title>
    <url>%2F2020%2F04%2F24%2F21.%E6%80%BB%E7%BB%93%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%2FELK%2F</url>
    <content type="text"><![CDATA[ES(分布式全文检索引擎) 怎么解决大数据的写入和查询瓶颈的 存储数据时按照有序存储 将数据和索引分离 对存储数据进行压缩 ES 与 Lucence 的关系 1）Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。 2）Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 ES核心概念 1）Cluster: 集群。ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。 2）Node: 节点。形成集群的每个服务器称为节点。 3）Index: 索引。index是一系列document 的集合。比方说我们可以有一个index叫做 customer data，另外一个叫做product catalog，还有一个叫做 order data。一个index被一个唯一的小写字母名字标记。这个名字将会影响里面数据的查询，更新操作 4）Type: 分组。在一个Index中，我们可以定义很多的type。一个type是一个逻辑分组。大多数情况下，同一个type中的数据会有相同的数据结构。 5）Document: 记录。一个Document是一个可以被索引的基本单元。 6）Shard: 分片。当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。 7）Replia: 副本。为提高查询吞吐量或实现高可用性，可以使用分片副本。副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。 8）全文检索: 全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。 Es数据架构和Mysql对比 mysql Elastic Search Database Index Table Type Row Document Column Field Schema Mapping(定义索引如果建立如何使用) Index Everything is indexed Es 的特点和优势 1）分布式实时文件存储，可将每一个字段存入索引，使其可以被检索到。 2）实时分析的分布式搜索引擎。 分布式：索引分拆成多个分片，每个分片可有零个或多个副本。集群中的每个数据节点都可承载一个或多个分片，并且协调和处理各种操作； 负载再平衡和路由在大多数情况下自动完成。 3）可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。也可以运行在单台PC上（已测试） 4）支持插件机制，分词插件、同步插件、Hadoop插件、可视化插件等。 Es 的节点角色 1）master node ​ 主要功能：维护元数据，管理集群节点状态；不负责数据写入和查询。 ​ 配置要点：内存可以相对小一些，但是机器一定要稳定，最好是独占的机器。 2）client node ​ 主要功能：负责任务分发和结果汇聚，分担数据节点压力。 ​ 配置要点：大内存，最好是独占的机器 3）data node ​ 主要功能：负责数据的写入与查询，压力大。 ​ 配置要点：大内存，最好是独占的机器。 3）mixed node ​ 主要功能：综合上述三个节点的功能。 ​ 配置要点：大内存，最好是独占的机器。 Es 过滤器 term: 精确过滤 terms: 多次条匹配 range: 范围过滤 exists: 存在某些条件 missing: 不存在某些条件 ids: 指定多个id过滤器 bool 过滤器; must, or, and 过滤器]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink Time]]></title>
    <url>%2F2020%2F04%2F11%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F10-Flink%20Time%2F</url>
    <content type="text"><![CDATA[时间（Time）时间类型 Flink中的时间与现实世界中的时间是不一致的，在flink中被划分为事件时间，摄入时间，处理时间三种。 如果以EventTime为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime 如果以IngesingtTime为基准来定义时间窗口将形成IngestingTimeWindow,以source的systemTime为准。 如果以ProcessingTime基准来定义时间窗口将形成ProcessingTimeWindow，以operator的systemTime为准。 时间详解Processing Time Processing Time 是指事件被处理时机器的系统时间。 当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。 例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。 Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。 Event Time Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。 完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。 假设所有数据都已到达， Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。 请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。 Ingestion Time Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。 Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。 与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。 在 Flink 中，，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。 时间戳和水印在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图： 那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？Apache Flink的时间类型开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图： 那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？ Apache Flink的时间类型开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图： ProcessingTime 是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。 IngestionTime IngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。 EventTime EventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。 开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。 什么是WatermarkWatermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: Watermark的产生方式目前Apache Flink 有两种生产Watermark的方式，如下： Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。 在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。 Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。 所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。 Watermark的接口定义对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下： Periodic Watermarks - AssignerWithPeriodicWatermarks 1234567891011121314151617181920212223/*** Returns the current watermark. This method is periodically called by the* system to retrieve the current watermark. The method may return &#123;@code null&#125; to* indicate that no new Watermark is available.** The returned watermark will be emitted only if it is non-null and itsTimestamp* is larger than that of the previously emitted watermark (to preserve the contract of* ascending watermarks). If the current watermark is still* identical to the previous one, no progress in EventTime has happened since* the previous call to this method. If a null value is returned, or theTimestamp* of the returned watermark is smaller than that of the last emitted one, then no* new watermark will be generated.** The interval in which this method is called and Watermarks are generated* depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;.** @see org.Apache.flink.streaming.api.watermark.Watermark* @see ExecutionConfig#getAutoWatermarkInterval()** @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.*/@NullableWatermark getCurrentWatermark(); Punctuated Watermarks - AssignerWithPunctuatedWatermarks 1234567891011121314151617181920public interface AssignerWithPunctuatedWatermarks&lt;T&gt; extends TimestampAssigner&lt;T&gt; &#123;/*** Asks this implementation if it wants to emit a watermark. This method is called right after* the &#123;@link #extractTimestamp(Object, long)&#125; method.** The returned watermark will be emitted only if it is non-null and itsTimestamp* is larger than that of the previously emitted watermark (to preserve the contract of* ascending watermarks). If a null value is returned, or theTimestamp of the returned* watermark is smaller than that of the last emitted one, then no new watermark will* be generated.** For an example how to use this method, see the documentation of* &#123;@link AssignerWithPunctuatedWatermarks this class&#125;.** @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.*/@NullableWatermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);&#125; AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner 1234567891011121314151617public interfaceTimestampAssigner&lt;T&gt; extends Function &#123;/*** Assigns aTimestamp to an element, in milliseconds since the Epoch.** The method is passed the previously assignedTimestamp of the element.* That previousTimestamp may have been assigned from a previous assigner,* by ingestionTime. If the element did not carry aTimestamp before, this value is* &#123;@code Long.MIN_VALUE&#125;.** @param element The element that theTimestamp is wil be assigned to.* @param previousElementTimestamp The previous internalTimestamp of the element,* or a negative value, if noTimestamp has been assigned, yet.* @return The newTimestamp.*/long extractTimestamp(T element, long previousElementTimestamp);&#125; 从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。 Watermark解决如上问题从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。 回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下： 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： 1234567CREATE TABLE source(...,Event_timeTimeStamp,WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0) ) with (...); 如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下： 上面对应的DDL(Alibaba 内部的DDL语法，目前正在和社区讨论)定义如下： 1234567CREATE TABLE source(...,Event_timeTimeStamp,WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000) ) with (...); 上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s. 多流的Watermark处理在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示： Apache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图: 本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。 使用Event Time 处理实时数据 如下是一段 log 日志，我们根据该日志格式，来分析客户的下单量情况。 日志格式：1581490623000,James,51581490624150,John,2… 接下来，我们从并行Source 和 非并行Source 两个方向，来使用 EventTime 处理实时数据。(接下来示例，设置延迟为0s，即不延迟) 非并行Source非并行Source，以 socketTextStream为例来介绍 Flink使用 EventTime 处理实时数据。 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/*** TODO 非并行Source EventTime ** @author liuzebiao * @Date 2020-2-12 15:25*/public class EventTimeDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置EventTime作为时间标准 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); //读取source，并指定(1581490623000,Mary,3)中哪个字段为EventTime时间 //WaterMarks:是Flink中窗口延迟触发的机制。Time.seconds(0)表示无延迟。 SingleOutputStreamOperator&lt;String&gt; source = env.socketTextStream("localhost", 8888).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) &#123; @Override public long extractTimestamp(String line) &#123; String[] split = line.split(","); return Long.parseLong(split[0]); &#125; &#125;); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; mapOperator = source.map(line -&gt; &#123; String[] split = line.split(","); return Tuple2.of(split[1], Integer.parseInt(split[2])); &#125;).returns(Types.TUPLE(Types.STRING,Types.INT)); KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = mapOperator.keyBy(0); //EventTime滚动窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(5))); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = windowedStream.sum(1); sum.print(); env.execute("EventTimeDemo"); &#125;&#125; 测试结果 结果分析备注： (1581490623000转换后为：2020-02-12 14:57:03​ 1581490624000转换后为：2020-02-12 14:57:04) 当我们在Socket中输入如下数据：1581490623000,Mary,21581490624000,John,31581490624500,Clerk,11581490624998,Maria,41581490624999,Mary,31581490626000,Mary,31581490630800,Steve,3 (2020-02-12 14:57:10.800) 窗口定义的时间是：含头不含尾。即：[0,5)，图片解析：(我们定义滚动窗口为5s，我们分析图片发现到4998时,并没有输出内容。因为4998还没超过5s，窗口规定是&gt;=临界值时触发，所以当我们输入4999临界时，我们发现输出内容了，说明一个窗口滚动完成，输出内容包含4999这个时间的值；当输入6000时，6000在[5,10)之间没有&gt;10，所以不输出。输入30800【2020-02-12 14:57:10.800)】，已经超过10s，所以结果只输出1个 (Mary,3)，因为Steve已经被分到另一个窗口了) 还有一个问题，就是：当输入到 4999 时，只是Mary这个分组满足5s这个条件，但是其它分组John，Clerk 等也同步输出结果了。显然这不符合逻辑。为什么会出现这种情况呢？是因为SocketStream 是非并行数据流，所以才会出现这样子的结果。(接下来我们就是用并行数据流KafkaSource来分析) 并行Source并行Source，以 KafkaSouce 为例来介绍 Flink使用 EventTime 处理实时数据。 代码 并行KafkaSource EventTime示例(读取 topic为 window_demo中的消息)，代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/*** TODO 并行KafkaSource EventTime示例(读取 topic为 window_demo中的消息)** @author liuzebiao* @Date 2020-2-12 15:25*/public class EventTimeDemo &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置EventTime作为时间标准 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); //Kafka props Properties properties = new Properties(); //指定Kafka的Broker地址 properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.204.210:9092,192.168.204.211:9092,192.168.204.212:9092"); //指定组ID properties.put(ConsumerConfig.GROUP_ID_CONFIG, "flinkDemoGroup"); //如果没有记录偏移量，第一次从最开始消费 properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); FlinkKafkaConsumer&lt;String&gt; kafkaSource = new FlinkKafkaConsumer("window_demo", new SimpleStringSchema(), properties); //2.通过addSource()方式，创建 Kafka DataStream //读取source，并指定(1581490623000,Mary,3)中哪个字段为EventTime时间 SingleOutputStreamOperator&lt;String&gt; source = env.addSource(kafkaSource).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) &#123; @Override public long extractTimestamp(String line) &#123; String[] split = line.split(","); return Long.parseLong(split[0]); &#125; &#125;); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; mapOperator = source.map(line -&gt; &#123; String[] split = line.split(","); return Tuple2.of(split[1], Integer.parseInt(split[2])); &#125;).returns(Types.TUPLE(Types.STRING,Types.INT)); KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = mapOperator.keyBy(0); //EventTime滚动窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(5))); SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = windowedStream.sum(1); sum.print(); env.execute("EventTimeDemo"); &#125;&#125; 测试结果 创建 Topic 命令如下： 12bin/kafka-topics.sh --create --zookeeper 192.168.204.210:2181,192.168.204.211:2181,192.168.204.212:2181 --replication-factor 1 --partitions 3 --topic window_demo# (特别注意一下：此处创建了3个分区) 创建 Topic 成功截图(点击放大查看)： 使用命令，写入数据到Kafka： 1bin/kafka-console-producer.sh --broker-list 192.168.204.210:9092 --topic window_demo 使用命令写入以下数据： 123451581490623000,Mary,21581490624000,John,31581490624500,Clerk,11581490624998,Maria,41581490624999,Mary,3 测试结果： 结果分析​ 在并行Source一例中，当我们输入1581490624999,Mary,3时，我们看到控制台会直接帮我们输出计算结果。 ​ 但是，在使用 KafkaSource 时，我们连续输入了 3次1581490624999,Mary,3，我们才看到控制台帮我们输出计算了结果。 ​ 那这是为什么呢？这是 并行Source 和 非并行Source 的原因导致的（这里涉及到 KafkaSource 创建的 topic，有 3 个分区的原因，如下图所示）]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 广播变量]]></title>
    <url>%2F2020%2F04%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F12-Flink%20%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[广播变量简介在Flink中，同一个算子可能存在若干个不同的并行实例，计算过程可能不在同一个Slot中进行，不同算子之间更是如此，因此不同算子的计算数据之间不能像Java数组之间一样互相访问，而广播变量Broadcast便是解决这种情况的。 我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份、 用法123456789101：初始化数据 DataSet&lt;Integer&gt; num = env.fromElements(1, 2, 3) 2：广播数据 .withBroadcastSet(toBroadcast, &quot;num&quot;); 3：获取数据 Collection&lt;Integer&gt; broadcastSet = getRuntimeContext().getBroadcastVariable(&quot;num&quot;); 注意： 1：广播出去的变量存在于每个节点的内存中，所以这个数据集不能太大。因为广播出去的数据，会常驻内存，除非程序执行结束 2：广播变量在初始化广播出去以后不支持修改，这样才能保证每个节点的数据都是一致的。 注意事项使用广播状态，task 之间不会相互通信只有广播的一边可以修改广播状态的内容。用户必须保证所有 operator 并发实例上对广播状态的修改行为都是一致的。或者说，如果不同的并发实例拥有不同的广播状态内容，将导致不一致的结果。 广播状态中事件的顺序在各个并发实例中可能不尽相同广播流的元素保证了将所有元素（最终）都发给下游所有的并发实例，但是元素的到达的顺序可能在并发实例之间并不相同。因此，对广播状态的修改不能依赖于输入数据的顺序。 所有operator task都会快照下他们的广播状态在checkpoint时，所有的 task 都会 checkpoint 下他们的广播状态，随着并发度的增加，checkpoint 的大小也会随之增加 广播变量存在内存中广播出去的变量存在于每个节点的内存中，所以这个数据集不能太大，百兆左右可以接受，Gb不能接受 案例12345678910111213141516171819202122232425public class BroadCastTest &#123; public static void main(String[] args) throws Exception&#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); //1.封装一个DataSet DataSet&lt;Integer&gt; broadcast = env.fromElements(1, 2, 3); DataSet&lt;String&gt; data = env.fromElements(&quot;a&quot;, &quot;b&quot;); data.map(new RichMapFunction&lt;String, String&gt;() &#123; private List list = new ArrayList(); @Override public void open(Configuration parameters) throws Exception &#123; // 3. 获取广播的DataSet数据 作为一个Collection Collection&lt;Integer&gt; broadcastSet = getRuntimeContext().getBroadcastVariable(&quot;number&quot;); list.addAll(broadcastSet); &#125; @Override public String map(String value) throws Exception &#123; return value + &quot;: &quot;+ list; &#125; &#125;).withBroadcastSet(broadcast, &quot;number&quot;) // 2. 广播的broadcast .printToErr();//打印到err方便查看 &#125;&#125; 输出结果： 12a: [1, 2, 3]b: [1, 2, 3]]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 并行度]]></title>
    <url>%2F2020%2F04%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F11-Flink%20%E5%B9%B6%E8%A1%8C%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[并行执行本节介绍如何在Flink中配置程序的并行执行。FLink程序由多个任务（转换/操作符、数据源和sinks）组成。任务被分成多个并行实例来执行，每个并行实例处理任务的输入数据的子集。任务的并行实例的数量称之为并行性。 如果要使用保存点，还应该考虑设置最大并行性（或最大并行性）。当从保存点还原时，可以改变特定运算符或整个程序的并行性，并且该设置指定并行性的上限。这是必需的，因为FLINK内部将状态划分为key-groups，并且我们不能拥有+INF的key-group数，因为这将对性能有害。 Flink中人物的并行度可以从多个不同层面设置： 1， 操作算子层面 2， 执行环境层面‘ 3， 客户端层面 4， 系统层面 5，设置slots 操作算子层操作算子，数据源，数据接收器等这些并行度都可以通过调用他们的setParallelism()方法设置。例如： 1234567891011121314151617val env = StreamExecutionEnvironment.getExecutionEnvironmentval text = [...]val wordCounts = text .flatMap&#123; _.split(" ") map &#123; (_, 1) &#125; &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1).setParallelism(5)wordCounts.print()env.execute("Word Count Example") 执行环境层面flink程序执行需要执行环境上下文。执行环境为其要执行的操作算子，数据源，数据sinks都是设置了默认的并行度。执行环境的并行度可以通过操作算子显示指定并行度来覆盖掉。 默认的执行环境并行度可以通过调用setParallelism()来设置。例如，操作算子，数据源，数据接收器，并行度都设置为3，那么在执行环境层面，设置方式如下： 12345678910111213141516171819val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(3)val text = [...]val wordCounts = text .flatMap&#123; _.split(" ") map &#123; (_, 1) &#125; &#125; .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1)wordCounts.print()env.execute("Word Count Example") 客户端层在提交job 到flink的时候，在客户端侧也可以设置flink的并行度。客户端即可以是java工程，也可以是scala工程。Flink的Command-line Interface (CLI)就是这样一种客户端。 在客户端侧flink可以通过-p参数来设置并行度。例如： 1./bin/flink run -p 10 ../examples/*WordCount-java*.jar 在java/scala客户端，并行度设置方式如下： 12345678910111213141516171819try &#123; PackagedProgram program = new PackagedProgram(file, args) InetSocketAddress jobManagerAddress = RemoteExecutor.getInetFromHostport("localhost:6123") Configuration config = new Configuration() Client client = new Client(jobManagerAddress, new Configuration(), program.getUserCodeClassLoader()) // set the parallelism to 10 here client.run(program, 10, true)&#125; catch &#123; case e: Exception =&gt; e.printStackTrace&#125; 系统层面系统层面的并行度设置，会针对所有的执行环境生效，可以通过parallelism.default，属性在conf/flink-conf.yaml文件中设置。 设置最大并行度设置最大并行度，实际上调用的方法是setMaxParallelism()，其调用位置和setParallelism()一样。 默认的最大并行度是近似于operatorParallelism + (operatorParallelism / 2)，下限是127，上线是32768. 值得注意的是将最大的并行的设置为超级大的数可能会对性能造成不利的影响，因为一些状态后端是必须要保存内部数据结构的，这个数据结构跟key-group数量相匹配（这是可重定状态的内部实现机制）。 配置taskmanagerslotflink通过将项目分成tasks，来实现并行的执行项目，划分的tasks会被发到slot去处理。 集群中Flink的taskmanager提供处理slot。Slots数量最合适的是跟taskmanager的cores数量成正比。当然，taskmanager.numberOfTaskSlots的推荐值就是cpu核心的数目。 当启动一个任务的时候，我们可以为其提供默认的slot数目，其实也即是flink工程的并行度，设置方式在上面已经有详细介绍。]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 分布式缓存]]></title>
    <url>%2F2020%2F04%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F9-Flink%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[分布式缓存Flink提供了一个分布式缓存，类似于hadoop，可以使用户在并行函数中很方便的读取本地文件，并把它放在taskmanager节点中，防止task重复拉取。此缓存的工作机制如下：程序注册一个文件或者目录(本地或者远程文件系统，例如hdfs或者s3)，通过ExecutionEnvironment注册缓存文件并为它起一个名称。当程序执行，Flink自动将文件或者目录复制到所有taskmanager节点的本地文件系统，仅会执行一次。用户可以通过这个指定的名称查找文件或者目录，然后从taskmanager节点的本地文件系统访问它。 示例在ExecutionEnvironment中注册一个文件： 12345//获取运行环境ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();//1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试env.registerCachedFile("/Users/wangzhiwu/WorkSpace/quickstart/text","a.txt"); 在用户函数中访问缓存文件或者目录(这里是一个map函数)。这个函数必须继承RichFunction,因为它需要使用RuntimeContext读取数据: 1234567891011121314151617181920212223242526DataSet&lt;String&gt; result = data.map(new RichMapFunction&lt;String, String&gt;() &#123; private ArrayList&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); //2：使用文件 File myFile = getRuntimeContext().getDistributedCache().getFile("a.txt"); List&lt;String&gt; lines = FileUtils.readLines(myFile); for (String line : lines) &#123; this.dataList.add(line); System.err.println("分布式缓存为:" + line); &#125; &#125; @Override public String map(String value) throws Exception &#123; //在这里就可以使用dataList System.err.println("使用datalist：" + dataList + "------------" +value); //业务逻辑 return dataList +"：" + value; &#125; &#125;); result.printToErr(); &#125; 完整代码如下,仔细看注释： 123456789101112131415161718192021222324252627282930313233343536373839public class DisCacheTest &#123; public static void main(String[] args) throws Exception&#123; //获取运行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); //1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试 //text 中有4个单词:hello flink hello FLINK env.registerCachedFile("/Users/wangzhiwu/WorkSpace/quickstart/text","a.txt"); DataSource&lt;String&gt; data = env.fromElements("a", "b", "c", "d"); DataSet&lt;String&gt; result = data.map(new RichMapFunction&lt;String, String&gt;() &#123; private ArrayList&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); //2：使用文件 File myFile = getRuntimeContext().getDistributedCache().getFile("a.txt"); List&lt;String&gt; lines = FileUtils.readLines(myFile); for (String line : lines) &#123; this.dataList.add(line); System.err.println("分布式缓存为:" + line); &#125; &#125; @Override public String map(String value) throws Exception &#123; //在这里就可以使用dataList System.err.println("使用datalist：" + dataList + "------------" +value); //业务逻辑 return dataList +"：" + value; &#125; &#125;); result.printToErr(); &#125;&#125;// 输出结果如下： 1234[hello, flink, hello, FLINK]：a[hello, flink, hello, FLINK]：b[hello, flink, hello, FLINK]：c[hello, flink, hello, FLINK]：d]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink DataStreamAPI]]></title>
    <url>%2F2020%2F04%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F7-Flink%20DataStreamAPI%2F</url>
    <content type="text"><![CDATA[DataStream算子将一个或多个DataStream转换为新DataStream。程序可以将多个转换组合成复杂的数据流拓扑。DataStreamAPI和DataSetAPI主要的区别在于Transformation部分。 DataStream Transformationmap DataStream→DataStream用一个数据元生成一个数据元。一个map函数，它将输入流的值加倍： 1234567DataStream&lt;Integer&gt; dataStream = //...dataStream.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer value) throws Exception &#123; return 2 * value; &#125;&#125;); FlatMap DataStream→DataStream 采用一个数据元并生成零个，一个或多个数据元。将句子分割为单词的flatmap函数： 123456789dataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for(String word: value.split(" "))&#123; out.collect(word); &#125; &#125;&#125;); Filter DataStream→DataStream计算每个数据元的布尔函数，并保存函数返回true的数据元。过滤掉零值的过滤器： 123456dataStream.filter(new FilterFunction&lt;Integer&gt;() &#123; @Override public boolean filter(Integer value) throws Exception &#123; return value != 0; &#125;&#125;); KeyBy DataStream→KeyedStream 逻辑上将流分区为不相交的分区。具有相同Keys的所有记录都分配给同一分区。在内部，keyBy（）是使用散列分区实现的。指定键有不同的方法。 此转换返回KeyedStream，其中包括使用被Keys化状态所需的KeyedStream。 12dataStream.keyBy("someKey") // Key by field "someKey"dataStream.keyBy(0) // Key by the first element of a Tuple 🌺注意： 如果出现以下情况，则类型不能成为key： 它是POJO类型但不覆盖hashCode（）方法并依赖于Object.hashCode（）实现 任何类型的数组 ReduceKeyedStream→DataStream 将当前数据元与最后一个Reduce的值组合并发出新值。例如：reduce函数，用于创建部分和的流： 1234567keyedStream.reduce(new ReduceFunction&lt;Integer&gt;() &#123; @Override public Integer reduce(Integer value1, Integer value2) throws Exception &#123; return value1 + value2; &#125;&#125;); FoldKeyedStream→DataStream 具有初始值的被Keys化数据流上的“滚动”折叠。将当前数据元与最后折叠的值组合并发出新值。 折叠函数，当应用于序列（1,2,3,4,5）时，发出序列“start-1”，“start-1-2”，“start-1-2-3”,. .. 1234567DataStream&lt;String&gt; result = keyedStream.fold("start", new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String current, Integer value) &#123; return current + "-" + value; &#125; &#125;); 聚合 KeyedStream→DataStream 在被Keys化数据流上滚动聚合。min和minBy之间的差异是min返回最小值，而minBy返回该字段中具有最小值的数据元(max和maxBy相同)。 12345678910keyedStream.sum(0);keyedStream.sum("key");keyedStream.min(0);keyedStream.min("key");keyedStream.max(0);keyedStream.max("key");keyedStream.minBy(0);keyedStream.minBy("key");keyedStream.maxBy(0);keyedStream.maxBy("key"); Window函数关于Flink的窗口概念，我们会在后面有详细介绍。 WindowKeyedStream→WindowedStream 可以在已经分区的KeyedStream上定义Windows。Windows根据某些特征（例如，在最后5秒内到达的数据）对每个Keys中的数据进行分组。 123dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data Window ApplyWindowedStream→DataStreamAllWindowedStream→DataStream 将一般函数应用于整个窗口。下面是一个手动求和窗口数据元的函数。 注意：如果您正在使用windowAll转换，则需要使用AllWindowFunction。 12345678910111213141516171819202122232425windowedStream.apply (new WindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Tuple, Window&gt;() &#123; public void apply (Tuple tuple, Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;);// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply (new AllWindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Window&gt;() &#123; public void apply (Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;); Window ReduceWindowedStream→DataStream 将reduce函数应用于窗口并返回reduce后的值。 12345windowedStream.reduce (new ReduceFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123; public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception &#123; return new Tuple2&lt;String,Integer&gt;(value1.f0, value1.f1 + value2.f1); &#125;&#125;); 提取时间戳 关于Time我们在后面有专门的章节进行介绍 DataStream→DataStream 从记录中提取时间戳，以便使用使用事件时间语义的窗口。 1stream.assignTimestamps (new TimeStampExtractor() &#123;...&#125;); Partition 分区 自定义分区DataStream→DataStream使用用户定义的分区程序为每个数据元选择目标任务。 12dataStream.partitionCustom(partitioner, "someKey");dataStream.partitionCustom(partitioner, 0); 随机分区DataStream→DataStream根据均匀分布随机分配数据元。 1dataStream.shuffle(); Rebalance （循环分区）DataStream→DataStream分区数据元循环，每个分区创建相等的负载。在存在数据倾斜时用于性能优化。 1dataStream.rebalance(); rescaleDataStream→DataStream 如果上游 算子操作具有并行性2并且下游算子操作具有并行性6，则一个上游 算子操作将分配元件到三个下游算子操作，而另一个上游算子操作将分配到其他三个下游 算子操作。另一方面，如果下游算子操作具有并行性2而上游 算子操作具有并行性6，则三个上游 算子操作将分配到一个下游算子操作，而其他三个上游算子操作将分配到另一个下游算子操作。 在不同并行度不是彼此的倍数的情况下，一个或多个下游 算子操作将具有来自上游 算子操作的不同数量的输入。 请参阅此图以获取上例中连接模式的可视化： 1dataStream.rescale(); 广播DataStream→DataStream向每个分区广播数据元。 1dataStream.broadcast();]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink DataSetAPI]]></title>
    <url>%2F2020%2F04%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F6-Flink%20DataSetAPI%2F</url>
    <content type="text"><![CDATA[编程结构1234567891011121314151617181920public class SocketTextStreamWordCount &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2)&#123;System.err.println("USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"); return; &#125; String hostName = args[0]; Integer port = Integer.parseInt(args[1]); final StreamExecutionEnvironment env = StreamExecutionEnvironment .getExecutionEnvironment(); DataStream&lt;String&gt; text = env.socketTextStream(hostName, port); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new LineSplitter()) .keyBy(0) .sum(1); counts.print(); env.execute("Java WordCount from SocketTextStream Example"); &#125; 上面的SocketTextStreamWordCount是一个典型的Flink程序，他由一下及格部分构成： 获得一个execution environment， 加载/创建初始数据， 指定此数据的转换， 指定放置计算结果的位置， 触发程序执行 DataSet API分类： Source: 数据源创建初始数据集，例如来自文件或Java集合 Transformation: 数据转换将一个或多个DataSet转换为新的DataSet Sink: 将计算结果存储或返回 DataSet Sources基于文件的 readTextFile(path)/ TextInputFormat- 按行读取文件并将其作为字符串返回。 readTextFileWithValue(path)/ TextValueInputFormat- 按行读取文件并将它们作为StringValues返回。StringValues是可变字符串。 readCsvFile(path)/ CsvInputFormat- 解析逗号（或其他字符）分隔字段的文件。返回元组或POJO的DataSet。支持基本java类型及其Value对应作为字段类型。 readFileOfPrimitives(path, Class)/ PrimitiveInputFormat- 解析新行（或其他字符序列）分隔的原始数据类型（如String或）的文件Integer。 readFileOfPrimitives(path, delimiter, Class)/ PrimitiveInputFormat- 解析新行（或其他字符序列）分隔的原始数据类型的文件，例如String或Integer使用给定的分隔符。 readSequenceFile(Key, Value, path)/ SequenceFileInputFormat- 创建一个JobConf并从类型为SequenceFileInputFormat，Key class和Value类的指定路径中读取文件，并将它们作为Tuple2 &lt;Key，Value&gt;返回。 基于集合 fromCollection(Collection) - 从Java Java.util.Collection创建数据集。集合中的所有数据元必须属于同一类型。 fromCollection(Iterator, Class) - 从迭代器创建数据集。该类指定迭代器返回的数据元的数据类型。 fromElements(T ...) - 根据给定的对象序列创建数据集。所有对象必须属于同一类型。 fromParallelCollection(SplittableIterator, Class)- 并行地从迭代器创建数据集。该类指定迭代器返回的数据元的数据类型。 generateSequence(from, to) - 并行生成给定间隔中的数字序列。 通用方法 readFile(inputFormat, path)/ FileInputFormat- 接受文件输入格式。 createInput(inputFormat)/ InputFormat- 接受通用输入格式。 代码示例123456789101112131415161718192021222324252627282930313233ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();// 从本地文件系统读DataSet&lt;String&gt; localLines = env.readTextFile("file:///path/to/my/textfile");// 读取HDFS文件DataSet&lt;String&gt; hdfsLines = env.readTextFile("hdfs://nnHost:nnPort/path/to/my/textfile");// 读取CSV文件DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; csvInput = env.readCsvFile("hdfs:///the/CSV/file").types(Integer.class, String.class, Double.class);// 读取CSV文件中的部分，字段选择1代表选择，0代表弃用DataSet&lt;Tuple2&lt;String, Double&gt;&gt; csvInput = env.readCsvFile("hdfs:///the/CSV/file").includeFields("10010").types(String.class, Double.class);// 读取CSV映射为一个java类DataSet&lt;Person&gt;&gt; csvInput = env.readCsvFile("hdfs:///the/CSV/file").pojoType(Person.class, "name", "age", "zipcode");// 读取一个指定位置序列化后的文件DataSet&lt;Tuple2&lt;IntWritable, Text&gt;&gt; tuples = env.readSequenceFile(IntWritable.class, Text.class, "hdfs://nnHost:nnPort/path/to/file");// 从输入字符创建DataSet&lt;String&gt; value = env.fromElements("Foo", "bar", "foobar", "fubar");// 创建一个数字序列DataSet&lt;Long&gt; numbers = env.generateSequence(1, 10000000);// 从关系型数据库读取DataSet&lt;Tuple2&lt;String, Integer&gt; dbData =env.createInput(JDBCInputFormat.buildJDBCInputFormat() .setDrivername("org.apache.derby.jdbc.EmbeddedDriver") .setDBUrl("jdbc:derby:memory:persons").setQuery("select name, age from persons").setRowTypeInfo(new RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO)).finish()); DataSet Transformation 详细可以参考官网:https://flink.sojb.cn/dev/batch/dataset_transformations.html#filter Map 采用一个数据元并生成一个数据元。 123data.map(new MapFunction&lt;String, Integer&gt;() &#123; public Integer map(String value) &#123; return Integer.parseInt(value); &#125;&#125;); FlatMap 采用一个数据元并生成零个，一个或多个数据元。 1234567data.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; public void flatMap(String value, Collector&lt;String&gt; out) &#123; for (String s : value.split(" ")) &#123; out.collect(s); &#125; &#125;&#125;); MapPartition 在单个函数调用中转换并行分区。该函数将分区作为Iterable流来获取，并且可以生成任意数量的结果值。每个分区中的数据元数量取决于并行度和先前的 算子操作。 123456789data.mapPartition(new MapPartitionFunction&lt;String, Long&gt;() &#123; public void mapPartition(Iterable&lt;String&gt; values, Collector&lt;Long&gt; out) &#123; long c = 0; for (String s : values) &#123; c++; &#125; out.collect(c); &#125;&#125;); Filter 计算每个数据元的布尔函数，并保存函数返回true的数据元。重要信息：系统假定该函数不会修改应用谓词的数据元。违反此假设可能会导致错误的结果。 123data.filter(new FilterFunction&lt;Integer&gt;() &#123; public boolean filter(Integer value) &#123; return value &gt; 1000; &#125;&#125;); Reduce 通过将两个数据元重复组合成一个数据元，将一组数据元组合成一个数据元。Reduce可以应用于完整数据集或分组数据集。 123data.reduce(new ReduceFunction&lt;Integer&gt; &#123; public Integer reduce(Integer a, Integer b) &#123; return a + b; &#125;&#125;); 如果将reduce应用于分组数据集，则可以通过提供CombineHint 来指定运行时执行reduce的组合阶段的方式 setCombineHint。在大多数情况下，基于散列的策略应该更快，特别是如果不同键的数量与输入数据元的数量相比较小（例如1/10）。 ReduceGroup 将一组数据元组合成一个或多个数据元。ReduceGroup可以应用于完整数据集或分组数据集。 123456789data.reduceGroup(new GroupReduceFunction&lt;Integer, Integer&gt; &#123; public void reduce(Iterable&lt;Integer&gt; values, Collector&lt;Integer&gt; out) &#123; int prefixSum = 0; for (Integer i : values) &#123; prefixSum += i; out.collect(prefixSum); &#125; &#125;&#125;); Aggregate 将一组值聚合为单个值。聚合函数可以被认为是内置的reduce函数。聚合可以应用于完整数据集或分组数据集。 12Dataset&lt;Tuple3&lt;Integer, String, Double&gt;&gt; input = // [...]DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; output = input.aggregate(SUM, 0).and(MIN, 2); 您还可以使用简写语法进行最小，最大和总和聚合。 12Dataset&lt;Tuple3&lt;Integer, String, Double&gt;&gt; input = // [...]DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; output = input.sum(0).andMin(2); Distinct 返回数据集的不同数据元。它相对于数据元的所有字段或字段子集从输入DataSet中删除重复条目。 1data.distinct(); 使用reduce函数实现Distinct。您可以通过提供CombineHint 来指定运行时执行reduce的组合阶段的方式 setCombineHint。在大多数情况下，基于散列的策略应该更快，特别是如果不同键的数量与输入数据元的数量相比较小（例如1/10）。 Join 通过创建在其键上相等的所有数据元对来连接两个数据集。可选地使用JoinFunction将数据元对转换为单个数据元，或使用FlatJoinFunction将数据元对转换为任意多个（包括无）数据元。请参阅键部分以了解如何定义连接键。 123result = input1.join(input2) .where(0) // key of the first input (tuple field 0) .equalTo(1); // key of the second input (tuple field 1) 您可以通过Join Hints指定运行时执行连接的方式。提示描述了通过分区或广播进行连接，以及它是使用基于排序还是基于散列的算法。如果未指定提示，系统将尝试估算输入大小，并根据这些估计选择最佳策略。 1234// This executes a join by broadcasting the first data set// using a hash table for the broadcast dataresult = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(0).equalTo(1); 请注意，连接转换仅适用于等连接。其他连接类型需要使用OuterJoin或CoGroup表示。 OuterJoin 在两个数据集上执行左，右或全外连接。外连接类似于常规（内部）连接，并创建在其键上相等的所有数据元对。此外，如果在另一侧没有找到匹配的Keys，则保存“外部”侧（左侧，右侧或两者都满）的记录。匹配数据元对（或一个数据元和null另一个输入的值）被赋予JoinFunction以将数据元对转换为单个数据元，或者转换为FlatJoinFunction以将数据元对转换为任意多个（包括无）数据元。请参阅键部分以了解如何定义连接键。 1234567891011input1.leftOuterJoin(input2) // rightOuterJoin or fullOuterJoin for right or full outer joins .where(0) // key of the first input (tuple field 0) .equalTo(1) // key of the second input (tuple field 1) .with(new JoinFunction&lt;String, String, String&gt;() &#123; public String join(String v1, String v2) &#123; // NOTE: // - v2 might be null for leftOuterJoin // - v1 might be null for rightOuterJoin // - v1 OR v2 might be null for fullOuterJoin &#125; &#125;); CoGroup reduce 算子操作的二维变体。将一个或多个字段上的每个输入分组，然后关联组。每对组调用转换函数。 12345678data1.coGroup(data2) .where(0) .equalTo(1) .with(new CoGroupFunction&lt;String, String, String&gt;() &#123; public void coGroup(Iterable&lt;String&gt; in1, Iterable&lt;String&gt; in2, Collector&lt;String&gt; out) &#123; out.collect(...); &#125; &#125;); Cross 构建两个输入的笛卡尔积（交叉乘积），创建所有数据元对。可选择使用CrossFunction将数据元对转换为单个数据元 123DataSet&lt;Integer&gt; data1 = // [...]DataSet&lt;String&gt; data2 = // [...]DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; result = data1.cross(data2); 注：交叉是一个潜在的非常计算密集型 算子操作它甚至可以挑战大的计算集群！建议使用crossWithTiny（）和crossWithHuge（）来提示系统的DataSet大小。 Union 生成两个数据集的并集。 123DataSet&lt;String&gt; data1 = // [...]DataSet&lt;String&gt; data2 = // [...]DataSet&lt;String&gt; result = data1.union(data2); Rebalance 均匀地Rebalance 数据集的并行分区以消除数据偏差。只有类似Map的转换可能会遵循Rebalance 转换。 123DataSet&lt;String&gt; in = // [...]DataSet&lt;String&gt; result = in.rebalance() .map(new Mapper()); Hash-Partition 散列分区给定键上的数据集。键可以指定为位置键，表达键和键选择器函数。 123DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.partitionByHash(0) .mapPartition(new PartitionMapper()); Range-Partition Range-Partition给定键上的数据集。键可以指定为位置键，表达键和键选择器函数。 123DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.partitionByRange(0) .mapPartition(new PartitionMapper()); Custom Partitioning 手动指定数据分区。注意：此方法仅适用于单个字段键。 12DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.partitionCustom(Partitioner&lt;K&gt; partitioner, key) Sort Partition 本地按指定顺序对指定字段上的数据集的所有分区进行排序。可以将字段指定为元组位置或字段表达式。通过链接sortPartition（）调用来完成对多个字段的排序。 123DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.sortPartition(1, Order.ASCENDING) .mapPartition(new PartitionMapper()); First-n 返回数据集的前n个（任意）数据元。First-n可以应用于常规数据集，分组数据集或分组排序数据集。分组键可以指定为键选择器函数或字段位置键。 1234567DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]// regular data setDataSet&lt;Tuple2&lt;String,Integer&gt;&gt; result1 = in.first(3);// grouped data setDataSet&lt;Tuple2&lt;String,Integer&gt;&gt; result2 = in.groupBy(0) .first(3);// grouped-sorted data setDataSet&lt;Tuple2&lt;String,Integer&gt;&gt; result3 = in.groupBy(0) .sortGroup(1, Order.ASCENDING) .first(3); DataSet Sink数据接收器使用DataSet用于存储或返回。使用OutputFormat描述数据接收器算子操作 。Flink带有各种内置输出格式，这些格式封装在DataSet上的算子操作中： writeAsText()/ TextOutputFormat- 按字符串顺序写入数据元。通过调用每个数据元的toString（）方法获得字符串。 writeAsFormattedText()/ TextOutputFormat- 按字符串顺序写数据元。通过为每个数据元调用用户定义的format（）方法来获取字符串。 writeAsCsv(…)/ CsvOutputFormat- 将元组写为逗号分隔值文件。行和字段分隔符是可配置的。每个字段的值来自对象的toString（）方法。 print()/ printToErr()/ print(String msg)/ printToErr(String msg)- 在标准输出/标准错误流上打印每个数据元的toString（）值。可选地，可以提供前缀（msg），其前缀为输出。这有助于区分不同的打印调用。如果并行度大于1，则输出也将与生成输出的任务的标识符一起添加。 write()/ FileOutputFormat- 自定义文件输出的方法和基类。支持自定义对象到字节的转换。 output()/ OutputFormat- 大多数通用输出方法，用于非基于文件的数据接收器（例如将结果存储在数据库中）。 可以将DataSet输入到多个 算子操作。程序可以编写或打印数据集，同时对它们执行其他转换。 示例： 1234567891011121314151617181920212223242526// text dataDataSet&lt;String&gt; textData = // [...]// write DataSet to a file on the local file systemtextData.writeAsText("file:///my/result/on/localFS");// write DataSet to a file on a HDFS with a namenode running at nnHost:nnPorttextData.writeAsText("hdfs://nnHost:nnPort/my/result/on/localFS");// write DataSet to a file and overwrite the file if it existstextData.writeAsText("file:///my/result/on/localFS", WriteMode.OVERWRITE);// tuples as lines with pipe as the separator "a|b|c"DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; values = // [...]values.writeAsCsv("file:///path/to/the/result/file", "\n", "|");// this writes tuples in the text formatting "(a, b, c)", rather than as CSV linesvalues.writeAsText("file:///path/to/the/result/file");// this writes values as strings using a user-defined TextFormatter objectvalues.writeAsFormattedText("file:///path/to/the/result/file", new TextFormatter&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123; public String format (Tuple2&lt;Integer, Integer&gt; value) &#123; return value.f1 + " - " + value.f0; &#125; &#125;); 使用自定义输出格式： 1234567891011DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; myResult = [...]// write Tuple DataSet to a relational databasemyResult.output( // build and configure OutputFormat JDBCOutputFormat.buildJDBCOutputFormat() .setDrivername("org.apache.derby.jdbc.EmbeddedDriver") .setDBUrl("jdbc:derby:memory:persons") .setQuery("insert into persons (name, age, height) values (?,?,?)") .finish() ); 序列化器 Flink自带了针对诸如int，long，String等标准类型的序列化器 针对Flink无法实现序列化的数据类型，我们可以交给Avro和Kryo 使用方法：ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 123使用avro序列化：env.getConfig().enableForceAvro();使用kryo序列化：env.getConfig().enableForceKryo();使用自定义序列化：env.getConfig().addDefaultKryoSerializer(Class&lt;?&gt; type, Class&lt;? extends Serializer&lt;?&gt;&gt; serializerClass) 数据类型 Java Tuple 和 Scala case class Java POJOs：java实体类 Primitive Types默认支持java和scala基本数据类型 General Class Types默认支持大多数java和scala class Hadoop Writables支持hadoop中实现了org.apache.hadoop.Writable的数据类型 Special Types例如scala中的Either Option 和Try]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 重启策略]]></title>
    <url>%2F2020%2F04%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F8-Flink%20%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Flink的重启策略Flink支持不同的重启策略，这些重启策略控制着job失败后如何重启。集群可以通过默认的重启策略来重启，这个默认的重启策略通常在未指定重启策略的情况下使用，而如果Job提交的时候指定了重启策略，这个重启策略就会覆盖掉集群的默认重启策略。 概览默认的重启策略是通过Flink的flink-conf.yaml来指定的，这个配置参数restart-strategy定义了哪种策略会被采用。如果checkpoint未启动，就会采用no restart策略，如果启动了checkpoint机制，但是未指定重启策略的话，就会采用fixed-delay策略，重试Integer.MAX_VALUE次。请参考下面的可用重启策略来了解哪些值是支持的。 每个重启策略都有自己的参数来控制它的行为，这些值也可以在配置文件中设置，每个重启策略的描述都包含着各自的配置值信息。 重启策略 重启策略值 Fixed delay fixed-delay Failure rate failure-rate No restart None 除了定义一个默认的重启策略之外，你还可以为每一个Job指定它自己的重启策略，这个重启策略可以在ExecutionEnvironment中调用setRestartStrategy()方法来程序化地调用，主意这种方式同样适用于StreamExecutionEnvironment。 下面的例子展示了我们如何为我们的Job设置一个固定延迟重启策略，一旦有失败，系统就会尝试每10秒重启一次，重启3次。 Java代码 12345ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 尝试重启次数 Time.of(10, TimeUnit.SECONDS) // 延迟时间间隔)); Scala代码: 12345val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 重启次数 Time.of(10, TimeUnit.SECONDS) // 延迟时间间隔)) 重启策略下面部分描述了重启策略特定的配置项 固定延迟重启策略(Fixed Delay Restart Strategy)固定延迟重启策略会尝试一个给定的次数来重启Job，如果超过了最大的重启次数，Job最终将失败。在连续的两次重启尝试之间，重启策略会等待一个固定的时间。 重启策略可以配置flink-conf.yaml的下面配置参数来启用，作为默认的重启策略: 1restart-strategy: fixed-delay 配置参数 描述 默认值 restart-strategy.fixed-delay.attempts 在Job最终宣告失败之前，Flink尝试执行的次数 1，如果启用checkpoint的话是Integer.MAX_VALUE restart-strategy.fixed-delay.delay 延迟重启意味着一个执行失败之后，并不会立即重启，而是要等待一段时间。 akka.ask.timeout,如果启用checkpoint的话是1s 例子: 12restart-strategy.fixed-delay.attempts: 3restart-strategy.fixed-delay.delay: 10 s 固定延迟重启也可以在程序中设置: Java代码: 12345ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 重启次数 Time.of(10, TimeUnit.SECONDS) // 重启时间间隔)); Scala代码: 12345val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3, // 重启次数 Time.of(10, TimeUnit.SECONDS) // 重启时间间隔)) 失败率重启策略失败率重启策略在Job失败后会重启，但是超过失败率后，Job会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间。 失败率重启策略可以在flink-conf.yaml中设置下面的配置参数来启用: 1restart-strategy:failure-rate 配置参数 描述 默认值 restart-strategy.failure-rate.max-failures-per-interval 在一个Job认定为失败之前，最大的重启次数 1 restart-strategy.failure-rate.failure-rate-interval 计算失败率的时间间隔 1分钟 restart-strategy.failure-rate.delay 两次连续重启尝试之间的时间间隔 akka.ask.timeout 123restart-strategy.failure-rate.max-failures-per-interval: 3restart-strategy.failure-rate.failure-rate-interval: 5 minrestart-strategy.failure-rate.delay: 10 s 失败率重启策略也可以在程序中设置: Java代码: 123456ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // 每个测量时间间隔最大失败次数 Time.of(5, TimeUnit.MINUTES), //失败率测量的时间间隔 Time.of(10, TimeUnit.SECONDS) // 两次连续重启尝试的时间间隔)); Scala代码: 123456val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.failureRateRestart( 3, // 每个测量时间间隔最大失败次数 Time.of(5, TimeUnit.MINUTES), //失败率测量的时间间隔 Time.of(10, TimeUnit.SECONDS) // 两次连续重启尝试的时间间隔)) 无重启策略Job直接失败，不会尝试进行重启 1restart-strategy: none 无重启策略也可以在程序中设置 Java代码: 12ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.setRestartStrategy(RestartStrategies.noRestart()); Scala代码: 12val env = ExecutionEnvironment.getExecutionEnvironment()env.setRestartStrategy(RestartStrategies.noRestart())]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 集群部署]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F5-Flink%20%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、部署模式Flink 支持使用多种部署模式来满足不同规模应用的需求，常见的有单机模式，Standalone Cluster 模式，同时 Flink 也支持部署在其他第三方平台上，如 YARN，Mesos，Docker，Kubernetes 等。以下主要介绍其单机模式和 Standalone Cluster 模式的部署。 二、单机模式单机模式是一种开箱即用的模式，可以在单台服务器上运行，适用于日常的开发和调试。具体操作步骤如下： 2.1 安装部署1. 前置条件 Flink 的运行依赖 JAVA 环境，故需要预先安装好 JDK，具体步骤可以参考：Linux 环境下 JDK 安装 2. 下载 &amp; 解压 &amp; 运行 Flink 所有版本的安装包可以直接从其官网进行下载，这里我下载的 Flink 的版本为 1.9.1 ，要求的 JDK 版本为 1.8.x +。 下载后解压到指定目录： 1tar -zxvf flink-1.9.1-bin-scala_2.12.tgz -C /usr/app 不需要进行任何配置，直接使用以下命令就可以启动单机版本的 Flink： 1bin/start-cluster.sh 3. WEB UI 界面 Flink 提供了 WEB 界面用于直观的管理 Flink 集群，访问端口为 8081： Flink 的 WEB UI 界面支持大多数常用功能，如提交作业，取消作业，查看各个节点运行情况，查看作业执行情况等，大家可以在部署完成后，进入该页面进行详细的浏览。 2.2 作业提交启动后可以运行安装包中自带的词频统计案例，具体步骤如下： 1. 开启端口 1nc -lk 9999 2. 提交作业 1bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9999 该 JAR 包的源码可以在 Flink 官方的 GitHub 仓库中找到，地址为 ：SocketWindowWordCount ，可选传参有 hostname， port，对应的词频数据需要使用空格进行分割。 3. 输入测试数据 1a a b b c c c a e 4. 查看控制台输出 可以通过 WEB UI 的控制台查看作业统运行情况： 也可以通过 WEB 控制台查看到统计结果： 2.3 停止作业可以直接在 WEB 界面上点击对应作业的 Cancel Job 按钮进行取消，也可以使用命令行进行取消。使用命令行进行取消时，需要先获取到作业的 JobId，可以使用 flink list 命令查看，输出如下： 123456[root@hadoop001 flink-1.9.1]# ./bin/flink listWaiting for response...------------------ Running/Restarting Jobs -------------------05.11.2019 08:19:53 : ba2b1cc41a5e241c32d574c93de8a2bc : Socket Window WordCount (RUNNING)--------------------------------------------------------------No scheduled jobs. 获取到 JobId 后，就可以使用 flink cancel 命令取消作业： 1bin/flink cancel ba2b1cc41a5e241c32d574c93de8a2bc 2.4 停止 Flink命令如下： 1bin/stop-cluster.sh 三、Standalone ClusterStandalone Cluster 模式是 Flink 自带的一种集群模式，具体配置步骤如下： 3.1 前置条件使用该模式前，需要确保所有服务器间都已经配置好 SSH 免密登录服务。这里我以三台服务器为例，主机名分别为 hadoop001，hadoop002，hadoop003 , 其中 hadoop001 为 master 节点，其余两台为 slave 节点，搭建步骤如下： 3.2 搭建步骤修改 conf/flink-conf.yaml 中 jobmanager 节点的通讯地址为 hadoop001: 1jobmanager.rpc.address: hadoop001 修改 conf/slaves 配置文件，将 hadoop002 和 hadoop003 配置为 slave 节点： 12hadoop002hadoop003 将配置好的 Flink 安装包分发到其他两台服务器上： 12scp -r /usr/app/flink-1.9.1 hadoop002:/usr/appscp -r /usr/app/flink-1.9.1 hadoop003:/usr/app 在 hadoop001 上使用和单机模式相同的命令来启动集群： 1bin/start-cluster.sh 此时控制台输出如下： 启动完成后可以使用 Jps 命令或者通过 WEB 界面来查看是否启动成功。 3.3 可选配置除了上面介绍的 jobmanager.rpc.address 是必选配置外，Flink h还支持使用其他可选参数来优化集群性能，主要如下： jobmanager.heap.size：JobManager 的 JVM 堆内存大小，默认为 1024m 。 taskmanager.heap.size：Taskmanager 的 JVM 堆内存大小，默认为 1024m 。 taskmanager.numberOfTaskSlots：Taskmanager 上 slots 的数量，通常设置为 CPU 核心的数量，或其一半。 parallelism.default：任务默认的并行度。 io.tmp.dirs：存储临时文件的路径，如果没有配置，则默认采用服务器的临时目录，如 LInux 的 /tmp 目录。 更多配置可以参考 Flink 的官方手册：Configuration 4.3 常见异常如果进程没有启动，可以通过查看 log 目录下的日志来定位错误，常见的一个错误如下： 12345678910112019-11-05 09:18:35,877 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnosticsjava.io.IOException: Could not create FileSystem for highly available storage (high-availability.storageDir).......Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded......Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Hadoop is not in the classpath/dependencies....... 可以看到是因为在 classpath 目录下找不到 Hadoop 的相关依赖，此时需要检查是否在环境变量中配置了 Hadoop 的安装路径，如果路径已经配置但仍然存在上面的问题，可以从 Flink 官网下载对应版本的 Hadoop 组件包： 下载完成后，将该 JAR 包上传至所有 Flink 安装目录的 lib 目录即可。 四、On YarnYarn的简介： ResourceManagerResourceManager 负责整个集群的资源管理和分配，是一个全局的资源管理系统。 NodeManager 以心跳的方式向 ResourceManager 汇报资源使用情况（目前主要是 CPU 和内存的使用情况）。RM 只接受 NM 的资源回报信息，对于具体的资源处理则交给 NM 自己处理。 NodeManagerNodeManager 是每个节点上的资源和任务管理器，它是管理这台机器的代理，负责该节点程序的运行，以及该节点资源的管理和监控。YARN 集群每个节点都运行一个NodeManager。NodeManager 定时向 ResourceManager 汇报本节点资源（CPU、内存）的使用情况和Container 的运行状态。当 ResourceManager 宕机时 NodeManager 自动连接 RM 备用节点。NodeManager 接收并处理来自 ApplicationMaster 的 Container 启动、停止等各种请求。 ApplicationMaster负责与 RM 调度器协商以获取资源（用 Container 表示）。将得到的任务进一步分配给内部的任务(资源的二次分配)。与 NM 通信以启动/停止任务。监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务 Flink on yarn 集群启动步骤 步骤1 用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。 步骤2 ResourceManager为该应用程序分配第一个Container，并与对应的Node-Manager通信，要求它在这个Container中启动应用程序的ApplicationMaster。 步骤3 ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。 步骤4 ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。 步骤5 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。 步骤6 NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 步骤7 各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。 步骤8 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己 on yarn 集群部署设置Hadoop环境变量： 12[root@hadoop2 flink-1.7.2]# vi /etc/profileexport HADOOP_CONF_DIR=这里是你自己的hadoop路径 bin/yarn-session.sh -h 查看使用方法: 在启动的是可以指定TaskManager的个数以及内存(默认是1G)，也可以指定JobManager的内存，但是JobManager的个数只能是一个 我们开启动一个YARN session： 1./bin/yarn-session.sh -n 4 -tm 8192 -s 8 上面命令启动了4个TaskManager，每个TaskManager内存为8G且占用了8个核(是每个TaskManager，默认是1个核)。在启动YARN session的时候会加载conf/flink-config.yaml配置文件，我们可以根据自己的需求去修改里面的相关参数. YARN session启动之后就可以使用bin/flink来启动提交作业: 例如： 1./bin/flink run -c com.demo.wangzhiwu.WordCount $DEMO_DIR/target/flink-demo-1.0.SNAPSHOT.jar --port 9000 flink run的用法如下： 1234567用法: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; &quot;run&quot; 操作参数: -c,--class &lt;classname&gt; 如果没有在jar包中指定入口类，则需要在这里通过这个参数指定 -m,--jobmanager &lt;host:port&gt; 指定需要连接的jobmanager(主节点)地址 使用这个参数可以指定一个不同于配置文件中的jobmanager -p,--parallelism &lt;parallelism&gt; 指定程序的并行度。可以覆盖配置文件中的默认值。 使用run 命令向yarn集群提交一个job。客户端可以确定jobmanager的地址。当然，你也可以通过-m参数指定jobmanager。jobmanager的地址在yarn控制台上可以看到。 值得注意的是： 上面的YARN session是在Hadoop YARN环境下启动一个Flink cluster集群，里面的资源是可以共享给其他的Flink作业。我们还可以在YARN上启动一个Flink作业。这里我们还是使用./bin/flink，但是不需要事先启动YARN session： 123./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar \ --input hdfs://user/hadoop/input.txt \ --output hdfs://user/hadoop/output.txt 上面的命令同样会启动一个类似于YARN session启动的页面。其中的-yn是指TaskManager的个数，必须要指定。 后台运行 yarn session如果你不希望flink yarn client一直运行，也可以启动一个后台运行的yarn session。使用这个参数：-d 或者 –detached在这种情况下，flink yarn client将会只提交任务到集群然后关闭自己。注意：在这种情况下，无法使用flink停止yarn session。必须使用yarn工具来停止yarn session 1yarn application -kill &lt;applicationId&gt; flink on yarn的故障恢复flink 的 yarn 客户端通过下面的配置参数来控制容器的故障恢复。这些参数可以通过conf/flink-conf.yaml 或者在启动yarn session的时候通过-D参数来指定。 yarn.reallocate-failed：这个参数控制了flink是否应该重新分配失败的taskmanager容器。默认是true。 yarn.maximum-failed-containers：applicationMaster可以接受的容器最大失败次数，达到这个参数，就会认为yarn session失败。默认这个次数和初始化请求的taskmanager数量相等(-n 参数指定的)。 yarn.application-attempts：applicationMaster重试的次数。如果这个值被设置为1(默认就是1)，当application master失败的时候，yarn session也会失败。设置一个比较大的值的话，yarn会尝试重启applicationMaster。 日志文件查看在某种情况下，flink yarn session 部署失败是由于它自身的原因，用户必须依赖于yarn的日志来进行分析。最有用的就是yarn log aggregation 。启动它，用户必须在yarn-site.xml文件中设置yarn.log-aggregation-enable 属性为true。一旦启用了，用户可以通过下面的命令来查看一个失败的yarn session的所有详细日志。 1yarn logs -applicationId &lt;application ID&gt; 五、Standalone集群高可用性概述JobManager 协调每个 Flink 部署。它负责调度和资源管理。默认情况下，每个 Flink 集群只有一个 JobManager 实例。 这会产生单点故障(SPOF)：如果 JobManager 崩溃，则无法提交新作业并且导致运行中的作业运行失败。使用 JobManager 高可用性模式，可以避免这个问题，从而消除 SPOF。您可以为Standalone和 YARN 集群配置高可用性。 针对 Standalone 集群的 JobManager 高可用性的一般概念是，任何时候都有一个 主 JobManager 和多个备 JobManagers，以便在主节点失败时有备 JobManagers 来接管集群。这保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以正常运行。主备 JobManager 实例之间没有明显的区别。每个 JobManager 都可以充当主备节点。例如，请考虑以下三个 JobManager 实例的设置: 配置要启用 JobManager 高可用性，您必须将高可用性模式设置为 zookeeper，配置 zookeeper quorum 将所有 JobManager 主机及其 web UI 端口写入配置文件。Flink利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。 ZooKeeper 是独立于 Flink 的服务，通过 Leader 选举和轻量级一致状态存储提供高可靠的分布式协调。 Masters文件 (masters服务器)要启动HA集群，请在以下位置配置Master文件 conf/masters:masters文件：masters文件包含启动 jobmanager 的所有主机和 web 用户界面绑定的端口。 123jobManagerAddress1:webUIPort1 [...]jobManagerAddressX:webUIPortX 默认情况下，job manager选一个随机端口作为进程随机通信端口。您可以通过 high-availability.jobmanager.port 键修改此设置。此配置接受单个端口（例如50010），范围（50000-50025）或两者的组合（50010,50011,50020-50025,50050-50075）。 配置文件（flink-conf.yaml）要启动HA集群，请将以下配置键添加到 conf/flink-conf.yaml: 高可用性模式（必需）：在 conf/flink-conf.yaml 中，必须将高可用性模式设置为zookeeper，以打开高可用模式。或者将此选项设置为工厂类的 FQN，Flink 通过创建 HighAvailabilityServices 实例使用。 1high-availability: zookeeper Zookeeper quorum（必需）： ZooKeeper quorum 是 ZooKeeper 服务器的复制组，它提供分布式协调服务。 1high-availability.zookeeper.quorum:address1:2181[,...],addressX:2181 每个 addressX:port 都是一个 ZooKeeper 服务器的ip及其端口，Flink 可以在指定的地址和端口访问zookeeper。 ZooKeeper root （推荐）： ZooKeeper 根节点，在该节点下放置所有集群节点。 1high-availability.zookeeper.path.root: /flink ZooKeeper cluster-id（推荐）： ZooKeeper的cluster-id节点，在该节点下放置集群的所有相关数据。 1high-availability.cluster-id: /default_ns # important: customize per cluster 重要： 在运行 YARN 或其他群集管理器中运行时，不要手动设置此值。在这些情况下，将根据应用程序 ID 自动生成 cluster-id。 手动设置 cluster-id 会覆盖 YARN 中的自动生成的 ID。反过来，使用 -z CLI 选项指定 cluster-id 会覆盖手动配置。如果在裸机上运行多个 Flink HA 集群，则必须为每个集群手动配置单独的 cluster-id。 存储目录（必需）： JobManager 元数据保存在文件系统 storageDir 中，在 ZooKeeper 中仅保存了指向此状态的指针。 1high-availability.storageDir: hdfs://xxx/flink/recovery 该storageDir 中保存了 JobManager 恢复状态所需的所有元数据。配置 master 文件和 ZooKeeper quorum 之后，您可以使用提供的集群启动脚本。它们将启动 HA 群集。请注意，启动 Flink HA 集群前，必须启动 Zookeeper 集群，并确保为要启动的每个 HA 群集配置单独的 ZooKeeper 根路径。 示例：具有2个 JobManager 的 Standalone 集群 在conf/flink-conf.yaml 中配置高可用模式和 ZooKeeper quorum： 12345high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181high-availability.zookeeper.path.root: /flinkhigh-availability.cluster-id: /cluster_onehigh-availability.storageDir: hdfs://xx/flink/recovery 在 conf/master 中配置 master: 12localhost:8081localhost:8082 在 conf/zoo.cfg 中配置 ZooKeeper 服务（目前，每台机器只能运行一个 ZooKeeper 进程） 1server.0=localhost:2888:3888 启动 ZooKeeper quorum： 12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 启动 Flink HA 集群： 12345$ bin/start-cluster.shStarting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.Starting jobmanager daemon on host localhost.Starting jobmanager daemon on host localhost.Starting taskmanager daemon on host localhost. 停止 Zookeeper quorum 和集群： 123456$ bin/stop-cluster.shStopping taskmanager daemon (pid: 7647) on localhost.Stopping jobmanager daemon (pid: 7495) on host localhost.Stopping jobmanager daemon (pid: 7349) on host localhost.$ bin/stop-zookeeper-quorum.shStopping zookeeper daemon (pid: 7101) on host localhost. 六、YARN 集群的高可用性在运行高可用性 YARN 集群时，我们不会运行多个 JobManager (ApplicationMaster) 实例，而只运行一个，该JobManager实例失败时，YARN会将其重新启动。Yarn的具体行为取决于您使用的 YARN 版本。 配置Application Master最大重试次数（yarn-site.xml）在YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数： 1234567&lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt; The maximum number of application master execution attempts. &lt;/description&gt;&lt;/property&gt; 当前 YARN 版本的默认值是2(表示允许单个JobManager失败两次)。 Application Attempts（flink-conf.yaml）：除了HA配置(参考上文)之外，您还必须配置最大重试次数 conf/flink-conf.yaml: 1yarn.application-attempts: 10 这意味着在如果程序启动失败，YARN会再重试9次（9 次重试 + 1次启动）。如果 YARN 操作需要，如果启动10次作业还失败，yarn才会将该任务的状态置为失败。如果抢占，节点硬件故障或重启，NodeManager 重新同步等操作需要，YARN继续尝试启动应用。 这些重启不计入 yarn.application-attempts 个数中。重要的是要注意 yarn.resourcemanager.am.max-attempts 为yarn中程序重启上限。因此， Flink 中设置的程序尝试次数不能超过 YARN 的集群设置。 示例：高可用的YARN Session1.配置 HA 模式和 ZooKeeper 集群在 conf/flink-conf.yaml 中： 12345high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181high-availability.storageDir: hdfs:///flink/recoveryhigh-availability.zookeeper.path.root: /flinkyarn.application-attempts: 10 配置 ZooKeeper 服务在 conf/zoo.cfg 中(目前每台机器只能运行一个 ZooKeeper 进程)： 1server.0=localhost:2888:3888 启动 ZooKeeper 集群： 12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 启动 HA 集群： 1$ bin / yarn-session.sh -n 2 配置 Zookeeper 安全性如果 ZooKeeper 使用 Kerberos 以安全模式运行，flink-conf.yaml 根据需要覆盖以下配置： 1234567zookeeper.sasl.service-name: zookeeper # 默认设置是 “zookeeper” 。如果 ZooKeeper 集群配置了# 不同的服务名称，那么可以在这里提供。zookeeper.sasl.login-context-name: Client # 默认设置是 “Client”。该值配置需要匹配# &quot;security.kerberos.login.contexts&quot;中的其中一个值。 有关 Kerberos 安全性的 Flink 配置的更多信息，请参阅 此处。您还可以在 此处 找到关于 Flink 内部如何设置基于 kerberos 的安全性的详细信息。 Bootstrap ZooKeeper如果您没有正在运行的ZooKeeper，则可以使用Flink程序附带的脚本。这是一个 ZooKeeper 配置模板 conf/zoo.cfg。您可以为主机配置为使用 server.X 条目运行 ZooKeeper，其中 X 是每个服务器的唯一IP: 123server.X=addressX:peerPort:leaderPort[...]server.Y=addressY:peerPort:leaderPort 该脚本 bin/start-zookeeper-quorum.sh 将在每个配置的主机上启动 ZooKeeper 服务器。 Flink wrapper 会启动 ZooKeeper 服务，该 wraper 从 conf/zoo.cfg 中读取配置，并设置一些必需的配置项。在生产设置中，建议您使用自己安装的 ZooKeeper。]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 状态管理与检查点机制]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F4-Flink%20%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一、状态分类相对于其他流计算框架，Flink 一个比较重要的特性就是其支持有状态计算。即你可以将中间的计算结果进行保存，并提供给后续的计算使用： 具体而言，Flink 又将状态 (State) 分为 Keyed State 与 Operator State： 2.1 算子状态算子状态 (Operator State)：顾名思义，状态是和算子进行绑定的，一个算子的状态不能被其他算子所访问到。官方文档上对 Operator State 的解释是：each operator state is bound to one parallel operator instance，所以更为确切的说一个算子状态是与一个并发的算子实例所绑定的，即假设算子的并行度是 2，那么其应有两个对应的算子状态： 2.2 键控状态键控状态 (Keyed State) ：是一种特殊的算子状态，即状态是根据 key 值进行区分的，Flink 会为每类键值维护一个状态实例。如下图所示，每个颜色代表不同 key 值，对应四个不同的状态实例。需要注意的是键控状态只能在 KeyedStream 上进行使用，我们可以通过 stream.keyBy(...) 来得到 KeyedStream 。 二、状态编程2.1 键控状态Flink 提供了以下数据格式来管理和存储键控状态 (Keyed State)： ValueState：存储单值类型的状态。可以使用 update(T) 进行更新，并通过 T value() 进行检索。 ListState：存储列表类型的状态。可以使用 add(T) 或 addAll(List) 添加元素；并通过 get() 获得整个列表。 ReducingState：用于存储经过 ReduceFunction 计算后的结果，使用 add(T) 增加元素。 AggregatingState：用于存储经过 AggregatingState 计算后的结果，使用 add(IN) 添加元素。 FoldingState：已被标识为废弃，会在未来版本中移除，官方推荐使用 AggregatingState 代替。 MapState：维护 Map 类型的状态。 以上所有增删改查方法不必硬记，在使用时通过语法提示来调用即可。这里给出一个具体的使用示例：假设我们正在开发一个监控系统，当监控数据超过阈值一定次数后，需要发出报警信息。这里之所以要达到一定次数，是因为由于偶发原因，偶尔一次超过阈值并不能代表什么，故需要达到一定次数后才触发报警，这就需要使用到 Flink 的状态编程。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839public class ThresholdWarning extends RichFlatMapFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, List&lt;Long&gt;&gt;&gt; &#123; // 通过ListState来存储非正常数据的状态 private transient ListState&lt;Long&gt; abnormalData; // 需要监控的阈值 private Long threshold; // 触发报警的次数 private Integer numberOfTimes; public void ThresholdWarning(Long threshold, Integer numberOfTimes) &#123; this.threshold = threshold; this.numberOfTimes = numberOfTimes; &#125; @Override public void open(Configuration parameters) &#123; // 通过状态名称(句柄)获取状态实例，如果不存在则会自动创建 abnormalData = getRuntimeContext().getListState( new ListStateDescriptor&lt;&gt;("abnormalData", Long.class)); &#125; @Override public void flatMap(Tuple2&lt;String, Long&gt; value, Collector&lt;Tuple2&lt;String, List&lt;Long&gt;&gt;&gt; out) throws Exception &#123; Long inputValue = value.f1; // 如果输入值超过阈值，则记录该次不正常的数据信息 if (inputValue &gt;= threshold) &#123; abnormalData.add(inputValue); &#125; ArrayList&lt;Long&gt; list = Lists.newArrayList(abnormalData.get().iterator()); // 如果不正常的数据出现达到一定次数，则输出报警信息 if (list.size() &gt;= numberOfTimes) &#123; out.collect(Tuple2.of(value.f0 + " 超过指定阈值 ", list)); // 报警信息输出后，清空状态 abnormalData.clear(); &#125; &#125;&#125; 调用自定义的状态监控，这里我们使用 a，b 来代表不同类型的监控数据，分别对其数据进行监控： 1234567891011final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.fromElements( Tuple2.of("a", 50L), Tuple2.of("a", 80L), Tuple2.of("a", 400L), Tuple2.of("a", 100L), Tuple2.of("a", 200L), Tuple2.of("a", 200L), Tuple2.of("b", 100L), Tuple2.of("b", 200L), Tuple2.of("b", 200L), Tuple2.of("b", 500L), Tuple2.of("b", 600L), Tuple2.of("b", 700L));tuple2DataStreamSource .keyBy(0) .flatMap(new ThresholdWarning(100L, 3)) // 超过100的阈值3次后就进行报警 .printToErr();env.execute("Managed Keyed State"); 输出如下结果如下： 2.2 状态有效期以上任何类型的 keyed state 都支持配置有效期 (TTL) ，示例如下： 1234567891011StateTtlConfig ttlConfig = StateTtlConfig // 设置有效期为 10 秒 .newBuilder(Time.seconds(10)) // 设置有效期更新规则，这里设置为当创建和写入时，都重置其有效期到规定的10秒 .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) /*设置只要值过期就不可见，另外一个可选值是ReturnExpiredIfNotCleanedUp， 代表即使值过期了，但如果还没有被物理删除，就是可见的*/ .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build();ListStateDescriptor&lt;Long&gt; descriptor = new ListStateDescriptor&lt;&gt;("abnormalData", Long.class);descriptor.enableTimeToLive(ttlConfig); 2.3 算子状态相比于键控状态，算子状态目前支持的存储类型只有以下三种： ListState：存储列表类型的状态。 UnionListState：存储列表类型的状态，与 ListState 的区别在于：如果并行度发生变化，ListState 会将该算子的所有并发的状态实例进行汇总，然后均分给新的 Task；而 UnionListState 只是将所有并发的状态实例汇总起来，具体的划分行为则由用户进行定义。 BroadcastState：用于广播的算子状态。 这里我们继续沿用上面的例子，假设此时我们不需要区分监控数据的类型，只要有监控数据超过阈值并达到指定的次数后，就进行报警，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class ThresholdWarning extends RichFlatMapFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, List&lt;Tuple2&lt;String, Long&gt;&gt;&gt;&gt; implements CheckpointedFunction &#123; // 非正常数据 private List&lt;Tuple2&lt;String, Long&gt;&gt; bufferedData; // checkPointedState private transient ListState&lt;Tuple2&lt;String, Long&gt;&gt; checkPointedState; // 需要监控的阈值 private Long threshold; // 次数 private Integer numberOfTimes; public void ThresholdWarning(Long threshold, Integer numberOfTimes) &#123; this.threshold = threshold; this.numberOfTimes = numberOfTimes; this.bufferedData = new ArrayList&lt;&gt;(); &#125; @Override public void initializeState(FunctionInitializationContext context) throws Exception &#123; // 注意这里获取的是OperatorStateStore checkPointedState = context.getOperatorStateStore(). getListState(new ListStateDescriptor&lt;&gt;("abnormalData", TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; &#125;))); // 如果发生重启，则需要从快照中将状态进行恢复 if (context.isRestored()) &#123; for (Tuple2&lt;String, Long&gt; element : checkPointedState.get()) &#123; bufferedData.add(element); &#125; &#125; &#125; @Override public void flatMap(Tuple2&lt;String, Long&gt; value, Collector&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Long&gt;&gt;&gt;&gt; out) &#123; Long inputValue = value.f1; // 超过阈值则进行记录 if (inputValue &gt;= threshold) &#123; bufferedData.add(value); &#125; // 超过指定次数则输出报警信息 if (bufferedData.size() &gt;= numberOfTimes) &#123; // 顺便输出状态实例的hashcode out.collect(Tuple2.of(checkPointedState.hashCode() + "阈值警报！", bufferedData)); bufferedData.clear(); &#125; &#125; @Override public void snapshotState(FunctionSnapshotContext context) throws Exception &#123; // 在进行快照时，将数据存储到checkPointedState checkPointedState.clear(); for (Tuple2&lt;String, Long&gt; element : bufferedData) &#123; checkPointedState.add(element); &#125; &#125;&#125; 调用自定义算子状态，这里需要将并行度设置为 1： 1234567891011121314final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 开启检查点机制env.enableCheckpointing(1000);// 设置并行度为1DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.setParallelism(1).fromElements( Tuple2.of("a", 50L), Tuple2.of("a", 80L), Tuple2.of("a", 400L), Tuple2.of("a", 100L), Tuple2.of("a", 200L), Tuple2.of("a", 200L), Tuple2.of("b", 100L), Tuple2.of("b", 200L), Tuple2.of("b", 200L), Tuple2.of("b", 500L), Tuple2.of("b", 600L), Tuple2.of("b", 700L));tuple2DataStreamSource .flatMap(new ThresholdWarning(100L, 3)) .printToErr();env.execute("Managed Keyed State");&#125; 此时输出如下： 在上面的调用代码中，我们将程序的并行度设置为 1，可以看到三次输出中状态实例的 hashcode 全是一致的，证明它们都同一个状态实例。假设将并行度设置为 2，此时输出如下： 可以看到此时两次输出中状态实例的 hashcode 是不一致的，代表它们不是同一个状态实例，这也就是上文提到的，一个算子状态是与一个并发的算子实例所绑定的。同时这里只输出两次，是因为在并发处理的情况下，线程 1 可能拿到 5 个非正常值，线程 2 可能拿到 4 个非正常值，因为要大于 3 次才能输出，所以在这种情况下就会出现只输出两条记录的情况，所以需要将程序的并行度设置为 1。 三、检查点机制3.1 CheckPoints为了使 Flink 的状态具有良好的容错性，Flink 提供了检查点机制 (CheckPoints) 。通过检查点机制，Flink 定期在数据流上生成 checkpoint barrier ，当某个算子收到 barrier 时，即会基于当前状态生成一份快照，然后再将该 barrier 传递到下游算子，下游算子接收到该 barrier 后，也基于当前状态生成一份快照，依次传递直至到最后的 Sink 算子上。当出现异常后，Flink 就可以根据最近的一次的快照数据将所有算子恢复到先前的状态。 3.2 开启检查点默认情况下，检查点机制是关闭的，需要在程序中进行开启： 1234567891011121314151617// 开启检查点机制，并指定状态检查点之间的时间间隔env.enableCheckpointing(1000); // 其他可选配置如下：// 设置语义env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// 设置两个检查点之间的最小时间间隔env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// 设置执行Checkpoint操作时的超时时间env.getCheckpointConfig().setCheckpointTimeout(60000);// 设置最大并发执行的检查点的数量env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// 将检查点持久化到外部存储env.getCheckpointConfig().enableExternalizedCheckpoints( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// 如果有更近的保存点时，是否将作业回退到该检查点env.getCheckpointConfig().setPreferCheckpointForRecovery(true); 3.3 保存点机制保存点机制 (Savepoints) 是检查点机制的一种特殊的实现，它允许你通过手工的方式来触发 Checkpoint，并将结果持久化存储到指定路径中，主要用于避免 Flink 集群在重启或升级时导致状态丢失。示例如下： 12# 触发指定id的作业的Savepoint，并将结果存储到指定目录下bin/flink savepoint :jobId [:targetDirectory] 更多命令和配置可以参考官方文档：savepoints 四、状态后端4.1 状态管理器分类默认情况下，所有的状态都存储在 JVM 的堆内存中，在状态数据过多的情况下，这种方式很有可能导致内存溢出，因此 Flink 该提供了其它方式来存储状态数据，这些存储方式统一称为状态后端 (或状态管理器)： 主要有以下三种： 1. MemoryStateBackend默认的方式，即基于 JVM 的堆内存进行存储，主要适用于本地开发和调试。 2. FsStateBackend基于文件系统进行存储，可以是本地文件系统，也可以是 HDFS 等分布式文件系统。 需要注意而是虽然选择使用了 FsStateBackend ，但正在进行的数据仍然是存储在 TaskManager 的内存中的，只有在 checkpoint 时，才会将状态快照写入到指定文件系统上。 3. RocksDBStateBackendRocksDBStateBackend 是 Flink 内置的第三方状态管理器，采用嵌入式的 key-value 型数据库 RocksDB 来存储正在进行的数据。等到 checkpoint 时，再将其中的数据持久化到指定的文件系统中，所以采用 RocksDBStateBackend 时也需要配置持久化存储的文件系统。之所以这样做是因为 RocksDB 作为嵌入式数据库安全性比较低，但比起全文件系统的方式，其读取速率更快；比起全内存的方式，其存储空间更大，因此它是一种比较均衡的方案。 4.2 配置方式Flink 支持使用两种方式来配置后端管理器： 第一种方式：基于代码方式进行配置，只对当前作业生效： 1234// 配置 FsStateBackendenv.setStateBackend(new FsStateBackend("hdfs://namenode:40010/flink/checkpoints"));// 配置 RocksDBStateBackendenv.setStateBackend(new RocksDBStateBackend("hdfs://namenode:40010/flink/checkpoints")); 配置 RocksDBStateBackend 时，需要额外导入下面的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt;&lt;/dependency&gt; 第二种方式：基于 flink-conf.yaml 配置文件的方式进行配置，对所有部署在该集群上的作业都生效： 12state.backend: filesystemstate.checkpoints.dir: hdfs://namenode:40010/flink/checkpoints]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 窗口]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F3-Flink%20%E7%AA%97%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[一、窗口概念在大多数场景下，我们需要统计的数据流都是无界的，因此我们无法等待整个数据流终止后才进行统计。通常情况下，我们只需要对某个时间范围或者数量范围内的数据进行统计分析：如每隔五分钟统计一次过去一小时内所有商品的点击量；或者每发生1000次点击后，都去统计一下每个商品点击率的占比。在 Flink 中，我们使用窗口 (Window) 来实现这类功能。按照统计维度的不同，Flink 中的窗口可以分为 时间窗口 (Time Windows) 和 计数窗口 (Count Windows) 。 二、窗口类型 flink支持两种划分窗口的方式(time和count) 如果根据时间划分窗口，那么它就是一个time-window，如果根据数据数量进行划分，那么它就是一个count-window flink支持窗口的两个重要属性(size和interval) 如果 size=interval，那么就会形成trumbiling-window(滚动窗口/无重叠数据) 如果 size&gt;interval，那么就会形成sliding-window(滑动窗口/有重叠数据) 如果 size&lt;interval, 这种窗口将会丢失数据。比如每5秒钟，统计过去3秒钟通过路口汽车的数据，将会漏掉2秒钟的数据。 通过组合可以得出四种基本窗口 time-tumbling-window 无重叠数据之间窗口，设置方式示例: timeWindow(Time.seconds(5)) time-sliding-window 有重叠数据的时间窗口，设置方式示例: timeWindow(Time.seconds(5), Time.seconds(3)) count-tumbling-window 无重叠数据的数量窗口，设置方式示例: countWindow(5) count-sliding-window 有重叠数据的数量窗口，设置方式示例: countWindow(5, 3) flink 支持在stream上的通过key去区分多个窗口 二、Time WindowsTime Windows 用于以时间为维度来进行数据聚合，具体分为以下四类： 2.1 Tumbling Windows滚动窗口 (Tumbling Windows) 是指彼此之间没有重叠的窗口。例如：每隔1小时统计过去1小时内的商品点击量，那么 1 天就只能分为 24 个窗口，每个窗口彼此之间是不存在重叠的，具体如下： 这里我们以词频统计为例，给出一个具体的用例，代码如下： 12345678910111213final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 接收socket上的数据输入DataStreamSource&lt;String&gt; streamSource = env.socketTextStream("hadoop001", 9999, "\n", 3);streamSource.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; String[] words = value.split("\t"); for (String word : words) &#123; out.collect(new Tuple2&lt;&gt;(word, 1L)); &#125; &#125;&#125;).keyBy(0).timeWindow(Time.seconds(3)).sum(1).print(); //每隔3秒统计一次每个单词出现的数量env.execute("Flink Streaming"); 测试结果如下： 假如我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成不重叠的窗口，每一个事件只能属于一个窗口。 123456789// 用户id和购买数量 streamval counts: DataStream[(Int, Int)] = ...val tumblingCnts: DataStream[(Int, Int)] = counts // 用userId分组 .keyBy(0) // 1分钟的翻滚窗口宽度 .timeWindow(Time.minutes(1)) // 计算购买数量 .sum(1) 2.2 Sliding Windows滑动窗口用于滚动进行聚合分析，例如：每隔 6 分钟统计一次过去一小时内所有商品的点击量，那么统计窗口彼此之间就是存在重叠的，即 1天可以分为 240 个窗口。图示如下： 可以看到 window 1 - 4 这四个窗口彼此之间都存在着时间相等的重叠部分。想要实现滑动窗口，只需要在使用 timeWindow 方法时额外传递第二个参数作为滚动时间即可，具体如下： 12// 每隔3秒统计一次过去1分钟内的数据keyBy(0).timeWindow(Time.minutes(1),Time.seconds(3)).sum(1) 2.3 Session Windows当用户在进行持续浏览时，可能每时每刻都会有点击数据，例如在活动区间内，用户可能频繁的将某类商品加入和移除购物车，而你只想知道用户本次浏览最终的购物车情况，此时就可以在用户持有的会话结束后再进行统计。想要实现这类统计，可以通过 Session Windows 来进行实现。 具体的实现代码如下： 1234// 以处理时间为衡量标准，如果10秒内没有任何数据输入，就认为会话已经关闭，此时触发统计window(ProcessingTimeSessionWindows.withGap(Time.seconds(10)))// 以事件时间为衡量标准 window(EventTimeSessionWindows.withGap(Time.seconds(10))) 2.4 Global Windows最后一个窗口是全局窗口， 全局窗口会将所有 key 相同的元素分配到同一个窗口中，其通常配合触发器 (trigger) 进行使用。如果没有相应触发器，则计算将不会被执行。 这里继续以上面词频统计的案例为例，示例代码如下： 12// 当单词累计出现的次数每达到10次时，则触发计算，计算整个窗口内该单词出现的总数window(GlobalWindows.create()).trigger(CountTrigger.of(10)).sum(1).print(); 三、Count WindowsCount Windows 用于以数量为维度来进行数据聚合，同样也分为滚动窗口和滑动窗口，实现方式也和时间窗口完全一致，只是调用的 API 不同，具体如下： 1234// 滚动计数窗口，每1000次点击则计算一次countWindow(1000)// 滑动计数窗口，每10次点击发生后，则计算过去1000次点击的情况countWindow(1000,10) 实际上计数窗口内部就是调用的我们上一部分介绍的全局窗口来实现的，其源码如下： 123456789public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size) &#123; return window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));&#125;public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size, long slide) &#123; return window(GlobalWindows.create()) .evictor(CountEvictor.of(size)) .trigger(CountTrigger.of(slide));&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink-Kafka-Connector]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F13-Flink-Kafka-Connector%2F</url>
    <content type="text"><![CDATA[简介Flink-kafka-connector用来做什么？ Kafka中的partition机制和Flink的并行度机制结合，实现数据恢复Kafka可以作为Flink的source和sink任务失败，通过设置kafka的offset来恢复应用 kafka简单介绍关于kafka，我们会有专题文章介绍，这里简单介绍几个必须知道的概念。 1.生产者（Producer）​ 顾名思义，生产者就是生产消息的组件，它的主要工作就是源源不断地生产出消息，然后发送给消息队列。生产者可以向消息队列发送各种类型的消息，如狭义的字符串消息，也可以发送二进制消息。生产者是消息队列的数据源，只有通过生产者持续不断地向消息队列发送消息，消息队列才能不断处理消息。2.消费者（Consumer）​ 所谓消费者，指的是不断消费（获取）消息的组件，它获取消息的来源就是消息队列（即Kafka本身）。换句话说，生产者不断向消息队列发送消息，而消费者则不断从消息队列中获取消息。3.主题（Topic）​ 主题是Kafka中一个极为重要的概念。首先，主题是一个逻辑上的概念，它用于从逻辑上来归类与存储消息本身。多个生产者可以向一个Topic发送消息，同时也可以有多个消费者消费一个Topic中的消息。Topic还有分区和副本的概念。Topic与消息这两个概念之间密切相关，Kafka中的每一条消息都归属于某一个Topic，而一个Topic下面可以有任意数量的消息。 kafka简单操作启动zk：nohup bin/zookeeper-server-start.sh config/zookeeper.properties &amp; 启动server: nohup bin/kafka-server-start.sh config/server.properties &amp; 创建一个topic：bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test 查看topic：bin/kafka-topics.sh –list –zookeeper localhost:2181 发送数据：bin/kafka-console-producer.sh –broker-list localhost:9092 –topic test 启动一个消费者：bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –from-beginning 删除topic： bin/kafka-topics.sh –delete –zookeeper localhost:2181 –topic topn Flink消费Kafka注意事项 setStartFromGroupOffsets()【默认消费策略】 默认读取上次保存的offset信息如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据 setStartFromEarliest()从最早的数据开始进行消费，忽略存储的offset信息 setStartFromLatest()从最新的数据进行消费，忽略存储的offset信息 setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)从指定位置进行消费 当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。 为了能够使用支持容错的kafka Consumer，需要开启checkpointenv.enableCheckpointing(5000); // 每5s checkpoint一次 搭建Kafka单机环境我本地安装了一个kafka_2.11-2.1.0版本的kafka 启动Zookeeper和kafka server: 123启动zk：nohup bin/zookeeper-server-start.sh config/zookeeper.properties &amp;启动server: nohup bin/kafka-server-start.sh config/server.properties &amp; 创建一个topic: 1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test Kafka作为Flink Sink首先pom依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 向kafka写入数据： 1234567891011121314151617181920public class KafkaProducer &#123; public static void main(String[] args) throws Exception&#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; text = env.addSource(new MyNoParalleSource()).setParallelism(1); Properties properties = new Properties(); properties.setProperty("bootstrap.servers", "localhost:9092"); //new FlinkKafkaProducer("topn",new KeyedSerializationSchemaWrapper(new SimpleStringSchema()),properties,FlinkKafkaProducer.Semantic.EXACTLY_ONCE); FlinkKafkaProducer&lt;String&gt; producer = new FlinkKafkaProducer("test",new SimpleStringSchema(),properties);/* //event-timestamp事件的发生时间 producer.setWriteTimestampToKafka(true);*/ text.addSink(producer); env.execute(); &#125;&#125;// 大家这里特别注意，我们实现了一个并行度为1的MyNoParalleSource来生产数据，代码如下： 12345678910111213141516171819202122232425262728293031323334353637//使用并行度为1的sourcepublic class MyNoParalleSource implements SourceFunction&lt;String&gt; &#123;//1 //private long count = 1L; private boolean isRunning = true; /** * 主要的方法 * 启动一个source * 大部分情况下，都需要在这个run方法中实现一个循环，这样就可以循环产生数据了 * * @param ctx * @throws Exception */ @Override public void run(SourceContext&lt;String&gt; ctx) throws Exception &#123; while(isRunning)&#123; //图书的排行榜 List&lt;String&gt; books = new ArrayList&lt;&gt;(); books.add("Pyhton从入门到放弃");//10 books.add("Java从入门到放弃");//8 books.add("Php从入门到放弃");//5 books.add("C++从入门到放弃");//3 books.add("Scala从入门到放弃");//0-4 int i = new Random().nextInt(5); ctx.collect(books.get(i)); //每2秒产生一条数据 Thread.sleep(2000); &#125; &#125; //取消一个cancel的时候会调用的方法 @Override public void cancel() &#123; isRunning = false; &#125;&#125; 代码实现了一个发送器，来发送书名&lt;Pyhton从入门到放弃&gt;&lt;Java从入门到放弃&gt;等… 然后右键运行我们的程序，控制台输出如下： 开始源源不断的生产数据了。 然后我们用命令去查看一下 kafka test这个topic： 1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning 输出如下： Kafka作为Flink Source直接上代码： 123456789101112131415161718public class KafkaConsumer &#123; public static void main(String[] args) throws Exception&#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty("bootstrap.servers", "localhost:9092"); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;("test", new SimpleStringSchema(), properties); //从最早开始消费 consumer.setStartFromEarliest(); DataStream&lt;String&gt; stream = env .addSource(consumer); stream.print(); //stream.map(); env.execute(); &#125;&#125; 控制台输出如下： 将我们之前发往kafka的消息全部打印出来了。]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink 介绍]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F1-Flink%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[一、Flink 简介Apache Flink 诞生于柏林工业大学的一个研究性项目，原名 StratoSphere 。2014 年，由 StratoSphere 项目孵化出 Flink，并于同年捐赠 Apache，之后成为 Apache 的顶级项目。2019 年 1 年，阿里巴巴收购了 Flink 的母公司 Data Artisans，并宣布开源内部的 Blink，Blink 是阿里巴巴基于 Flink 优化后的版本，增加了大量的新功能，并在性能和稳定性上进行了各种优化，经历过阿里内部多种复杂业务的挑战和检验。同时阿里巴巴也表示会逐步将这些新功能和特性 Merge 回社区版本的 Flink 中，因此 Flink 成为目前最为火热的大数据处理框架。 简单来说，Flink 是一个分布式的流处理框架，它能够对有界和无界的数据流进行高效的处理。Flink 的核心是流处理，当然它也能支持批处理，Flink 将批处理看成是流处理的一种特殊情况，即数据流是有明确界限的。这和 Spark Streaming 的思想是完全相反的，Spark Streaming 的核心是批处理，它将流处理看成是批处理的一种特殊情况， 即把数据流进行极小粒度的拆分，拆分为多个微批处理。 Flink 有界数据流和无界数据流： Spark Streaming 数据流的拆分： 二、Flink 核心架构2.0 Flink组件栈Flink 采用分层的架构设计，从而保证各层在功能和职责上的清晰。如下图所示，由上而下分别是 API &amp; Libraries 层、Runtime 核心层以及物理部署层： 2.1 API &amp; Libraries 层这一层主要提供了编程 API 和 顶层类库： 编程 API : 用于进行流处理的 DataStream API 和用于进行批处理的 DataSet API； 顶层类库：包括用于复杂事件处理的 CEP 库；用于结构化数据查询的 SQL &amp; Table 库，以及基于批处理的机器学习库 FlinkML 和 图形处理库 Gelly。 2.2 Runtime 核心层这一层是 Flink 分布式计算框架的核心实现层，包括作业转换，任务调度，资源分配，任务执行等功能，基于这一层的实现，可以在流式引擎下同时运行流处理程序和批处理程序。比如：支持分布式Stream处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务 2.3 物理部署层Flink 的物理部署层，用于支持在不同平台上部署运行 Flink 应用。 主要涉及了Flink的部署模式，Flink支持多种部署模式：本地、集群（Standalone/YARN）、云（GCE/EC2） 三、Flink 分层 API在上面介绍的 API &amp; Libraries 这一层，Flink 又进行了更为具体的划分。具体如下： 按照如上的层次结构，API 的一致性由下至上依次递增，接口的表现能力由下至上依次递减，各层的核心功能如下： 3.1 SQL &amp; Table APISQL &amp; Table API 同时适用于批处理和流处理，这意味着你可以对有界数据流和无界数据流以相同的语义进行查询，并产生相同的结果。除了基本查询外， 它还支持自定义的标量函数，聚合函数以及表值函数，可以满足多样化的查询需求。 3.2 DataStream &amp; DataSet APIDataStream &amp; DataSet API 是 Flink 数据处理的核心 API，支持使用 Java 语言或 Scala 语言进行调用，提供了数据读取，数据转换和数据输出等一系列常用操作的封装。 3.3 Stateful Stream ProcessingStateful Stream Processing 是最低级别的抽象，它通过 Process Function 函数内嵌到 DataStream API 中。 Process Function 是 Flink 提供的最底层 API，具有最大的灵活性，允许开发者对于时间和状态进行细粒度的控制。 四、Flink 集群架构4.1 核心组件按照上面的介绍，Flink 核心架构的第二层是 Runtime 层， 该层采用标准的 Master - Slave 结构， 其中，Master 部分又包含了三个核心组件：Dispatcher、ResourceManager 和 JobManager，而 Slave 则主要是 TaskManager 进程。它们的功能分别如下： JobManagers (也称为 masters) ：JobManagers 接收由 Dispatcher 传递过来的执行程序，该执行程序包含了作业图 (JobGraph)，逻辑数据流图 (logical dataflow graph) 及其所有的 classes 文件以及第三方类库 (libraries) 等等 。紧接着 JobManagers 会将 JobGraph 转换为执行图 (ExecutionGraph)，然后向 ResourceManager 申请资源来执行该任务，一旦申请到资源，就将执行图分发给对应的 TaskManagers 。因此每个作业 (Job) 至少有一个 JobManager；高可用部署下可以有多个 JobManagers，其中一个作为 leader，其余的则处于 standby 状态。 TaskManagers (也称为 workers) : TaskManagers 负责实际的子任务 (subtasks) 的执行，每个 TaskManagers 都拥有一定数量的 slots。Slot 是一组固定大小的资源的合集 (如计算能力，存储空间)。TaskManagers 启动后，会将其所拥有的 slots 注册到 ResourceManager 上，由 ResourceManager 进行统一管理。 Dispatcher：负责接收客户端提交的执行程序，并传递给 JobManager 。除此之外，它还提供了一个 WEB UI 界面，用于监控作业的执行情况。 Client： 用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群， Client会将用户提交的Flink程序组装一个JobGraph， 并且是以JobGraph的形式提交的 ResourceManager ：负责管理 slots 并协调集群资源。ResourceManager 接收来自 JobManager 的资源请求，并将存在空闲 slots 的 TaskManagers 分配给 JobManager 执行任务。Flink 基于不同的部署平台，如 YARN , Mesos，K8s 等提供了不同的资源管理器，当 TaskManagers 没有足够的 slots 来执行任务时，它会向第三方平台发起会话来请求额外的资源。 4.2 Task &amp; SubTask上面我们提到：TaskManagers 实际执行的是 SubTask，而不是 Task，这里解释一下两者的区别： 在执行分布式计算时，Flink 将可以链接的操作 (operators) 链接到一起，这就是 Task。之所以这样做， 是为了减少线程间切换和缓冲而导致的开销，在降低延迟的同时可以提高整体的吞吐量。 但不是所有的 operator 都可以被链接，如下 keyBy 等操作会导致网络 shuffle 和重分区，因此其就不能被链接，只能被单独作为一个 Task。 简单来说，一个 Task 就是一个可以链接的最小的操作链 (Operator Chains) 。如下图，source 和 map 算子被链接到一块，因此整个作业就只有三个 Task： 解释完 Task ，我们在解释一下什么是 SubTask，其准确的翻译是： A subtask is one parallel slice of a task，即一个 Task 可以按照其并行度拆分为多个 SubTask。如上图，source &amp; map 具有两个并行度，KeyBy 具有两个并行度，Sink 具有一个并行度，因此整个虽然只有 3 个 Task，但是却有 5 个 SubTask。Jobmanager 负责定义和拆分这些 SubTask，并将其交给 Taskmanagers 来执行，每个 SubTask 都是一个单独的线程。 4.3 资源管理理解了 SubTasks ，我们再来看看其与 Slots 的对应情况。一种可能的分配情况如下： 这时每个 SubTask 线程运行在一个独立的 TaskSlot， 它们共享所属的 TaskManager 进程的TCP 连接（通过多路复用技术）和心跳信息 (heartbeat messages)，从而可以降低整体的性能开销。此时看似是最好的情况，但是每个操作需要的资源都是不尽相同的，这里假设该作业 keyBy 操作所需资源的数量比 Sink 多很多 ，那么此时 Sink 所在 Slot 的资源就没有得到有效的利用。 基于这个原因，Flink 允许多个 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，但只要它们来自同一个 Job 就可以。假设上面 souce &amp; map 和 keyBy 的并行度调整为 6，而 Slot 的数量不变，此时情况如下： 可以看到一个 Task Slot 中运行了多个 SubTask 子任务，此时每个子任务仍然在一个独立的线程中执行，只不过共享一组 Sot 资源而已。那么 Flink 到底如何确定一个 Job 至少需要多少个 Slot 呢？Flink 对于这个问题的处理很简单，默认情况一个 Job 所需要的 Slot 的数量就等于其 Operation 操作的最高并行度。如下， A，B，D 操作的并行度为 4，而 C，E 操作的并行度为 2，那么此时整个 Job 就需要至少四个 Slots 来完成。通过这个机制，Flink 就可以不必去关心一个 Job 到底会被拆分为多少个 Tasks 和 SubTasks。 4.4 组件通讯 Flink是基于Master-Slave风格的架构 Flink集群启动时，会启动一个JobManager进程、至少一个TaskManager进程 Flink 的所有组件都基于 Actor System 来进行通讯。Actor system是多种角色的 actor 的容器，它提供调度，配置，日志记录等多种服务，并包含一个可以启动所有 actor 的线程池，如果 actor 是本地的，则消息通过共享内存进行共享，但如果 actor 是远程的，则通过 RPC 的调用来传递消息。 五、Flink编程模型 Flink程序的基础构建模块是流(streams) 与 转换(transformations) 每一个数据流起始于一个或多个 source，并终止于一个或多个 sink 每一个数据流起始于一个或多个 source，并终止于一个或多个 sink 下面是一个由Flink程序映射为Streaming Dataflow的示意图: 并行数据流示意图: 五、Flink的优势 支持高吞吐、低延迟、高性能的流处理 支持高度灵活的窗口（Window）操作 支持有状态计算的Exactly-once语义 提供DataStream API和DataSet API 六、Flink 的优点最后基于上面的介绍，来总结一下 Flink 的优点： Flink 是基于事件驱动 (Event-driven) 的应用，能够同时支持流处理和批处理； 基于内存的计算，能够保证高吞吐和低延迟，具有优越的性能表现； 支持精确一次 (Exactly-once) 语意，能够完美地保证一致性和正确性； 分层 API ，能够满足各个层次的开发需求； 支持高可用配置，支持保存点机制，能够提供安全性和稳定性上的保证； 多样化的部署方式，支持本地，远端，云端等多种部署方案； 具有横向扩展架构，能够按照用户的需求进行动态扩容； 活跃度极高的社区和完善的生态圈的支持。 七、 开发环境搭建使用 IDEA 构建如果你使用的是开发工具是 IDEA ，可以直接在项目创建页面选择 Maven Flink Archetype 进行项目初始化： 如果你的 IDEA 没有上述 Archetype， 可以通过点击右上角的 ADD ARCHETYPE ，来进行添加，依次填入所需信息，这些信息都可以从上述的 archetype:generate 语句中获取。点击 OK 保存后，该 Archetype 就会一直存在于你的 IDEA 中，之后每次创建项目时，只需要直接选择该 Archetype 即可： 选中 Flink Archetype ，然后点击 NEXT 按钮，之后的所有步骤都和正常的 Maven 工程相同。 项目结构创建完成后的自动生成的项目结构如下： 其中 BatchJob 为批处理的样例代码，源码如下： 123456789import org.apache.flink.api.scala._object BatchJob &#123; def main(args: Array[String]) &#123; val env = ExecutionEnvironment.getExecutionEnvironment .... env.execute("Flink Batch Scala API Skeleton") &#125;&#125; getExecutionEnvironment 代表获取批处理的执行环境，如果是本地运行则获取到的就是本地的执行环境；如果在集群上运行，得到的就是集群的执行环境。如果想要获取流处理的执行环境，则只需要将 ExecutionEnvironment 替换为 StreamExecutionEnvironment， 对应的代码样例在 StreamingJob 中： 123456789import org.apache.flink.streaming.api.scala._object StreamingJob &#123; def main(args: Array[String]) &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment ... env.execute("Flink Streaming Scala API Skeleton") &#125;&#125; 需要注意的是对于流处理项目 env.execute() 这句代码是必须的，否则流处理程序就不会被执行，但是对于批处理项目则是可选的。 主要依赖基于 Maven 骨架创建的项目主要提供了以下核心依赖：其中 flink-scala 用于支持开发批处理程序 ；flink-streaming-scala 用于支持开发流处理程序 ；scala-library 用于提供 Scala 语言所需要的类库。如果在使用 Maven 骨架创建时选择的是 Java 语言，则默认提供的则是 flink-java 和 flink-streaming-java 依赖。 12345678910111213141516171819202122&lt;!-- Apache Flink dependencies --&gt;&lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- Scala Library, provided by Flink as well. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 需要特别注意的以上依赖的 scope 标签全部被标识为 provided ，这意味着这些依赖都不会被打入最终的 JAR 包。因为 Flink 的安装包中已经提供了这些依赖，位于其 lib 目录下，名为 flink-dist_*.jar ，它包含了 Flink 的所有核心类和依赖： scope 标签被标识为 provided 会导致你在 IDEA 中启动项目时会抛出 ClassNotFoundException 异常。基于这个原因，在使用 IDEA 创建项目时还自动生成了以下 profile 配置： 1234567891011121314151617181920212223242526272829303132333435&lt;!-- This profile helps to make things run out of the box in IntelliJ --&gt;&lt;!-- Its adds Flink's core classes to the runtime class path. --&gt;&lt;!-- Otherwise they are missing in IntelliJ, because the dependency is 'provided' --&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;add-dependencies-for-IDEA&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;idea.version&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/profile&gt;&lt;/profiles&gt; 在 id 为 add-dependencies-for-IDEA 的 profile 中，所有的核心依赖都被标识为 compile，此时你可以无需改动任何代码，只需要在 IDEA 的 Maven 面板中勾选该 profile，即可直接在 IDEA 中运行 Flink 项目： 词频统计案例项目创建完成后，可以先书写一个简单的词频统计的案例来尝试运行 Flink 项目，以下以 Scala 语言为例，分别介绍流处理程序和批处理程序的编程示例： 批处理示例 123456789101112131415import org.apache.flink.api.scala._object WordCountBatch &#123; def main(args: Array[String]): Unit = &#123; val benv = ExecutionEnvironment.getExecutionEnvironment val dataSet = benv.readTextFile("D:\\wordcount.txt") dataSet.flatMap &#123; _.toLowerCase.split(",")&#125; .filter (_.nonEmpty) .map &#123; (_, 1) &#125; .groupBy(0) .sum(1) .print() &#125;&#125; 其中 wordcount.txt 中的内容如下： 1234a,a,a,a,ab,b,bc,cd,d 本机不需要配置其他任何的 Flink 环境，直接运行 Main 方法即可 流处理示例 1234567891011121314151617181920import org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.windowing.time.Timeobject WordCountStreaming &#123; def main(args: Array[String]): Unit = &#123; val senv = StreamExecutionEnvironment.getExecutionEnvironment val dataStream: DataStream[String] = senv.socketTextStream("192.168.0.229", 9999, '\n') dataStream.flatMap &#123; line =&gt; line.toLowerCase.split(",") &#125; .filter(_.nonEmpty) .map &#123; word =&gt; (word, 1) &#125; .keyBy(0) .timeWindow(Time.seconds(3)) .sum(1) .print() senv.execute("Streaming WordCount") &#125;&#125; 这里以监听指定端口号上的内容为例，使用以下命令来开启端口服务： 1nc -lk 9999 之后输入测试数据即可观察到流处理程序的处理情况。 八、使用 Scala Shell对于日常的 Demo 项目，如果你不想频繁地启动 IDEA 来观察测试结果，可以像 Spark 一样，直接使用 Scala Shell 来运行程序，这对于日常的学习来说，效果更加直观，也更省时。Flink 安装包的下载地址如下： 1https://flink.apache.org/downloads.html Flink 大多数版本都提供有 Scala 2.11 和 Scala 2.12 两个版本的安装包可供下载： 下载完成后进行解压即可，Scala Shell 位于安装目录的 bin 目录下，直接使用以下命令即可以本地模式启动： 1./start-scala-shell.sh local 命令行启动完成后，其已经提供了批处理 （benv 和 btenv）和流处理（senv 和 stenv）的运行环境，可以直接运行 Scala Flink 程序，示例如下： 最后解释一个常见的异常：这里我使用的 Flink 版本为 1.9.1，启动时会抛出如下异常。这里因为按照官方的说明，目前所有 Scala 2.12 版本的安装包暂时都不支持 Scala Shell，所以如果想要使用 Scala Shell，只能选择 Scala 2.11 版本的安装包。 12[root@hadoop001 bin]# ./start-scala-shell.sh local错误: 找不到或无法加载主类 org.apache.flink.api.scala.FlinkShell]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink-Redis-Sink]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F15-Flink-Redis-Sink%2F</url>
    <content type="text"><![CDATA[简介流式计算中，我们经常有一些场景是消费Kafka数据，进行处理，然后存储到其他的数据库或者缓存或者重新发送回其他的消息队列中。本文讲述一个简单的Redis作为Sink的案例。 关于Redis SinkFlink提供了封装好的写入Redis的包给我们用，首先我们要新增一个依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.10&lt;/artifactId&gt; &lt;version&gt;1.1.5&lt;/version&gt;&lt;/dependency&gt; 然后我们实现一个自己的RedisSinkExample： 1234567891011121314//指定Redis setpublic static final class RedisSinkExample implements RedisMapper&lt;Tuple2&lt;String,Integer&gt;&gt; &#123; public RedisCommandDescription getCommandDescription() &#123; return new RedisCommandDescription(RedisCommand.SET, null); &#125; public String getKeyFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f0; &#125; public String getValueFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f1.toString(); &#125;&#125; 我们用最简单的单机Redis的SET命令进行演示。 完整的代码如下，实现一个读取Kafka的消息，然后进行WordCount，并把结果更新到redis中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class RedisSinkTest &#123;public static void main(String[] args) throws Exception&#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); env.enableCheckpointing(2000); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); //连接kafka Properties properties = new Properties(); properties.setProperty("bootstrap.servers", "127.0.0.1:9092"); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;("test", new SimpleStringSchema(), properties); consumer.setStartFromEarliest(); DataStream&lt;String&gt; stream = env.addSource(consumer); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = stream.flatMap(new LineSplitter()).keyBy(0).sum(1); //实例化FlinkJedisPoolConfig 配置redis FlinkJedisPoolConfig conf = new FlinkJedisPoolConfig.Builder().setHost("127.0.0.1").setPort("6379").build(); //实例化RedisSink，并通过flink的addSink的方式将flink计算的结果插入到redis counts.addSink(new RedisSink&lt;&gt;(conf,new RedisSinkExample())); env.execute("WordCount From Kafka To Redis"); &#125;// public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123; String[] tokens = value.toLowerCase().split("\\W+"); for (String token : tokens) &#123; if (token.length() &gt; 0) &#123; out.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125; &#125; &#125; //指定Redis set public static final class RedisSinkExample implements RedisMapper&lt;Tuple2&lt;String,Integer&gt;&gt; &#123; public RedisCommandDescription getCommandDescription() &#123; return new RedisCommandDescription(RedisCommand.SET, null); &#125; public String getKeyFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f0; &#125; public String getValueFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f1.toString(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink SQL]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F14-Flink%20SQL%2F</url>
    <content type="text"><![CDATA[简介Apache Flink具有两个关系API - 表API和SQL - 用于统一流和批处理。Table API是Scala和Java的语言集成查询API，允许以非常直观的方式组合来自关系运算符的查询，Table API和SQL接口彼此紧密集成，以及Flink的DataStream和DataSet API。您可以轻松地在基于API构建的所有API和库之间切换。例如，您可以使用CEP库从DataStream中提取模式，然后使用Table API分析模式，或者可以在预处理上运行Gelly图算法之前使用SQL查询扫描，过滤和聚合批处理表数据。 Flink SQL的编程模型创建一个TableEnvironmentTableEnvironment是Table API和SQL集成的核心概念，它主要负责: 1、在内部目录中注册一个Table 2、注册一个外部目录 3、执行SQL查询 4、注册一个用户自定义函数(标量、表及聚合) 5、将DataStream或者DataSet转换成Table 6、持有ExecutionEnvironment或者StreamExecutionEnvironment的引用一个Table总是会绑定到一个指定的TableEnvironment中，相同的查询不同的TableEnvironment是无法通过join、union合并在一起。TableEnvironment有一个在内部通过表名组织起来的表目录，Table API或者SQL查询可以访问注册在目录中的表，并通过名称来引用它们。 在目录中注册表TableEnvironment允许通过各种源来注册一个表: 1、一个已存在的Table对象，通常是Table API或者SQL查询的结果​ Table projTable = tableEnv.scan(“X”).select(…); 2、TableSource，可以访问外部数据如文件、数据库或者消息系统​ TableSource csvSource = new CsvTableSource(“/path/to/file”, …); 3、程序中的DataStream或者DataSet​ 将DataSet转换为Table​ Table table= tableEnv.fromDataSet(dataSet); 注册TableSink注册TableSink可用于将 Table API或SQL查询的结果发送到外部存储系统，例如数据库，键值存储，消息队列或文件系统（在不同的编码中，例如，CSV，Apache [Parquet] ，Avro，ORC]，……）: 1234TableSink csvSink = new CsvTableSink(&quot;/path/to/file&quot;, ...); String[] fieldNames = &#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;; TypeInformation[] fieldTypes = &#123;Types.INT, Types.STRING, Types.LONG&#125;; tableEnv.registerTableSink(&quot;CsvSinkTable&quot;, fieldNames, fieldTypes, csvSink); 实战案例一基于Flink SQL的WordCount: 12345678910111213141516171819202122232425262728293031323334353637383940public class WordCountSQL &#123; public static void main(String[] args) throws Exception&#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tEnv = TableEnvironment.getTableEnvironment(env); List list = new ArrayList(); String wordsStr = "Hello Flink Hello TOM"; String[] words = wordsStr.split("\\W+"); for(String word : words)&#123; WC wc = new WC(word, 1); list.add(wc); &#125; DataSet&lt;WC&gt; input = env.fromCollection(list); tEnv.registerDataSet("WordCount", input, "word, frequency"); Table table = tEnv.sqlQuery( "SELECT word, SUM(frequency) as frequency FROM WordCount GROUP BY word"); DataSet&lt;WC&gt; result = tEnv.toDataSet(table, WC.class); result.print(); &#125;//main public static class WC &#123; public String word;//hello public long frequency;//1 // public constructor to make it a Flink POJO public WC() &#123;&#125; public WC(String word, long frequency) &#123; this.word = word; this.frequency = frequency; &#125; @Override public String toString() &#123; return "WC " + word + " " + frequency; &#125; &#125;&#125; 输出如下： 123WC TOM 1WC Hello 2WC Flink 1 实战案例二本例稍微复杂，首先读取一个文件中的内容进行统计，并写入到另外一个文件中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class SQLTest &#123; public static void main(String[] args) throws Exception&#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = BatchTableEnvironment.getTableEnvironment(env); env.setParallelism(1); DataSource&lt;String&gt; input = env.readTextFile("test.txt"); input.print(); //转换成dataset DataSet&lt;Orders&gt; topInput = input.map(new MapFunction&lt;String, Orders&gt;() &#123; @Override public Orders map(String s) throws Exception &#123; String[] splits = s.split(" "); return new Orders(Integer.valueOf(splits[0]), String.valueOf(splits[1]),String.valueOf(splits[2]), Double.valueOf(splits[3])); &#125; &#125;); //将DataSet转换为Table Table order = tableEnv.fromDataSet(topInput); //orders表名 tableEnv.registerTable("Orders",order); Table tapiResult = tableEnv.scan("Orders").select("name"); tapiResult.printSchema(); Table sqlQuery = tableEnv.sqlQuery("select name, sum(price) as total from Orders group by name order by total desc"); //转换回dataset DataSet&lt;Result&gt; result = tableEnv.toDataSet(sqlQuery, Result.class); //将dataset map成tuple输出 /*result.map(new MapFunction&lt;Result, Tuple2&lt;String,Double&gt;&gt;() &#123; @Override public Tuple2&lt;String, Double&gt; map(Result result) throws Exception &#123; String name = result.name; Double total = result.total; return Tuple2.of(name,total); &#125; &#125;).print();*/ TableSink sink = new CsvTableSink("SQLTEST.txt", "|"); //writeToSink /*sqlQuery.writeToSink(sink); env.execute();*/ String[] fieldNames = &#123;"name", "total"&#125;; TypeInformation[] fieldTypes = &#123;Types.STRING, Types.DOUBLE&#125;; tableEnv.registerTableSink("SQLTEST", fieldNames, fieldTypes, sink); sqlQuery.insertInto("SQLTEST"); env.execute(); &#125; /** * 源数据的映射类 */ public static class Orders &#123; /** * 序号，姓名，书名，价格 */ public Integer id; public String name; public String book; public Double price; public Orders() &#123; super(); &#125; public Orders(Integer id, String name, String book, Double price) &#123; this.id = id; this.name = name; this.book = book; this.price = price; &#125; &#125; /** * 统计结果对应的类 */ public static class Result &#123; public String name; public Double total; public Result() &#123;&#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink(Source-KafkaSink-Mysql)]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F16-Flink(Source-KafkaSink-Mysql)%2F</url>
    <content type="text"><![CDATA[本文介绍消费Kafka的消息实时写入Mysql maven新增依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt;&lt;/dependency&gt; 2.重写RichSinkFunction,实现一个Mysql Sink 123456789101112131415161718192021222324252627public class MysqlSink extends RichSinkFunction&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; &#123; private Connection connection; private PreparedStatement preparedStatement; String username = ""; String password = ""; String drivername = ""; //配置改成自己的配置 String dburl = ""; @Override public void invoke(Tuple3&lt;Integer, String, Integer&gt; value) throws Exception &#123; Class.forName(drivername); connection = DriverManager.getConnection(dburl, username, password); String sql = "replace into table(id,num,price) values(?,?,?)"; //假设mysql 有3列 id,num,price preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.setInt(3, value.f2); preparedStatement.executeUpdate(); if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125;&#125; Flink主类 123456789101112131415161718192021222324252627public class MysqlSinkTest &#123; public static void main(String[] args) throws Exception &#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty("bootstrap.servers", "localhost:9092"); // 1,abc,100 类似这样的数据，当然也可以是很复杂的json数据，去做解析 FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;("test", new SimpleStringSchema(), properties); env.getConfig().disableSysoutLogging(); //设置此可以屏蔽掉日记打印情况 env.getConfig().setRestartStrategy( RestartStrategies.fixedDelayRestart(5, 5000)); env.enableCheckpointing(2000); DataStream&lt;String&gt; stream = env .addSource(consumer); DataStream&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; sourceStream = stream.filter((FilterFunction&lt;String&gt;) value -&gt; StringUtils.isNotBlank(value)).map((MapFunction&lt;String, Tuple3&lt;Integer, String, Integer&gt;&gt;) value -&gt; &#123; String[] args1 = value.split(","); return new Tuple3&lt;Integer, String, Integer&gt;(Integer .valueOf(args1[0]), args1[1],Integer .valueOf(args1[2])); &#125;); sourceStream.addSink(new MysqlSink()); env.execute("data to mysql start"); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Source、Transform、Sink]]></title>
    <url>%2F2020%2F04%2F09%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FFlink%2F2-Source%E3%80%81Transform%E3%80%81Sink%2F</url>
    <content type="text"><![CDATA[Data Source一、内置 Data SourceFlink Data Source 用于定义 Flink 程序的数据来源，Flink 官方提供了多种数据获取方法，用于帮助开发者简单快速地构建输入流，具体如下： 1.1 基于文件构建1. readTextFile(path)：按照 TextInputFormat 格式读取文本文件，并将其内容以字符串的形式返回。示例如下： 1env.readTextFile(filePath).print(); 2. readFile(fileInputFormat, path) ：按照指定格式读取文件。 3. readFile(inputFormat, filePath, watchType, interval, typeInformation)：按照指定格式周期性的读取文件。其中各个参数的含义如下： inputFormat：数据流的输入格式。 filePath：文件路径，可以是本地文件系统上的路径，也可以是 HDFS 上的文件路径。 watchType：读取方式，它有两个可选值，分别是 FileProcessingMode.PROCESS_ONCE 和 FileProcessingMode.PROCESS_CONTINUOUSLY：前者表示对指定路径上的数据只读取一次，然后退出；后者表示对路径进行定期地扫描和读取。需要注意的是如果 watchType 被设置为 PROCESS_CONTINUOUSLY，那么当文件被修改时，其所有的内容 (包含原有的内容和新增的内容) 都将被重新处理，因此这会打破 Flink 的 exactly-once 语义。 interval：定期扫描的时间间隔。 typeInformation：输入流中元素的类型。 使用示例如下： 12345678final String filePath = "D:\\log4j.properties";final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.readFile(new TextInputFormat(new Path(filePath)), filePath, FileProcessingMode.PROCESS_ONCE, 1, BasicTypeInfo.STRING_TYPE_INFO).print();env.execute(); 1.2 基于集合构建1. fromCollection(Collection)：基于集合构建，集合中的所有元素必须是同一类型。示例如下： 1env.fromCollection(Arrays.asList(1,2,3,4,5)).print(); 2. fromElements(T …)： 基于元素构建，所有元素必须是同一类型。示例如下： 1env.fromElements(1,2,3,4,5).print(); 3. generateSequence(from, to)：基于给定的序列区间进行构建。示例如下： 1env.generateSequence(0,100); 4. fromCollection(Iterator, Class)：基于迭代器进行构建。第一个参数用于定义迭代器，第二个参数用于定义输出元素的类型。使用示例如下： 1env.fromCollection(new CustomIterator(), BasicTypeInfo.INT_TYPE_INFO).print(); 其中 CustomIterator 为自定义的迭代器，这里以产生 1 到 100 区间内的数据为例，源码如下。需要注意的是自定义迭代器除了要实现 Iterator 接口外，还必须要实现序列化接口 Serializable ，否则会抛出序列化失败的异常： 1234567891011121314151617import java.io.Serializable;import java.util.Iterator;public class CustomIterator implements Iterator&lt;Integer&gt;, Serializable &#123; private Integer i = 0; @Override public boolean hasNext() &#123; return i &lt; 100; &#125; @Override public Integer next() &#123; i++; return i; &#125;&#125; 5. fromParallelCollection(SplittableIterator, Class)：方法接收两个参数，第二个参数用于定义输出元素的类型，第一个参数 SplittableIterator 是迭代器的抽象基类，它用于将原始迭代器的值拆分到多个不相交的迭代器中。 1.3 基于 Socket 构建Flink 提供了 socketTextStream 方法用于构建基于 Socket 的数据流，socketTextStream 方法有以下四个主要参数： hostname：主机名； port：端口号，设置为 0 时，表示端口号自动分配； delimiter：用于分隔每条记录的分隔符； maxRetry：当 Socket 临时关闭时，程序的最大重试间隔，单位为秒。设置为 0 时表示不进行重试；设置为负值则表示一直重试。示例如下： 1env.socketTextStream("192.168.0.229", 9999, "\n", 3).print(); 二、自定义 Data Source2.1 SourceFunction除了内置的数据源外，用户还可以使用 addSource 方法来添加自定义的数据源。自定义的数据源必须要实现 SourceFunction 接口，这里以产生 [0 , 1000) 区间内的数据为例，代码如下： 123456789101112131415161718192021final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.addSource(new SourceFunction&lt;Long&gt;() &#123; private long count = 0L; private volatile boolean isRunning = true; public void run(SourceContext&lt;Long&gt; ctx) &#123; while (isRunning &amp;&amp; count &lt; 1000) &#123; // 通过collect将输入发送出去 ctx.collect(count); count++; &#125; &#125; public void cancel() &#123; isRunning = false; &#125;&#125;).print();env.execute(); 2.2 ParallelSourceFunction 和 RichParallelSourceFunction上面通过 SourceFunction 实现的数据源是不具有并行度的，即不支持在得到的 DataStream 上调用 setParallelism(n) 方法，此时会抛出如下的异常： 1Exception in thread "main" java.lang.IllegalArgumentException: Source: 1 is not a parallel source 如果你想要实现具有并行度的输入流，则需要实现 ParallelSourceFunction 或 RichParallelSourceFunction 接口，其与 SourceFunction 的关系如下图： ParallelSourceFunction 直接继承自 ParallelSourceFunction，具有并行度的功能。RichParallelSourceFunction 则继承自 AbstractRichFunction，同时实现了 ParallelSourceFunction 接口，所以其除了具有并行度的功能外，还提供了额外的与生命周期相关的方法，如 open() ，closen() 。 三、Streaming Connectors3.1 内置连接器除了自定义数据源外， Flink 还内置了多种连接器，用于满足大多数的数据收集场景。当前内置连接器的支持情况如下： Apache Kafka (支持 source 和 sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) Hadoop FileSystem (sink) RabbitMQ (source/sink) Apache NiFi (source/sink) Twitter Streaming API (source) Google PubSub (source/sink) 除了上述的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink 相关的连接器如下： Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) Netty (source) 随着 Flink 的不断发展，可以预见到其会支持越来越多类型的连接器，关于连接器的后续发展情况，可以查看其官方文档：Streaming Connectors 。在所有 DataSource 连接器中，使用的广泛的就是 Kafka，所以这里我们以其为例，来介绍 Connectors 的整合步骤。 3.2 整合 Kakfa1. 导入依赖整合 Kafka 时，一定要注意所使用的 Kafka 的版本，不同版本间所需的 Maven 依赖和开发时所调用的类均不相同，具体如下： Maven 依赖 Flink 版本 Consumer and Producer 类的名称 Kafka 版本 flink-connector-kafka-0.8_2.11 1.0.0 + FlinkKafkaConsumer08 FlinkKafkaProducer08 0.8.x flink-connector-kafka-0.9_2.11 1.0.0 + FlinkKafkaConsumer09 FlinkKafkaProducer09 0.9.x flink-connector-kafka-0.10_2.11 1.2.0 + FlinkKafkaConsumer010 FlinkKafkaProducer010 0.10.x flink-connector-kafka-0.11_2.11 1.4.0 + FlinkKafkaConsumer011 FlinkKafkaProducer011 0.11.x flink-connector-kafka_2.11 1.7.0 + FlinkKafkaConsumer FlinkKafkaProducer &gt;= 1.0.0 这里我使用的 Kafka 版本为 kafka_2.12-2.2.0，添加的依赖如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt;&lt;/dependency&gt; 2. 代码开发这里以最简单的场景为例，接收 Kafka 上的数据并打印，代码如下： 123456789final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Properties properties = new Properties();// 指定Kafka的连接位置properties.setProperty("bootstrap.servers", "hadoop001:9092");// 指定监听的主题，并定义Kafka字节消息到Flink对象之间的转换规则DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;("flink-stream-in-topic", new SimpleStringSchema(), properties));stream.print();env.execute("Flink Streaming"); 3.3 整合测试1. 启动 KakfaKafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的： 12345# zookeeper启动命令bin/zkServer.sh start# 内置zookeeper启动命令bin/zookeeper-server-start.sh config/zookeeper.properties 启动单节点 kafka 用于测试： 1# bin/kafka-server-start.sh config/server.properties 2. 创建 Topic123456789# 创建用于测试主题bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 \ --partitions 1 \ --topic flink-stream-in-topic# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3. 启动 Producer这里 启动一个 Kafka 生产者，用于发送测试数据： 1bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic flink-stream-in-topic 4. 测试结果在 Producer 上输入任意测试数据，之后观察程序控制台的输出： 程序控制台的输出如下： 可以看到已经成功接收并打印出相关的数据。 Transform一、Transformations 分类Flink 的 Transformations 操作主要用于将一个和多个 DataStream 按需转换成新的 DataStream。它主要分为以下三类： DataStream Transformations：进行数据流相关转换操作； Physical partitioning：物理分区。Flink 提供的底层 API ，允许用户定义数据的分区规则； Task chaining and resource groups：任务链和资源组。允许用户进行任务链和资源组的细粒度的控制。 以下分别对其主要 API 进行介绍： 二、DataStream Transformations2.1 Map [DataStream → DataStream]对一个 DataStream 中的每个元素都执行特定的转换操作： 123DataStream&lt;Integer&gt; integerDataStream = env.fromElements(1, 2, 3, 4, 5);integerDataStream.map((MapFunction&lt;Integer, Object&gt;) value -&gt; value * 2).print();// 输出 2,4,6,8,10 2.2 FlatMap [DataStream → DataStream]FlatMap 与 Map 类似，但是 FlatMap 中的一个输入元素可以被映射成一个或者多个输出元素，示例如下： 12345678910111213String string01 = "one one one two two";String string02 = "third third third four";DataStream&lt;String&gt; stringDataStream = env.fromElements(string01, string02);stringDataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for (String s : value.split(" ")) &#123; out.collect(s); &#125; &#125;&#125;).print();// 输出每一个独立的单词，为节省排版，这里去掉换行，后文亦同one one one two two third third third four 2.3 Filter [DataStream → DataStream]用于过滤符合条件的数据： 1env.fromElements(1, 2, 3, 4, 5).filter(x -&gt; x &gt; 3).print(); 2.4 KeyBy 和 Reduce KeyBy [DataStream → KeyedStream] ：用于将相同 Key 值的数据分到相同的分区中； Reduce [KeyedStream → DataStream] ：用于对数据执行归约计算。 如下例子将数据按照 key 值分区后，滚动进行求和计算： 12345678910111213DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2DataStream = env.fromElements(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("a", 2), new Tuple2&lt;&gt;("b", 3), new Tuple2&lt;&gt;("b", 5));KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = tuple2DataStream.keyBy(0);keyedStream.reduce((ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;) (value1, value2) -&gt; new Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1)).print();// 持续进行求和计算，输出：(a,1)(a,3)(b,3)(b,8) KeyBy 操作存在以下两个限制： KeyBy 操作用于用户自定义的 POJOs 类型时，该自定义类型必须重写 hashCode 方法； KeyBy 操作不能用于数组类型。 2.5 Aggregations [KeyedStream → DataStream]Aggregations 是官方提供的聚合算子，封装了常用的聚合操作，如上利用 Reduce 进行求和的操作也可以利用 Aggregations 中的 sum 算子重写为下面的形式： 1tuple2DataStream.keyBy(0).sum(1).print(); 除了 sum 外，Flink 还提供了 min , max , minBy，maxBy 等常用聚合算子： 123456789101112// 滚动计算指定key的最小值，可以通过index或者fieldName来指定keykeyedStream.min(0);keyedStream.min("key");// 滚动计算指定key的最大值keyedStream.max(0);keyedStream.max("key");// 滚动计算指定key的最小值，并返回其对应的元素keyedStream.minBy(0);keyedStream.minBy("key");// 滚动计算指定key的最大值，并返回其对应的元素keyedStream.maxBy(0);keyedStream.maxBy("key"); 2.6 Union [DataStream → DataStream]用于连接两个或者多个元素类型相同的 DataStream 。当然一个 DataStream 也可以与其本生进行连接，此时该 DataStream 中的每个元素都会被获取两次： 123456DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("a", 2));DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource02 = env.fromElements(new Tuple2&lt;&gt;("b", 1), new Tuple2&lt;&gt;("b", 2));streamSource01.union(streamSource02);streamSource01.union(streamSource01,streamSource02); 2.7 Connect [DataStream,DataStream → ConnectedStreams]Connect 操作用于连接两个或者多个类型不同的 DataStream ，其返回的类型是 ConnectedStreams ，此时被连接的多个 DataStreams 可以共享彼此之间的数据状态。但是需要注意的是由于不同 DataStream 之间的数据类型是不同的，如果想要进行后续的计算操作，还需要通过 CoMap 或 CoFlatMap 将 ConnectedStreams 转换回 DataStream： 12345678910111213141516171819DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("b", 5));DataStreamSource&lt;Integer&gt; streamSource02 = env.fromElements(2, 3, 9);// 使用connect进行连接ConnectedStreams&lt;Tuple2&lt;String, Integer&gt;, Integer&gt; connect = streamSource01.connect(streamSource02);connect.map(new CoMapFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;() &#123; @Override public Integer map1(Tuple2&lt;String, Integer&gt; value) throws Exception &#123; return value.f1; &#125; @Override public Integer map2(Integer value) throws Exception &#123; return value; &#125;&#125;).map(x -&gt; x * 100).print();// 输出：300 500 200 900 300 2.8 Split 和 Select Split [DataStream → SplitStream]：用于将一个 DataStream 按照指定规则进行拆分为多个 DataStream，需要注意的是这里进行的是逻辑拆分，即 Split 只是将数据贴上不同的类型标签，但最终返回的仍然只是一个 SplitStream； Select [SplitStream → DataStream]：想要从逻辑拆分的 SplitStream 中获取真实的不同类型的 DataStream，需要使用 Select 算子，示例如下： 12345678910111213DataStreamSource&lt;Integer&gt; streamSource = env.fromElements(1, 2, 3, 4, 5, 6, 7, 8);// 标记SplitStream&lt;Integer&gt; split = streamSource.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); output.add(value % 2 == 0 ? "even" : "odd"); return output; &#125;&#125;);// 获取偶数数据集split.select("even").print();// 输出 2,4,6,8 2.9 project [DataStream → DataStream]project 主要用于获取 tuples 中的指定字段集，示例如下： 12345678DataStreamSource&lt;Tuple3&lt;String, Integer, String&gt;&gt; streamSource = env.fromElements( new Tuple3&lt;&gt;("li", 22, "2018-09-23"), new Tuple3&lt;&gt;("ming", 33, "2020-09-23"));streamSource.project(0,2).print();// 输出(li,2018-09-23)(ming,2020-09-23) 三、物理分区物理分区 (Physical partitioning) 是 Flink 提供的底层的 API，允许用户采用内置的分区规则或者自定义的分区规则来对数据进行分区，从而避免数据在某些分区上过于倾斜，常用的分区规则如下： 3.1 Random partitioning [DataStream → DataStream]随机分区 (Random partitioning) 用于随机的将数据分布到所有下游分区中，通过 shuffle 方法来进行实现： 1dataStream.shuffle(); 3.2 Rebalancing [DataStream → DataStream]Rebalancing 采用轮询的方式将数据进行分区，其适合于存在数据倾斜的场景下，通过 rebalance 方法进行实现： 1dataStream.rebalance(); 3.3 Rescaling [DataStream → DataStream]当采用 Rebalancing 进行分区平衡时，其实现的是全局性的负载均衡，数据会通过网络传输到其他节点上并完成分区数据的均衡。 而 Rescaling 则是低配版本的 rebalance，它不需要额外的网络开销，它只会对上下游的算子之间进行重新均衡，通过 rescale 方法进行实现： 1dataStream.rescale(); ReScale 这个单词具有重新缩放的意义，其对应的操作也是如此，具体如下：如果上游 operation 并行度为 2，而下游的 operation 并行度为 6，则其中 1 个上游的 operation 会将元素分发到 3 个下游 operation，另 1 个上游 operation 则会将元素分发到另外 3 个下游 operation。反之亦然，如果上游的 operation 并行度为 6，而下游 operation 并行度为 2，则其中 3 个上游 operation 会将元素分发到 1 个下游 operation，另 3 个上游 operation 会将元素分发到另外 1 个下游operation： 3.4 Broadcasting [DataStream → DataStream]将数据分发到所有分区上。通常用于小数据集与大数据集进行关联的情况下，此时可以将小数据集广播到所有分区上，避免频繁的跨分区关联，通过 broadcast 方法进行实现： 1dataStream.broadcast(); 3.5 Custom partitioning [DataStream → DataStream]Flink 运行用户采用自定义的分区规则来实现分区，此时需要通过实现 Partitioner 接口来自定义分区规则，并指定对应的分区键，示例如下： 12345678910111213141516171819202122 DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource = env.fromElements(new Tuple2&lt;&gt;("Hadoop", 1), new Tuple2&lt;&gt;("Spark", 1), new Tuple2&lt;&gt;("Flink-streaming", 2), new Tuple2&lt;&gt;("Flink-batch", 4), new Tuple2&lt;&gt;("Storm", 4), new Tuple2&lt;&gt;("HBase", 3));streamSource.partitionCustom(new Partitioner&lt;String&gt;() &#123; @Override public int partition(String key, int numPartitions) &#123; // 将第一个字段包含flink的Tuple2分配到同一个分区 return key.toLowerCase().contains("flink") ? 0 : 1; &#125;&#125;, 0).print();// 输出如下：1&gt; (Flink-streaming,2)1&gt; (Flink-batch,4)2&gt; (Hadoop,1)2&gt; (Spark,1)2&gt; (Storm,4)2&gt; (HBase,3) 四、任务链和资源组任务链和资源组 ( Task chaining and resource groups ) 也是 Flink 提供的底层 API，用于控制任务链和资源分配。默认情况下，如果操作允许 (例如相邻的两次 map 操作) ，则 Flink 会尝试将它们在同一个线程内进行，从而可以获取更好的性能。但是 Flink 也允许用户自己来控制这些行为，这就是任务链和资源组 API： 4.1 startNewChainstartNewChain 用于基于当前 operation 开启一个新的任务链。如下所示，基于第一个 map 开启一个新的任务链，此时前一个 map 和 后一个 map 将处于同一个新的任务链中，但它们与 filter 操作则分别处于不同的任务链中： 1someStream.filter(...).map(...).startNewChain().map(...); 4.2 disableChaining disableChaining 操作用于禁止将其他操作与当前操作放置于同一个任务链中，示例如下： 1someStream.map(...).disableChaining(); 4.3 slotSharingGroupslot 是任务管理器 (TaskManager) 所拥有资源的固定子集，每个操作 (operation) 的子任务 (sub task) 都需要获取 slot 来执行计算，但每个操作所需要资源的大小都是不相同的，为了更好地利用资源，Flink 允许不同操作的子任务被部署到同一 slot 中。slotSharingGroup 用于设置操作的 slot 共享组 (slot sharing group) ，Flink 会将具有相同 slot 共享组的操作放到同一个 slot 中 。示例如下： 1someStream.filter(...).slotSharingGroup("slotSharingGroupName"); Sink一、Data Sinks在使用 Flink 进行数据处理时，数据经 Data Source 流入，然后通过系列 Transformations 的转化，最终可以通过 Sink 将计算结果进行输出，Flink Data Sinks 就是用于定义数据流最终的输出位置。Flink 提供了几个较为简单的 Sink API 用于日常的开发，具体如下： 1.1 writeAsTextwriteAsText 用于将计算结果以文本的方式并行地写入到指定文件夹下，除了路径参数是必选外，该方法还可以通过指定第二个参数来定义输出模式，它有以下两个可选值： WriteMode.NO_OVERWRITE：当指定路径上不存在任何文件时，才执行写出操作； WriteMode.OVERWRITE：不论指定路径上是否存在文件，都执行写出操作；如果原来已有文件，则进行覆盖。 使用示例如下： 1streamSource.writeAsText("D:\\out", FileSystem.WriteMode.OVERWRITE); 以上写出是以并行的方式写出到多个文件，如果想要将输出结果全部写出到一个文件，需要设置其并行度为 1： 1streamSource.writeAsText("D:\\out", FileSystem.WriteMode.OVERWRITE).setParallelism(1); 1.2 writeAsCsvwriteAsCsv 用于将计算结果以 CSV 的文件格式写出到指定目录，除了路径参数是必选外，该方法还支持传入输出模式，行分隔符，和字段分隔符三个额外的参数，其方法定义如下： 1writeAsCsv(String path, WriteMode writeMode, String rowDelimiter, String fieldDelimiter) 1.3 print / printToErrprint / printToErr 是测试当中最常用的方式，用于将计算结果以标准输出流或错误输出流的方式打印到控制台上。 1.4 writeUsingOutputFormat采用自定义的输出格式将计算结果写出，上面介绍的 writeAsText 和 writeAsCsv 其底层调用的都是该方法，源码如下： 12345public DataStreamSink&lt;T&gt; writeAsText(String path, WriteMode writeMode) &#123; TextOutputFormat&lt;T&gt; tof = new TextOutputFormat&lt;&gt;(new Path(path)); tof.setWriteMode(writeMode); return writeUsingOutputFormat(tof);&#125; 1.5 writeToSocketwriteToSocket 用于将计算结果以指定的格式写出到 Socket 中，使用示例如下： 1streamSource.writeToSocket("192.168.0.226", 9999, new SimpleStringSchema()); 二、整合 Kafka Sink3.1 addSinkFlink 提供了 addSink 方法用来调用自定义的 Sink 或者第三方的连接器，想要将计算结果写出到 Kafka，需要使用该方法来调用 Kafka 的生产者 FlinkKafkaProducer，具体代码如下： 12345678910111213141516171819202122232425final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 1.指定Kafka的相关配置属性Properties properties = new Properties();properties.setProperty("bootstrap.servers", "192.168.200.0:9092");// 2.接收Kafka上的数据DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;("flink-stream-in-topic", new SimpleStringSchema(), properties));// 3.定义计算结果到 Kafka ProducerRecord 的转换KafkaSerializationSchema&lt;String&gt; kafkaSerializationSchema = new KafkaSerializationSchema&lt;String&gt;() &#123; @Override public ProducerRecord&lt;byte[], byte[]&gt; serialize(String element, @Nullable Long timestamp) &#123; return new ProducerRecord&lt;&gt;("flink-stream-out-topic", element.getBytes()); &#125;&#125;;// 4. 定义Flink Kafka生产者FlinkKafkaProducer&lt;String&gt; kafkaProducer = new FlinkKafkaProducer&lt;&gt;("flink-stream-out-topic", kafkaSerializationSchema, properties, FlinkKafkaProducer.Semantic.AT_LEAST_ONCE, 5);// 5. 将接收到输入元素*2后写出到Kafkastream.map((MapFunction&lt;String, String&gt;) value -&gt; value + value).addSink(kafkaProducer);env.execute("Flink Streaming"); 3.2 创建输出主题创建用于输出测试的主题： 12345678bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 \ --partitions 1 \ --topic flink-stream-out-topic# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3.3 启动消费者启动一个 Kafka 消费者，用于查看 Flink 程序的输出情况： 1bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic flink-stream-out-topic 3.4 测试结果在 Kafka 生产者上发送消息到 Flink 程序，观察 Flink 程序转换后的输出情况，具体如下： 可以看到 Kafka 生成者发出的数据已经被 Flink 程序正常接收到，并经过转换后又输出到 Kafka 对应的 Topic 上。 三、自定义 Sink除了使用内置的第三方连接器外，Flink 还支持使用自定义的 Sink 来满足多样化的输出需求。想要实现自定义的 Sink ，需要直接或者间接实现 SinkFunction 接口。通常情况下，我们都是实现其抽象类 RichSinkFunction，相比于 SinkFunction ，其提供了更多的与生命周期相关的方法。两者间的关系如下： 这里我们以自定义一个 FlinkToMySQLSink 为例，将计算结果写出到 MySQL 数据库中，具体步骤如下： 4.1 导入依赖首先需要导入 MySQL 相关的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.16&lt;/version&gt;&lt;/dependency&gt; 4.2 自定义 Sink继承自 RichSinkFunction，实现自定义的 Sink ： 123456789101112131415161718192021222324252627282930313233343536public class FlinkToMySQLSink extends RichSinkFunction&lt;Employee&gt; &#123; private PreparedStatement stmt; private Connection conn; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName("com.mysql.cj.jdbc.Driver"); conn = DriverManager.getConnection("jdbc:mysql://192.168.0.229:3306/employees" + "?characterEncoding=UTF-8&amp;serverTimezone=UTC&amp;useSSL=false", "root", "123456"); String sql = "insert into emp(name, age, birthday) values(?, ?, ?)"; stmt = conn.prepareStatement(sql); &#125; @Override public void invoke(Employee value, Context context) throws Exception &#123; stmt.setString(1, value.getName()); stmt.setInt(2, value.getAge()); stmt.setDate(3, value.getBirthday()); stmt.executeUpdate(); &#125; @Override public void close() throws Exception &#123; super.close(); if (stmt != null) &#123; stmt.close(); &#125; if (conn != null) &#123; conn.close(); &#125; &#125;&#125; 4.3 使用自定义 Sink想要使用自定义的 Sink，同样是需要调用 addSink 方法，具体如下： 12345678final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Date date = new Date(System.currentTimeMillis());DataStreamSource&lt;Employee&gt; streamSource = env.fromElements( new Employee("hei", 10, date), new Employee("bai", 20, date), new Employee("ying", 30, date));streamSource.addSink(new FlinkToMySQLSink());env.execute(); 4.4 测试结果启动程序，观察数据库写入情况： 数据库成功写入，代表自定义 Sink 整合成功。 Streaming Connectors除了上述 API 外，Flink 中还内置了系列的 Connectors 连接器，用于将计算结果输入到常用的存储系统或者消息中间件中，具体如下： Apache Kafka (支持 source 和 sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) Hadoop FileSystem (sink) RabbitMQ (source/sink) Apache NiFi (source/sink) Google PubSub (source/sink) 除了内置的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink Sink 相关的连接器如下： Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) 这里接着在 Data Sources 章节介绍的整合 Kafka Source 的基础上，将 Kafka Sink 也一并进行整合，具体步骤如下。]]></content>
      <categories>
        <category>大数据</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark介绍]]></title>
    <url>%2F2020%2F03%2F31%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FSpark%2F1.Spark%20%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Spark介绍什么是Spark Spark是一种快速、通用、可扩展的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。 Spark基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了高容错性和高可伸缩性，允许用户将Spark部署在大量廉价硬件之上，形成集群。 Spark 的特点 中间结果输出：基于MapReduce的计算引擎通常会将中间结果输出到磁盘上，进行存储和容错。出于任务管道承接的考虑，当一些查询翻译到MapReduce任务时，往往会产生多个Stage，而这些串联的Stage又依赖于底层文件系统（如HDFS）来存储每一个Stage的输出结果; Spark 则将中间结果保存到内存中，这样大大提高了数据处理的实时性和速度； 兼容HDFS，HIVE: Spark是MapReduce的替代方案，而且兼容HDFS、Hive，可融入Hadoop的生态系统，以弥补MapReduce的不足。 速度快: 与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。 接口丰富: Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。 通用性: Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。 兼容性: Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。 Spark 架构图 Spark集群安装和运行Spark 集群安装 环境准备 准备两台以上的Linux服务器，并安装好JDK(&gt;=1.7) 下载Spark安装包 下载地址: http://www.apache.org/dyn/closer.lua/spark 解压安装包: tar -zxvf spark-xxx.tgz -C /usr/local/ 配置Spark(1.5.2版本为例) 进入到Spark安装目录， 进入conf目录并重命名并修改spark-env.sh.template文件 123cd /usr/local/spark-1.5.2-bin-hadoop2.6cd conf/mv spark-env.sh.template spark-env.sh 在该配置文件中添加如下配置 1234vi spark-env.shexport JAVA_HOME=/usr/java/jdk1.7.0_45export SPARK_MASTER_IP=node1.itcast.cnexport SPARK_MASTER_PORT=7077 保存退出 重命名并修改slaves.template文件 在该文件中添加子节点所在的位置（Worker节点） 12345mv slaves.template slavesvi slavesnode2.itcast.cnnode3.itcast.cnnode4.itcast.cn 保存退出 将配置好的Spark拷贝到其他节点上 123scp -r spark-1.5.2-bin-hadoop2.6/ node2.itcast.cn:/usr/local/scp -r spark-1.5.2-bin-hadoop2.6/ node3.itcast.cn:/usr/local/scp -r spark-1.5.2-bin-hadoop2.6/ node4.itcast.cn:/usr/local/ Spark集群配置完毕，目前是1个Master，3个Work，在node1.itcast.cn上启动Spark集群 /usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh 启动后执行jps命令，主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：http://node1.itcast.cn:8080/ 到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单： Spark集群规划：node1，node2是Master；node3，node4，node5是Worker 安装配置zk集群，并启动zk集群 停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置 1export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark" 在node1节点上修改slaves配置文件内容指定worker节点 在node1上执行sbin/start-all.sh脚本，然后在node2上执行sbin/start-master.sh启动第二个Master 执行Spark应用Spark任务提交命令格式 123456spark-submit \--class $class \--master spark://$spark_master_node \--executor-memory $memory_use \--total-executor-cores $core_use \$jar [args] 命令示例 1234567/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://node1.itcast.cn:7077 \--executor-memory 1G \--total-executor-cores 2 \/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-examples-1.5.2-hadoop2.6.0.jar \100 Spark shellspark shell作用 spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。 spark shell 启动 1234/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \--master spark://node1.itcast.cn:7077 \--executor-memory 2g \--total-executor-cores 2 参数: –master spark://node1.itcast.cn:7077 指定Master的地址 –executor-memory 2g 指定每个 executor 可用内存为2G –total-executor-cores 2 指定整个集群使用的cup核数为2个 如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。 Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可 编写Spark程序Spark Shell编写程序示例 首先启动hdfs 向hdfs上传一个文件到hdfs://node1.itcast.cn:9000/words.txt 在spark shell中用scala语言编写spark程序(先启动spark shell) 1sc.textFile("hdfs://node1.itcast.cn:9000/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://node1.itcast.cn:9000/out") 使用hdfs命令查看结果 ​ hdfs dfs -ls hdfs://node1.itcast.cn:9000/out/p* ​ 说明： ​ sc是SparkContext对象，该对象时提交spark程序的入口 ​ 每一个方法都是spark的一个算子，对应spark的一个RDD数据结构，对应一种数据的操作，又分为Action和Transform，后面会介绍 ​ textFile(hdfs://node1.itcast.cn:9000/words.txt)是hdfs中读取数据 ​ flatMap(_.split(“ “))先map在压平 ​ map((_,1))将单词和1构成元组 ​ reduceByKey(_+_)按照key进行reduce，并将value累加 ​ saveAsTextFile(“hdfs://node1.itcast.cn:9000/out”)将结果写入到hdfs中 制作job jar包示例spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，下面以maven项目为例 ​ 配置pom.xml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.itcast.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mvn&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.10.6&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-make:transitive&lt;/arg&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;mainClass&gt;cn.itcast.spark.WordCount&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 编写 spark 程序 12345678910111213141516package cn.itcast.sparkimport org.apache.spark.&#123;SparkContext, SparkConf&#125;object WordCount &#123; def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val conf = new SparkConf().setAppName("WC") //创建SparkContext，该对象是提交spark App的入口 val sc = new SparkContext(conf) //使用sc创建RDD并执行相应的transformation和action sc.textFile(args(0)).flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile(args(1)) //停止sc，结束该任务 sc.stop() &#125;&#125; 打包maven项目 修改pom.xml 中的main class 打包 点击idea右侧的Maven Project选项 击Lifecycle,选择clean和package，然后点击Run Maven Build 编译成功后的jar包，放到spark集群中 使用spark-submit命令提交Spark应用 12345678/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \--class cn.itcast.spark.WordCount \--master spark://node1.itcast.cn:7077 \--executor-memory 2G \--total-executor-cores 4 \/root/spark-mvn-1.0-SNAPSHOT.jar \hdfs://node1.itcast.cn:9000/words.txt \hdfs://node1.itcast.cn:9000/out 查看执行结果 12345hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-00000#(hello,6)#(tom,3)#(kitty,2)#(jerry,1)]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDD]]></title>
    <url>%2F2020%2F03%2F31%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FSpark%2F2.Spark%20RDD%2F</url>
    <content type="text"><![CDATA[RDD 介绍什么是RDDRDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。 RDD的属性1）一组分区（Partition），即数据集的基本组成单位。对于RDD来说，每个分区都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分区个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 2）一个计算每个分区的函数。Spark中RDD的计算是以分区为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 4）一个Partitioner，即RDD的分区函数。当前Spark中实现了两种类型的分区函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分区数量，也决定了parent RDD Shuffle输出时的分区数量。 5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 创建RDD的方式1）由一个已经存在的Scala集合创建。 val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8)) 2）由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等 val rdd2 = sc.textFile(“hdfs://node1.itcast.cn:9000/words.txt”) RDD 编程 APIRDD中的所有转换（Transform）都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果的计算(Action)给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。 Transformation常用transformation 转换 含义 使用 map(func) 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2) filter(func) 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 rdd2.filter(_&gt;10) flatMap(func) 返回一个新的RDD，RDD中元素经过func函数计算后，并将所返回的内容打散平铺 sc.textFile(“/root/words.txt”).flatMap(x=&gt;x.split(“ “)) mapPartitions(func) 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U] val rdd4 = rdd3.partitionBy(hostParitioner).mapPartitions(it =&gt; { it.toList.sortBy(_._2._2).reverse.take(2).iterator }) rdd4.saveAsTextFile(“c://out4”) mapPartitionsWithIndex(func) 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U] val func = (index: Int, iter: Iterator[(Int)]) =&gt; { iter.toList.map(x =&gt; “[partID:” + index + “, val: “ + x + “]”).iterator};val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func).collect sample(withReplacement, fraction, seed) 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子 union(otherDataset) 对源RDD和参数RDD求并集后返回一个新的RDD，要求列数要一样，类型可以不同 intersection(otherDataset) 对源RDD和参数RDD求交集后返回一个新的RDD val rdd9 = rdd6.intersection(rdd7) distinct([numTasks])) 对源RDD进行去重后返回一个新的RDD val rdd6 = sc.parallelize(List(5,6,4,7))val rdd7 = sc.parallelize(List(1,2,3,4)); val rdd8 = rdd6.union(rdd7)； rdd8.distinct.sortBy(x=&gt;x).collect groupByKey([numTasks]) 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD val rdd1 = sc.parallelize(List((“tom”, 1), (“jerry”, 2), (“kitty”, 3))) ;val rdd2 = sc.parallelize(List((“jerry”, 9), (“tom”, 8), (“shuke”, 7))); val rdd3 = rdd1 union rdd2; rdd3.groupByKey.map(x=&gt;(x._1,x._2.sum)) reduceByKey(func, [numTasks]) 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 sc.textFile(“/root/words.txt”).flatMap(x=&gt;x.split(“ “)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) aggregate接收两个函数，和一个初始化值。seqOp函数用于聚集每一个分区，combOp用于聚集所有分区聚集后的结果。每一个分区的聚集，和最后所有分区的聚集都需要初始化值的参与。 val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2); rdd1.aggregate(0)(math.max(_,_), _ + _) sortByKey([ascending], [numTasks]) 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD sortBy(func,[ascending], [numTasks]) 与sortByKey类似，但是更灵活 rdd8.distinct.sortBy(x=&gt;x).collect join(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD val rdd1 = sc.parallelize(List((“tom”, 1), (“jerry”, 2), (“kitty”, 3))); val rdd2 = sc.parallelize(List((“jerry”, 9), (“tom”, 8), (“shuke”, 7))); val rdd3 = rdd1.join(rdd2) cogroup(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))类型的RDD val rdd1 = sc.parallelize(List((“tom”, 1), (“tom”, 2), (“jerry”, 3), (“kitty”, 2))); val rdd2 = sc.parallelize(List((“jerry”, 2), (“tom”, 1), (“shuke”, 2))); val rdd3 = rdd1.cogroup(rdd2) cartesian(otherDataset) 笛卡尔积 val rdd1 = sc.parallelize(List(“tom”, “jerry”)); val rdd2 = sc.parallelize(List(“tom”, “kitty”, “shuke”)); val rdd3 = rdd1.cartesian(rdd2) pipe(command, [envVars]) 调用外部命令 coalesce(numPartitions) 重新分区 repartition(numPartitions) 重新分区 ### Action 常用的Action 动作 含义 reduce(func) 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的 collect() 在驱动程序中，以数组的形式返回数据集的所有元素 count() 返回RDD的元素个数 first() 返回RDD的第一个元素（类似于take(1)） take(n) 返回一个由数据集的前n个元素组成的数组 takeSample(withReplacement,num, [seed]) 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 takeOrdered(n, [ordering]) saveAsTextFile(path) 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 saveAsSequenceFile(path) 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 saveAsObjectFile(path) countByKey() 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 foreach(func) 在数据集的每一个元素上，运行函数func进行更新。 WordCount 中的RDD 场景训练练习1：批量操作后过滤 val rdd1 = sc.parallelize(List(5, 6, 4, 7, 3, 8, 2, 9, 1, 10)) //对rdd1里的每一个元素乘2然后排序 val rdd2 = rdd1.map(_ * 2).sortBy(x =&gt; x, true) //过滤出大于等于十的元素 val rdd3 = rdd2.filter(_ &gt;= 10) //将元素以数组的方式在客户端显示 rdd3.collect 练习2：数据打平 val rdd1 = sc.parallelize(Array(“a b c”, “d e f”, “h i j”)) //将rdd1里面的每一个元素先切分在压平 val rdd2 = rdd1.flatMap(_.split(‘ ‘)) rdd2.collect 练习3：求两个RDD的交集或并集 val rdd1 = sc.parallelize(List(5, 6, 4, 3)) val rdd2 = sc.parallelize(List(1, 2, 3, 4)) //求并集 val rdd3 = rdd1.union(rdd2) //求交集 val rdd4 = rdd1.intersection(rdd2) //去重 rdd3.distinct.collect rdd4.collect 练习4：数据合并分组 val rdd1 = sc.parallelize(List((“tom”, 1), (“jerry”, 3), (“kitty”, 2))) val rdd2 = sc.parallelize(List((“jerry”, 2), (“tom”, 1), (“shuke”, 2))) //求jion val rdd3 = rdd1.join(rdd2) rdd3.collect //求并集 val rdd4 = rdd1 union rdd2 //按key进行分组 rdd4.groupByKey rdd4.collect 练习5：数据合并分组 val rdd1 = sc.parallelize(List((“tom”, 1), (“tom”, 2), (“jerry”, 3), (“kitty”, 2))) val rdd2 = sc.parallelize(List((“jerry”, 2), (“tom”, 1), (“shuke”, 2))) //cogroup val rdd3 = rdd1.cogroup(rdd2) //注意cogroup与groupByKey的区别 rdd3.collect 练习6：数据聚合 val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5)) //reduce聚合 val rdd2 = rdd1.reduce(_ + _) rdd2.collect 练习7：数据合并聚合并排序 val rdd1 = sc.parallelize(List((“tom”, 1), (“jerry”, 3), (“kitty”, 2), (“shuke”, 1))) val rdd2 = sc.parallelize(List((“jerry”, 2), (“tom”, 3), (“shuke”, 2), (“kitty”, 5))) val rdd3 = rdd1.union(rdd2) //按key进行聚合 val rdd4 = rdd3.reduceByKey(_ + _) rdd4.collect //按value的降序排序 val rdd5 = rdd4.map(t =&gt; (t._2, t._1)).sortByKey(false).map(t =&gt; (t._2, t._1)) rdd5.collect 想要了解更多，访问下面的地址 http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html RDD 的依赖关系RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。 窄依赖窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用 总结：窄依赖我们形象的比喻为独生子女 宽依赖宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition 总结：窄依赖我们形象的比喻为超生 RDD的血统RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区 RDD 的缓存RDD缓存的作用Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。 RDD 缓存的方式RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。 缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。 DAG的生成DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 Spark 中的checkpointcheckpoint在spark中主要有两块应用：一块是在spark core中对RDD做checkpoint，可以将checkpoint RDD的依赖关系，RDD数据保存到可靠存储（如HDFS）以便数据恢复；另外一块是应用在spark streaming中，使用checkpoint用来保存DStreamGraph以及相关配置信息，以便在Driver崩溃重启的时候能够接着之前进度继续进行处理（如之前waiting batch的job会在重启后继续处理）。 本文主要将详细分析checkpoint在以上两种场景的读写过程。 checkpoint的使用方法使用checkpoint对RDD做快照大体如下： 1234&gt; sc.setCheckpointDir(checkpointDir.toString)&gt; val rdd = sc.makeRDD(1 to 20, numSlices = 1)&gt; rdd.checkpoint()&gt; 首先，设置checkpoint的目录（一般是hdfs目录），这个目录用来将RDD相关的数据（包括每个partition实际数据，以及partitioner（如果有的话））。然后在RDD上调用checkpoint的方法即可。 checkpoint写流程可以看到checkpoint使用非常简单，设置checkpoint目录，然后调用RDD的checkpoint方法。针对checkpoint的写入流程，主要有以下四个问题： Q1：RDD中的数据是什么时候写入的？是在rdd调用checkpoint方法时候吗？ Q2：在做checkpoint的时候，具体写入了哪些数据到HDFS了？ Q3：在对RDD做完checkpoint以后，对做RDD的本省又做了哪些收尾工作？ Q4：实际过程中，使用RDD做checkpoint的时候需要注意什么问题？ 弄清楚了以上四个问题，我想对checkpoint的写过程也就基本清楚了。接下来将一一回答上面提出的问题。 A1：首先看一下RDD中checkpoint方法，可以看到在该方法中是只是新建了一个ReliableRDDCheckpintData的对象，并没有做实际的写入工作。实际触发写入的时机是在runJob生成改RDD后，调用RDD的doCheckpoint方法来做的。 A2：在经历调用RDD.doCheckpoint → RDDCheckpintData.checkpoint → ReliableRDDCheckpintData.doCheckpoint → ReliableRDDCheckpintData.writeRDDToCheckpointDirectory后，在writeRDDToCheckpointDirectory方法中可以看到：将作为一个单独的任务（RunJob）将RDD中每个parition的数据依次写入到checkpoint目录（writePartitionToCheckpointFile），此外如果该RDD中的partitioner如果不为空，则也会将该对象序列化后存储到checkpoint目录。所以，在做checkpoint的时候，写入的hdfs中的数据主要包括：RDD中每个parition的实际数据，以及可能的partitioner对象（writePartitionerToCheckpointDir）。 A3：在写完checkpoint数据到hdfs以后，将会调用rdd的markCheckpoined方法，主要斩断该rdd的对上游的依赖，以及将paritions置空等操作。 A4：通过A1，A2可以知道，在RDD计算完毕后，会再次通过RunJob将每个partition数据保存到HDFS。这样RDD将会计算两次，所以为了避免此类情况，最好将RDD进行cache。即1.1中rdd的推荐使用方法如下： 12345&gt; sc.setCheckpointDir(checkpointDir.toString)&gt; val rdd = sc.makeRDD(1 to 20, numSlices = 1)&gt; rdd.cache()&gt; rdd.checkpoint()&gt; checkpoint 读流程在做完checkpoint后，获取原来RDD的依赖以及partitions数据都将从CheckpointRDD中获取。也就是说获取原来rdd中每个partition数据以及partitioner等对象，都将转移到CheckPointRDD中。 在CheckPointRDD的一个具体实现ReliableRDDCheckpintRDD中的compute方法中可以看到，将会从hdfs的checkpoint目录中恢复之前写入的partition数据。而partitioner对象（如果有）也会从之前写入hdfs的paritioner对象恢复。 总的来说，checkpoint读取过程是比较简单的。 checkpoint 和cache 区别 cache 机制保证了需要访问重复数据的应用（如迭代型算法和交互式应用）可以运行的更快。 与 Hadoop MapReduce job 不同的是 Spark 的逻辑/物理执行图可能很庞大，task 中 computing chain 可能会很长，计算某些 RDD 也可能会很耗时。这时，如果 task 中途运行出错，那么 task 的整个 computing chain 需要重算，代价太高。因此，有必要将计算代价较大的 RDD checkpoint 一下，这样，当下游 RDD 计算出错时，可以直接从 checkpoint 过的 RDD 那里读取数据继续算。 RDD分区规则 通过集合方式指定 通过scala 集合方式parallelize生成rdd， 如， val rdd = sc.parallelize(1 to 10) 这种方式下，如果在parallelize操作时没有指定分区数，则 rdd的分区数 = sc.defaultParallelism textFile 分区规则 1.如果textFile指定分区数量为0或者1的话，defaultMinPartitions值为1，则有多少个文件，就会有多少个分区。 2.如果不指定默认分区数量，则默认分区数量为2，则会根据所有文件字节大小totalSize除以分区数量partitons的值goalSize，然后比较goalSize和hdfs指定分块大小（这里是32M）作比较，以较小的最为goalSize作为切分大小，对每个文件进行切分，若文件大于大于goalSize，则会生成该文件大小/goalSize + 1个分区。 3.如果指定分区数量大于等于2，则默认分区数量为指定值，生成分区数量规则同2中的规则。 Dataframe(SQL)什么是Dataframe与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上 看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。 创建Dataframe 在本地创建一个文件，有三列，分别是id、name、age，用空格分隔，然后上传到hdfs上 hdfs dfs -put person.txt / 在spark shell执行下面命令，读取数据，将每一行的数据使用列分隔符分割 val lineRDD = sc.textFile(“hdfs://node1.itcast.cn:9000/person.txt”).map(_.split(“ “)) 定义case class（相当于表的schema） case class Person(id:Int, name:String, age:Int) 将RDD和case class关联 val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt)) 将RDD转换成DataFrame val personDF = personRDD.toDF 对DataFrame进行处理 personDF.show Dataframe 常用操作查看Dataframe 中内容 personDF.show 查看DataFrame部分列中的内容 personDF.select(personDF.col(“name”)).show personDF.select(col(“name”), col(“age”)).show personDF.select(“name”).show 打印dataFrame的Schema信息 personDF.printSchema 查看内容，并对字段进行计算 personDF.select(col(“id”), col(“name”), col(“age”) + 1).show personDF.select(personDF(“id”), personDF(“name”), personDF(“age”) + 1).show 字段条件过滤 personDF.filter(col(“age”) &gt;= 18).show 分组计算 personDF.groupBy(“age”).count().show() SQL语法风格将Dataframe注册成表 personDF.registerTempTable(“t_person”) sql排序查询 sqlContext.sql(“select * from t_person order by age desc limit 2”).show 查询schema信息 sqlContext.sql(“desc t_person”).show Spark StreamingSparkStreaming 的特殊Transformation UpdateStateByKey Operation UpdateStateByKey原语用于记录历史记录，上文中Word Count示例中就用到了该特性。若不用UpdateStateByKey来更新状态，那么每次数据进来后分析完成后，结果输出后将不在保存 Transform Operation Transform原语允许DStream上执行任意的RDD-to-RDD函数。通过该函数可以方便的扩展Spark API。此外，MLlib（机器学习）以及Graphx也是通过本函数来进行结合的。 Window Operations Window Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态 Output Operations on DStreamsOutput Operations可以将DStream的数据输出到外部的数据库或文件系统，当某个Output Operations原语被调用时（与RDD的Action相同），streaming程序才会开始真正的计算过程。 Output Operation Meaning print() Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. saveAsTextFiles(prefix, [suffix]) Save this DStream’s contents as text files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”. saveAsObjectFiles(prefix, [suffix]) Save this DStream’s contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”. saveAsHadoopFiles(prefix, [suffix]) Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”. foreachRDD(func) The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. Spark 任务提交任务提交的主要四个阶段DAG的生成 =&gt; stage切分 =&gt; task的生成 =&gt; 任务提交 构建DAG用户提交的job将首先被转换成一系列RDD并通过RDD之间的依赖关系构建DAG,然后将DAG提交到调度系统； DAGScheduler将DAG切分stage（切分依据是shuffle）,将stage中生成的task以taskset的形式发送给TaskScheduler Scheduler 调度task（根据资源情况将task调度到Executors） Executors接收task，然后将task交给线程池执行。 任务提交详细步骤 spark集群启动后，Worker向Master注册信息 spark-submit命令提交程序后，driver和application也会向Master注册信息 创建SparkContext对象：主要的对象包含DAGScheduler和TaskScheduler Driver把Application信息注册给Master后，Master会根据App信息去Worker节点启动Executor Executor内部会创建运行task的线程池，然后把启动的Executor反向注册给Dirver DAGScheduler：负责把Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；同时DAGScheduler还会处理由于Shuffle数据丢失导致的失败； TaskScheduler：维护所有TaskSet，分发Task给各个节点的Executor（根据数据本地化策略分发Task），监控task的运行状态，负责重试失败的task； 所有task运行完成后，SparkContext向Master注销，释放资源； Spark stage 切分流程 划分stage 的思路park划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。 stage 作用这是个复杂是业务逻辑（将多台机器上具有相同属性的数据聚合到一台机器上:shuffle）如果有shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，下一个阶段的计算依赖上一个阶段的数据在同一个stage中，会有多个算子，可以合并到一起，我们很难‘’称其为pipeline（流水线，严格按照流程、顺序执行） Spark Driver 给Executor 提交task 时序图 Spark Executor 启动和任务接受和执行时序图 Spark Shuffle概述Shuffle就是对数据进行重组，由于分布式计算的特性和要求，在实现细节上更加繁琐和复杂 在MapReduce框架，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I/O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己的shuffle实现过程 在DAG调度的过程中，Stage阶段的划分是根据是否有shuffle过程，也就是存在ShuffleDependency宽依赖的时候，需要进行shuffle,这时候会将作业job划分成多个Stage；并且在划分Stage的时候，构建ShuffleDependency的时候进行shuffle注册，获取后续数据读取所需要的ShuffleHandle,最终每一个job提交后都会生成一个ResultStage和若干个ShuffleMapStage，其中ResultStage表示生成作业的最终结果所在的Stage. ResultStage与ShuffleMapStage中的task分别对应着ResultTask与ShuffleMapTask。一个作业，除了最终的ResultStage外，其他若干ShuffleMapStage中各个ShuffleMapTask都需要将最终的数据根据相应的Partitioner对数据进行分组，然后持久化分区的数据。 HashShuffle机制HashShuffle概述在spark-1.6版本之前，一直使用HashShuffle，在spark-1.6版本之后使用Sort-Base Shuffle，因为HashShuffle存在的不足所以就替换了HashShuffle. 我们知道，Spark的运行主要分为2部分：一部分是驱动程序，其核心是SparkContext；另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle。 没有优化之前的HashShuffle机制 在HashShuffle没有优化之前，每一个ShufflleMapTask会为每一个ReduceTask创建一个bucket缓存，并且会为每一个bucket创建一个文件。这个bucket存放的数据就是经过Partitioner操作(默认是HashPartitioner)之后找到对应的bucket然后放进去，最后将数据刷新bucket缓存的数据到磁盘上，即对应的block file. 然后ShuffleMapTask将输出作为MapStatus发送到DAGScheduler的MapOutputTrackerMaster，每一个MapStatus包含了每一个ResultTask要拉取的数据的位置和大小 ResultTask然后去利用BlockStoreShuffleFetcher向MapOutputTrackerMaster获取MapStatus，看哪一份数据是属于自己的，然后底层通过BlockManager将数据拉取过来 拉取过来的数据会组成一个内部的ShuffleRDD，优先放入内存，内存不够用则放入磁盘，然后ResulTask开始进行聚合，最后生成我们希望获取的那个MapPartitionRDD 这种方式的缺点 如上图所示：在这里有1个worker，2个executor，每一个executor运行2个ShuffleMapTask，有三个ReduceTask，所以总共就有4 * 3=12个bucket和12个block file。 如果数据量较大，将会生成MR个小文件，比如ShuffleMapTask有100个，ResultTask有100个，这就会产生100100=10000个小文件 bucket缓存很重要，需要将ShuffleMapTask所有数据都写入bucket，才会刷到磁盘，那么如果Map端数据过多，这就很容易造成内存溢出，尽管后面有优化，bucket写入的数据达到刷新到磁盘的阀值之后，就会将数据一点一点的刷新到磁盘，但是这样磁盘I/O就多了 优化后的HashShuffle 每一个Executor进程根据核数，决定Task的并发数量，比如executor核数是2，就是可以并发运行两个task，如果是一个则只能运行一个task 假设executor核数是1，ShuffleMapTask数量是M,那么它依然会根据ResultTask的数量R，创建R个bucket缓存，然后对key进行hash，数据进入不同的bucket中，每一个bucket对应着一个block file,用于刷新bucket缓存里的数据 然后下一个task运行的时候，那么不会再创建新的bucket和block file，而是复用之前的task已经创建好的bucket和block file。即所谓同一个Executor进程里所有Task都会把相同的key放入相同的bucket缓冲区中 这样的话，生成文件的数量就是(本地worker的executor数量*executor的cores*ResultTask数量)如上图所示，即2 * 1* 3 = 6个文件，每一个Executor的shuffleMapTask数量100,ReduceTask数量为100，那么 未优化的HashShuffle的文件数是2 *1* 100*100 =20000，优化之后的数量是2*1*100 = 200文件，相当于少了100倍 这种方式的缺点： 如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。 Sort-Based ShuffleSort-Based Shuffle概述HashShuffle回顾 io\GC\内存占用大: HashShuffle写数据的时候，内存有一个bucket缓冲区，同时在本地磁盘有对应的本地文件，如果本地有文件，那么在内存应该也有文件句柄也是需要耗费内存的。也就是说，从内存的角度考虑，即有一部分存储数据，一部分管理文件句柄。如果Mapper分片数量为1000,Reduce分片数量为1000,那么总共就需要1000000个小文件。所以就会有很多内存消耗，频繁IO以及GC频繁或者出现内存溢出。 容易造成网络异常: 而且Reducer端读取Map端数据时，Mapper有这么多小文件，就需要打开很多网络通道读取，很容易造成Reducer（下一个stage）通过driver去拉取上一个stage数据的时候，说文件找不到，其实不是文件找不到而是程序不响应，因为正在GC. Sorted-Based Shuffle介绍为了缓解Shuffle过程产生文件数过多和Writer缓存开销过大的问题，spark引入了类似于hadoop Map-Reduce的shuffle机制。该机制每一个ShuffleMapTask不会为后续的任务创建单独的文件，而是会将所有的Task结果写入同一个文件，并且对应生成一个索引文件。以前的数据是放在内存缓存中，等到数据完了再刷到磁盘，现在为了减少内存的使用，在内存不够用的时候，可以将输出溢写到磁盘，结束的时候，再将这些不同的文件联合内存的数据一起进行归并，从而减少内存的使用量。一方面文件数量显著减少，另一方面减少Writer缓存所占用的内存大小，而且同时避免GC的风险和频率。 Sort-Based Shuffle有几种不同的策略：BypassMergeSortShuffleWriter、SortShuffleWriter和UnasfeSortShuffleWriter。 对于BypassMergeSortShuffleWriter 使用这个模式特点： 主要用于处理不需要排序和聚合的Shuffle操作，所以数据是直接写入文件，数据量较大的时候，网络I/O和内存负担较重 主要适合处理Reducer任务数量比较少的情况下 将每一个分区写入一个单独的文件，最后将这些文件合并,减少文件数量；但是这种方式需要并发打开多个文件，对内存消耗比较大 因为BypassMergeSortShuffleWriter这种方式比SortShuffleWriter更快，所以如果在Reducer数量不大，又不需要在map端聚合和排序，而且最终产生的文件数量少，尽量使用这种方式进行shuffle。 Reducer的数目 &lt; spark.shuffle.sort.bypassMergeThrshold指定的阀值，就是用的是这种方式。 对于SortShuffleWriter 使用这个模式特点： 比较适合数据量很大的场景或者集群规模很大 引入了外部外部排序器，可以支持在Map端进行本地聚合或者不聚合 如果外部排序器enable了spill功能，如果内存不够，可以先将输出溢写到本地磁盘，最后将内存结果和本地磁盘的溢写文件进行合并 另外这个Sort-Based Shuffle跟Executor核数没有关系，即跟并发度没有关系，它是每一个ShuffleMapTask都会产生一个data文件和index文件，所谓合并也只是将该ShuffleMapTask的各个partition对应的分区文件合并到data文件而已。所以这个就需要个Hash-BasedShuffle的consolidation机制区别开来。]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark原理]]></title>
    <url>%2F2020%2F03%2F31%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FSpark%2F5.Spark%20%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Spark 任务提交任务提交的主要四个阶段DAG的生成 =&gt; stage切分 =&gt; task的生成 =&gt; 任务提交 构建DAG用户提交的job将首先被转换成一系列RDD并通过RDD之间的依赖关系构建DAG,然后将DAG提交到调度系统； DAGScheduler将DAG切分stage（切分依据是shuffle）,将stage中生成的task以taskset的形式发送给TaskScheduler Scheduler 调度task（根据资源情况将task调度到Executors） Executors接收task，然后将task交给线程池执行。 任务提交详细步骤 spark集群启动后，Worker向Master注册信息 spark-submit命令提交程序后，driver和application也会向Master注册信息 创建SparkContext对象：主要的对象包含DAGScheduler和TaskScheduler Driver把Application信息注册给Master后，Master会根据App信息去Worker节点启动Executor Executor内部会创建运行task的线程池，然后把启动的Executor反向注册给Dirver DAGScheduler：负责把Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；同时DAGScheduler还会处理由于Shuffle数据丢失导致的失败； TaskScheduler：维护所有TaskSet，分发Task给各个节点的Executor（根据数据本地化策略分发Task），监控task的运行状态，负责重试失败的task； 所有task运行完成后，SparkContext向Master注销，释放资源； Spark stage 切分流程 划分stage 的思路park划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。 stage 作用这是个复杂是业务逻辑（将多台机器上具有相同属性的数据聚合到一台机器上:shuffle）如果有shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，下一个阶段的计算依赖上一个阶段的数据在同一个stage中，会有多个算子，可以合并到一起，我们很难‘’称其为pipeline（流水线，严格按照流程、顺序执行） Spark Driver 给Executor 提交task 时序图 Spark Executor 启动和任务接受和执行时序图 Spark Shuffle概述Shuffle就是对数据进行重组，由于分布式计算的特性和要求，在实现细节上更加繁琐和复杂 在MapReduce框架，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I/O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己的shuffle实现过程 在DAG调度的过程中，Stage阶段的划分是根据是否有shuffle过程，也就是存在ShuffleDependency宽依赖的时候，需要进行shuffle,这时候会将作业job划分成多个Stage；并且在划分Stage的时候，构建ShuffleDependency的时候进行shuffle注册，获取后续数据读取所需要的ShuffleHandle,最终每一个job提交后都会生成一个ResultStage和若干个ShuffleMapStage，其中ResultStage表示生成作业的最终结果所在的Stage. ResultStage与ShuffleMapStage中的task分别对应着ResultTask与ShuffleMapTask。一个作业，除了最终的ResultStage外，其他若干ShuffleMapStage中各个ShuffleMapTask都需要将最终的数据根据相应的Partitioner对数据进行分组，然后持久化分区的数据。 HashShuffle机制HashShuffle概述在spark-1.6版本之前，一直使用HashShuffle，在spark-1.6版本之后使用Sort-Base Shuffle，因为HashShuffle存在的不足所以就替换了HashShuffle. 我们知道，Spark的运行主要分为2部分：一部分是驱动程序，其核心是SparkContext；另一部分是Worker节点上Task,它是运行实际任务的。程序运行的时候，Driver和Executor进程相互交互：运行什么任务，即Driver会分配Task到Executor，Driver 跟 Executor 进行网络传输; 任务数据从哪儿获取，即Task要从 Driver 抓取其他上游的 Task 的数据结果，所以有这个过程中就不断的产生网络结果。其中，下一个 Stage 向上一个 Stage 要数据这个过程，我们就称之为 Shuffle。 没有优化之前的HashShuffle机制 在HashShuffle没有优化之前，每一个ShufflleMapTask会为每一个ReduceTask创建一个bucket缓存，并且会为每一个bucket创建一个文件。这个bucket存放的数据就是经过Partitioner操作(默认是HashPartitioner)之后找到对应的bucket然后放进去，最后将数据刷新bucket缓存的数据到磁盘上，即对应的block file. 然后ShuffleMapTask将输出作为MapStatus发送到DAGScheduler的MapOutputTrackerMaster，每一个MapStatus包含了每一个ResultTask要拉取的数据的位置和大小 ResultTask然后去利用BlockStoreShuffleFetcher向MapOutputTrackerMaster获取MapStatus，看哪一份数据是属于自己的，然后底层通过BlockManager将数据拉取过来 拉取过来的数据会组成一个内部的ShuffleRDD，优先放入内存，内存不够用则放入磁盘，然后ResulTask开始进行聚合，最后生成我们希望获取的那个MapPartitionRDD 这种方式的缺点 如上图所示：在这里有1个worker，2个executor，每一个executor运行2个ShuffleMapTask，有三个ReduceTask，所以总共就有4 * 3=12个bucket和12个block file。 如果数据量较大，将会生成MR个小文件，比如ShuffleMapTask有100个，ResultTask有100个，这就会产生100100=10000个小文件 bucket缓存很重要，需要将ShuffleMapTask所有数据都写入bucket，才会刷到磁盘，那么如果Map端数据过多，这就很容易造成内存溢出，尽管后面有优化，bucket写入的数据达到刷新到磁盘的阀值之后，就会将数据一点一点的刷新到磁盘，但是这样磁盘I/O就多了 优化后的HashShuffle 每一个Executor进程根据核数，决定Task的并发数量，比如executor核数是2，就是可以并发运行两个task，如果是一个则只能运行一个task 假设executor核数是1，ShuffleMapTask数量是M,那么它依然会根据ResultTask的数量R，创建R个bucket缓存，然后对key进行hash，数据进入不同的bucket中，每一个bucket对应着一个block file,用于刷新bucket缓存里的数据 然后下一个task运行的时候，那么不会再创建新的bucket和block file，而是复用之前的task已经创建好的bucket和block file。即所谓同一个Executor进程里所有Task都会把相同的key放入相同的bucket缓冲区中 这样的话，生成文件的数量就是(本地worker的executor数量*executor的cores*ResultTask数量)如上图所示，即2 * 1* 3 = 6个文件，每一个Executor的shuffleMapTask数量100,ReduceTask数量为100，那么 未优化的HashShuffle的文件数是2 *1* 100*100 =20000，优化之后的数量是2*1*100 = 200文件，相当于少了100倍 这种方式的缺点： 如果 Reducer 端的并行任务或者是数据分片过多的话则 Core * Reducer Task 依旧过大，也会产生很多小文件。 Sort-Based ShuffleSort-Based Shuffle概述HashShuffle回顾 io\GC\内存占用大: HashShuffle写数据的时候，内存有一个bucket缓冲区，同时在本地磁盘有对应的本地文件，如果本地有文件，那么在内存应该也有文件句柄也是需要耗费内存的。也就是说，从内存的角度考虑，即有一部分存储数据，一部分管理文件句柄。如果Mapper分片数量为1000,Reduce分片数量为1000,那么总共就需要1000000个小文件。所以就会有很多内存消耗，频繁IO以及GC频繁或者出现内存溢出。 容易造成网络异常: 而且Reducer端读取Map端数据时，Mapper有这么多小文件，就需要打开很多网络通道读取，很容易造成Reducer（下一个stage）通过driver去拉取上一个stage数据的时候，说文件找不到，其实不是文件找不到而是程序不响应，因为正在GC. Sorted-Based Shuffle介绍为了缓解Shuffle过程产生文件数过多和Writer缓存开销过大的问题，spark引入了类似于hadoop Map-Reduce的shuffle机制。该机制每一个ShuffleMapTask不会为后续的任务创建单独的文件，而是会将所有的Task结果写入同一个文件，并且对应生成一个索引文件。以前的数据是放在内存缓存中，等到数据完了再刷到磁盘，现在为了减少内存的使用，在内存不够用的时候，可以将输出溢写到磁盘，结束的时候，再将这些不同的文件联合内存的数据一起进行归并，从而减少内存的使用量。一方面文件数量显著减少，另一方面减少Writer缓存所占用的内存大小，而且同时避免GC的风险和频率。 Sort-Based Shuffle有几种不同的策略：BypassMergeSortShuffleWriter、SortShuffleWriter和UnasfeSortShuffleWriter。 对于BypassMergeSortShuffleWriter 使用这个模式特点： 主要用于处理不需要排序和聚合的Shuffle操作，所以数据是直接写入文件，数据量较大的时候，网络I/O和内存负担较重 主要适合处理Reducer任务数量比较少的情况下 将每一个分区写入一个单独的文件，最后将这些文件合并,减少文件数量；但是这种方式需要并发打开多个文件，对内存消耗比较大 因为BypassMergeSortShuffleWriter这种方式比SortShuffleWriter更快，所以如果在Reducer数量不大，又不需要在map端聚合和排序，而且最终产生的文件数量少，尽量使用这种方式进行shuffle。 Reducer的数目 &lt; spark.shuffle.sort.bypassMergeThrshold指定的阀值，就是用的是这种方式。 对于SortShuffleWriter 使用这个模式特点： 比较适合数据量很大的场景或者集群规模很大 引入了外部外部排序器，可以支持在Map端进行本地聚合或者不聚合 如果外部排序器enable了spill功能，如果内存不够，可以先将输出溢写到本地磁盘，最后将内存结果和本地磁盘的溢写文件进行合并 另外这个Sort-Based Shuffle跟Executor核数没有关系，即跟并发度没有关系，它是每一个ShuffleMapTask都会产生一个data文件和index文件，所谓合并也只是将该ShuffleMapTask的各个partition对应的分区文件合并到data文件而已。所以这个就需要个Hash-BasedShuffle的consolidation机制区别开来。]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL]]></title>
    <url>%2F2020%2F03%2F31%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FSpark%2F3.Spark%20SQL%2F</url>
    <content type="text"><![CDATA[Spark SQL 介绍什么是Spark SQLSpark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。 Spark的作用我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！ Spark SQL 的特点 易整合 统一的数据访问方式 兼容Hive 标准的数据连接 Dataframe介绍什么是Dataframe与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上 看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。 如何创建Dataframe 在Spark SQL中SQLContext是创建DataFrames和执行SQL的入口，在spark-1.5.2中已经内置了一个sqlContext 示例: 在本地创建一个文件，有三列，分别是id、name、age，用空格分隔，然后上传到hdfs上 hdfs dfs -put person.txt / 在spark shell执行下面命令，读取数据，将每一行的数据使用列分隔符分割 val lineRDD = sc.textFile(“hdfs://node1.itcast.cn:9000/person.txt”).map(_.split(“ “)) 定义case class（相当于表的schema） case class Person(id:Int, name:String, age:Int) 将RDD和case class关联 val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt)) 将RDD转换成DataFrame val personDF = personRDD.toDF 对DataFrame进行处理 personDF.show Dataframe常用操作查看Dataframe 中内容 personDF.show 查看DataFrame部分列中的内容 personDF.select(personDF.col(“name”)).show personDF.select(col(“name”), col(“age”)).show personDF.select(“name”).show 打印dataFrame的Schema信息 personDF.printSchema 查看内容，并对字段进行计算 personDF.select(col(“id”), col(“name”), col(“age”) + 1).show personDF.select(personDF(“id”), personDF(“name”), personDF(“age”) + 1).show 字段条件过滤 personDF.filter(col(“age”) &gt;= 18).show 分组计算 personDF.groupBy(“age”).count().show() SQL 语法风格将Dataframe注册成表 personDF.registerTempTable(“t_person”) sql排序查询 sqlContext.sql(“select * from t_person order by age desc limit 2”).show 查询schema信息 sqlContext.sql(“desc t_person”).show 编写 Spark SQL查询任务环境准备创建scala maven项目，在maven项目的pom.xml中添加Spark SQL的依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt;&lt;/dependency&gt; 两种推断表Schema的方式通过反射来推断Schema 创建主类 12345678910111213141516171819202122232425262728293031323334353637package cn.itcast.spark.sqlimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.SQLContextobject InferringSchema &#123; def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val conf = new SparkConf().setAppName("SQL-1") //SQLContext要依赖SparkContext val sc = new SparkContext(conf) //创建SQLContext val sqlContext = new SQLContext(sc) //从指定的地址创建RDD val lineRDD = sc.textFile(args(0)).map(_.split(" ")) //创建case class //将RDD和case class关联 val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt)) //导入隐式转换，如果不到人无法将RDD转换成DataFrame //将RDD转换成DataFrame import sqlContext.implicits._ val personDF = personRDD.toDF //注册表 personDF.registerTempTable("t_person") //传入SQL val df = sqlContext.sql("select * from t_person order by age desc limit 2") //将结果以JSON的方式存储到指定位置 df.write.json(args(1)) //停止Spark Context sc.stop() &#125;&#125;//case class一定要放到外面case class Person(id: Int, name: String, age: Int) 将程序打成jar包，上传到spark集群，提交Spark任务 123456/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \--class cn.itcast.spark.sql.InferringSchema \--master spark://node1.itcast.cn:7077 \/root/spark-mvn-1.0-SNAPSHOT.jar \hdfs://node1.itcast.cn:9000/person.txt \hdfs://node1.itcast.cn:9000/out 查看运行结果 hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-r-* 通过StructType直接指定Schema 创建主类 1234567891011121314151617181920212223242526272829303132333435363738394041package cn.itcast.spark.sqlimport org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.sql.types._import org.apache.spark.&#123;SparkContext, SparkConf&#125;/** * Created by ZX on 2015/12/11. */object SpecifyingSchema &#123; def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val conf = new SparkConf().setAppName("SQL-2") //SQLContext要依赖SparkContext val sc = new SparkContext(conf) //创建SQLContext val sqlContext = new SQLContext(sc) //从指定的地址创建RDD val personRDD = sc.textFile(args(0)).map(_.split(" ")) //通过StructType直接指定每个字段的schema val schema = StructType( List( StructField("id", IntegerType, true), StructField("name", StringType, true), StructField("age", IntegerType, true) ) ) //将RDD映射到rowRDD val rowRDD = personRDD.map(p =&gt; Row(p(0).toInt, p(1).trim, p(2).toInt)) //将schema信息应用到rowRDD上 val personDataFrame = sqlContext.createDataFrame(rowRDD, schema) //注册表 personDataFrame.registerTempTable("t_person") //执行SQL val df = sqlContext.sql("select * from t_person order by age desc limit 4") //将结果以JSON的方式存储到指定位置 df.write.json(args(1)) //停止Spark Context sc.stop() &#125;&#125; 将程序打成jar包，上传到spark集群，提交Spark任务 123456/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \--class cn.itcast.spark.sql.InferringSchema \--master spark://node1.itcast.cn:7077 \/root/spark-mvn-1.0-SNAPSHOT.jar \hdfs://node1.itcast.cn:9000/person.txt \hdfs://node1.itcast.cn:9000/out 查看运行结果 hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-r-* Spark SQL JDBC 直接对接数据源Spark JDBC 作用Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。 Spark JDBC使用(spark shell示例) 启动Spark Shell，必须指定mysql连接驱动jar包 /usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \ –master spark://node1.itcast.cn:7077 \ –jars /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \ –driver-class-path /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar 从mysql中加载数据 1val jdbcDF = sqlContext.read.format("jdbc").options(Map("url" -&gt; "jdbc:mysql://192.168.10.1:3306/bigdata", "driver" -&gt; "com.mysql.jdbc.Driver", "dbtable" -&gt; "person", "user" -&gt; "root", "password" -&gt; "123456")).load() 对数据进行查询 jdbcDF.show() Spark JDBS使用(jar包方式) 创建主类 Spark SQL程序 12345678910111213141516171819202122232425262728293031import org.apache.spark.&#123;SparkConf, SparkContext&#125;object JdbcRDD &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("MySQL-Demo") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //通过并行化创建RDD val personRDD = sc.parallelize(Array("1 tom 5", "2 jerry 3", "3 kitty 6")).map(_.split(" ")) //通过StructType直接指定每个字段的schema val schema = StructType( List( StructField("id", IntegerType, true), StructField("name", StringType, true), StructField("age", IntegerType, true) ) ) //将RDD映射到rowRDD val rowRDD = personRDD.map(p =&gt; Row(p(0).toInt, p(1).trim, p(2).toInt)) //将schema信息应用到rowRDD上 val personDataFrame = sqlContext.createDataFrame(rowRDD, schema) //创建Properties存储数据库相关属性 val prop = new Properties() prop.put("user", "root") prop.put("password", "123456") //将数据追加到数据库 personDataFrame.write.mode("append").jdbc("jdbc:mysql://192.168.10.1:3306/bigdata", "bigdata.person", prop) //停止SparkContext sc.stop() &#125;&#125; 用maven将程序打包 将jar包提交到集群执行 123456/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \--class cn.itcast.spark.sql.JdbcRDD \--master spark://node1.itcast.cn:7077 \--jars /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \--driver-class-path /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \/root/spark-mvn-1.0-SNAPSHOT.jar]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming]]></title>
    <url>%2F2020%2F03%2F31%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FSpark%2F4.Spark%20Streaming%2F</url>
    <content type="text"><![CDATA[Spark Streaming介绍什么是Spark StreamingSpark Streaming类似于Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。 为什么使用Spark Streaming 易用 容错 容易整合到Spark体系 DStream介绍什么是DStreamDiscretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据，如下图： 对数据的操作也是按照RDD为单位来进行的 计算过程由Spark engine来完成 DStream 相关操作DStream上的原语与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。 Transformations on DStreams Transformation Meaning map(func) Return a new DStream by passing each element of the source DStream through a function func. flatMap(func) Similar to map, but each input item can be mapped to 0 or more output items. filter(func) Return a new DStream by selecting only the records of the source DStream on which func returns true. repartition(numPartitions) Changes the level of parallelism in this DStream by creating more or fewer partitions. union(otherStream) Return a new DStream that contains the union of the elements in the source DStream and otherDStream. count() Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream. reduce(func) Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative so that it can be computed in parallel. countByValue() When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream. reduceByKey(func, [numTasks]) When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks. join(otherStream, [numTasks]) When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key. cogroup(otherStream, [numTasks]) When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples. transform(func) Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream. updateStateByKey(func) Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key. 特殊的 Transformations UpdateStateByKey Operation UpdateStateByKey原语用于记录历史记录，上文中Word Count示例中就用到了该特性。若不用UpdateStateByKey来更新状态，那么每次数据进来后分析完成后，结果输出后将不在保存 Transform Operation Transform原语允许DStream上执行任意的RDD-to-RDD函数。通过该函数可以方便的扩展Spark API。此外，MLlib（机器学习）以及Graphx也是通过本函数来进行结合的。 Window Operations Window Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态 Output Operations on DStreamsOutput Operations可以将DStream的数据输出到外部的数据库或文件系统，当某个Output Operations原语被调用时（与RDD的Action相同），streaming程序才会开始真正的计算过程。 Output Operation Meaning print() Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. saveAsTextFiles(prefix, [suffix]) Save this DStream’s contents as text files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”. saveAsObjectFiles(prefix, [suffix]) Save this DStream’s contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”. saveAsHadoopFiles(prefix, [suffix]) Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: “prefix-TIME_IN_MS[.suffix]”. foreachRDD(func) The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs. Spark Streaming 实战实现 word count 启动生产者服务(socket 服务为例) 首先在一台Linux（ip：192.168.10.101）上用YUM安装nc工具 yum install -y nc 启动一个服务端并监听9999端口 nc -lk 9999 编写Spark Streaming程序 12345678910111213141516171819202122232425262728293031package cn.itcast.spark.streamingimport cn.itcast.spark.util.LoggerLevelimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object NetworkWordCount &#123; def main(args: Array[String]) &#123; //设置日志级别 LoggerLevel.setStreamingLogLevels() //创建SparkConf并设置为本地模式运行 //注意local[2]代表开两个线程 val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount") //设置DStream批次时间间隔为2秒 val ssc = new StreamingContext(conf, Seconds(2)) //通过网络读取数据 val lines = ssc.socketTextStream("192.168.10.101", 9999) //将读到的数据用空格切成单词 val words = lines.flatMap(_.split(" ")) //将单词和1组成一个pair val pairs = words.map(word =&gt; (word, 1)) //按单词进行分组求相同单词出现的次数 val wordCounts = pairs.reduceByKey(_ + _) //打印结果到控制台 wordCounts.print() //开始计算 ssc.start() //等待停止 ssc.awaitTermination() &#125;&#125; 启动Spark Streaming 由于使用的是本地模式“local[2]”所以可以直接在本地运行该程序 注意：要指定并行度，如在本地运行设置setMaster(“local[2]”)，相当于启动两个线程，一个给receiver(1.3版本之前)，一个给computer。如果是在集群中运行，必须要求集群中可用core数大于1 开始生产 在Linux nc生产者端命令行中输入单词 查看到spark的流处理输出 问题：结果每次在Linux段输入的单词次数都被正确的统计出来，但是结果不能累加！如果需要累加需要使用updateStateByKey(func)来更新状态，下面给出一个例子 实现Spark Streaming 累加123456789101112131415161718192021222324252627282930313233package cn.itcast.spark.streamingimport cn.itcast.spark.util.LoggerLevelimport org.apache.spark.&#123;HashPartitioner, SparkConf&#125;import org.apache.spark.streaming.&#123;StreamingContext, Seconds&#125;object NetworkUpdateStateWordCount &#123; /** * String : 单词 hello * Seq[Int] ：单词在当前批次出现的次数 * Option[Int] ： 历史结果 */ val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) =&gt; &#123; //iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x))) iter.flatMap&#123;case(x,y,z)=&gt;Some(y.sum + z.getOrElse(0)).map(m=&gt;(x, m))&#125; &#125; def main(args: Array[String]) &#123; LoggerLevel.setStreamingLogLevels() val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkUpdateStateWordCount") val ssc = new StreamingContext(conf, Seconds(5)) //做checkpoint 写入共享存储中 ssc.checkpoint("c://aaa") val lines = ssc.socketTextStream("192.168.10.100", 9999) //reduceByKey 结果不累加 //val result = lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_) //updateStateByKey结果可以累加但是需要传入一个自定义的累加函数：updateFunc val results = lines.flatMap(_.split(" ")).map((_,1)).updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true) results.print() ssc.start() ssc.awaitTermination() &#125;&#125; Spark Streaming 结合Kafka示例 安装并配置zk 安装并配置Kafka 启动zk 启动Kafka 创建topic 1bin/kafka-topics.sh --create --zookeeper node1.itcast.cn:2181,node2.itcast.cn:2181 --replication-factor 3 --partitions 3 --topic urlcount 编写Spark Streaming 程序 12345678910111213141516171819202122232425262728293031323334353637package cn.itcast.spark.streamingpackage cn.itcast.sparkimport org.apache.spark.&#123;HashPartitioner, SparkConf&#125;import org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.kafka.KafkaUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object UrlCount &#123; val updateFunc = (iterator: Iterator[(String, Seq[Int], Option[Int])]) =&gt; &#123; iterator.flatMap&#123;case(x,y,z)=&gt; Some(y.sum + z.getOrElse(0)).map(n=&gt;(x, n))&#125; &#125; def main(args: Array[String]) &#123; //接收命令行中的参数 val Array(zkQuorum, groupId, topics, numThreads, hdfs) = args //创建SparkConf并设置AppName val conf = new SparkConf().setAppName("UrlCount") //创建StreamingContext val ssc = new StreamingContext(conf, Seconds(2)) //设置检查点 ssc.checkpoint(hdfs) //设置topic信息 val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap //重Kafka中拉取数据创建DStream val lines = KafkaUtils.createStream(ssc, zkQuorum ,groupId, topicMap, StorageLevel.MEMORY_AND_DISK).map(_._2) //切分数据，截取用户点击的url val urls = lines.map(x=&gt;(x.split(" ")(6), 1)) //统计URL点击量 val result = urls.updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true) //将结果打印到控制台 result.print() ssc.start() ssc.awaitTermination() &#125;&#125; 提交任务并执行]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 拓展]]></title>
    <url>%2F2020%2F03%2F15%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FKafka%2FKafka%20%E6%8B%93%E5%B1%95%2F</url>
    <content type="text"><![CDATA[Kafka 介绍什么是Kafka在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。 KAFKA + STORM +REDIS Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。 Kafka最初是由LinkedIn开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。 Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。 Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性 什么是JMSJMS是什么：JMS是Java提供的一套技术规范 JMS干什么用：用来异构系统 集成通信，缓解系统瓶颈，提高系统的伸缩性增强系统用户体验，使得系统模块化和组件化变得可行并更加灵活 通过什么方式：生产消费者模式（生产者、服务器、消费者） 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除） 点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息*一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。 发布/订阅模式（一对多，数据生产后，推送给所有订阅者） 发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即时当前订阅者不可用，处于离线状态。 12queue.put（object） 数据生产queue.take(object) 数据消费 JMS 核心组件 Destination：消息发送的目的地，也就是前面说的Queue和Topic。 Message ：从字面上就可以看出是被发送的消息。 Producer： 消息的生产者，要发送一个消息，必须通过这个生产者来发送。 MessageConsumer： 与生产者相对应，这是消息的消费者或接收者，通过它来接收一个消息。 为什么需要JMS组件（消息队列）消息系统的核心作用就是三点：解耦，异步和并行 以用户注册的案列来说明消息系统的作用 Kafka 核心组件 Topic ：消息根据Topic进行归类 Producer：发送消息者 Consumer：消息接受者 broker：每个kafka实例(server) Zookeeper：依赖集群保存meta信息。 Kafka 部署步骤 下载安装包 http://kafka.apache.org/downloads.html 在linux中使用wget命令下载安装包 wget http://mirrors.hust.edu.cn/apache/kafka/0.8.2.2/kafka_2.11-0.8.2.2.tgz 解压安装包 tar -zxvf /export/software/kafka_2.11-0.8.2.2.tgz -C /export/servers/ cd /export/servers/ ln -s kafka_2.11-0.8.2.2 kafka 修改配置文件 cp /export/servers/kafka/config/server.properties /export/servers/kafka/config/server.properties.bak vi /export/servers/kafka/config/server.properties 输入以下内容 分发安装包 scp -r /export/servers/kafka_2.11-0.8.2.2 kafka02:/export/servers 然后分别在各机器上创建软连 cd /export/servers/ ln -s kafka_2.11-0.8.2.2 kafka 再次修改配置文件（重要） 依次修改各服务器上配置文件的的broker.id，分别是0,1,2不得重复。 启动集群 依次在各节点上启动kafka bin/kafka-server-start.sh config/server.properties kafka集群命令 查看当前服务器中的所有topic 1bin/kafka-topics.sh --list --zookeeper zk01:2181 创建topic 1./kafka-topics.sh --create --zookeeper mini1:2181 --replication-factor 1 --partitions 3 --topic first 删除topic 1sh bin/kafka-topics.sh --delete --zookeeper zk01:2181 --topic test 需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。 通过shell命令发送消息 1kafka-console-producer.sh --broker-list kafka01:9092 --topic itheima 通过shell消费消息 1sh bin/kafka-console-consumer.sh --zookeeper zk01:2181 --from-beginning --topic test1 查看消费位置 1sh kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk01:2181 --group testGroup 查看某个Topic的详情 1sh kafka-topics.sh --topic test --describe --zookeeper zk01:2181 Kafka java api 生产者相关api 消费者相关api Kafka 架构介绍Kafka 整体结构介绍 Producer ：消息生产者，就是向kafka broker发消息的客户端。 Consumer ：消息消费者，向kafka broker取消息的客户端 Topic ：咋们可以理解为一个队列。 Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。 Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。 Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。 Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka Consumer 与topic 的关系本质上kafka只支持Topic； 每个group中可以有多个consumer，每个consumer属于一个consumer group； 通常情况下，一个group中会包含多个consumer，这样不仅可以提高topic中消息的并发消费能力，而且还能提高”故障容错”性，如果group中的某个consumer失效那么其消费的partitions将会有其他consumer自动接管。 对于Topic中的一条特定的消息，只会被订阅此Topic的每个group中的其中一个consumer消费，此消息不会发送给一个group的多个consumer； 那么一个group中所有的consumer将会交错的消费整个Topic，每个group中consumer消息消费互相独立，我们可以认为一个group是一个”订阅”者。 在kafka中,一个partition中的消息只会被group中的一个consumer消费(同一时刻)； 一个Topic中的每个partions，只会被一个”订阅者”中的一个consumer消费，不过一个consumer可以同时消费多个partitions中的消息。 kafka的设计原理决定,对于一个topic，同一个group中不能有多于partitions个数的consumer同时消费，否则将意味着某些consumer将无法得到消息。 kafka只能保证一个partition中的消息被某个consumer消费时是顺序的；事实上，从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。 Kafka 消息分发Producer客户端负责消息的分发 kafka集群中的任何一个broker都可以向producer提供metadata信息,这些metadata中包含”集群中存活的servers列表”/“partitions leader列表”等信息； 当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接； 消息由producer直接通过socket发送到broker，中间不会经过任何”路由层”，事实上，消息被路由到哪个partition上由producer客户端决定； 比如可以采用”random””key-hash””轮询”等,如果一个topic中有多个partitions,那么在producer端实现”消息均衡分发”是必要的。 在producer端的配置文件中,开发者可以指定partition路由的方式。 Producer消息发送的应答机制 设置发送数据是否需要服务端的反馈,有三个值0,1,-1 0: producer不会等待broker发送ack 1: 当leader接收到消息之后发送ack -1: 当所有的follower都同步消息成功后发送ack 1request.required.acks=0 Kafka 消费负载均衡当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力，步骤如下： 1、 假如topic1,具有如下partitions: P0,P1,P2,P3 2、 加入group中,有如下consumer: C1,C2 3、 首先根据partition索引号对partitions排序: P0,P1,P2,P3 4、 根据consumer.id排序: C0,C1 5、 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整) 6、 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)] Kafka生产者分区机制所谓分区策略是决定生产者将消息发送到哪个分区的算法。Kafka 为我们提供了默认的分区策略，同时它也支持你自定义分区策略。 如果要自定义分区策略，你需要显式地配置生产者端的参数partitioner.class。这个参数该怎么设定呢？方法很简单，在编写生产者程序时，你可以编写一个具体的类实现org.apache.kafka.clients.producer.Partitioner接口。这个接口也很简单，只定义了两个方法：partition()和close()，通常你只需要实现最重要的 partition 方法。我们来看看这个方法的方法签名： 1int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); 这里的topic、key、keyBytes、value和valueBytes都属于消息数据，cluster则是集群信息（比如当前 Kafka 集群共有多少主题、多少 Broker 等）。Kafka 给你这么多信息，就是希望让你能够充分地利用这些信息对消息进行分区，计算出它要被发送到哪个分区中。只要你自己的实现类定义好了 partition 方法，同时设置partitioner.class参数为你自己实现类的 Full Qualified Name，那么生产者程序就会按照你的代码逻辑对消息进行分区。虽说可以有无数种分区的可能，但比较常见的分区策略也就那么几种，下面我来详细介绍一下。 轮询策略也称 Round-robin 策略，即顺序分配。比如一个主题下有 3 个分区，那么第一条消息被发送到分区 0，第二条被发送到分区 1，第三条被发送到分区 2，以此类推。当生产第 4 条消息时又会重新开始，即将其分配到分区 0，就像下面这张图展示的那样。 这就是所谓的轮询策略。轮询策略是 Kafka Java 生产者 API 默认提供的分区策略。如果你未指定partitioner.class参数，那么你的生产者程序会按照轮询的方式在主题的所有分区间均匀地“码放”消息。 轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上，故默认情况下它是最合理的分区策略，也是我们最常用的分区策略之一。 随机策略也称 Randomness 策略。所谓随机就是我们随意地将消息放置到任意一个分区上，如下面这张图所示。 如果要实现随机策略版的 partition 方法，很简单，只需要两行代码即可： 12List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);return ThreadLocalRandom.current().nextInt(partitions.size()); 先计算出该主题总的分区数，然后随机地返回一个小于它的正整数。 本质上看随机策略也是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于轮询策略，所以如果追求数据的均匀分布，还是使用轮询策略比较好。事实上，随机策略是老版本生产者使用的分区策略，在新版本中已经改为轮询了。 按消息key保存策略也称 Key-ordering 策略。这个可以理解为是自定义的策略之一。 Kafka 允许为每条消息定义消息键，简称为 Key。这个 Key 的作用非常大，它可以是一个有着明确业务含义的字符串，比如客户代码、部门编号或是业务 ID 等；也可以用来表征消息元数据。特别是在 Kafka 不支持时间戳的年代，在一些场景中，工程师们都是直接将消息创建时间封装进 Key 里面的。一旦消息被定义了 Key，那么你就可以保证同一个 Key 的所有消息都进入到相同的分区里面，由于每个分区下的消息处理都是有顺序的，故这个策略被称为按消息键保序策略，如下图所示。 实现这个策略的 partition 方法同样简单，只需要下面两行代码即可： 12List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);return Math.abs(key.hashCode()) % partitions.size(); 前面提到的 Kafka 默认分区策略实际上同时实现了两种策略：如果指定了 Key，那么默认实现按消息Key保序策略；如果没有指定 Key，则使用轮询策略。 其他分区策略上面这几种分区策略都是比较基础的策略，除此之外你还能想到哪些有实际用途的分区策略？其实还有一种比较常见的，即所谓的基于地理位置的分区策略。当然这种策略一般只针对那些大规模的 Kafka 集群，特别是跨城市、跨国家甚至是跨大洲的集群。 假设有个厂商所有服务都部署在北京的一个机房（这里我假设它是自建机房，不考虑公有云方案。其实即使是公有云，实现逻辑也差不多），现在该厂商考虑在南方找个城市（比如深圳）再创建一个机房；另外从两个机房中选取一部分机器共同组成一个大的 Kafka 集群。显然，这个集群中必然有一部分机器在北京，另外一部分机器在深圳。 假设该厂商计划为每个新注册用户提供一份注册礼品，比如南方的用户注册的可以免费得到一碗“甜豆腐脑”，而北方的新注册用户可以得到一碗“咸豆腐脑”。如果用 Kafka 来实现则很简单，只需要创建一个双分区的主题，然后再创建两个消费者程序分别处理南北方注册用户逻辑即可。 但问题是你需要把南北方注册用户的注册消息正确地发送到位于南北方的不同机房中，因为处理这些消息的消费者程序只可能在某一个机房中启动着。换句话说，送甜豆腐脑的消费者程序只在深圳机房启动着，而送咸豆腐脑的程序只在北京的机房中，如果你向深圳机房中的 Broker 发送北方注册用户的消息，那么这个用户将无法得到礼品！ 此时我们就可以根据 Broker 所在的 IP 地址实现定制化的分区策略。比如下面这段代码： 12List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);return partitions.stream().filter(p -&gt; isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get() 我们可以从所有分区中找出那些 Leader 副本在南方的所有分区，然后随机挑选一个进行消息发送。 Kafka 文件存储机制Kafka 文件存储基本结构 在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。默认保留7天的数据。 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除） 数据有序的讨论？ ​ 一个partition的数据是否是有序的？ 间隔性有序，不连续 ​ 针对一个topic里面的数据，只能做到partition内部有序，不能做到全局有序。 ​ 特别加入消费者的场景后，如何保证消费者消费的数据全局有序的？伪命题。 ​ 只有一种情况下才能保证全局有序？就是只有一个partition。 Kafka Partition Segment Segment file组成：由2大部分组成，分别为index file和data file，此2个文件一 一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件。 Segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。 索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 ​ 3，497：当前log文件中的第几条信息，存放在磁盘上的那个地方 上述图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。 segment data file由许多message组成， 物理结构如下： Kafka 查找Message读取offset=368776的message，需要通过下面2个步骤查找。 查找segment file00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0 00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1 00000000000000737337.index的起始偏移量为737338=737337 + 1 其他后续文件依次类推。 以起始偏移量命名并排序这些文件，只要根据offset 二分查找文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index和对应log文件。 通过segment file 查找 message当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址 然后再通过00000000000000368769.log顺序查找直到offset=368776为止。]]></content>
      <categories>
        <category>数据存储</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>数据存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm]]></title>
    <url>%2F2020%2F03%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FStorm%2FStorm%2F</url>
    <content type="text"><![CDATA[流式计算和storm介绍什么是离线计算离线计算：批量获取数据、批量传输数据、周期性批量计算数据、数据展示 代表技术：Sqoop批量导入数据、HDFS批量存储数据、MapReduce批量计算数据、Hive批量计算数据、任务调度 什么是流式计算流式计算：数据实时产生、数据实时传输、数据实时计算、实时展示 代表技术：Flume实时获取数据、Kafka/metaq实时数据存储、Storm/JStorm实时数据计算、Redis实时结果缓存、持久化存储(mysql)。 一句话总结：将源源不断产生的数据实时收集并实时计算，尽可能快的得到计算结果 离线计算和实时计算的区别最大的区别：实时收集、实时计算、实时展示 什么是stormStorm用来实时处理数据，特点：低延迟、高可用、分布式、可扩展、数据不丢失。提供简单容易理解的接口，便于开发。 Storm 与 Hadoop的区别 Storm用于实时计算，Hadoop用于离线计算。 Storm处理的数据保存在内存中，源源不断；Hadoop处理的数据保存在文件系统中，一批一批。 Storm的数据通过网络传输进来；Hadoop的数据保存在磁盘中。 Storm与Hadoop的编程模型相似(对比图如下) 应用场景 日志分析 从海量日志中分析出特定的数据，并将分析的结果存入外部存储器用来辅佐决策。 管道系统 将一个数据从一个系统传输到另外一个系统，比如将数据库同步到Hadoop 消息转化器 将接受到的消息按照某种格式进行转化，存储到另外一个系统如消息中间件 Storm 架构Storm核心组件 Nimbus：负责资源分配和任务调度。 Supervisor：负责接受nimbus分配的任务，启动和停止属于自己管理的worker进程。—通过配置文件设置当前supervisor上启动多少个worker。 Worker：运行具体处理组件逻辑的进程。Worker运行的任务类型只有两种，一种是Spout任务，一种是Bolt任务。 Task：worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，不同spout/bolt的task可能会共享一个物理线程，该线程称为executor。 Storm 编程模型 Topology：Storm中运行的一个实时应用程序的名称。（拓扑） Spout：在一个topology中获取源数据流的组件。 通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。 Bolt：接受数据然后执行处理的组件,用户可以在其中执行自己想要的操作。 Tuple：一次消息传递的基本单元，理解为一组消息就是一个Tuple。 StreamGrouping:数据分组策略， 一共有7种 流式计算的一般架构 其中flume用来获取数据。 Kafka用来临时保存数据。 Strom用来计算数据。 Redis是个内存数据库，用来保存数据。 Storm 集群部署与操作集群部署流程基础环境准备 配置所有机器hosts vi /etc/hosts 123xx.xx.xx.xx storm01 zk01 hadoop01xx.xx.xx.xx9 storm02 zk02 hadoop02xx.xx.xx.xx storm03 zk03 hadoop03 关闭防火墙 1chkconfig iptables off &amp;&amp; setenforce 0 创建storm用户 1groupadd stormuser &amp;&amp; useradd stormuser &amp;&amp; usermod -a -G stormuser stormuser 创建工作目录 12345mkdir /exportmkdir /export/serverschmod 755 -R /export# 切换到realtime用户下su stormuser 安装包下载 下载安装包 1wget http://124.202.164.6/files/1139000006794ECA/apache.fayea.com/storm/apache-storm-0.9.5/apache-storm-0.9.5.tar.gz 解压安装包 123tar -zxvf apache-storm-0.9.5.tar.gz -C /export/servers/cd /export/servers/ln -s apache-storm-0.9.5 storm 配置文件修改与安装包同步 修改配置文件 12mv /export/servers/storm/conf/storm.yaml /export/servers/storm/conf/storm.yaml.bakvi /export/servers/storm/conf/storm.yaml 输入以下内容： ​ 指定集群中的机器，nimbus host 机器， JVM的参数配置， 以及supervisor启动worker对应的端口号 分发安装包 1234scp -r /export/servers/apache-storm-0.9.5 storm02:/export/servers# 然后分别在各机器上创建软连接cd /export/servers/ln -s apache-storm-0.9.5 storm 启动集群 在nimbus.host所属的机器上启动 nimbus服务 12cd /export/servers/storm/bin/nohup ./storm nimbus &amp; 在nimbus.host所属的机器上启动ui服务 12cd /export/servers/storm/bin/nohup ./storm ui &amp; 在其它个点击上启动supervisor服务(拓展节点的时候，重复分发安装包，并执行该命令) 12cd /export/servers/storm/bin/nohup ./storm supervisor &amp; 查看集群​ 访问nimbus.host:/8080，即可看到storm的ui界面。 ​ Storm 的集群命令有许多简单且有用的命令可以用来管理拓扑，它们可以提交、杀死、禁用、再平衡拓扑。 任务提交命令命令格式 1storm jar 【jar路径】 【拓扑包名.拓扑类名】 【拓扑名称】 命令示例 1bin/storm jar examples/storm-starter/storm-starter-topologies-0.9.6.jar storm.starter.WordCountTopology wordcount 杀死任务命令命令格式 1storm kill 【拓扑名称】 -w 10（执行kill命令时可以通过-w [等待秒数]指定拓扑停用以后的等待时间） 命令示例 1storm kill topology-name -w 10 停用任务命令命令格式 1storm deactivte 【拓扑名称】 命令示例 1storm deactivte topology-name 我们能够挂起或停用运行中的拓扑。当停用拓扑时，所有已分发的元组都会得到处理，但是spouts的nextTuple方法不会被调用。销毁一个拓扑，可以使用kill命令。它会以一种安全的方式销毁一个拓扑，首先停用拓扑，在等待拓扑消息的时间段内允许拓扑完成当前的数据流。 启用任务命令命令格式 1storm activate【拓扑名称】 命令示例 1storm activate topology-name 重新部署任务命令命令格式 1storm rebalance 【拓扑名称】 命令示例 1storm rebalance topology-name ​ 再平衡使你重分配集群任务。这是个很强大的命令。比如，你向一个运行中的集群增加了节点。再平衡命令将会停用拓扑，然后在相应超时时间之后重分配工人，并重启拓扑。 Storm日志nimbus的日志信息在nimbus的服务器上 12cd /export/servers/storm/logstail -100f /export/servers/storm/logs/nimbus.log 查看ui运行日志信息在ui的服务器上，一般和nimbus一个服务器 12cd /export/servers/storm/logstail -100f /export/servers/storm/logs/ui.log 查看supervisor运行日志信息在supervisor服务上 12cd /export/servers/storm/logstail -100f /export/servers/storm/logs/supervisor.log 查看supervisor上worker运行日志信息在supervisor服务上 12cd /export/servers/storm/logstail -100f /export/servers/storm/logs/worker-6702.log Storm流式计算生命周期介绍 storm 执行流式数据处理函数的生命周期主要是通过Spout和Bolt 这两个组件的生命周期 Spout组件涉及到的方法有： declareOutputFields() (调用一次) open() (调用一次) activate() (调用一次) nextTuple() （循环调用 ） disactive() (手动调用) Bolt组件涉及到的方法有 declareOutputFileds() (调用一次) prepare() (调用一次) execute() （循环执行） 执行顺序？ 在客户端将jar包提交到集群上的时候，执行 spout 、bolt 的构造方法以及declareOutputFields（） spout执行open方法一次，得到conf一些配置信息 activate让处理的信息发送到bolt中 Bolt 中 prepare 方法只执行一次 Spout 中 nextTuple不断的循环执行 Blot 中 execute 不断的循环执行 Stream GroupingStream Grouping 的作用Storm 的Tuple 从 Spout 中 分发到 Bolt， 以及从Bolt 分发的Bolt 的过程中，Storm定义了一些内置的分发方法和规则， 这些规则通过一些条件或者随机将具有相同特征的tuple分发到同一Bolt中进行处理，这样有利于对不同特征的数据进行集中统计 下面介绍一些常用的Grouping 种类 Stream Grouping 种类 Shuffle Grouping: 随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。 Fields Grouping：按字段分组，比如按userid来分组，具有同样userid的tuple会被分到相同的Bolts里的一个task，而不同的userid则会被分配到不同的bolts里的task。 All Grouping：广播发送，对于每一个tuple，所有的bolts都会收到。 Global Grouping：全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。 Non Grouping：不分组，这stream grouping个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果，有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行， 即减少跨主机socket通讯。 Direct Grouping： 直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id （OutputCollector.emit方法也会返回task的id）。 Local or shuffle grouping：如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。 单体统计案例功能说明设计一个topology，来实现对文档里面的单词出现的频率进行统计。 整个topology分为三个部分 RandomSentenceSpout：数据源，在已知的英文句子中，随机发送一条句子出去。 SplitSentenceBolt：负责将单行文本记录（句子）切分成单词 WordCountBolt：负责对单词的频率进行累加 项目结构设计 RandomSentenceSpout的实现 SplitSentenceBolt的实现 WordCountBolt的实现 Storm 程序的并发机制概念 Workers (JVMs): 在一个物理节点上可以运行一个或多个独立的JVM 进程。一个Topology可以包含一个或多个worker(并行的跑在不同的物理机上), 所以worker process就是执行一个topology的子集, 并且worker只能对应于一个topology Executors (threads): 在一个worker JVM进程中运行着多个Java线程。一个executor线程可以执行一个或多个tasks。但一般默认每个executor只执行一个task。一个worker可以包含一个或多个executor, 每个component (spout或bolt)至少对应于一个executor, 所以可以说executor执行一个compenent的子集, 同时一个executor只能对应于一个component。 Tasks(bolt/spout instances)：Task就是具体的处理逻辑对象，每一个Spout和Bolt会被当作很多task在整个集群里面执行。每一个task对应到一个线程，而stream grouping则是定义怎么从一堆task发射tuple到另外一堆task。你可以调用TopologyBuilder.setSpout和TopBuilder.setBolt来设置并行度 — 也就是有多少个task。 配置并行度通过修改配置文件和代码配置冰心额度 对于并发度的配置, 在storm里面可以在多个地方进行配置, 优先级为： defaults.yaml &lt; storm.yaml &lt; topology-specific configuration &lt; internal component-specific configuration &lt; external component-specific configuration worker processes的数目, 可以通过配置文件和代码中配置, worker就是执行进程, 所以考虑并发的效果, 数目至少应该大于machines的数目 executor的数目, component的并发线程数，只能在代码中配置(通过setBolt和setSpout的参数), 例如, setBolt(“green-bolt”, new GreenBolt(), 2) tasks的数目, 可以不配置, 默认和executor1:1, 也可以通过setNumTasks()配置 Topology的worker数通过config设置，即执行该topology的worker（java）进程数。它可以通过 storm rebalance 命令任意调整。 123456Config conf = newConfig();conf.setNumWorkers(2); //用2个workertopologyBuilder.setSpout("blue-spout", newBlueSpout(), 2); //设置2个并发度topologyBuilder.setBolt("green-bolt", newGreenBolt(), 2).setNumTasks(4).shuffleGrouping("blue-spout"); //设置2个并发度，4个任务topologyBuilder.setBolt("yellow-bolt", newYellowBolt(), 6).shuffleGrouping("green-bolt"); //设置6个并发度StormSubmitter.submitTopology("mytopology", conf, topologyBuilder.createTopology()); 动态改变任务的并行度Storm支持在不 restart topology 的情况下, 动态的改变(增减) worker processes 的数目和 executors 的数目, 称为rebalancing. 通过Storm web UI，或者通过storm rebalance命令实现： 1storm rebalance mytopology -n 5 -e a-spout=3 -e b-bolt=10 Storm 的技术分析Storm的通讯技术通讯机制Worker间的通信经常需要通过网络跨节点进行，Storm使用ZeroMQ或Netty(0.9以后默认使用)作为进程间通信的消息框架。 Worker进程内部通信：不同worker的thread通信使用LMAX Disruptor来完成。 不同topologey之间的通信，Storm不负责，需要自己想办法实现，例如使用kafka等； Worker 进程间通讯机制worker进程间消息传递机制，消息的接收和处理的大概流程见下图 对于worker进程来说，为了管理流入和传出的消息，每个worker进程有一个独立的接收线程(对配置的TCP端口supervisor.slots.ports进行监听); 对应Worker接收线程，每个worker存在一个独立的发送线程，它负责从worker的transfer-queue中读取消息，并通过网络发送给其他worker 每个executor有自己的incoming-queue和outgoing-queue。 Worker接收线程将收到的消息通过task编号传递给对应的executor(一个或多个)的incoming-queues; 每个executor有单独的线程分别来处理spout/bolt的业务逻辑，业务逻辑输出的中间数据会存放在outgoing-queue中，当executor的outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到transfer-queue中。 每个worker进程控制一个或多个executor线程，用户可在代码中进行配置。其实就是我们在代码中设置的并发度个数。 Worker 进程间通讯机制分析 Worker接受线程通过网络接受数据，并根据Tuple中包含的taskId，匹配到对应的executor；然后根据executor找到对应的incoming-queue，将数据存发送到incoming-queue队列中。 业务逻辑执行现成消费incoming-queue的数据，通过调用Bolt的execute(xxxx)方法，将Tuple作为参数传输给用户自定义的方法 业务逻辑执行完毕之后，将计算的中间数据发送给outgoing-queue队列，当outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到Worker的transfer-queue中 Worker发送线程消费transfer-queue中数据，计算Tuple的目的地，连接不同的node+port将数据通过网络传输的方式传送给另一个的Worker。 另一个worker执行以上步骤1的操作。 Disruptor 队列disruptor是什么 1、 简单理解：Disruptor是一个Queue。Disruptor是实现了“队列”的功能，而且是一个有界队列。而队列的应用场景自然就是“生产者-消费者”模型。 2、 在JDK中Queue有很多实现类，包括不限于ArrayBlockingQueue、LinkBlockingQueue，这两个底层的数据结构分别是数组和链表。数组查询快，链表增删快，能够适应大多数应用场景。 3、 但是ArrayBlockingQueue、LinkBlockingQueue都是线程安全的。涉及到线程安全，就会有synchronized、lock等关键字，这就意味着CPU会打架。 4、 Disruptor一种线程之间信息无锁的交换方式（使用CAS（Compare And Swap/Set）操作）。 disruptor主要特点 Disruptor可以看成一个事件监听或消息机制，在队列中一边生产者放入消息，另外一边消费者并行取出处理. 底层是单个数据结构：一个ring buffer。 每个生产者和消费者都有一个次序计算器，以显示当前缓冲工作方式。 每个生产者消费者能够操作自己的次序计数器的能够读取对方的计数器，生产者能够读取消费者的计算器确保其在没有锁的情况下是可写的。 核心组件 Ring Buffer 环形的缓冲区，负责对通过 Disruptor 进行交换的数据（事件）进行存储和更新。 Sequence 通过顺序递增的序号来编号管理通过其进行交换的数据（事件），对数据(事件)的处理过程总是沿着序号逐个递增处理。 RingBuffer底层是个数组，次序计算器是一个64bit long 整数型，平滑增长。 ​ 1. 接受数据并写入到脚标31的位置，之后会沿着序号一直写入，但是不会绕过消费者所在的脚标。 ​ 2. Joumaler和replicator同时读到24的位置，他们可以批量读取数据到30 ​ 3. 消费逻辑线程读到了14的位置，但是没法继续读下去，因为他的sequence暂停在15的位置上，需要等到他的sequence给他序号。如果sequence能正常工作，就能读取到30的数据。 Worker间的网络通讯Netty Netty是一个NIO client-server(客户端服务器)框架，使用Netty可以快速开发网络应用，例如服务器和客户端协议。Netty提供了一种新的方式来使开发网络应用程序，这种新的方式使得它很容易使用和有很强的扩展性。Netty的内部实现时很复杂的，但是Netty提供了简单易用的api从网络处理代码中解耦业务逻辑。Netty是完全基于NIO实现的，所以整个Netty都是异步的。 ZeroMQ ZeroMQ是一种基于消息队列的多线程网络库，其对套接字类型、连接处理、帧、甚至路由的底层细节进行抽象，提供跨越多种传输协议的套接字。ZeroMQ是网络通信中新的一层，介于应用层和传输层之间（按照TCP/IP划分），其是一个可伸缩层，可并行运行，分散在分布式系统间。 ZeroMQ定位为：一个简单好用的传输层，像框架一样的一个socket library，他使得Socket编程更加简单、简洁和性能更高。是一个消息处理队列库，可在多个线程、内核和主机盒之间弹性伸缩。ZMQ的明确目标是“成为标准网络协议栈的一部分，之后进入Linux内核”。 Storm 组件本地目录结构 Storm zookeeper 目录树 Storm 任务提交过程过程示意图 nimbus 创建的Topo对象示意图 详细任务提交过程 Storm 的容错机制总体介绍 在storm中，可靠的信息处理机制是从spout开始的。 一个提供了可靠的处理机制的spout需要记录他发射出去的tuple，当下游bolt处理tuple或者子tuple失败时spout能够重新发射。 Storm通过调用Spout的nextTuple()发送一个tuple。为实现可靠的消息处理，首先要给每个发出的tuple带上唯一的ID，并且将ID作为参数传递给SoputOutputCollector的emit()方法：collector.emit(new Values(“value1”,”value2”), msgId); messageid就是用来标示唯一的tuple的，而rootid是随机生成的 给每个tuple指定ID告诉Storm系统，无论处理成功还是失败，spout都要接收tuple树上所有节点返回的通知。如果处理成功，spout的ack()方法将会对编号是msgId的消息应答确认；如果处理失败或者超时，会调用fail()方法。 基本实现Storm 系统中有一组叫做”acker”的特殊的任务，它们负责跟踪DAG（有向无环图）中的每个消息。 acker任务保存了spout id到一对值的映射。第一个值就是spout的任务id，通过这个id，acker就知道消息处理完成时该通知哪个spout任务。第二个值是一个64bit的数字，我们称之为”ack val”， 它是树中所有消息的随机id的异或计算结果。 ack val表示了整棵树的的状态，无论这棵树多大，只需要这个固定大小的数字就可以跟踪整棵树。当消息被创建和被应答的时候都会有相同的消息id发送过来做异或。 每当acker发现一棵树的ack val值为0的时候，它就知道这棵树已经被完全处理了 可靠性配置有三种方法可以去掉消息的可靠性： 将参数Config.TOPOLOGY_ACKERS设置为0，通过此方法，当Spout发送一个消息的时候，它的ack方法将不会被调用; Spout发送一个消息时，不指定此消息的messageID。当需要关闭特定消息可靠性的时候，可以使用此方法； 最后，如果你不在意某个消息派生出来的子孙消息的可靠性，则此消息派生出来的子消息在发送时不要做锚定，即在emit方法中不指定输入消息。因为这些子孙消息没有被锚定在任何tuple tree中，因此他们的失败不会引起任何spout重新发送消息。]]></content>
      <categories>
        <category>大数据</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase]]></title>
    <url>%2F2020%2F02%2F23%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7%2Fhbase%2F</url>
    <content type="text"><![CDATA[hbase简介hbase介绍HBASE是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBASE技术可在廉价PC Server上搭建起大规模结构化存储集群。 HBASE的目标是存储并处理大型的数据，更具体来说是仅需使用普通的硬件配置，就能够处理由成千上万的行和列所组成的大型数据。 HBASE是Google Bigtable的开源实现，但是也有很多不同之处。比如：Google Bigtable利用GFS作为其文件存储系统，HBASE利用Hadoop HDFS作为其文件存储系统；Google运行MAPREDUCE来处理Bigtable中的海量数据，HBASE同样利用Hadoop MapReduce来处理HBASE中的海量数据；Google Bigtable利用Chubby作为协同服务，HBASE利用Zookeeper作为对应。 与传统数据库对比传统数据库的问题 1）数据量很大的时候无法存储 2）没有很好的备份机制 3）数据达到一定数量开始缓慢，很大的话基本无法支撑 hbase的优势 1）线性扩展，随着数据量增多可以通过节点扩展进行支撑 2）数据存储在hdfs上，备份机制健全 3）通过zookeeper协调查找数据，访问速度快。 hbase 集群中的角色 一个或者多个主节点，Hmaster 管理用户对Table表的增、删、改、查操作； 管理HRegion服务器的负载均衡，调整HRegion分布； 在HRegion分裂后，负责新HRegion的分配； 在HRegion服务器停机后，负责失效HRegion服务器上的HRegion迁移。 多个从节点，HregionServer 表的增删改查数据。 和hdfs交互，存取数据。 zookeeper在hbase 中作用 保存Hmaster的地址和backup-master地址 保存表-ROOT-的地址- hbase默认的根表，检索表。 HRegionServer列表 hbase的安装 上传hbase安装包到服务器集群 hbase-x.xx.x.tar.gz 解压缩安装包 tar -zxvf hbase-0.94.6.tar.gz 重命名解压后目录 mv hbase-0.94.6.hbase 修改环境变量 1234567su - rootvi /etc/profile# 添加内容# export HBASE_HOME=/home/hadoop/hbase# export PATH=$PATH:$HBASE_HOME/binsource /etc/profilesu - hadoop 修改配置文件 进入 配置文件目录 1cd $HABASE_HOME/conf 修改配置文件 hbase-env.sh (配置hbase环境变量) 1234export JAVA_HOME=/usr/jdk //jdk安装目录export JAVA_CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport HBASE_CLASSPATH=/home/hadoop/hadoop/conf //hadoop配置文件的位置export HBASE_MANAGES_ZK=true #如果使用独立安装的zookeeper这个地方就是false 修改配置文件 hbase-site.xml (配置hbase集群) 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; #hbasemaster的主机和端口 &lt;value&gt;master1:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; #时间同步允许的时间差 &lt;value&gt;180000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs:// hadoop-cluster1/hbase&lt;/value&gt;#hbase共享目录，持久化hbase数据 &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; #是否分布式运行，false即为单机 &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;#zookeeper地址 &lt;value&gt;slave1, slave2,slave3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;#zookeeper配置信息快照的位置 &lt;value&gt;/home/hadoop/hbase/tmp/zookeeper&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改配置文件 regionservers 集群从节点配置 123slave1slave2slave3 把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf下 12cp /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml /home/hadoop/hbase/confcp /home/hadoop/hadoop/etc/hadoop/core-site.xml /home/hadoop/hbase/conf 将hbase分发到各个主机 123scp -r /home/hadoop/hbase hadoop@slave1:/home/hadoop/scp -r /home/hadoop/hbase hadoop@slave2:/home/hadoop/scp -r /home/hadoop/hbase hadoop@slave3:/home/hadoop/ 启动hbase 1start-hbase.sh 备注： 在master节点执行 查看状态 查看hbase进行: jps 进入hbase shell : hbase shell 退出hbase shell: quit 查看hbase web: http://master:60010/ 启动Hmaster 主备 在任意安装了hbase机器上启动hmaster 1local-master-backup.sh start 1 添加hbase节点 复制hbase安装目录到新节点上, 执行启动命令 1hbase-daemon.sh start regionserver hbase 数据模型row key与nosql数据库们一样,row key是用来检索记录的主键。访问HBASE table中的行，只有三种方式： 1.通过单个row key访问 2.通过row key的range（正则） 3.全表扫描 Row key行键 (Row key)可以是任意字符串(最大长度 是 64KB，实际应用中长度一般为 10-100bytes)，在HBASE内部，row key保存为字节数组。存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性) 列簇(Columns Family)列簇 ：HBASE表中的每个列，都归属于某个列族。列族是表的schema的一部 分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如 courses:history，courses:math都属于courses 这个列族。 Cell由{row key, columnFamily, version} 唯一确定的单元。cell中 的数据是没有类型的，全部是字节码形式存贮。 关键字：无类型、字节码 时间戳(Time Stamp)HBASE 中通过rowkey和columns确定的为一个存贮单元称为cell。每个 cell都保存 着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由HBASE(在数据写入时自动 )赋值，此时时间戳是精确到毫秒 的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版 本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。 为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供 了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段 时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。 数据模型示意图 hbase 命令 名称 命令表达式 创建表 create ‘表名’, ‘列族名1’,’列族名2’,’列族名N’ 查看所有表 list 描述表 describe ‘表名’ 判断表存在 exists ‘表名’ 判断是否禁用启用表 is_enabled ‘表名’ ; is_disabled ‘表名’ 添加记录 put ‘表名’, ‘rowKey’, ‘列族 : 列‘ , ‘值’ 查看记录rowkey下的所有数据 get ‘表名’ , ‘rowKey’ 查看表中的记录总数 count ‘表名’ 获取某个列族 get ‘表名’,’rowkey’,’列族’ 获取某个列族的某个列 get ‘表名’,’rowkey’,’列族：列’ 删除记录 delete ‘表名’ ,‘行名’ , ‘列族：列’ 删除整行 deleteall ‘表名’,’rowkey’ 删除一张表 先要屏蔽该表，才能对该表进行删除第一步 disable ‘表名’ ，第二步 drop ‘表名’ 清空表 truncate ‘表名‘ 查看所有记录 scan “表名” 查看某个表某个列中所有数据 scan 表名” , {COLUMNS=&gt;’列族名:列名’} 更新记录 就是重写一遍，进行覆盖，hbase没有修改，都是追加 hbase javaApi 操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351import java.util.ArrayList;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.MasterNotRunningException;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.ZooKeeperConnectionException;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Delete;import org.apache.hadoop.hbase.client.Get;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HConnection;import org.apache.hadoop.hbase.client.HConnectionManager;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.client.ResultScanner;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.filter.ColumnPrefixFilter;import org.apache.hadoop.hbase.filter.CompareFilter;import org.apache.hadoop.hbase.filter.FilterList;import org.apache.hadoop.hbase.filter.FilterList.Operator;import org.apache.hadoop.hbase.filter.RegexStringComparator;import org.apache.hadoop.hbase.filter.RowFilter;import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;import org.apache.hadoop.hbase.util.Bytes;import org.junit.After;import org.junit.Before;import org.junit.Test;public class HbaseTest &#123; /** * 配置ss */ static Configuration config = null; private Connection connection = null; private Table table = null; @Before public void init() throws Exception &#123; config = HBaseConfiguration.create();// 配置 config.set("hbase.zookeeper.quorum", "master,work1,work2");// zookeeper地址 config.set("hbase.zookeeper.property.clientPort", "2181");// zookeeper端口 connection = ConnectionFactory.createConnection(config); table = connection.getTable(TableName.valueOf("user")); &#125; /** * 创建一个表 * * @throws Exception */ @Test public void createTable() throws Exception &#123; // 创建表管理类 HBaseAdmin admin = new HBaseAdmin(config); // hbase表管理 // 创建表描述类 TableName tableName = TableName.valueOf("test3"); // 表名称 HTableDescriptor desc = new HTableDescriptor(tableName); // 创建列族的描述类 HColumnDescriptor family = new HColumnDescriptor("info"); // 列族 // 将列族添加到表中 desc.addFamily(family); HColumnDescriptor family2 = new HColumnDescriptor("info2"); // 列族 // 将列族添加到表中 desc.addFamily(family2); // 创建表 admin.createTable(desc); // 创建表 &#125; @Test @SuppressWarnings("deprecation") public void deleteTable() throws MasterNotRunningException, ZooKeeperConnectionException, Exception &#123; HBaseAdmin admin = new HBaseAdmin(config); admin.disableTable("test3"); admin.deleteTable("test3"); admin.close(); &#125; /** * 向hbase中增加数据 * * @throws Exception */ @SuppressWarnings(&#123; "deprecation", "resource" &#125;) @Test public void insertData() throws Exception &#123; table.setAutoFlushTo(false); table.setWriteBufferSize(534534534); ArrayList&lt;Put&gt; arrayList = new ArrayList&lt;Put&gt;(); for (int i = 21; i &lt; 50; i++) &#123; Put put = new Put(Bytes.toBytes("1234"+i)); put.add(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes("wangwu"+i)); put.add(Bytes.toBytes("info"), Bytes.toBytes("password"), Bytes.toBytes(1234+i)); arrayList.add(put); &#125; //插入数据 table.put(arrayList); //提交 table.flushCommits(); &#125; /** * 修改数据 * * @throws Exception */ @Test public void uodateData() throws Exception &#123; Put put = new Put(Bytes.toBytes("1234")); put.add(Bytes.toBytes("info"), Bytes.toBytes("namessss"), Bytes.toBytes("lisi1234")); put.add(Bytes.toBytes("info"), Bytes.toBytes("password"), Bytes.toBytes(1234)); //插入数据 table.put(put); //提交 table.flushCommits(); &#125; /** * 删除数据 * * @throws Exception */ @Test public void deleteDate() throws Exception &#123; Delete delete = new Delete(Bytes.toBytes("1234")); table.delete(delete); table.flushCommits(); &#125; /** * 单条查询 * * @throws Exception */ @Test public void queryData() throws Exception &#123; Get get = new Get(Bytes.toBytes("1234")); Result result = table.get(get); System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password")))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("namessss")))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex")))); &#125; /** * 全表扫描 * * @throws Exception */ @Test public void scanData() throws Exception &#123; Scan scan = new Scan(); //scan.addFamily(Bytes.toBytes("info")); //scan.addColumn(Bytes.toBytes("info"), Bytes.toBytes("password")); scan.setStartRow(Bytes.toBytes("wangsf_0")); scan.setStopRow(Bytes.toBytes("wangwu")); ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password")))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name")))); //System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("password")))); //System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name")))); &#125; &#125; /** * 全表扫描的过滤器 * 列值过滤器 * * @throws Exception */ @Test public void scanDataByFilter1() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //过滤器：列值过滤器 SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes("info"), Bytes.toBytes("name"), CompareFilter.CompareOp.EQUAL, Bytes.toBytes("zhangsan2")); // 设置过滤器 scan.setFilter(filter); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password")))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name")))); //System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("password")))); //System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name")))); &#125; &#125; /** * rowkey过滤器 * @throws Exception */ @Test public void scanDataByFilter2() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //匹配rowkey以wangsenfeng开头的 RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator("^12341")); // 设置过滤器 scan.setFilter(filter); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("password")))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name")))); //System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("password")))); //System.out.println(Bytes.toString(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name")))); &#125; &#125; /** * 匹配列名前缀 * @throws Exception */ @Test public void scanDataByFilter3() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //匹配rowkey以wangsenfeng开头的 ColumnPrefixFilter filter = new ColumnPrefixFilter(Bytes.toBytes("na")); // 设置过滤器 scan.setFilter(filter); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println("rowkey：" + Bytes.toString(result.getRow())); System.out.println("info:name：" + Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name")))); // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("age")) != null) &#123; System.out.println("info:age：" + Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("age")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex")) != null) &#123; System.out.println("infi:sex：" + Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name")) != null) &#123; System.out .println("info2:name：" + Bytes.toString(result.getValue( Bytes.toBytes("info2"), Bytes.toBytes("name")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("age")) != null) &#123; System.out.println("info2:age：" + Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("age")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("sex")) != null) &#123; System.out.println("info2:sex：" + Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("sex")))); &#125; &#125; &#125; /** * 过滤器集合 * @throws Exception */ @Test public void scanDataByFilter4() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //过滤器集合：MUST_PASS_ALL（and）,MUST_PASS_ONE(or) FilterList filterList = new FilterList(Operator.MUST_PASS_ONE); //匹配rowkey以wangsenfeng开头的 RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator("^wangsenfeng")); //匹配name的值等于wangsenfeng SingleColumnValueFilter filter2 = new SingleColumnValueFilter(Bytes.toBytes("info"), Bytes.toBytes("name"), CompareFilter.CompareOp.EQUAL, Bytes.toBytes("zhangsan")); filterList.addFilter(filter); filterList.addFilter(filter2); // 设置过滤器 scan.setFilter(filterList); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println("rowkey：" + Bytes.toString(result.getRow())); System.out.println("info:name：" + Bytes.toString(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name")))); // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("age")) != null) &#123; System.out.println("info:age：" + Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("age")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex")) != null) &#123; System.out.println("infi:sex：" + Bytes.toInt(result.getValue(Bytes.toBytes("info"), Bytes.toBytes("sex")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("name")) != null) &#123; System.out .println("info2:name：" + Bytes.toString(result.getValue( Bytes.toBytes("info2"), Bytes.toBytes("name")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("age")) != null) &#123; System.out.println("info2:age：" + Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("age")))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("sex")) != null) &#123; System.out.println("info2:sex：" + Bytes.toInt(result.getValue(Bytes.toBytes("info2"), Bytes.toBytes("sex")))); &#125; &#125; &#125; @After public void close() throws Exception &#123; table.close(); connection.close(); &#125;&#125; MapReduce操作Hbase介绍hbase 对Mapreduce提供了支持， 它实现了TableMapper类和TableReducer类，我们只需要继承这两个类即可。 TableMapper: 实现了从hbase读取数据，并可以实现按行处理的功能 TableReducer: 接受TableMapper的处理结果，并可以将数据输出到hbase新的表上 实现方法 写个mapper继承TableMapper&lt;Text, IntWritable&gt; ​ 参数：Text：mapper的输出key类型； IntWritable：mapper的输出value类型。 ​ 其中的map方法如下： ​ map(ImmutableBytesWritable key, Result value,Context context) ​ 参数：key：rowKey；value： Result ，一行数据； context上下文 写个reduce继承TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt; ​ 参数：Text:reducer的输入key； IntWritable：reduce的输入value； ​ ImmutableBytesWritable：reduce输出到hbase中的rowKey类型。 ​ 其中的reduce方法如下： ​ reduce(Text key, Iterable values,Context context) ​ 参数： key：reduce的输入key；values：reduce的输入value； 实现示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;/** * mapreduce操作hbase * @author wilson * */public class HBaseMr &#123; /** * 创建hbase配置 */ static Configuration config = null; static &#123; config = HBaseConfiguration.create(); config.set("hbase.zookeeper.quorum", "slave1,slave2,slave3"); config.set("hbase.zookeeper.property.clientPort", "2181"); &#125; /** * 表信息 */ public static final String tableName = "word";//表名1 public static final String colf = "content";//列族 public static final String col = "info";//列 public static final String tableName2 = "stat";//表名2 /** * 初始化表结构，及其数据 */ public static void initTB() &#123; HTable table=null; HBaseAdmin admin=null; try &#123; admin = new HBaseAdmin(config);//创建表管理 /*删除表*/ if (admin.tableExists(tableName)||admin.tableExists(tableName2)) &#123; System.out.println("table is already exists!"); admin.disableTable(tableName); admin.deleteTable(tableName); admin.disableTable(tableName2); admin.deleteTable(tableName2); &#125; /*创建表*/ HTableDescriptor desc = new HTableDescriptor(tableName); HColumnDescriptor family = new HColumnDescriptor(colf); desc.addFamily(family); admin.createTable(desc); HTableDescriptor desc2 = new HTableDescriptor(tableName2); HColumnDescriptor family2 = new HColumnDescriptor(colf); desc2.addFamily(family2); admin.createTable(desc2); /*插入数据*/ table = new HTable(config,tableName); table.setAutoFlush(false); table.setWriteBufferSize(500); List&lt;Put&gt; lp = new ArrayList&lt;Put&gt;(); Put p1 = new Put(Bytes.toBytes("1")); p1.add(colf.getBytes(), col.getBytes(), ("The Apache Hadoop software library is a framework").getBytes()); lp.add(p1); Put p2 = new Put(Bytes.toBytes("2"));p2.add(colf.getBytes(),col.getBytes(),("The common utilities that support the other Hadoop modules").getBytes()); lp.add(p2); Put p3 = new Put(Bytes.toBytes("3")); p3.add(colf.getBytes(), col.getBytes(),("Hadoop by reading the documentation").getBytes()); lp.add(p3); Put p4 = new Put(Bytes.toBytes("4")); p4.add(colf.getBytes(), col.getBytes(),("Hadoop from the release page").getBytes()); lp.add(p4); Put p5 = new Put(Bytes.toBytes("5")); p5.add(colf.getBytes(), col.getBytes(),("Hadoop on the mailing list").getBytes()); lp.add(p5); table.put(lp); table.flushCommits(); lp.clear(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if(table!=null)&#123; table.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * MyMapper 继承 TableMapper * TableMapper&lt;Text,IntWritable&gt; * Text:输出的key类型， * IntWritable：输出的value类型 */ public static class MyMapper extends TableMapper&lt;Text, IntWritable&gt; &#123; private static IntWritable one = new IntWritable(1); private static Text word = new Text(); @Override //输入的类型为：key：rowKey； value：一行数据的结果集Result protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; //获取一行数据中的colf：col String words = Bytes.toString(value.getValue(Bytes.toBytes(colf), Bytes.toBytes(col)));// 表里面只有一个列族，所以我就直接获取每一行的值 //按空格分割 String itr[] = words.toString().split(" "); //循环输出word和1 for (int i = 0; i &lt; itr.length; i++) &#123; word.set(itr[i]); context.write(word, one); &#125; &#125; &#125; /** * MyReducer 继承 TableReducer * TableReducer&lt;Text,IntWritable&gt; * Text:输入的key类型， * IntWritable：输入的value类型， * ImmutableBytesWritable：输出类型，表示rowkey的类型 */ public static class MyReducer extends TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //对mapper的数据求和 int sum = 0; for (IntWritable val : values) &#123;//叠加 sum += val.get(); &#125; // 创建put，设置rowkey为单词 Put put = new Put(Bytes.toBytes(key.toString())); // 封装数据 put.add(Bytes.toBytes(colf), Bytes.toBytes(col),Bytes.toBytes(String.valueOf(sum))); //写到hbase,需要指定rowkey、put context.write(new ImmutableBytesWritable(Bytes.toBytes(key.toString())),put); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; config.set("df.default.name", "hdfs://master:9000/");//设置hdfs的默认路径 config.set("hadoop.job.ugi", "hadoop,hadoop");//用户名，组 config.set("mapred.job.tracker", "master:9001");//设置jobtracker在哪 //初始化表 initTB();//初始化表 //创建job Job job = new Job(config, "HBaseMr");//job job.setJarByClass(HBaseMr.class);//主类 //创建scan Scan scan = new Scan(); //可以指定查询某一列 scan.addColumn(Bytes.toBytes(colf), Bytes.toBytes(col)); //创建查询hbase的mapper，设置表名、scan、mapper类、mapper的输出key、mapper的输出value TableMapReduceUtil.initTableMapperJob(tableName, scan, MyMapper.class,Text.class, IntWritable.class, job); //创建写入hbase的reducer，指定表名、reducer类、job TableMapReduceUtil.initTableReducerJob(tableName2, MyReducer.class, job); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; hbase 原理示意图 存储原理 HBase中的每张表都通过行键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，由HRegionServer管理，管理哪些HRegion由HMaster分配。 HRegionServer存取一个子表时，会创建一个HRegion对象，表的Region信息存在.meta表中，该表信息可通过Zookeeper进行追踪。然后对表的每个列族(Column Family)创建一个Store实例，每个Store都会有0个或多个StoreFile与之对应，每个StoreFile都会对应一个HFile， HFile就是实际的存储文件。因此，一个HRegion有多少个列族就有多少个Store。另外，每个HRegion还拥有一个MemStore实例。memStore存储在内存中，StoreFile存储在HDFS上。 Region虽然是分布式存储的最小单元，但并不是存储的最小单元。Region由一个或者多个Store组成，每个store保存一个columns family； 本质上MemStore就是一个内存里放着一个保存KEY/VALUE的MAP，当MemStore（默认64MB）写满之后，会开始刷磁盘操作。 在进行表增加时，会先写memStore，当memStore文件大小达到一定大小时，会flush到StoreFile中。当Region中的StoreFile文件过多时，会进行Compact操作，将StoreFile合并。 写流程ZooKeeper—meta–regionserver–Hlog|MemStore–storefile 1、 通过zookeeper的-ROOT- 找到表 .META. 对应的hregionserver。 2、通过META表rowkey，表名等信息找到数据对应的regine。 3、通过zookerpeer 的信息找到对应的regioneserver。 4、 hregionserver将数据写到hlog（write ahead log）。为了数据的持久化和恢复。 5、 hregionserver将数据写到内存（memstore） 6、 反馈client写成功。 数据flush过程1、 当memstore数据达到阈值（默认是64M），将内存中的数据删除。 2、 将数据刷到硬盘(storefile文件)。 3、将内存中数据删除 4、删除Hlog中的历史数据，并在hlog中做标记点。 数据合并过程1、 当storefile数据块达到4块，hmaster将数据块加载到本地，进行合并 2、 当合并的数据超过256M，进行拆分，将拆分后的region分配给不同的hregionserver管理 3、 当hregionser宕机后，将hregionserver上的hlog拆分，然后分配给不同的hregionserver加载，修改.META. 4、 注意：hlog会同步到hdfs做持久化 hbase 读流程ZooKeeper—meta–regionserver–region–memstore–storefile 1、 通过zookeeper的-ROOT- 找到表 .META. 对应的hregionserver。 2、通过META表rowkey，表名等信息找到数据对应的regine。 3、通过zookerpeer 的信息找到对应的regioneserver。 4、连接到对应regineserver上的regine。 5、先从Memstore找数据，如果没有，再到StoreFile上读 6、 数据块会缓存 hmaster的职责1、管理用户对Table的增、删、改、查操作 (维护表对应的meta表地址，regine 和 regineserver之间的关系等)； 2、记录region在哪台Hregion server上 3、在Region Split后，负责新Region的分配； 4、新机器加入时，管理HRegion Server的负载均衡，调整Region分布 5、在HRegion Server宕机后，负责失效HRegion Server 上的Regions迁移。 hregionserver 的职责HRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBASE中最核心的模块。 HRegion Server管理了很多table的分区，也就是region。 client 职责Client HBASE Client使用HBASE的RPC机制与HMaster和RegionServer进行通信 管理类操作：Client与HMaster进行RPC； 数据读写类操作：Client与HRegionServer进行RPC。]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据工具</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop]]></title>
    <url>%2F2020%2F02%2F14%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7%2Fsqoop%2F</url>
    <content type="text"><![CDATA[Sqoop 的介绍Sqoop概述sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。 Apache Sqoop是用来实现结构型数据（如关系数据库）和Hadoop之间进行数据迁移的工具。它充分利用了MapReduce的并行特点以批处理的方式加快数据的传输，同时也借助MapReduce实现了容错。 导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统； 导出数据：从Hadoop的文件系统中导出数据到关系数据库 Sqoop 支持的数据库 Database version –direct support? connect string matches HSQLDB 1.8.0+ No jdbc:hsqldb:*// MySQL 5.0+ Yes jdbc:mysql:// Oracle 10.2.0+ No jdbc:oracle:*// PostgreSQL 8.3+ Yes (import only) jdbc:postgresql:/ 工作机制将导入或导出命令翻译成mapreduce程序来实现 在翻译出的mapreduce中主要是对inputformat和outputformat进行定制 Sqoop安装 安装sqoop的前提是已经具备java和hadoop的环境 下载并解压: 下载地址http://ftp.wayne.edu/apache/sqoop/1.4.6/ 修改配置文件 1234567cd $SQOOP_HOME/confmv sqoop-env-template.sh sqoop-env.sh# 打开sqoop-env.sh并编辑下面几行：export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.1/ export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.1/export HIVE_HOME=/home/hadoop/apps/hive-1.2.1 加如mysql的jdbc驱动包 1cp ~/app/hive/lib/mysql-connector-java-5.1.28.jar $SQOOP_HOME/lib/ 验证启动 12cd $SQOOP_HOME/binsqoop-version 预期的输出: 1234515/12/17 14:52:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6Sqoop 1.4.6 git commit id 5b34accaca7de251fc91161733f906af2eddbe83Compiled by abe on Fri Aug 1 11:19:26 PDT 2015 Sqoop命令介绍命令详情1sqoop help 123456789101112131415usage: sqoop COMMAND [ARGS]Available commands:codegen Generate code to interact with database recordscreate-hive-table Import a table definition into Hiveeval Evaluate a SQL statement and display the resultsexport Export an HDFS directory to a database tablehelp List available commandsimport Import a table from a database to HDFSimport-all-tables Import tables from a database to HDFSjob Work with saved jobslist-databases List available databases on a serverlist-tables List available tables in a databasemerge Merge results of incremental importsmetastore Run a standalone Sqoop metastoreversion Display version information 数据导入关系型数据到导入到HDFS命令格式 1sqoop import (generic-args) (import-args) 命令示例 1sqoop import --connect jdbc:mysql://192.168.81.176/hivemeta2db --username root -password passwd --table sds 默认目录是/user/${user.name}/${tablename}，可以通过–target-dir设置hdfs上的目标目录。 可以通过–m设置并行数据，即map的数据，决定文件的个数。 如果想要将整个数据库中的表全部导入到hdfs上，可以使用import-all-tables命令 如果想要指定所需的列, 使用–columns 参数, 多个字段有,隔开 如果想导出为SequenceFiles, 使用–as-sequencefile参数，为类文件命名使用–class-name参数 导出文本可以指定分隔符, –fields-terminated-by,–lines-terminated-by, –optionally-enclosed-by 指定过滤条件, 示例： –where “sd_id &gt; 100” HDFS导出到关系型数据库命令格式 1sqoop export (generic-args) (export-args) 命令示例 1sqoop export --connect jdbc:mysql://192.168.81.176/sqoop --username root -password passwd --table sds --export-dir /user/guojian/sds sqoop事务 上例中sqoop数据中的sds表需要先把表结构创建出来，否则export操作会直接失败。 由于sqoop是通过map完成数据的导入，各个map过程是独立的，没有事物的概念，可能会有部分map数据导入失败的情况。为了解决这一问题，sqoop中有一个折中的办法，即是指定中间 staging 表，成功后再由中间表导入到结果表。 这一功能是通过 –staging-table staging-table-name 指定，同时staging表结构也是需要提前创建出来的: 需要说明的是，在使用 –direct ， –update-key 或者–call存储过程的选项时，staging中间表是不可用的。 结果验证: 数据会首先写到sds_tmp表，导入操作成功后，再由sds_tmp表导入到sds结果表中，同时会清除sds_tmp表。 如果有map失败，则成功的map会将数据写入tmp表，export任务失败，同时tmp表的数据会被保留。 如果tmp中已有数据，则此export操作会直接失败，可以使用 –clear-staging-table 指定在执行前清除中间表。 export 选项 参数 说明 –direct 直接使用 mysqlimport 工具导入mysql –export-dir 需要export的hdfs数据路径 -m,–num-mappers 并行export的map个数n –table 导出到的目标表 –call 调用存储过程 –update-key 指定需要更新的列名，可以将数据库中已经存在的数据进行更新 –update-mode 更新模式，包括 updateonly (默认）和allowinsert 前者只允许更新，后者允许新的列数据写入 –input-null-string The string to be interpreted as null for string columns –input-null-non-string The string to be interpreted as null for non-string columns –staging-table 指定中间staging表 –clear-staging-table 执行export前将中间staging表数据清除 –batch Use batch mode for underlying statement execution. 关系型数据库导入到HIVE中命令格式 1sqoop import (generic-args) (create-args) 要指定–hive-import 参数 命令示例 1bin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import --m 1 在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。用–target-dir 参数，来创建外部表 其他命令列出数据库中所有库命令示例 1sqoop list-databases --connect jdbc:mysql://192.168.81.176/ --username root -password passwd 列出数据库中所有表命令示例 1sqoop list-tables --connect jdbc:mysql://192.168.81.176/sqoop --username root -password passwd sqoop jobjob 相关参数 参数 说明 –create 创业一个新的sqoop作业. –delete 删除一个sqoop job –exec 执行一个 –create 保存的作业 –show 显示一个作业的参数 –list 显示所有创建的sqoop作业 创建job 1sqoop job --create myimportjob -- import --connectjdbc:mysql://192.168.81.176/hivemeta2db --username root -password passwd --table TBLS 查看job 1sqoop job --list 检查job 1sqoop job --show myjob 执行job 1sqoop job --exec myjob]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据工具</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[azkaban]]></title>
    <url>%2F2020%2F02%2F14%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7%2Fazkaban%2F</url>
    <content type="text"><![CDATA[azkaban 介绍为甚么使用azkaban工作流调度 一个完整的数据分析系统通常都是由大量任务单元组成： shell脚本程序，java程序，mapreduce程序、hive脚本等 各任务单元之间存在时间先后及前后依赖关系 为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行； 例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示： 通过Hadoop先将原始数据同步到HDFS上； 借助MapReduce计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张Hive表中； 需要对Hive中多个表的数据进行JOIN处理，得到一个明细数据Hive大表； 将明细数据进行复杂的统计分析，得到结果报表信息； 需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。 工作流调度实现方式 简单的任务调度： 直接使用linux的crontab来定义 负载的任务调度: 1. 开发调度平台; 2. 使用贤臣的开源调度系统, 比如ooize、azkaban等； Azkaban简介Azkaban是由Linkedin开源的一歌批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系， 并提供一个易于使用的web用户界面维护和跟踪你的工作流； 其有如下功能特点： Web用户界面 方便上传工作流 方便设置任务之间的关系 调度工作流 认证/授权(权限的工作) 能够杀死并重新启动工作流 模块化和可插拔的插件机制 项目工作区 工作流和任务的日志记录和审计 Azkaban安装将安装文件上传到集群，最好上传到安装了hive、sqoop的机器上，方便命令的执行 在当前用户目录下新建azkabantools目录，用于存放源安装文件，新建azkaban目录，用于存放azkaban运行程序； azkaban web 服务器安装 下载安装包: azkaban-web-server-2.5.0.tar.gz 解压安装包: tar -zxvf azkaban-web-server-2.5.0.tar.gz 将压缩后的 azkaban-web-server-2.5.0 移动到 azkaban目录中, 并重新命名为webserver mv azkaban-web-server-2.5.0 ../azkaban/server azkaban 执行服务器安装 下载安装包: azkaban-executor-server-2.5.0.tar.gz 解压安装包: tar -zxvf azkaban-executor-server-2.5.0.tar.gz 将解压后的azkaban-executor-server-2.5.0 移动到 azkaban目录中,并重新命名 executor mv azkaban-executor-server-2.5.0 ../azkaban/executor azkaban 脚本导入azkaban 执行计划是通过配置在数据库中的任务来进行调度的，所以我们需要初始化数据库的任务相关的表结构 下载脚本: azkaban-sql-script-2.5.0.tar.gz 解压数据库脚本: tar –zxvf azkaban-sql-script-2.5.0.tar.gz 将解压后的mysql脚本，导入到mysql中: 1234mysql&gt; create database azkaban;mysql&gt; use azkaban;Database changedmysql&gt; source /home/hadoop/azkaban-2.5.0/create-all-sql-2.5.0.sql; 创建SSL配置参考地址: http://docs.codehaus.org/display/JETTY/How+to+configure+SSL 命令： keytool -keystore keystore -alias jetty -genkey -keyalg RSA 运行此命令后,会提示输入当前生成 keystor的密码及相应信息,输入的密码请劳记,信息如下: 输入keystore密码： 再次输入新密码: 您的名字与姓氏是什么？ [Unknown]： 您的组织单位名称是什么？ [Unknown]： 您的组织名称是什么？ [Unknown]： 您所在的城市或区域名称是什么？ [Unknown]： 您所在的州或省份名称是什么？ [Unknown]： 该单位的两字母国家代码是什么 [Unknown]： CN CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？ [否]： y 输入&lt;jetty&gt;的主密码 ​ （如果和 keystore 密码相同，按回车）： 再次输入新密码: 完成上述工作后,将在当前目录生成 keystore 证书文件,将keystore 考贝到 azkaban web服务器根目录中.如:cp keystore azkaban/webserver 配置时区先配置好服务器节点上的时区 先生成时区配置文件Asia/Shanghai, 用交互式命令tzselect即可 拷贝改时间文件，覆盖系统本地的时区配置 cp xxx /usr/share/zoneinfo/Aisa/Shanghai/etc/localtime 配置文件 azkaban web服务器配置 进入azkaban web服务器安装目录 conf目录 修改azkaban.properties文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#Azkaban Personalization Settingsazkaban.name=Test #服务器UI名称,用于服务器上方显示的名字azkaban.label=My Local Azkaban #描述azkaban.color=#FF3601 #UI颜色azkaban.default.servlet.path=/index #web.resource.dir=web/ #默认根web目录default.timezone.id=Asia/Shanghai #默认时区,已改为亚洲/上海 默认为美国 #Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManager #用户权限管理默认类user.manager.xml.file=conf/azkaban-users.xml #用户配置,具体配置参加下文 #Loader for projectsexecutor.global.properties=conf/global.properties # global配置文件所在位置azkaban.project.dir=projects # database.type=mysql #数据库类型mysql.port=3306 #端口号mysql.host=hadoop03 #数据库连接IPmysql.database=azkaban #数据库实例名mysql.user=root #数据库用户名mysql.password=root #数据库密码mysql.numconnections=100 #最大连接数 # Velocity dev modevelocity.dev.mode=false# Jetty服务器属性.jetty.maxThreads=25 #最大线程数jetty.ssl.port=8443 #Jetty SSL端口jetty.port=8081 #Jetty端口jetty.keystore=keystore #SSL文件名jetty.password=123456 #SSL文件密码jetty.keypassword=123456 #Jetty主密码 与 keystore文件相同jetty.truststore=keystore #SSL文件名jetty.trustpassword=123456 # SSL文件密码 # 执行服务器属性executor.port=12321 #执行服务器端口 # 邮件设置mail.sender=xxxxxxxx@163.com #发送邮箱mail.host=smtp.163.com #发送邮箱smtp地址mail.user=xxxxxxxx #发送邮件时显示的名称mail.password=********** #邮箱密码job.failure.email=xxxxxxxx@163.com #任务失败时发送邮件的地址job.success.email=xxxxxxxx@163.com #任务成功时发送邮件的地址lockdown.create.projects=false #cache.directory=cache #缓存目录 azkaban 执行服务器配置 进入执行服务器安装目录conf,修改azkaban.properties azkaban.properties 文件 1234567891011121314151617181920212223#Azkabandefault.timezone.id=Asia/Shanghai #时区 # Azkaban JobTypes 插件配置azkaban.jobtype.plugin.dir=plugins/jobtypes #jobtype 插件所在位置 #Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects #数据库设置database.type=mysql #数据库类型(目前只支持mysql)mysql.port=3306 #数据库端口号mysql.host=192.168.20.200 #数据库IP地址mysql.database=azkaban #数据库实例名mysql.user=azkaban #数据库用户名mysql.password=oracle #数据库密码mysql.numconnections=100 #最大连接数 # 执行服务器配置executor.maxThreads=50 #最大线程数executor.port=12321 #端口号(如修改,请与web服务中一致)executor.flow.threads=30 #线程数 用户配置 进入azkaban web服务器conf目录,修改azkaban-users.xml， 增加管理员用户 azkaban-users.xml 文件 1234567&lt;azkaban-users&gt; &lt;user username="azkaban" password="azkaban" roles="admin" groups="azkaban" /&gt; &lt;user username="metrics" password="metrics" roles="metrics"/&gt; &lt;user username="admin" password="admin" roles="admin,metrics" /&gt; &lt;role name="admin" permissions="ADMIN" /&gt; &lt;role name="metrics" permissions="METRICS"/&gt;&lt;/azkaban-users&gt; Azkaban启动 启动web服务器 在azkaban web 根目录运行，服务器目录下执行启动命令 bin/azkaban-web-start.sh 启动执行服务器 在执行服务器目录下执行启动命令, 在执行服务器根目录运行 bin/azkaban-executor-start.sh 浏览器访问web 服务 在浏览器(建议使用谷歌浏览器)中输入https://服务器IP地址:8443 ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login. Azkaban实战Azkaba内置的任务类型支持command、java Command类型–单一job示例 创建job描述文件 vi command.job 123#command.jobtype=command command=echo 'hello' 将job资源文件打包成zip文件 zip command.job 通过azkaban的web管理平台创建project并上传job压缩包 首先创建project 上传zip包 启动执行该job Command 类型–多job工作流flow 创建有依赖关系的多job描述 第一个job: foo.job 123# foo.jobtype=commandcommand=echo foo 第二个job: bar.job 依赖foo.job 1234# bar.jobtype=commanddependencies=foocommand=echo bar 将所有job资源文件打到一个zip包中 在azkaban的web管理界面创建工程，并上传zip包 启动工作流flow HDFS操作人物 创建job描述文件 1234# bar.jobtype=commanddependencies=foocommand=echo bar 将job资源文件打包成zip文件 通过azkaban的web管理平台创建project并上传job压缩包 启动执行该job MAPREDUCE 任务Mr任务依然可以使用command的job类型来执行 创建1、job描述文件，及mr程序jar包（示例中直接使用hadoop自带的example jar） 123# mrwc.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout 将所有job资源文件打到一个zip包中 在azkaban的web管理界面创建工程并上传zip包 启动job HIVE脚本任务 创建hive脚本 test.sql 123456use default;drop table aztest;create table aztest(id int,name string) row format delimited fields terminated by ',';load data inpath '/aztest/hiveinput' into table aztest;create table azres as select * from aztest;insert overwrite directory '/aztest/hiveoutput' select count(1) from aztest; 创建job描述文件: hievf.job 123# hivef.jobtype=commandcommand=/home/hadoop/apps/hive/bin/hive -f 'test.sql' 将所有job资源文件打到一个zip包中 在azkaban的web管理界面创建工程并上传zip包 启动job]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据工具</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume]]></title>
    <url>%2F2020%2F02%2F13%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7%2Fflume%2F</url>
    <content type="text"><![CDATA[Flume介绍概述 Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。 Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中 一般的采集需求，通过对flume的简单配置即可实现 Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景 运行机制 Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成 每一个agent相当于一个数据传递员，内部有三个组件： Source：采集源，用于跟数据源对接，以获取数据 Sink：下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数据 Channel：angent内部的数据传输通道，用于从source将数据传递到sink 一个完整的工作流程：source不断的接收数据，将数据封装成一个一个的event，然后将event发送给channel，chanel作为一个缓冲区会临时存放这些event数据，随后sink会将channel中的event数据发送到指定的地方—-例如HDFS等。 Flume采集系统结构图简单的结构单个agent采集数据 复杂结构多级agent之间串联 Flume安装安装以及启动步骤 下载二进制包 apache-flume-1.6.0-bin.tar.gz 上传到数据源所在节点 解压二进制包 tar -zxvf apache-flume-1.6.0-bin.tar.gz 修改配置文件 进入到flume目录， 修改conf下的flume-env.sh, 在里面配置JAVA_HOME 配置采集方案 根据数据采集的需求配置采集方案，描述在配置文件中(文件名可以任意自定义) 启动flume agent 指定采集方案配置文件, 在相应的节点上启动flume agent 采集方案配置示例 创建采集方案 现在flume的conf目录下新建一个文件， 以netcat-logger.conf 为例(netcat 为一种socket数据源) 123456789101112131415161718192021# 定义这个agent中各组件的名字a1.sources = r1a1.sinks = k1a1.channels = c1# 描述和配置source组件：r1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# 描述和配置sink组件：k1a1.sinks.k1.type = logger# 描述和配置channel组件，此处使用是内存缓存的方式a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# 描述和配置source channel sink之间的连接关系a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动agent去采集数据 1bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1 -Dflume.root.logger=INFO,console 备注: ​ -c conf 指定flume自身的配置文件所在目录 ​ -f conf/netcat-logger.con 指定我们所描述的采集方案 ​ -n a1 指定我们这个agent的名字 测试数据采集效果 先要往agent采集监听的端口上发送数据， 让agent有数据可以采集 1telnet localhost 44444 连接后，发送数据，查看flume的logger界面显示内容 Flume采集案例目录到HDFS采集需求 某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去 根据需求，首先定义以下3大要素： 采集源，即source——监控文件目录 : spooldir 下沉目标，即sink——HDFS文件系统 : hdfs sink source和sink之间的传递通道——channel，可用file channel 也可以用内存channel 采集方案编写 123456789101112131415161718192021222324252627282930313233343536373839#定义三大组件的名称agent1.sources = source1agent1.sinks = sink1agent1.channels = channel1# 配置source组件agent1.sources.source1.type = spooldiragent1.sources.source1.spoolDir = /home/hadoop/logs/agent1.sources.source1.fileHeader = false#配置拦截器agent1.sources.source1.interceptors = i1agent1.sources.source1.interceptors.i1.type = hostagent1.sources.source1.interceptors.i1.hostHeader = hostname# 配置sink组件agent1.sinks.sink1.type = hdfsagent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%Magent1.sinks.sink1.hdfs.filePrefix = access_logagent1.sinks.sink1.hdfs.maxOpenFiles = 5000agent1.sinks.sink1.hdfs.batchSize= 100agent1.sinks.sink1.hdfs.fileType = DataStreamagent1.sinks.sink1.hdfs.writeFormat =Textagent1.sinks.sink1.hdfs.rollSize = 102400agent1.sinks.sink1.hdfs.rollCount = 1000000agent1.sinks.sink1.hdfs.rollInterval = 60#agent1.sinks.sink1.hdfs.round = true#agent1.sinks.sink1.hdfs.roundValue = 10#agent1.sinks.sink1.hdfs.roundUnit = minuteagent1.sinks.sink1.hdfs.useLocalTimeStamp = true# Use a channel which buffers events in memoryagent1.channels.channel1.type = memoryagent1.channels.channel1.keep-alive = 120agent1.channels.channel1.capacity = 500000agent1.channels.channel1.transactionCapacity = 600# Bind the source and sink to the channelagent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1 Channel参数解释： capacity：默认该通道中最大的可以存储的event数量 trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量 keep-alive：event添加到通道中或者移出的允许时间 HDFS SINK参数解释: path：hdfs的路径，需要包含文件系统标识，比如：hdfs://namenode/flume/webdata/ filePrefix：默认值：FlumeData，写入hdfs的文件名前缀 fileSuffix：写入 hdfs 的文件名后缀，比如：.lzo .log等。 inUsePrefix：临时文件的文件名前缀，hdfs sink 会先往目标目录中写临时文件，再根据相关规则重命名成最终目标文件； inUseSuffix：默认值：.tmp，临时文件的文件名后缀。 rollInterval：默认值：30：hdfs sink 间隔多长将临时文件滚动成最终目标文件，单位：秒; 如果设置成0，则表示不根据时间来滚动文件； 注：滚动（roll）指的是，hdfs sink 将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据； rollSize：默认值：1024：当临时文件达到多少（单位：bytes）时，滚动成目标文件；如果设置成0，则表示不根据临时文件大小来滚动文件； rollCount：默认值：10：当 events 数据达到该数量时候，将临时文件滚动成目标文件；如果设置成0，则表示不根据events数据来滚动文件； idleTimeout：默认值：0：当目前被打开的临时文件在该参数指定的时间（秒）内，没有任何数据写入，则将该临时文件关闭并重命名成目标文件； batchSize：默认值：100：每个批次刷新到 HDFS 上的 events 数量； codeC：文件压缩格式，包括：gzip, bzip2, lzo, lzop, snappy fileType：默认值：SequenceFile，文件格式，包括：SequenceFile, DataStream,CompressedStream ​ 当使用DataStream时候，文件不会被压缩，不需要设置hdfs.codeC; ​ 当使用CompressedStream时候，必须设置一个正确的hdfs.codeC值； maxOpenFiles：默认值：5000：最大允许打开的HDFS文件数，当打开的文件数达到该值，最早打开的文件将会被关闭； minBlockReplicas：默认值：HDFS副本数，写入 HDFS 文件块的最小副本数。 ​ 该参数会影响文件的滚动配置，一般将该参数配置成1，才可以按照配置正确滚动文件。 writeFormat：写 sequence 文件的格式。包含：Text, Writable（默认） callTimeout：默认值：10000，执行HDFS操作的超时时间（单位：毫秒）； threadsPoolSize：默认值：10，hdfs sink 启动的操作HDFS的线程数。 rollTimerPoolSize：默认值：1，hdfs sink 启动的根据时间滚动文件的线程数。 kerberosPrincipal：HDFS安全认证kerberos配置； kerberosKeytab：HDFS安全认证kerberos配置； proxyUser：代理用户 round：默认值：false，是否启用时间上的”舍弃”； roundValue：默认值：1，时间上进行“舍弃”的值； roundUnit：默认值：seconds，时间上进行”舍弃”的单位，包含：second,minute,hour 文件到HDFS采集需求 比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs 根据需求，首先定义以下3大要素： 采集源，即source——监控文件内容更新 : exec ‘tail -F file’ 下沉目标，即sink——HDFS文件系统 : hdfs sink Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel 采集方案编写 12345678910111213141516171819202122232425262728293031323334353637383940agent1.sources = source1agent1.sinks = sink1agent1.channels = channel1# Describe/configure tail -F source1agent1.sources.source1.type = execagent1.sources.source1.command = tail -F /home/hadoop/logs/access_logagent1.sources.source1.channels = channel1#configure host for sourceagent1.sources.source1.interceptors = i1agent1.sources.source1.interceptors.i1.type = hostagent1.sources.source1.interceptors.i1.hostHeader = hostname# Describe sink1agent1.sinks.sink1.type = hdfs#a1.sinks.k1.channel = c1agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%Magent1.sinks.sink1.hdfs.filePrefix = access_logagent1.sinks.sink1.hdfs.maxOpenFiles = 5000agent1.sinks.sink1.hdfs.batchSize= 100agent1.sinks.sink1.hdfs.fileType = DataStreamagent1.sinks.sink1.hdfs.writeFormat =Textagent1.sinks.sink1.hdfs.rollSize = 102400agent1.sinks.sink1.hdfs.rollCount = 1000000agent1.sinks.sink1.hdfs.rollInterval = 60agent1.sinks.sink1.hdfs.round = trueagent1.sinks.sink1.hdfs.roundValue = 10agent1.sinks.sink1.hdfs.roundUnit = minuteagent1.sinks.sink1.hdfs.useLocalTimeStamp = true# Use a channel which buffers events in memoryagent1.channels.channel1.type = memoryagent1.channels.channel1.keep-alive = 120agent1.channels.channel1.capacity = 500000agent1.channels.channel1.transactionCapacity = 600# Bind the source and sink to the channelagent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1 跟过source和sink组件Flume支持众多的source和sink类型，详细手册可参考官方文档 http://flume.apache.org/FlumeUserGuide.html]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据工具</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive]]></title>
    <url>%2F2020%2F02%2F11%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHive%2Fhive%2F</url>
    <content type="text"><![CDATA[Hive基本概念什么是HiveHive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类似SQL查询的功能； 为什么使用Hive 直接使用hadoop需要掌握大量的知识，上手难度较大，mapReduce开发难度太大且复杂； Hive接口采用类似SQL的语法，加快了开发的速度； 避免重复的去写MapReduce，减少了开发学习成本； 扩展功能比较方便； Hive的特点 可扩展 Hive可以自由的拓展集群的规模，一般情况下不需要重启服务； 延展性 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数； 容错 良好的容错性，节点出现问题SQL仍可完成执行； Hive与Hadoop的关系Hive利用HDFS存储数据，利用MapReduce查询数据 Hive与传统数据库的对比 hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析 Hive的数据存储 Hive中所有的数据都存储在 HDFS 中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等） 只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据。 Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。 db：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在hdfs中表现所属db目录下一个文件夹 external table：外部表, 与table类似，不过其数据存放位置可以在任意指定路径 普通表: 删除表后, hdfs上的文件都删了 External外部表: 删除后, hdfs上的文件没有删除, 只是把文件删除了 partition：在hdfs中表现为table目录下的子目录 bucket：桶, 在hdfs中表现为同一个表目录下根据hash散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中 Hive架构Hive架构图 Jobtracker： 是hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster TaskTracker： 相当于 Nodemanager + yarnchild 基本组成 用户接口: CLI、JDBC/ODBC、WebGUI CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。 元数据存储：一般存储在关系型数据库中如 mysql； Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。 解释器、编译器、优化器、执行器 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。 Hive 安装部署Hive三种模式 内嵌模式：元数据保持在内嵌的derby模式，只允许一个会话连接 本地独立模式：在本地安装Mysql，把元数据放到mySql内 远程模式：元数据放置在远程的Mysql数据库 安装步骤 下载Hive安装包 地址: http://hive.apache.org/downloads.html 将hive安装包上传到hadoop集群，并解压 123tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /export/servers/cd /export/servers/ln -s apache-hive-1.2.1-bin hive 配置环境变量，编辑/etc/profile 1234567vi /etc/profile# 新增行# export HIVE_HOME=/export/servers/hive# export PATH=$&#123;HIVE_HOME&#125;/bin:$PATH# 保存退出source /etc/profile 修改hive配置文件 12345678910cd /export/servers/hive/conf# 修改hive-env.sh文件， 将如下内容写到hive-env.sh文件中cp hive-env.sh template hive-env.sh# export JAVA_HOME=/export/servers/jdk# export HADOOP_HOME=/export/servers/hadoop# export HIVE_HOME=/export/servers/hive# 将如下信息写到hive-site.xml文件中touch hive-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop02:3306/hivedb?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 安装mysql并配置hive数据库及权限 安装mysql客户端 123yum install mysql-serveryum install mysqlservice mysqld start 配置hive元数据库 1create database hivedb 对hive源数据库及逆行赋权，开放远程连接 12GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;FLUSH PRIVILEGES; 启动hive 1hive 备注: 如果报错 Terminal initialization failed; falling back to unsupported, 将/export/servers/hive/lib 里面的jline2.12替换了hadoop 中hadoop-2.6.1/share/hadoop/yarn/lib/jline-0.09*.jar 如果报错 Hive metastore database is not initialized. 执行: schematool -dbType mysql -initSchema 使用方式 交互式Shell 直接输入命令，启动hive客户端 1bin/hive Hive thrift服务 启动服务 启动方式（假如是在hadoop01上）: 启动为前台: bin/hiveserver2 启动为后台: nohup bin/hiveserver2 1&gt;/var/log/hiveserver.log 2&gt;/var/log/hiveserver.err &amp; 启动成功后, 可以在别的节点上用beeline去连接 客户端连接 方式一: hive/bin/beeline beeline&gt; !connet jdbc:hive2//mini1:10000 方式二: bin/beeline -u jdbc:hive2://mini1:10000 -n hadoop 直接执行SQL 直接通过hive -e 执行命令 1hive -e 'sql' Hive基本操作创建表建表语法 123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 LIKE 允许用户复制现有的表结构，但是不复制数据。 ROW FORMAT DELIMITED [FIELDS TERMINATED BY char][COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char][LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。 STORED AS SEQUENCEFILE|TEXTFILE|RCFILE 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE(默认)。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 CLUSTERED BY 对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 把表（或者分区）组织成桶（Bucket）有两个理由： （1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。 （2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。 建表实例 创建内部表mytable 1create table if not exists mytable(sid int, sname string) row format delimited fields terminated by ',' stored as textfile; 创建外部表pageview 1create external table if not exists pageview(pageid int, page_url string comment 'The page URL') row format delimited fields terminated by ',' location 'hdfs://192.168.11.191.9000/user/hibe/warehouse/'; 创建分区表student_p 1create table student_p(Sno int,Sname string,Sex string,Sage int,Sdept string) partitioned by(part string) row format delimited fields terminated by ','stored as textfile; 创建带桶的表student 1create table student(id int, age int, name string) partitioned by(stat_date string) clustered by(id) sorted by(age) into 2 buckets row format delimited fields terminated by','; 修改表增加/删除分区语法结构 12345# 增加分区ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ...# 删除分区ALTER TABLE table_name DROP partition_spec, partition_spec,... 实例 添加分区 1alter table student_p add partition(part='a') partition(part='b'); 删除分区 1alter table student drop partition(stat_date='20140101'), partition(stat_date=20140102); 重命名表语法结构 1alter table table_name RENAME TO new_table_name 实例 1alter table student rename to student1; 增加/更新列语法结构 1ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) 1ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 实例 1alter table student replace columns (id int, age int, name string); 显示命令show tables: 查看所有护具库表 show databases: 查看所有数据库 show partitions t_name: 查看所有分区 show functions: 查看所有function desc extended t_name: 查看表的详细信息 desc formatted t_name: 格式化显示表的详细信息 数据查询语法结构 123456SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference[WHERE where_condition] [GROUP BY col_list [HAVING condition]] [ [CLUSTER BY col_list] | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list] ] [LIMIT number] 备注: order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。 根据distribute by指定的字段内容将数据分到同一个reducer。 Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by 实例 获取三个最大的学生数据 1select id, age, name from stuent where stat_date='20140101' order by age desc limit 3; 查询学生信息，按照年龄降序排序 12set mapred.reduce.tasks=4;select id, age, name from stuent distribute by age sort by age desc; 按照学生名称汇总学生年龄 1select name, sum(age) from student group by name; 数据导入导出Load语法结构 12LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] 备注: Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。 LOCAL关键字 如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。 如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件 OVERWRITE关键字 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 实例 加载相对路径的数据 1load data local inpath 'buckets.txt' into table student partition(stat_date='20131231'); 加载绝对路径数据 1load data local inpath '/root/app/datafile/buckets.txt' into table student partition(stat_date='20131230'); 加载包含模式数据 1load data local inpath 'hdfs://192.168.11.11:9000/user/hibe/warehouse/student/stat_date=20131230/buckets.txt' into table student partition(stat_date='20131229'); overwrite 关键字使用 1load data local inpath 'buckets.txt' overwrite into table student partition(stat_date='20131229'); Insert语法结构 12345678910111213141516171819202122232425# 基本模式插入INSERT OVERWRITE/INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement FROM from_statement# 多插入模式FROM from_statement INSERT OVERWRITE/INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [INSERT OVERWRITE/INTO TABLE tablename2 [PARTITION ...] select_statement2] ...# 自动分区模式(查询结果有分区字段)INSERT OVERWRITE/INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement# 分桶数据导入set mapreduce.job.reduces=4;insert OVERWRITE/INTO TABLE tablenameselect_statement FROM tablename distribute by(var1) sort by(var2 asc);insert OVERWRITE/INTO table tablenameselect_statement cluster by(var1);# 基本数据导出INSERT OVERWRITE/INTO [LOCAL] DIRECTORY directory1 SELECT ... FROM ...# 多导出模式FROM from_statementINSERT OVERWRITE/INTO [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE/INTO [LOCAL] DIRECTORY directory2 select_statement2] ... 备注: insert into 与 insert overwrite 都可以向hive表中插入数据，但是insert into直接追加到表中数据的尾部，而insert overwrite会重写数据，既先进行删除，再写入。如果存在分区的情况，insert overwrite会只重写当前分区数据。 LOCAL: LOCAL关键字是将数据导入到本地文件系统，如果不加LOCAL关键字是将数据导出到HDFS文件系统 实例 基本模式插入 12insert overwrite table student partition(stat_date='20140101')select id, age, name from student where stat_date='20131229'; 多插入模式 12345from student insert overwrite table student partition(stat_date='20140102')select id, age, name where stat_date='20131229'insert overwrite table stu dent partition(stat_date='20140103')select id, age, name where stat_date='20131229'; 自动分区模式 12insert overwrite table student1 partition(stat_date)select id, age, stat_date from student where stat_date='20140101'; 分通表数据导入 123456789set mapreduce.job.reduces=4;insert into table stu_buckselect Sno,Sname,Sex,Sage,Sdept from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student cluster by(Sno); 导出文件到本地 12insert overwrite local directory '/root/app/datafile/student1'select * from student1 数据写入到文件系统时进行文本序列化，且每列用^A来区分，\n为换行符。用more命令查看时不容易看出分割符，可以使用: sed -e ‘s/\x01/|/g’ filename来查看。 导出数据到HDFS 12insert overwrite directory 'hdfs://192.168.11.191:9000/user/hive/warehouse/mystudent'select * from student1; Join操作语法结构 123table1 JOIN table2 [join_condition] | table1 &#123;LEFT|RIGHT|FULL&#125; [OUTER] JOIN table2 join_condition | table1 LEFT SEMI JOIN table2 join_condition 备注: ​ Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。另外，Hive 支持多于 2 个表的连接。 Join操作的说明 等值join 12345SELECT a.* FROM a JOIN b ON (a.id = b.id)；SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)；# 是正确的，然而:SELECT a.* FROM a JOIN b ON (a.id&gt;b.id)# 是错误的。 可以多于两张表 123456789SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);# 如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：SELECT a.val, b.val, c.val FROM a JOIN bON (a.key = b.key1) JOIN cON (c.key = b.key1)# 被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)JOIN c ON (c.key = b.key2)# 而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。 join时，每次map/reduce任务的逻辑 reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如： 12SELECT a.val, b.val, c.val FROM aJOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记 录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有： 12SELECT a.val, b.val, c.val FROM aJOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) 这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。 LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况 12SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key) 对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:a.val, NULL 所以 a 表中的所有记录都被保留了，a RIGHT OUTER JOIN b”会保留所有 b 表的记录。 Join 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况： 123SELECT a.val, b.val FROM aLEFT OUTER JOIN b ON (a.key=b.key)WHERE a.ds='2009-07-07' AND b.ds='2009-07-07' 会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法： 1234SELECT a.val, b.val FROM a LEFT OUTER JOIN bON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07') 这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。 Join 是不能交换位置的。 无论是 LEFT 还是 RIGHT join，都是左连接的。 123SELECT a.val1, a.val2, b.val, c.valFROM aJOIN b ON (a.key = b.key)LEFT OUTER JOIN c ON (a.key = c.key) 先 join a 表到 b 表，丢弃掉所有 join key 中不匹配的记录，然后用这一中间结果和 c 表做 join。这一表述有一个不太明显的问题，就是当一个 key 在 a 表和 c 表都存在，但是 b 表中不存在的时候：整个记录在第一次 join，即 a JOIN b 的时候都被丢掉了（包括a.val1，a.val2和a.key），然后我们再和 c 表 join 的时候，如果 c.key 与 a.key 或 b.key 相等，就会得到这样的结果：NULL, NULL, NULL, c.val Join实例获取已经分配班级的学生姓名 1select name, classname from student a join class b on (a.name = b.std_name); 获取尚未分配班级学生的姓名 1select name, classname from student a left join class b on (a.name = b.std_name) where b.std_name = null; left semi join 是 IN/EXISTS 的高效实现 只会打印左表的字段 1select id, name from stuent a left semi join class b on (a.name = b.std_name) Hive ShellHive命令行语法结构 1hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S] 说明: -i 从文件初始化HQL。 -e从命令行执行指定的HQL -f 执行HQL脚本 -v 输出执行的HQL语句到控制台 -p connect to Hive Server on port number -hiveconf x=y Use this to set hive/hadoop configuration variables. ,实例 1、执行一次查询 1hive -e 'select count(*) from student'; 2、运行一个文件 1hive -f query.hql 3、运行参数文件 123hive -i initHQl.conf# 文件内容# set mapred.reduce.tasks = 4 Hive 配置参数Hive参数文档地址: https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties 开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。 对于一般参数，有以下三种设定方式： 配置文件 命令行参数 参数声明 配置文件Hive的配置文件包括 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml 默认配置文件：$HIVE_CONF_DIR/hive-default.xml 用户自定义配置会覆盖默认配置。 另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。 配置文件的设定对本机启动的所有Hive进程都有效。 命令行参数启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如： 1bin/hive -hiveconf hive.root.logger=INFO,console 这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效 参数声明可以在HQL中使用SET关键字设定参数，例如： 1set mapred.reduce.tasks=100; 这一设定的作用域也是session级的。 上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。 Hive 自定义函数、Transform当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 自定义函数分类 UDF 作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数） UDAF（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（count，max） UDF开发步骤 先开发一个java类，继承UDF， 并重载evaluate方法 12345678910package cn.itcast.bigdata.udfimport org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public final class Lower extends UDF&#123; public Text evaluate(final Text s)&#123; if(s==null)&#123;return null;&#125; return new Text(s.toString().toLowerCase()); &#125;&#125; 打成jar包上传到服务器 将jar包添加到hive的classpath 1hive&gt;add JAR /home/hadoop/udf.jar; 创建临时函数与开发好的java class关联 1Hive&gt;create temporary function toprovince as 'cn.itcast.bigdata.udf.ToProvince'; 即可在hql中使用自定义函数strip 1Select strip(name),age from t_test; Transform实现步骤Hive的 TRANSFORM 关键字提供了在SQL中调用自写脚本的功能 适合实现Hive中没有的功能又不想写UDF的情况 编辑weekday_mapper.py内容 123456789#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split('\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\t'.join([movieid, rating, str(weekday),userid]) 添加transform方法 1hive&gt;add FILE weekday_mapper.py; 使用transform进行查询 123456INSERT OVERWRITE TABLE u_data_newSELECT TRANSFORM (movieid, rating, unixtime,userid) USING &apos;python weekday_mapper.py&apos; AS (movieid, rating, weekday,userid)FROM u_data; Hive实战例子需求有如下访客访问次数统计表 t_access_times 访客 月份 访问次数 A 2015-01 5 A 2015-01 15 B 2015-01 5 A 2015-01 8 B 2015-01 25 A 2015-01 5 A 2015-02 4 A 2015-02 6 B 2015-02 10 B 2015-02 5 …… …… …… 需要输出报表：t_access_times_accumulate 访客 月份 月访问总计 累计访问总计 A 2015-01 33 33 A 2015-02 10 43 ……. ……. ……. ……. B 2015-01 30 30 B 2015-02 15 45 ……. ……. ……. ……. 实现12345678910select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulatefrom (select username,month,sum(salary) as salary from t_access_times group by username,month) A inner join (select username,month,sum(salary) as salary from t_access_times group by username,month) BonA.username=B.usernamewhere B.month &lt;= A.monthgroup by A.username,A.monthorder by A.username,A.month; HQL示例表信息查询123show databases;show tables;desc test; 表分桶示例123456789101112131415161718192021222324set mapreduce.job.reduces=4;drop table stu_buck;create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)clustered by(Sno) sorted by(Sno DESC)into 4 bucketsrow format delimitedfields terminated by ',';insert overwrite table student_buckselect * from student cluster by(Sno) sort by(Sage); 报错,cluster 和 sort 不能共存insert into table stu_buckselect Sno,Sname,Sex,Sage,Sdept from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student cluster by(Sno); 多重插入12345from studentinsert into table student_p partition(part='a')select * where Sno&lt;95011;insert into table student_p partition(part='a')select * where Sno&lt;95011; 导出数据到本地12insert overwrite local directory '/home/hadoop/student.txt'select * from student;]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉搜索树]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[二叉搜索树的后续遍历题目输入一个非空整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 思路直接判断数组的数据能不能组成一个二叉搜索树我们使用递归的方法，先判断数组的左子树和右子树的位置，然后再判断左子树、右子树是不是二叉搜索树。 代码1234567891011121314151617181920212223242526272829class Solution: def VerifySquenceOfBST(self, sequence): # write code here if not len(sequence): return False if len(sequence) == 1: return True length = len(sequence) root = sequence[-1] # 1.找到左子树的位置 i = 0 while sequence[i] &lt; root: i = i + 1 # 2.如果右子树有小于根的元素，则不是搜索树 k = i for j in range(i, length-1): if sequence[j] &lt; root: return False # 3.切分出左子树 left_s = sequence[:k] # 4.切分出右子树 right_s = sequence[k:length-1] left, right = True, True # 5.递归的判断左右子树是否可以组成搜索二叉树 if len(left_s) &gt; 0: left = self.VerifySquenceOfBST(left_s) if len(right_s) &gt; 0: right = self.VerifySquenceOfBST(right_s) return left and right 二叉搜索树与双向链表题目输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 思路二叉搜索树如上图所示，我们将其转换为配需双向链表。 根据二叉搜索树的特点：左结点的值&lt;根结点的值&lt;右结点的值，我们不难发现，使用二叉树的中序遍历出来的数据的数序，就是排序的顺序。因此，首先，确定了二叉搜索树的遍历方法。 接下来，我们看下图，我们可以把树分成三个部分：值为10的结点、根结点为6的左子树、根结点为14的右子树。根据排序双向链表的定义，值为10的结点将和它的左子树的最大一个结点链接起来，同时它还将和右子树最小的结点链接起来。 按照中序遍历的顺序，当我们遍历到根结点时，它的左子树已经转换成一个排序的好的双向链表了，并且处在链表中最后一个的结点是当前值最大的结点。我们把值为8的结点和根结点链接起来，10就成了最后一个结点，接着我们就去遍历右子树，并把根结点和右子树中最小的结点链接起来。 代码1234567891011121314151617181920212223# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def __init__(self): self.listHead = None self.listTail = None def Convert(self, pRootOfTree): if pRootOfTree==None: return self.Convert(pRootOfTree.left) if self.listHead==None: self.listHead = pRootOfTree self.listTail = pRootOfTree else: self.listTail.right = pRootOfTree pRootOfTree.left = self.listTail self.listTail = pRootOfTree self.Convert(pRootOfTree.right) return self.listHead 二叉搜索树的第k个节点题目给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 思路先中序遍历，再返回前K个节点 代码1234567891011121314151617181920212223# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回对应节点TreeNode def KthNode(self, pRoot, k): # write code here if not pRoot or not k : return res = [] def midTraversal(node): if len(res)&gt;=k or not node : return midTraversal(node.left) res.append(node) midTraversal(node.right) midTraversal(pRoot) if len(res)&lt;k: return return res[k-1]]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回溯法]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E5%9B%9E%E6%BA%AF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[矩阵中的路径题目请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 思路回溯法 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Solution: """ 在矩阵中寻找是否存在路径等于给定的字符串 Returns: matrix -- 寻找路径的字符矩阵 rows -- 矩阵的行数 cols -- 矩阵的列数 path -- 待寻找的路径 """ def hasPath(self, matrix, rows, cols, path): # 为了保证当前路径上已经被访问过的结点不再重复访问 # 设置 vis 来标记某个元素是否被访问过，容量和 matrix 相同 vis = [True] * rows * cols for i in range(rows): for j in range(cols): # 从 matrix[i][j] 开始寻找是否存在路径 if self.hasPathAtAStartPoint(matrix, rows, cols, i, j, path, vis): # 如果存在则返回True return True # 如果从所有位置出发都不能找到，则返回False return False # 从某个点(x, y)出发寻找路径 def hasPathAtAStartPoint(self, matrix, rows, cols, i, j, path, vis): # 1. index 表示当前访问元素的位置 index = i * cols + j # 2. 递归的终止条件 # 如果路径被搜索到了最终位置，存在则返回True if not path: return True # 3. 如果访问越出边界、当前位置元素不是path的起始元素或者当前位置已被访问过，直接返回False if i &lt; 0 or i &gt;= rows or j &lt; 0 or j &gt;= cols or \ matrix[index]!=path[0] or vis[index]==False: return False # 如果没有找到则将当前位置重新置为False # 关键步骤， 先标记为False， 不让子寻找会使用该点 vis[index] = False # 递归条件,如果以(0, 0)作为原点 # 向上走: i - 1 # 向下走: i + 1 # 向前走: j + 1 # 向后走: j _ 1 if(self.hasPathAtAStartPoint(matrix,rows,cols,i+1,j,path[1:],vis) or self.hasPathAtAStartPoint(matrix,rows,cols,i-1,j,path[1:],vis) or self.hasPathAtAStartPoint(matrix,rows,cols,i,j-1,path[1:],vis) or self.hasPathAtAStartPoint(matrix,rows,cols,i,j+1,path[1:],vis)): return True # 关键步骤 # 如果没找到，不需要占用该点，需要释放 vis[index] = True return False 机器人的运动范围题目地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 思路回溯法 代码123456789101112131415161718192021222324class Solution: def movingCount(self, threshold, rows, cols): # 用来标记坐标是否走过的二维矩阵 matrix = [[0 for i in range(cols)] for j in range(rows)] count = self.findgrid(threshold, rows, cols, matrix, 0, 0) return count def findgrid(self, threshold, rows, cols, matrix, i, j): count = 0 # matrix[i][j]==0表示没走过这一格 if i&lt;rows and j&lt;cols and i&gt;=0 and j&gt;=0 and \ self.judge(threshold, i, j) and matrix[i][j] == 0: matrix[i][j] = 1 # 表示已经走过了 count = 1 + self.findgrid(threshold, rows, cols, matrix, i, j+1) \ + self.findgrid(threshold, rows, cols, matrix, i, j-1) \ + self.findgrid(threshold, rows, cols, matrix, i+1, j) \ + self.findgrid(threshold, rows, cols, matrix, i-1, j) return count def judge(self, threshold, i, j): if sum(map(int, str(i) + str(j))) &lt;= threshold: return True else: return False]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[其他]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E5%85%B6%E4%BB%96%2F</url>
    <content type="text"><![CDATA[二进制中1的个数题目输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 思路一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&amp;1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。 代码12345678910class Solution: def NumberOf1(self, n): # write code here count = 0 if n&lt;0: n = n &amp; 0xffffffff while n: count += 1 n = n &amp; (n-1) return count 数值的整数次方题目给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 保证base和exponent不同时为0 思路当指数为负数的时候，可以先对指数求绝对值，然后算出次方的结果之后再取倒数。如果底数为0，则直接返回0。此时的次方在数学上是没有意义的。 除此之外，我们要注意：由于计算机表示小数（包括float和double型小数）都有误差，我们不能直接用等号（==）判断两个小数是否相等。如果两个小数的差的绝对值很小，比如小于0.0000001，就可以认为它们相等。 代码1234567891011121314class Solution: def Power(self, base, exponent): # write code here flag = 0 result = 1 if base == 0: return False if exponent &lt; 0: flag = 1 for i in range(abs(exponent)): result *= base if flag == 1: result = 1 / result return result 顺时针打印矩阵题目输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 思路将结果存入vector数组，从左到右，再从上到下，再从右到左，最后从下到上遍历。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Solution: # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): # write code here rows = len(matrix) cols = len(matrix[0]) result = [] if rows == 0 and cols == 0: return result left, right, top, buttom = 0, cols - 1, 0, rows - 1 while left &lt;= right and top &lt;= buttom: for i in range(left, right+1): result.append(matrix[top][i]) for i in range(top+1, buttom+1): result.append(matrix[i][right]) if top != buttom: for i in range(left, right)[::-1]: result.append(matrix[buttom][i]) if left != right: for i in range(top+1, buttom)[::-1]: result.append(matrix[i][left]) left += 1 top += 1 right -= 1 buttom -= 1 return result def matrix(target): num = target ** 2 left, right, top, bottom = 0, target-1, 0, target-1 res = [ [0 for col in range(target)] for row in range(target)] each = 1 while left &lt;= right and top &lt;= bottom and each &lt;= num: for i in range(left, right+1): res[top][i] = each each += 1 for i in range(top+1, bottom+1): res[i][right] = each each += 1 if top != bottom: for i in range(left, right)[::-1]: res[bottom][i] = each each += 1 if left != right and each &lt;= num: for i in range(top+1, bottom)[::-1]: res[i][left] = each each += 1 top += 1 left += 1 bottom -= 1 right -= 1 for i in range(len(res)): print("\t".join('%s' %id for id in res[i])) 最小的k个数题目输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 思路一种方法，用数组来存储排序的最小k个数 另一种方法是使用最大堆 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution: # 用数组的方法, 用数组来存储排序的最小k个数，插入的时间复杂度比较大 def GetLeastNumbers_Solution1(self, tinput, k): if len(tinput) &lt; k: return [] tmp = sorted(tinput[:k]) for each in tinput[k:]: index = k - 1 flag = False while index &gt;= 0 and tmp[index] &gt; each: index -= 1 flag = True if flag == True: tmp.insert(index+1, each) tmp.pop() return tmp def HeadAdjust(self, input_list, parent, length): # 获取堆的根 temp = input_list[parent] # 获取根孩子的坐标 child = 2 * parent + 1 while child &lt; length: # 寻找最大孩子的坐标 if child + 1 &lt; length and input_list[child] &lt; input_list[child+1]: child += 1 # 如果入参的大根发现比他小的值，便停止搜索 if temp &gt;= input_list[child]: break # 如果当前根小于孩子节点 # 将根节点换成孩子节点 input_list[parent] = input_list[child] # 更新根节点位置 parent = child # 更新孩子节点位置 child = 2 * parent + 1 input_list[parent] = temp def GetLeastNumbers_Solution(self, tinput, k): # write code here res = [] length = len(tinput) change = True if length &lt;= 0 or k &lt;= 0 or k &gt; length: return res res = tinput[:k] for i in range(k, length+1): if change == True: #将初始待排序关键字序列(R1,R2....Rn)构建成大顶堆，此堆为初始的无须区； #将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,......Rn-1)和新的有序区(Rn), #且满足R[1,2...n-1]&lt;=R[n]; #由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,......Rn-1)调整为新堆， #然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2....Rn-2)和新的有序区(Rn-1,Rn)。 #不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 for j in range(0, k//2+1)[::-1]: self.HeadAdjust(res, j, k) for j in range(1, k)[::-1]: res[0], res[j] = res[j], res[0] self.HeadAdjust(res, 0, j) chage = False if i != length and res[k-1] &gt; tinput[i]: res[k-1] = tinput[i] chage = True return res 整数中1出现的次数题目求出113的整数中1出现的次数,并算出1001300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 思路设定整数点（如1、10、100等等）作为位置点i（对应n的各位、十位、百位等等），分别对每个数位上有多少包含1的点进行分析。 根据设定的整数位置，对n进行分割，分为两部分，高位n/i，低位n%i当i表示百位，且百位对应的数&gt;=2,如n=31456,i=100，则a=314,b=56，此时百位为1的次数有a/10+1=32（最高两位031），每一次都包含100个连续的点，即共有(a/10+1)*100个点的百位为1当i表示百位，且百位对应的数为1，如n=31156,i=100，则a=311,b=56，此时百位对应的就是1，则共有a/10(最高两位0-30)次是包含100个连续点，当最高两位为31（即a=311），本次只对应局部点0056，共b+1次，所有点加起来共有（a/10*100）+(b+1)，这些点百位对应为1当i表示百位，且百位对应的数为0,如n=31056,i=100，则a=310,b=56，此时百位为1的次数有a/10=31（最高两位0~30）综合以上三种情况，当百位对应0或&gt;=2时，有(a+8)/10次包含所有100个点，还有当百位为1(a%10==1)，需要增加局部点b+1 #之所以补8，是因为当百位为0，则a/10==(a+8)/10，当百位&gt;=2，补8会产生进位位，效果等同于(a/10+1) 代码123456789101112class Solution: def NumberOf1Between1AndN_Solution(self, n): # write code here count = 0 i = 1 while i &lt;= n: a = n / i b = n % i count += (a+8)/10 * i + (a%10 == 1)*(b+1) i *= 10 return count 丑数题目把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 思路所谓的一个数m是另一个数n的因子，是指n能被m整除，也就是n%m==0。根据丑数的定义，丑数只能被2、3和5整除。根据丑数的定义，丑数应该是另一个丑数乘以2、3或者5的结果（1除外）。因此我们可以创建一个数组，里面的数字是排好序的丑数，每一个丑数都是前面的丑数乘以2、3或者5得到的。 这个思路的关键问题在于怎样保证数组里面的丑数是排好序的。对乘以2而言，肯定存在某一个丑数T2，排在它之前的每一个丑数乘以2得到的结果都会小于已有最大的丑数，在它之后的每一个丑数乘以乘以2得到的结果都会太大。我们只需要记下这个丑数的位置，同时每次生成新的丑数的时候，去更新这个T2。对乘以3和5而言，也存在着同样的T3和T5\ 重点: 每种质因子的丑数乘以对应的质因子还是丑数 代码12345678910111213141516class Solution: def GetUglyNumber_Solution(self, index): # write code here if index &lt; 7: return index res = [1, 2, 3, 4, 5, 6] t2, t3, t5 = 3, 2, 1 for i in range(6, index): res.append(min(res[t2] * 2, min(res[t3] * 3, res[t5] * 5))) while res[t2] * 2 &lt;= res[i]: t2 += 1 while res[t3] * 3 &lt;= res[i]: t3 += 1 while res[t5] * 5 &lt;= res[i]: t5 += 1 return res[index - 1] 和为S的两个数字题目输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 思路对于一个数组，我们可以定义两个指针，一个从左往右遍历（pleft），另一个从右往左遍历（pright）。首先，我们比较第一个数字和最后一个数字的和curSum与给定数字sum，如果curSum &lt; sum，那么我们就要加大输入值，所以，pleft向右移动一位，重复之前的计算；如果curSum &gt; sum，那么我们就要减小输入值，所以，pright向左移动一位，重复之前的计算；如果相等，那么这两个数字就是我们要找的数字，直接输出即可。 这么做的好处是，也保证了乘积最小。 代码1234567891011121314class Solution: def FindNumbersWithSum(self, array, tsum): # write code here if len(array) &lt;= 1: return [] left, right = 0, len(array)-1 while left &lt; right: if array[left] + array[right] == tsum: return array[left], array[right] elif array[left] + array[right] &lt; tsum: left += 1 else: right -= 1 return [] 扑克牌的顺子题目LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 思路满足如下条件才可以认为是顺子：输入数据个数为5；输入数据都在0-13之间；没有相同的数字；最大值与最小值的差值不大于5。PS：大小王可以当成任意数。 这里可以使用一个技巧，即利用一个flag记录每个数字出现的次数。 代码12345678910111213141516171819202122232425262728class Solution: def IsContinuous(self, numbers): # write code here if len(numbers) != 5: return False max_num = None min_num = None # 用来保存所有数对应的 1&lt;&lt;num 结果 flag = 0 for number in numbers: if number &lt; 0 or number &gt; 13: return False # 如果是大小王不用统计 if number == 0: continue # 如果数据重复，则直接返回False if (flag &gt;&gt; number) &amp; 1 == 1: return False # 与当前flag想或，记录当前数的唯一标记 flag |= 1 &lt;&lt; number if number &lt; min_num or max_num is None: min_num = number if number &gt; max_num or max_num is None: max_num = number if max_num - min_num &gt;= 5: return False return True 孩子们的游戏题目每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) 如果没有小朋友，请返回-1 思路用数学归纳法推导出递推公式，设有n个人（编号0~(n-1))，从0开始报数，报到(m-1)的退出，剩下的人继续从0开始报数。令f[i]表示i个人时最后胜利者的编号，则有递推公式：f[1]=0;可用数学归纳求出递推公式F(N)=(F(N-1)+m)%n 代码123456789class Solution: def LastRemaining_Solution(self, n, m): # write code here if n == 0: return -1 s = 0 for i in range(2, n+1): s = (s+m) % i return s 求1+2+3+….+n题目求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 思路递归 代码12345class Solution: def Sum_Solution(self, n): # write code here ans = n return ans and ans + self.Sum_Solution(n-1) 不用加减乘除的加法题目写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 思路首先看十进制是如何做的： 5+7=12， 可以使用三步走： 第一步：相加各位的值，不算进位，得到2。第二步：计算进位值，得到10. 如果这一步的进位值为0，那么第一步得到的值就是最终结果。第三步：重复上述两步，只是相加的值变成上述两步的得到的结果2和10，得到12。 同样我们可以三步走的方式计算二进制值相加： 5-101，7-111 第一步：相加各位的值，不算进位，得到010，二进制每位相加就相当于各位做异或操作，101^111。第二步：计算进位值，得到1010，相当于各位做与操作得到101，再向左移一位得到1010，(101&amp;111)&lt;&lt;1。第三步：重复上述两步， 各位相加 010^1010=1000，进位值为100=(010&amp;1010)&lt;&lt;1。继续重复上述两步：1000^100 = 1100，进位值为0，跳出循环，1100为最终结果。 代码1234567891011class Solution: def Add(self, num1, num2): # write code here MAX = 0x7fffffff mask = 0xffffffff while num2 != 0: num1, num2 = (num1 ^ num2), ((num1 &amp; num2) &lt;&lt; 1) num1 = num1 &amp; mask num2 = num2 &amp; mask return num1 if num1 &lt;= MAX else ~(num1 ^ mask) 字符流中的第一个不重复字符题目请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 思路这道题还是很简单的。将字节流保存起来，通过哈希表统计字符流中每个字符出现的次数，顺便将字符流保存在string中，然后再遍历string，从哈希表中找到第一个出现一次的字符 代码12345678910111213141516171819class Solution: def __init__(self): self.s = '' self.count = &#123;&#125; # 返回对应char def FirstAppearingOnce(self): # write code here length = len(self.s) for i in range(length): if self.count[self.s[i]] == 1: return self.s[i] return '#' def Insert(self, char): # write code here self.s += char if char not in self.count: self.count[char] = 1 else: self.count[char] += 1 数据流中的中位数题目如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 思路先排序在返回 代码1234567891011class Solution: def __init__(self): self.nums = [] def Insert(self, num): self.nums.append(num) def GetMedian(self, fuck): self.nums.sort() if len(self.nums) % 2 == 1: return self.nums[(len(self.nums) - 1) / 2] else: return (self.nums[len(self.nums) / 2] + self.nums[len(self.nums) / 2 - 1]) / 2.0 滑动窗口的最大值题目给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 思路我们可以用STL中的deque来实现，接下来我们以数组{2,3,4,2,6,2,5,1}为例，来细说整体思路。 数组的第一个数字是2，把它存入队列中。第二个数字是3，比2大，所以2不可能是滑动窗口中的最大值， 因此把2从队列里删除，再把3存入队列中。第三个数字是4，比3大，同样的删3存4。此时滑动窗口中已经有3个数字，而它的最大值4位于队列的头部。 第四个数字2比4小，但是当4滑出之后它还是有可能成为最大值的，所以我们把2存入队列的尾部。下一个数字是6，比4和2都大，删4和2，存6。就这样依次进行，最大值永远位于队列的头部。 但是我们怎样判断滑动窗口是否包括一个数字？应该在队列里存入数字在数组里的下标，而不是数值。当一个数字的下标与当前处理的数字的下标之差大于或者相等于滑动窗口大小时，这个数字已经从窗口中滑出，可以从队列中删除。 代码1234567891011121314151617181920212223242526272829303132class Solution: def maxInWindows(self, num, size): # 如果数组 num 不存在，则返回 [] if not num: return [] # 如果滑动窗口的大小大于数组的大小，或者 size 小于 0，则返回 [] if size &gt; len(num) or size &lt;1: return [] # 如果滑动窗口的大小为 1 ，则直接返回原始数组 if size == 1: return num # 存放最大值，次大值的数组，和存放输出结果数组的初始化 temp = [0] res = [] # 对于数组中每一个元素进行判断 for i in range(len(num)): # 首先判断当前最大的元素是否过期 if i -temp[0] &gt; size-1: temp.pop(0) # 将第 i 个元素与 temp 中的值比较，将小于 i 的值都弹出 while (len(temp)&gt;0 and num[i] &gt;= num[temp [-1]]): temp.pop() # 如果现在 temp 的长度还没有达到最大规模，将元素 i 压入 if len(temp)&lt; size-1: temp.append(i) # 只有经过一个完整的窗口才保存当前的最大值 if i &gt;=size-1: res.append(num[temp [0]]) return res]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[重建二叉树题目输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 思路通常树有如下几种遍历方式： 前序遍历：先访问根结点，再访问左子结点，最后访问右子结点。 中序遍历：先访问左子结点，再访问根结点，最后访问右子结点。 后序遍历：先访问左子结点，再访问右子结点，最后访问根结点。 本题为前序遍历和中序遍历，最少需要两种遍历方式，才能重建二叉树。 前序遍历序列中，第一个数字总是树的根结点的值。在中序遍历序列中，根结点的值在序列的中间，左子树的结点的值位于根结点的值的左边，而右子树的结点的值位于根结点的值的右边。剩下的我们可以递归来实现 代码1234567891011121314151617181920212223# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回构造的TreeNode根节点 def reConstructBinaryTree(self, pre, tin): # write code here # 判断树的长度 if len(pre) == 0: return None elif len(pre) == 1: return TreeNode(pre[0]) else: # 先根据先序遍历的结果，直接拿到根元素的内容 root = TreeNode(pre[0]) # 通过中序遍历知道左树和右树的分界线 pos = tin.index(pre[0]) # 通过递归的方法获取左子树和右子树 root.left = self.reConstructBinaryTree(pre[1:pos+1], tin[:pos]) root.right = self.reConstructBinaryTree(pre[pos+1:], tin[pos+1:]) return root 树的子结构题目输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 思路要查找树A中是否存在和树B结构一样的子树，我们可以分为两步：第一步在树A中找到和B的根结点的值一样的结点R，第二步再判断树A中以R为根节点的子树是不是包含和树B一样的结构。 这里使用递归的方法即可。 代码12345678910111213141516171819# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if not pRoot1 or not pRoot2: return False return self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) or self.is_subtree(pRoot1, pRoot2) def is_subtree(self, A, B): if not B: return True if not A or A.val != B.val: return False return self.is_subtree(A.left, B.left) and self.is_subtree(A.right, B.right) 二叉树镜像题目操作给定的二叉树，将其变换为源二叉树的镜像。 思路先交换根节点的两个子结点之后，我们注意到值为10、6的结点的子结点仍然保持不变，因此我们还需要交换这两个结点的左右子结点。做完这两次交换之后，我们已经遍历完所有的非叶结点。此时变换之后的树刚好就是原始树的镜像。交换示意图如下所示： 代码12345678910111213141516171819# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回镜像树的根节点 def Mirror(self, root): # write code here if not root: return root left = root.left right = root.right tmp = left root.left = root.right root.right = tmp self.Mirror(root.left) self.Mirror(root.right) return root 从上往下打印二叉树题目从上往下打印出二叉树的每个节点，同层节点从左至右打印。 思路用一个数组来缓存每一层的节点 代码123456789101112131415class Solution: # 返回从上到下每个节点值列表，例：[1,2,3] def PrintFromTopToBottom(self, root): if not root: return [] result = [] tmp = [root] while tmp: cur = tmp.pop(0) result.append(cur.val) if cur.left: tmp.append(cur.left) if cur.right: tmp.append(cur.right) return result 二叉树中和为某一值得路径题目输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 思路深度优先搜索。使用前序遍历，使用两个全局变量result和tmp，result来存放最终结果，tmp用来存放临时结果。 每次遍历，我们先把root的值压入tmp，然后判断当前root是否同时满足： 与给定数值相减为0； 左子树为空； 右子树为空。 如果满足条件，就将tmp压入result中，否则，依次遍历左右子树。需要注意的是，遍历左右子树的时候，全局变量tmp是不清空的，直到到了根结点才请空tmp。 代码12345678910111213141516171819202122# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表，内部每个列表表示找到的路径 def FindPath(self, root, expectNumber): # write code here if not root: return [] # 如果是叶子节点，并且等于组成搜索路径的目标值，返回本身值，最为路径的最后一个元素 if not root.left and not root.right and root.val == expectNumber: return [[root.val]] # 递归向下寻找，直到找到叶子节点目标值 left = self.FindPath(root.left, expectNumber-root.val) right = self.FindPath(root.right, expectNumber-root.val) res = [] for i in left + right: res.append([root.val] + i) return res 二叉树的深度题目输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 思路可以是递归的方法，属于DFS（深度优先搜索）；另一种方法是按照层次遍历，属于BFS（广度优先搜索）。 代码123456789101112# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def TreeDepth(self, pRoot): if pRoot is None: return 0 left=self.TreeDepth(pRoot.left) right=self.TreeDepth(pRoot.right) return max(left,right)+1 平衡二叉树题目输入一棵二叉树，判断该二叉树是否是平衡二叉树。 在这里，我们只需要考虑其平衡性，不需要考虑其是不是排序二叉树 思路重复遍历会影响算法的性能，所以很有必要掌握不需要重复遍历的方法。如果我们用后序遍历的方式遍历二叉树的每一个结点，在遍历到一个结点之前我们就已经遍历了它的左右子树。只要在遍历每个结点的时候记录它的深度（某一结点的深度等于它到叶结点的路径的长度），我们就可以一边遍历一边判断每个结点是不是平衡的 代码123456789101112131415161718192021# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def IsBalanced_Solution(self, pRoot): # write code here if not pRoot: return 1 left = self.IsBalanced_Solution(pRoot.left) right = self.IsBalanced_Solution(pRoot.right) if not left: return False if not right: return False if abs(left-right) &lt;= 1: return 1 + max(left,right) else: return False 二叉树的下一个节点题目给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 思路1结点有右子树: 那么它的下一个结点就是它的右子树的最左子结点。 也就是说从右子结点出发一直沿着指向左子树结点的指针，我们就能找到它的下一个结点。 例如，图中结点b的下一个结点是h，结点a的下一个结点是f。 2节点是左子节点: 接着我们分析一下结点没有右子树的情形。如果结点是它父结点的左子结点， 那么它的下一个结点就是它的父结点。例如，途中结点d的下一个结点是b，f的下一个结点是c。 3节点的父节点是左子节点: 如果一个结点既没有右子树，并且它还是父结点的右子结点，这种情形就比较复杂。 我们可以沿着指向父结点的指针一直向上遍历，直到找到一个是它父结点的左子结点的结点。 如果这样的结点存在，那么这个结点的父结点就是我们要找的下一个结点。 例如，为了找到结点g的下一个结点，我们沿着指向父结点的指针向上遍历，先到达结点c。 由于结点c是父结点a的右结点，我们继续向上遍历到达结点a。由于结点a是树的根结点。 它没有父结点。因此结点g没有下一个结点。 代码1234567891011121314151617181920212223242526# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def GetNext(self, pNode): # write code here if not pNode: return pNode pNext = None # 如果当前节点有右子树，那么下一个节点就是右子树的最左节点 if pNode.right is not None: pNode = pNode.right while pNode.left is not None: pNode = pNode.left pNext = pNode # 如果当前节点没有右子树，则需要找他的父节点 elif pNode.next is not None: pParent = pNode.next while pParent is not None and pNode is pParent.right: pNode = pParent pParent = pNode.next pNext = pParent return pNext 对称的二叉树题目请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 思路 我们通常有三种不同的二叉树遍历算法，即前序遍历、中序遍历和后序遍历。在这三种遍历算法中，都是先遍历左子结点再遍历右子结点。以前序遍历为例，我们可以定义一个遍历算法，先遍历右子结点再遍历左子结点，暂且称其为前序遍历的对称遍历。 遍历第一棵树，前序遍历的遍历序列为{8,6,5,7,6,7,5}，其对称遍历的遍历序列为{8,6,5,7,6,7,5}。 遍历第二颗树，前序遍历的遍历序列为{8,6,5,7,9,7,5}，其对称遍历的遍历序列为{8,9,5,7,6,7,5}。 可以看到，使用此方法可以区分前两棵树，第一棵树为对称树，第二颗树不是对称树。但是当使用此方法，你会发现第三颗树的前序遍历和对称前序遍历的遍历序列是一样的。 怎么区分第三颗树呢？解决办法就是我们也要考虑NULL指针。此时，前序遍历的遍历序列{7,7,7,NULL,NULL,7,NULL,NULL,7,7,NLL,NULL,NULL}，其对称遍历的遍历序列为{7,7,NULL,7,NULL,NULL,7,7,NULL,NULL,7,NULL,NULL}。因为两种遍历的序列不同，因此这棵树不是对称树。 代码12345678910111213141516171819202122232425# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isSymmetrical(self, pRoot): # write code here if not pRoot: return True ## 判断左右子树是否互为镜像 return self.recursiveTree(pRoot.left, pRoot.right) def recursiveTree(self, left, right): # 如果左右子树都是空树，则是镜像树 if not left and not right: return True # 如果左右子树有一个为空，另一个不为空，那么不是镜像树 if not left or not right: return False # 如果左右子树的根节点的数值不想等，不互为镜像树 if left.val == right.val: # 递归判断左右自树是否是镜像树，注意左左比右右， 左右比右左 return self.recursiveTree(left.left, right.right) and self.recursiveTree(left.right, right.left) return False 按之字的顺序打印二叉树题目请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 思路 按之字顺序打印上图二叉树，打印顺序为： 1 3 2 4 5 6 7 15 14 13 12 12 10 9 8 为了达到这样打印的效果，我们需要使用两个栈。我们在打印某一行结点时，把下一层的子结点保存到相应的栈里。如果当前打印的是奇数层（第一层、第三层等），则先保存左子树结点再保存右子树结点到第一个栈里。如果当前打印的是偶数层（第二层、第四层等），则则先保存右子树结点再保存左子树结点到第二个栈里。 代码1234567891011121314151617181920212223242526# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: """docstring for Solution""" def Print(self, pRoot): resultArray = [] if not pRoot: return resultArray curLayerNodes = [pRoot] isEvenLayer = True while curLayerNodes: curLayerValues = [] nextLayerNodes = [] isEvenLayer = not isEvenLayer for node in curLayerNodes: curLayerValues.append(node.val) if node.left: nextLayerNodes.append(node.left) if node.right: nextLayerNodes.append(node.right) curLayerNodes = nextLayerNodes resultArray.append(curLayerValues[::-1]) if isEvenLayer else resultArray.append(curLayerValues) return resultArray 把二叉树打印成多行题目从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 思路使用队列即可 代码123456789101112131415161718192021222324252627# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回二维列表[[1,2],[4,5]] def Print(self, pRoot): # write code here if not pRoot: return [] result = [] curStack = [pRoot] while curStack: tmpStack = list() tmpResult = list() for node in curStack: tmpResult.append(node.val) if node.left: tmpStack.append(node.left) if node.right: tmpStack.append(node.right) curStack = tmpStack result.append(tmpResult) return result 序列化二叉树题目请实现两个函数，分别用来序列化和反序列化二叉树 二叉树的序列化是指：把一棵二叉树按照某种遍历方式的结果以某种格式保存为字符串，从而使得内存中建立起来的二叉树可以持久保存。序列化可以基于先序、中序、后序、层序的二叉树遍历方式来进行修改，序列化的结果是一个字符串，序列化时通过 某种符号表示空节点（#），以 ！ 表示一个结点值的结束（value!）。 二叉树的反序列化是指：根据某种遍历顺序得到的序列化字符串结果str，重构二叉树。 例如，我们可以把一个只有根节点为1的二叉树序列化为”1,”，然后通过自己的函数来解析回这个二叉树 思路使用前序遍历来序列化和发序列化即可。只要自己写的程序格式对应上即可。可以使用$符号表示NULL，同时每个结点之间，需要添加逗号，即’,’进行分隔。 代码1234567891011121314151617181920212223242526# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def Serialize(self, root): # write code here if not root: return "#" return str(root.val)+","+self.Serialize(root.left)+","+self.Serialize(root.right) def Deserialize(self, s): # write code here list = s.split(",") return self.deserializeTree(list) def deserializeTree(self, list): if len(list)&lt;=0: return None val = list.pop(0) root = None if val != '#': root = TreeNode(int(val)) root.left = self.deserializeTree(list) root.right = self.deserializeTree(list) return root]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[滑动窗口的最大值题目给你一根长度为n的绳子，请把绳子剪成整数长的m段（m、n都是整数，n&gt;1并且m&gt;1），每段绳子的长度记为k[0],k[1],…,k[m]。请问k[0]xk[1]x…xk[m]可能的最大乘积是多少？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。 思路我们先定义函数f(n)为把绳子剪成若干段之后的各段长度乘积的最大值.在剪第一刀的时候,我们会有n-1种可能的选择,也就是说剪出来的第一段绳子的长度可能为1,2,……n-1.因此就有了递归公式 f(n) = max(f(i)*f(n-i)),其中0&lt;i&lt;n. 代码12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: def cutRope1(self, number): # write code here if number &lt; 2: return 0 if number == 2: return 1 if number == 3: return 2 return self.cutRopeCore(number) def cutRopeCore(self, number): if number &lt; 4: return number max_ = 0 for i in range(1, number/2+1): max_ = max(self.cutRopeCore(i) * self.cutRopeCore(number - i), max_) return max_ def cutRope(self, number): if number &lt; 2: return 0 if number == 2: return 1 if number == 3: return 2 #申请辅助空间 products = [0]*(number+1) #定义前几个初始变量的值 products[0] = 0 products[1] = 1 products[2] = 2 products[3] = 3 #进行动态规划,也就是从下向上的进行求解 for i in range(4, number+1): max_ = 0 for j in range(1, i/2+1): max_ = max(products[j]*products[i-j], max_) products[i] = max_ max_ = products[number] return max_]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[栈]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E6%A0%88%2F</url>
    <content type="text"><![CDATA[用两个栈实现队列题目用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 思路创建两个栈stack1和stack2，使用两个“先进后出”的栈实现一个“先进先出”的队列。 我们通过一个具体的例子分析往该队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入到stack1，此时stack1中的元素有{a}，stack2为空。再压入两个元素b和c，还是插入到stack1中，此时stack1的元素有{a,b,c}，其中c位于栈顶，而stack2仍然是空的。 这个时候我们试着从队列中删除一个元素。按照先入先出的规则，由于a比b、c先插入队列中，最先删除的元素应该是a。元素a存储在stack1中，但并不在栈顶，因此不能直接进行删除操作。注意stack2我们一直没有使用过，现在是让stack2发挥作用的时候了。如果我们把stack1中的元素逐个弹出压入stack2，元素在stack2中的顺序正好和原来在stack1中的顺序相反。因此经过3次弹出stack1和要入stack2操作之后，stack1为空，而stack2中的元素是{c,b,a}，这个时候就可以弹出stack2的栈顶a了。此时的stack1为空，而stack2的元素为{b,a}，其中b在栈顶。 因此我们的思路是：当stack2中不为空时，在stack2中的栈顶元素是最先进入队列的元素，可以弹出。如果stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压倒stack1的栈底，经过弹出和压入之后就处于stack2的栈顶，有可以直接弹出。如果有新元素d插入，我们直接把它压入stack1即可。 代码12345678910111213class Solution: def __init__(self): self.stack1 = [] self.stack2 = [] def push(self, node): # write code here self.stack1.append(node) def pop(self): # 当栈二空了的时候，将栈一的数据反向推入到栈二中，实现了栈中数据反复 if len(self.stack2) == 0: while self.stack1: self.stack2.append(self.stack1.pop()) return self.stack2.pop() 包含min函数的栈题目定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 注意：保证测试中不会当栈为空的时候，对栈调用pop()或者min()或者top()方法。 思路使用两个stack，一个为数据栈，另一个为辅助栈。数据栈用于存储所有数据，辅助栈用于存储最小值。 举个例子： 入栈的时候：首先往空的数据栈里压入数字3，显然现在3是最小值，我们也把最小值压入辅助栈。接下来往数据栈里压入数字4。由于4大于之前的最小值，因此我们只要入数据栈，不压入辅助栈。 出栈的时候：当数据栈和辅助栈的栈顶元素相同的时候，辅助栈的栈顶元素出栈。否则，数据栈的栈顶元素出栈。 获得栈顶元素的时候：直接返回数据栈的栈顶元素。 栈最小元素：直接返回辅助栈的栈顶元素。 代码123456789101112131415161718192021222324252627282930313233class Solution: def __init__(self): self.Data = [] self.Min = [] def push(self, node): # write code here self.Data.append(node) # if self.Min: if self.Min[-1] &gt;= node: self.Min.append(node) else: self.Min.append(self.Min[-1]) else: self.Min.append(node) def pop(self): # write code here if self.Data == []: return None else: if self.Min: self.Min.pop() return self.Data.pop() def top(self): # write code here if self.Data == []: return None return self.Data[-1] def min(self): # write code here if not self.Min: return None return self.Min[-1] 栈的压入、弹出序列题目输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 思路借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。 代码1234567891011121314class Solution: def IsPopOrder(self, pushV, popV): # write code here if len(pushV) != len(popV): return False subStack = [] for i in pushV: subStack.append(i) while subStack and subStack[-1] == popV[0]: subStack.pop() popV.pop(0) if subStack: return False return True]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[二维数组中的查找题目在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 思路首先选取数组中右上角的数字。如果该数字等于要查找的数字，查找过程结束；如果该数字大于要查找的数组，剔除这个数字所在的列；如果该数字小于要查找的数字，剔除这个数字所在的行。也就是说如果要查找的数字不在数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 代码123456789101112131415161718192021class Solution: # array 二维列表 def Find(self, target, array): # write code here rows = len(array) if not rows: return False cols = len(array[0]) if not cols: return False row = 0 col = cols-1 while row &lt; rows and col &gt;=0: if target == array[row][col]: return True elif target &lt; array[row][col]: col -= 1 else: row += 1 return False 旋转数组的最小数字题目把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 思路将数据分为两个非递减数组来考虑 通过二分法来进行最小数据的查找 设置两个指针，一个存在于第一个比较大的数组, 一个存在于比较小的数组 每次二分判断是属于哪个数组，然后更新对应数组的指针，指导两个数组的指针相遇 代码12345678910111213141516171819202122232425262728293031class Solution: def minNumberInRotateArray(self, rotateArray): # write code here if len(rotateArray) == 0: return 0 left = 0 right = len(rotateArray) - 1 mid = 0 while rotateArray[left] &gt;= rotateArray[right]: # 二分结束标志 if right - left == 1: mid = right break mid = left + (right - left) // 2 # 特殊情况，如果二分位置的数据和左指针数据右指针数据都相等，此时无法进行判断，只能遍历查找最小值 if rotateArray[left] == rotateArray[mid] and rotateArray[mid] == rotateArray[right]: return self.minInorder(rotateArray, left, right) # 如果二分位置数据大于等于左指针数据，更新左指针 if rotateArray[mid] &gt;= rotateArray[left]: left = mid # 反之更新右指针 else: right = mid return rotateArray[mid] def minInorder(self, array, left, right): result = array[left] for i in range(left+1, right+1): if array[i] &lt; result: result = array[i] return result 调整数组顺序使奇数位于偶数前面题目输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变 思路创建双向队列，遍历数组，奇数前插入，偶数后插入。最后使用assign方法实现不同容器但相容的类型赋值。 代码12345678910111213141516171819class Solution: def reOrderArray2(self, array): # write code here res = [] array_length = len(array) for i in range(array_length): if array[l-i-1] % 2 == 0: res.insert(0, array[-i-1]) if array[i] % 2 == 0: res.insert(array[i]) return res def reOrderArray(self, array): pos = 0 for idx in range(len(array)): if array[idx] % 2: array.insert(pos, array.pop(idx)) pos += 1 return array 数组中出现次数超过一半的数字题目数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 思路数组中有一个数字出现的次数超过数组长度的一半，也就是说它出现的次数比其他所有数字出现次数的和还要多。因此我们可以考虑在遍历数组的时候保存两个值：一个是数组的一个数字，一个是次数。当我们遍历到下一个数字的时候，如果下一个数字和我们之前保存的数字相同，则次数加1；如果下一个数字和我们之前保存的数字不同，则次数减1。如果次数为零，我们需要保存下一个数字，并把次数设为1。由于我们要找的数字出现的次数比其他所有数字出现的次数之和还要多，那么要找的数字肯定是最后一次把次数设为1时对应的数字。 代码1234567891011121314151617181920212223242526class Solution: def MoreThanHalfNum_Solution(self, numbers): # write code here if not numbers: return 0 # 找到最后一个将counter置为1的数字，如果有目标值，它最有可能 maxTimesNumber = 0 counter = 0 for number in numbers: if counter == 0: maxTimesNumber = number counter = 1 elif maxTimesNumber != number: counter -= 1 elif maxTimesNumber == number: counter += 1 timeCounter = 0 for number in numbers: if number == maxTimesNumber: timeCounter += 1 return maxTimesNumber if len(numbers)//timeCounter&lt;2 else 0 return maxTimesNumber 连续子数组的最大和题目HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) 思路累加的子数组和，如果大于零，那么我们继续累加就行；否则，则需要剔除原来的累加和重新开始。 代码123456789101112131415class Solution: def FindGreatestSumOfSubArray(self, array): # write code here if not array: return 0 maxCount = array[0] curCount = array[0] for num in array[1:]: curCount += num if curCount &gt; maxCount: maxCount = curCount if curCount &lt;= 0: curCount = 0 return maxCount 把数组排成最小的数题目输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 思路遇到这个题，全排列当然可以做，但是时间复杂度为O(n!)。在这里我们自己定义一个规则，对拼接后的字符串进行比较。 排序规则如下： 若ab &gt; ba 则 a 大于 b， 若ab &lt; ba 则 a 小于 b， 若ab = ba 则 a 等于 b； 根据上述规则，我们需要先将数字转换成字符串再进行比较，因为需要串起来进行比较。比较完之后，按顺序输出即可。 代码12345678class Solution: def PrintMinNumber(self, numbers): # write code here if len(numbers) == 0: return '' compare = lambda a, b:cmp(str(a) + str(b), str(b) + str(a)) min_string = sorted(numbers, cmp = compare) return ''.join(str(s) for s in min_string) 数组中的逆序对题目在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 思路思路1： 暴力解法，顺序扫描整个数组，每扫描到一个数字的时候，逐个比较该数字和它后面的数字的大小。如果后面的数字比它小，则这两个数字就组成一个逆序对。假设数组中含有n个数字，由于每个数字都要和O(n)个数字作比较，因此这个算法的时间复杂度是O(n^2)。 思路2： 归并排序的改进版，把数据分成前后两个数组(递归分到每个数组仅有一个数据项)。合并数组，合并时，出现前面的数组值array[i]大于后面数组值array[j]时；则前面数组array[i]~array[mid]都是大于array[j]的，count += mid+1 - i。这个思路来自牛客网中 代码1234567891011121314151617181920212223242526272829class Solution: def InversePairs(self, data): # write code here global cnt cnt = 0 self.guibing(data) return cnt%1000000007 def guibing(self,data): global cnt if len(data) == 1: return data mid = len(data)//2 left = self.guibing(data[:mid]) right = self.guibing(data[mid:]) i = 0 j = 0 res = [] while i &lt; len(left) and j &lt; len(right): if left[i] &lt; right[j]: res.append(left[i]) i += 1 else: res.append(right[j]) cnt += len(left) - i #计算逆序对的数量 j += 1 res += left[i:] res += right[j:] return res 数字在排序数组中出现的次数题目统计一个数字在排序数组中出现的次数。 思路当然最好的方法是使用二分法先找到k的位置，在进行向前和向后遍历 代码1234567891011121314class Solution: def GetNumberOfK(self, data, k): # write code here i = 0 for i in range(len(data)): if data[i] == k: break n = 0 for item in data[i:]: if item == k: n += 1 else: break return n 数组中只出现一次的数字题目一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 思路我们可以将原始数组分成两个子数组，使得每个子数组包含一个只出现一次的数字，而其他数字都成对出现。这样，我们就可以用上述方法找到那个孤苦伶仃的元素。 我们还是从头到尾一次异或数组中的每一个数字，那么最终得到的结果就是两个只出现一次的数组的异或结果。因为其他数字都出现了两次，在异或中全部抵消了。由于两个数字肯定不一样，那么异或的结果肯定不为0，也就是说这个结果数组的二进制表示至少有一个位为1。我们在结果数组中找到第一个为1的位的位置，记为第n位。现在我们以第n位是不是1为标准把元数组中的数字分成两个子数组，第一个子数组中每个数字的第n位都是1，而第二个子数组中每个数字的第n位都是0。 举例：{2,4,3,6,3,2,5,5} 我们依次对数组中的每个数字做异或运行之后，得到的结果用二进制表示是0010。异或得到结果中的倒数第二位是1，于是我们根据数字的倒数第二位是不是1分为两个子数组。第一个子数组{2,3,6,3,2}中所有数字的倒数第二位都是1，而第二个子数组{4,5,5}中所有数字的倒数第二位都是0。接下来只要分别两个子数组求异或，就能找到第一个子数组中只出现一次的数字是6，而第二个子数组中只出现一次的数字是4。 代码123456789101112131415161718192021222324252627282930313233class Solution: # 返回[a,b] 其中ab是出现一次的两个数字 def FindNumsAppearOnce(self, array): # write code here if len(array) &lt; 2: return [] resultExclusiveOR = 0 # 将数组中的所有数字与或一遍，用来后面的分组条件 for item in array: resultExclusiveOR ^= item # 找到与或结果出现第一个一的索引 firstBitIs1 = self.FindFirstBitIsOne(resultExclusiveOR) # 遍历数组，分为两组数据进行相互与或，剩下的数字就是最终的两个数 num1, num2 = 0, 0 for item in array: if self.BitIsOne(firstBitIs1, item): num1 ^= item else: num2 ^= item return num1, num2 def FindFirstBitIsOne(self, num): indexBit = 0 while num &amp; 1 == 0 and indexBit &lt;= 32: indexBit += 1 num = num &gt;&gt; 1 return indexBit def BitIsOne(self, index, num): return (num &gt;&gt; index) &amp; 1 数组中重复的数字题目在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 思路1.可以把当前序列当成是一个下标和下标对应值是相同的数组（时间复杂度为O(n),空间复杂度为O(1)）；遍历数组，判断当前位的值和下标是否相等：若相等，则遍历下一位；2.若不等，则将当前位置i上的元素和a[i]位置上的元素比较：若它们相等，则找到了第一个相同的元素；若不等，则将它们两交换。换完之后a[i]位置上的值和它的下标是对应的，但i位置上的元素和下标并不一定对应；重复2的操作，直到当前位置i的值也为i，将i向后移一位，再重复2。 用数组的固定位置来存固定的数用来比较 代码123456789101112131415161718class Solution: # 这里要特别注意~找到任意重复的一个值并赋值到duplication[0] # 函数返回True/False def duplicate(self, numbers, duplication): # write code here n = len(numbers) if n == 0: return False for i in range(n): if numbers[i] &lt; 0 or numbers[i] &gt; n-1: return False for i in range(n): while numbers[i] != i: if numbers[i] == numbers[numbers[i]]: duplication[0] = numbers[i] return True numbers[numbers[i]], numbers[i] = numbers[i], numbers[numbers[i]] return False 构建乘积的数组题目给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]*A[1]…\A[i-1]*A[i+1]*…*A[n-1]。不能使用除法。（注意：规定B[0] = A[1] * A[2] * … * A[n-1]，B[n-1] = A[0] * A[1] * … * A[n-2];） 思路不断的将数组A第i 个数变成i 然后计算所有数的乘积 代码1234567891011121314151617class Solution: def multiply(self, A): # write code here B = [] if len(A) == 0: return B else: for i in range(len(A)): # 先将原本的数组A缓存 tmp = A[i] # 将第i位换成1 A[i] = 1 # 通过reduce， 计算所有数据乘积 B.append(reduce(lambda x,y:x*y, A)) # 重置数据A A[i] = tmp return B]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[替换空格题目请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 思路遍历 代码12345678910111213class Solution: # s 源字符串 def replaceSpace(self, s): # write code here target = "" i = 0 while i &lt; len(s): char = s[i] if char == " ": char = "%20" target += char i += 1 return target 字符串的排列题目输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 思路首先固定第一个字符，求后面所有字符的排列。这个时候我们仍把后面的所有字符分为两部分：后面的字符的第一个字符，以及这个字符之后的所有字符。然后把第一个字符逐一和它后面的字符交换。这个思路，是典型的递归思路。 代码12345678910111213141516171819202122232425class Solution: def __init__(self): self.result = [] def Permutation(self, ss): # write code here if len(ss) == 0: return [] # 进行切分组合 self.PermutationCore(list(ss), 0) sorted(self.result) return self.result def PermutationCore(self, char_list, begin): # 递归的终止条件,当起始位置等于字符串长度的时候，已经完成了这一次拼接 if begin == len(char_list): self.result.append("".join(char_list)) return for i in range(begin, len(char_list)): # 过滤掉字符串中和首字母相等的元素再排在首位，避免重复递归 if i != begin and char_list[i] == char_list[begin]: continue # 通过交换的方式，确定所有可能得首字母 char_list = list("".join(char_list)) char_list[i], char_list[begin] = char_list[begin], char_list[i] # 将剩下的字符递归进行排列组合 self.PermutationCore(char_list, begin+1) 第一个只出现一次的字符题目在一个字符串(0&lt;=字符串长度&lt;=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）.（从0开始计数） 思路建立一个哈希表，第一次扫描的时候，统计每个字符的出现次数。第二次扫描的时候，如果该字符出现的次数为1，则返回这个字符的位置。 代码12345678910111213141516class Solution: def FirstNotRepeatingChar(self, s): # write code here length = len(s) if length == 0: return -1 item = &#123;&#125; for i in range(length): if s[i] not in item.keys(): item[s[i]] = 1 else: item[s[i]] += 1 for i in range(length): if item[s[i]] == 1: return i return -1 左旋转字符串题目汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 思路裁剪 代码12345678class Solution: def LeftRotateString(self, s, n): length = len(s) if n &lt;= 0 or length == 0: return s if n &gt; length: n = n % length return s[n:] + s[:n] 翻转单词顺序序列题目牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 思路重新拼接 代码12345class Solution: def ReverseSentence(self, s): # write code here s_list = s.split(' ') return ' '.join(s_list[::-1]) 把字符串转换成整数题目将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0 思路这道题要考虑全面，对异常值要做出处理。 对于这个题目，需要注意的要点有： 指针是否为空指针以及字符串是否为空字符串；字符串对于正负号的处理；输入值是否为合法值，即小于等于’9’，大于等于’0’；int为32位，需要判断是否溢出；使用错误标志，区分合法值0和非法值0。代码中用两个函数来实现该功能，其中标志位g_nStatus用来表示是否为异常输出，minus标志位用来表示是否为负数。 代码1234567891011121314151617181920212223242526272829303132class Solution: def StrToInt(self, s): # write code here length = len(s) if length == 0: return 0 else: # 设置负数和是否有正负符号标志位 minus = False flag = False if s[0] == '+': flag = True if s[0] == '-': flag = True minus = True begin = 0 # 如果有正负符号标志位, 数字从1开始 if flag: begin = 1 num = 0 # 确定是否有负数系数 minus = -1 if minus else 1 # 不断遍历数字，将其*10 + 个数位 for each in s[begin:]: # 如果数字大于0 并小于9, 否则为非法字符 if each &gt;= '0' and each &lt;= '9': # 用 ord 计算字符和0的二进制数字差 num = num * 10 + minus * (ord(each) - ord('0')) else: num = 0 break return num 正则表达式匹配题目请实现一个函数用来匹配包括’.’和’‘的正则表达式。模式中的字符’.’表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”abaca”匹配，但是与”aa.a”和”ab*a”均不匹配 思路第一种方法:如果模式串中有星号，它会出现在第二个位置，即 \text{pattern[1]}pattern[1] 。这种情况下，我们可以直接忽略模式串中这一部分，或者删除匹配串的第一个字符，前提是它能够匹配模式串当前位置字符，即 \text{pattern[0]}pattern[0] 。如果两种操作中有任何一种使得剩下的字符串能匹配，那么初始时，匹配串和模式串就可以被匹配。 代码123456789101112131415class Solution(object): def match(self, text, pattern): # 如果没有匹配符号，并且没有了字符串，递归终结条件 if not pattern: return not text # 判断第一个字符和匹配符是否相匹配 first_match = bool(text) and pattern[0] in &#123;text[0], '.'&#125; # 如果第二个字符是* if len(pattern) &gt;= 2 and pattern[1] == '*': return (first_match and self.match(text[1:], pattern)) or self.match(text, pattern[2:]) # 如果不是* else: return first_match and self.match(text[1:], pattern[1:]) 表示数值的字符串题目请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 思路这道题还是比较简单的。表示数值的字符串遵循如下模式： [sign]integral-digits[.[fractional-digits]][e|E[sign]exponential-digits] 其中，(‘[‘和’]’之间的为可有可无的部分)。 在数值之前可能有一个表示正负的’+’或者’-‘。接下来是若干个0到9的数位表示数值的整数部分（在某些小数里可能没有数值的整数部分）。如果数值是一个小数，那么在小数后面可能会有若干个0到9的数位表示数值的小数部分。如果数值用科学记数法表示，接下来是一个’e’或者’E’，以及紧跟着的一个整数（可以有正负号）表示指数。 判断一个字符串是否符合上述模式时，首先看第一个字符是不是正负号。如果是，在字符串上移动一个字符，继续扫描剩余的字符串中0到9的数位。如果是一个小数，则将遇到小数点。另外，如果是用科学记数法表示的数值，在整数或者小数的后面还有可能遇到’e’或者’E’。 代码1234567891011121314151617181920212223242526272829303132class Solution: # s字符串 def isNumeric(self, s): # write code here # 标记符号、小数点、e是否出现过 sign = False decimal = False hasE = False for i in range(len(s)): # 如果字符是e，进行异常条件判断 if s[i] == 'e' or s[i] == 'E': if i == len(s)-1: # e后面一定要接数字, 如果是最后一位，直接报错 return False if hasE: # 不能同时存在两个e return False hasE = True # 如果字符是+-, 进行异常条件判断 elif s[i] == '+' or s[i] == '-': if sign and s[i-1] != 'e' and s[i-1] != 'E': # 第二次出现+-符号，则必须紧接在e之后 return False elif sign == False and i &gt; 0 and s[i-1] != 'e' and s[i-1] != 'E': # 第一次出现+-符号，且不是在字符串开头，则也必须紧接在e之后 return False sign = True # 如果字符是小数点，进行异常条件的判断 elif s[i] == '.': if (hasE or decimal): # e后面不能接小数点，小数点不能出现两次 return False decimal = True # 如果字符是非法字符，进行异常条件判断 elif s[i] &lt; '0' or s[i] &gt; '9': return False return True]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E9%80%92%E5%BD%92%2F</url>
    <content type="text"><![CDATA[斐波那契数列题目大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0，第1项是1）。 n&lt;=39 思路斐波那契 代码123456789101112class Solution: def Fibonacci(self, n): # write code here # write code here if n &lt;= 1: return n first, second, third = 0, 1, 0 for i in range(2, n+1): third = first + second first = second second = third return third 跳台阶题目一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果） 思路首先我们考虑最简单的情况。如果只有1级台阶，那么显然只一种跳法。 如果有2级台阶，那就有两种跳法：一种是分两次跳，每次跳1级；另一种是一次跳2级。 接着，我们来讨论一般情况。 我们把n级台阶时的跳法看成是n的函数，记为f(n)。 当n&gt;2时，第一次跳的时候就有两种不同的选择： 一是第一次只跳1级，此时跳法数目等于后面剩下的n-1级台阶的跳法数目，即为f(n-1)； 另外一种选择是跳一次跳2级，此时跳法数目等于后面剩下的n-2级台阶的跳法数目，即为f(n-2)。 因此n级台阶的不同跳法的总数f(n)=f(n-1)+f(n-2)。 分析到这里，我们不难看出这实际上就是斐波那契数列了。 代码1234567891011class Solution: def jumpFloor(self, number): # write code here if number &lt; 3: return number first, second, third = 1, 2, 0 for i in range(3, number+1): third = first + second first = second second = third return third 变态跳台阶题目一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 思路同上 代码123456class Solution: def jumpFloorII(self, number): # write code here if number &lt;= 2: return number return 2 ** (number-1) 矩形覆盖题目我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 比如n=3时，2*3的矩形块有3种覆盖方法： 思路菲波那切数列 代码1234567891011class Solution: def rectCover(self, number): # write code here if number &lt; 3: return number first, second, third = 1, 2, 0 for i in range(3, number+1): third = first + second first = second second = third return third]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表]]></title>
    <url>%2F2020%2F02%2F02%2F06.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%89%91%E6%8C%87offer%2F%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[从头到尾打印链表题目输入一个链表，返回一个反序的链表 思路通常，这种情况下，我们不希望修改原链表的结构。返回一个反序的链表，这就是经典的“后进先出”，我们可以使用栈实现这种顺序。每经过一个结点的时候，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出结点的值，给一个新的链表结构，这样链表就实现了反转。 代码1234567891011121314# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): # write code here result = [] while listNode: result.insert(0, listNode.val) listNode = listNode.next return result 链表中倒数第K个节点题目输入一个链表，输出该链表中倒数第k个结点。 思路我们可以定义两个指针。第一个指针从链表的头指针开始遍历向前走k-1，第二个指针保持不动；从第k步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在k-1，当第一个（走在前面的）指针到达链表的尾结点时，第二个指针（走在后面的）指针正好是倒数第k个结点。 代码123456789101112131415161718192021# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindKthToTail(self, head, k): # write code here if not head or k == 0: return None before_node = head after_node = None search_times = 0 while before_node: search_times += 1 before_node = before_node.next if search_times == k: after_node = head elif search_times &gt; k: after_node = after_node.next return after_node 反转链表题目输入一个链表，反转链表后，输出新链表的表头。 思路这个很简单，我们使用三个指针，分别指向当前遍历到的结点、它的前一个结点以及后一个结点。 在遍历的时候，做当前结点的尾结点和前一个结点的替换。 代码1234567891011121314151617# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead or not pHead.next: return pHead last = None while pHead: tmp = pHead.next pHead.next = last last = pHead pHead = tmp return last 合并两个排序链表题目输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 思路先判断输入的链表是否为空的指针。如果第一个链表为空，则直接返回第二个链表；如果第二个链表为空，则直接返回第一个链表。如果两个链表都是空链表，合并的结果是得到一个空链表。 两个链表都是排序好的，我们只需要从头遍历链表，判断当前指针，哪个链表中的值小，即赋给合并链表指针即可。使用递归就可以轻松实现。 代码12345678910111213141516171819# class ListNode:# def __init__(self, x):# self.val = x# self.next = None#使用递归就可以轻松实现。class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): if not pHead1: return pHead2 if not pHead2: return pHead1 if pHead1.val &lt; pHead2.val: pHead1.next = self.Merge(pHead1.next, pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1, pHead2.next) return pHead2 复杂链表的复制题目输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针random指向一个随机节点），请对此链表进行深拷贝，并返回拷贝后的头结点。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 思路我们这里采用三步：第一步：复制复杂指针的label和next。但是这次我们把复制的结点跟在元结点后面，而不是直接创建新的链表；第二步：设置复制出来的结点的random。因为新旧结点是前后对应关系，所以也是一步就能找到random；第三步：拆分链表。奇数是原链表，偶数是复制的链表。 代码12345678910111213141516171819202122232425262728293031323334353637383940class Solution: # 返回 RandomListNode def Clone(self, pHead): if not pHead: return None dummy = pHead # first step, N' to N next while dummy: dummynext = dummy.next copynode = RandomListNode(dummy.label) copynode.next = dummynext dummy.next = copynode dummy = dummynext dummy = pHead # second step, random' to random' while dummy: dummyrandom = dummy.random copynode = dummy.next if dummyrandom: copynode.random = dummyrandom.next dummy = copynode.next # third step, split linked list dummy = pHead copyHead = pHead.next while dummy: copyNode = dummy.next dummynext = copyNode.next dummy.next = dummynext if dummynext: copyNode.next = dummynext.next else: copyNode.next = None dummy = dummynext return copyHead 两个链表的第一个公共节点题目输入两个链表，找出它们的第一个公共结点。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的） 思路我们可以把两个链表拼接起来，一个pHead1在前pHead2在后，一个pHead2在前pHead1在后。这样，生成了两个相同长度的链表，那么我们只要同时遍历这两个表，就一定能找到公共结点。时间复杂度O(m+n)，空间复杂度O(m+n)。 代码123456789101112131415# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here if pHead1 == None or pHead2 == None: return None cur1, cur2 = pHead1, pHead2 while cur1 != cur2: cur1 = cur1.next if cur1 != None else pHead2 cur2 = cur2.next if cur2 != None else pHead1 return cur1 链表中环的入口节点题目给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 思路可以用两个指针来解决这个问题。先定义两个指针P1和P2指向链表的头结点。如果链表中的环有n个结点，指针P1先在链表上向前移动n步，然后两个指针以相同的速度向前移动。当第二个指针指向的入口结点时，第一个指针已经围绕着揍了一圈又回到了入口结点。以下图为例，指针P1和P2在初始化时都指向链表的头结点。由于环中有4个结点，指针P1先在链表上向前移动4步。接下来两个指针以相同的速度在链表上向前移动，直到它们相遇。它们相遇的结点正好是环的入口结点。 现在，关键问题在于怎么知道环中有几个结点呢？可以使用快慢指针，一个每次走一步，一个每次走两步。如果两个指针相遇，表明链表中存在环，并且两个指针相遇的结点一定在环中。 随后，我们就从相遇的这个环中结点出发，一边继续向前移动一边计数，当再次回到这个结点时，就可以得到环中结点数目了。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: def EntryNodeOfLoop(self, pHead): # write code here if pHead == None: return None # 判断环是否存在，并找到一个存在于环内的节点 meetingnode = self.MeetingNode(pHead) if meetingnode == None: return None # 通过将节点在环内遍历一次，确定环的长度 nodeslop = 1 node1 = meetingnode while node1.next != meetingnode: node1 = node1.next nodeslop += 1 # 找到距离头部节点环长度的节点 node1 = pHead for _ in range(nodeslop): node1 = node1.next # 设置一个后来节点，和上一个节点一起遍历，当他们相遇的时候，就是节点的入口 node2 = pHead while node1 != node2: node1 = node1.next node2 = node2.next return node1 # 先找到快慢节点相遇的节点， 存在的节点就处在环内 def MeetingNode(self, pHead): slow = pHead.next # 慢指针 if slow == None: return None fast = slow.next # 快指针 while fast != None and slow != None: # 任意一个指针为Nonde 说明不存在环 # 快慢节点相遇，找到存在于环内的节点 if slow == fast: return fast slow = slow.next fast = fast.next if fast != None: fast = fast.next return None 删除链表中重复节点题目在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 思路删除重复结点，只需要记录当前结点前的最晚访问过的不重复结点pPre、当前结点pCur、指向当前结点后面的结点pNext的三个指针即可。如果当前节点和它后面的几个结点数值相同，那么这些结点都要被剔除，然后更新pPre和pCur；如果不相同，则直接更新pPre和pCur。 需要考虑的是，如果第一个结点是重复结点我们该怎么办？这里我们分别处理一下就好，如果第一个结点是重复结点，那么就把头指针pHead也更新一下。 代码1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteDuplication(self, pHead): # write code here pPre = None # 记录前一个加点的指针，用来将新的不重复节点放到其后面 pCur = pHead # 记录当前处理的节点 while pCur != None: # 如果下一个几点不是None， 并且下一个节点的数据和当前节点数据相等 if pCur.next != None and pCur.val == pCur.next.val: pNext = pCur.next # 往下寻找不相等的节点 while pNext.next != None and pNext.next.val == pCur.val: pNext = pNext.next # 如果当前节点是头结点，将pNext.next 赋值给pHead if pCur == pHead: pHead = pNext.next # 否则将pNext.next 赋值给pPre.next else: pPre.next = pNext.next pCur = pNext.next else: pPre = pCur pCur = pCur.next return pHead]]></content>
      <categories>
        <category>数据结构与算法</category>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop HA高可用]]></title>
    <url>%2F2020%2F01%2F17%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHADOOP%2Fhadoop%20HA%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[HADOOP的HA机制正式引入HA机制是从hadoop2.0开始，之前的版本中没有HA机制 HA的运作机制介绍所谓HA，即高可用（7*24小时不中断服务） 实现高可用最关键的是消除单点故障 hadoop-ha严格来说应该分成各个组件的HA机制——HDFS的HA、YARN的HA HDFS的HA机制详解HDFS怎么实现HA机制 通过双namenode消除单点故障 双namenode的工作方式 元数据管理 元数据管理方式与传统方式不同，在每个namenode内存中各自保存一份元数据 Edits日志只能有一份，只有Active状态的namenode节点可以做写操作， 两个namenode都可以读取edits 共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现） 状态管理功能模块 实现了一个zkfailover，常驻在每一个namenode所在的节点 每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识 当需要进行状态切换时，由zkfailover来负责切换 切换时需要防止brain split现象的发生，即程序假死现象，会导致脑裂发生，所以需要在切换程序的时候，强制停止异常程序 服务名称 双namenode 对外提供服务会用一个逻辑名称来表示，所以client只需要请求namenode的逻辑名称即可，不用关注哪个namenode正在对外提供服务 HDFS的HA图解 YARN 的HA机制YARN怎么实现HA机制 YARN-resource manager 也是通过双节点的方式，来实现HA的 双resource manager的工作方式 状态管理 Yarn的高可用状态管理相对于HDFS非常简单，双resource通过zookeeper来同步每个节点的状态，根据对方的状态来更改自己的服务状态 如果客户端正在运行程序，这是resource发生了崩溃，这时resource会自动切换到另外一台机器上，MAPREDUCE会自动重试 服务名称 双resource 对外提供服务会用一个逻辑名称来表示，client不用关注哪个namenode正在对外提供服务 YARN高可用图解 HDFS联邦机制(namenode水平扩展)HDFS HA 还存在的问题 系统扩展性方面，元数据存储在NN内存中，会受到NN内存上限的制约。 整体性能方面，吞吐量受单个NN的影响。 隔离性方面，一个程序可能会影响其他运行的程序，如一个程序消耗过多资源导致其他程序无法顺利运行。HDFS HA本质上还是单名称节点。 HDFS联邦的作用 在HDFS联邦中，设计了多个相互独立的NN，使得HDFS的命名服务能够水平扩展，这些NN分别进行各自命名空间和块的管理，不需要彼此协调。每个DN要向集群中所有的NN注册，并周期性的发送心跳信息和块信息，报告自己的状态。 HDFS联邦拥有多个独立的命名空间，其中，每一个命名空间管理属于自己的一组块，这些属于同一个命名空间的块组成一个“块池”。每个DN会为多个块池提供块的存储，块池中的各个块实际上是存储在不同DN中的。 HDFS联邦图解 NameNode的安全模式什么是namenode的安全模式 在name刚启动的时候，内存中只有文件和文件的块id以及副本数量，但是并不知道所有块在哪个datanode上 namenode这个时候需要等待所有的datanode向他汇报自身持有的块信息，namenode才能再元数据中补全文件块信息中的位置信息 只有当namenode找到99.8%的块位置信息，才会退出安全模式，正常对外提供服务 namenode 安全模式图解 HA集群安装部署前期准备 准备7台主机(以7台来举例 mini1, mini2, mini3, mini4, mini5, mini6, mini7) 修改linux主机名 修改/etc/hosts 域名映射 关闭防火墙 配置ssh免密登录 安装JDK，配置JAVA环境变量 编译HADOOP安装包hadoop-2.6.4 集群规划1234567mini1 192.168.1.200 jdk、hadoop NameNode、DFSZKFailoverController(zkfc)mini2 192.168.1.201 jdk、hadoop NameNode、DFSZKFailoverController(zkfc)mini3 192.168.1.202 jdk、hadoop ResourceManager mini4 192.168.1.203 jdk、hadoop ResourceManagermini5 192.168.1.205 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMainmini6 192.168.1.206 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMainmini7 192.168.1.207 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain 说明: 在hadoop2.0中通常由两个NameNode组成，一个处于active状态，另一个处于standby状态。Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步active namenode的状态，以便能够在它失败时快速进行切换。hadoop2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这里我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode这里还配置了一个zookeeper集群，用于ZKFC（DFSZKFailoverController）故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为standby状态 hadoop-2.2.0中依然存在一个问题，就是ResourceManager只有一个，存在单点故障，hadoop-2.6.4解决了这个问题，有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调 安装步骤一. 配置zookeeper集群 解压zookeeper 1tar -zxvf zookeeper-3.4.5.tar.gz -C /home/hadoop/app/ 修改配置 1234567891011cd /home/hadoop/app/zookeeper-3.4.5/conf/cp zoo_sample.cfg zoo.cfgvim zoo.cfg# 修改 dataDir=/home/hadoop/app/zookeeper-3.4.5/tmp# 添加server.1=mini5:2888:3888server.2=mini6:2888:3888server.3=mini7:2888:3888# 创建tmp文件夹mkdir /home/hadoop/app/zookeeper-3.4.5/tmpecho 1 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid 拷贝zk到集群其他节点 12scp -r /home/hadoop/app/zookeeper-3.4.5/ mini6:/home/hadoop/app/scp -r /home/hadoop/app/zookeeper-3.4.5/ mini7:/home/hadoop/app/ 注意要修改其他节点的myid内容hadoop06： echo 2 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myidhadoop07： echo 3 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid 二. 配置hadoop集群 解压hadoop 1tar -zxvf hadoop-2.6.4.tar.gz -C /home/hadoop/app/ 配置HDFS 添加环境变量 12345678#将hadoop添加到环境变量中vim /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_55export HADOOP_HOME=/hadoop/hadoop-2.6.4export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin#hadoop2.0的配置文件全部在$HADOOP_HOME/etc/hadoop下cd /home/hadoop/app/hadoop-2.6.4/etc/hadoop 修改hadoop-env.sh 12vi hadoop-env.sh# 修改 export JAVA_HOME=/home/hadoop/app/jdk1.7.0_55 修改配置文件 core-site.xml 123456789101112131415161718&lt;configuration&gt;&lt;!-- 指定hdfs的nameservice为ns1 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://bi/&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定hadoop临时目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/app/hdpdata/&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定zookeeper地址 --&gt;&lt;property&gt;&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;&lt;value&gt;mini5:2181,mini6:2181,mini7:2181&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;configuration&gt;&lt;!--指定hdfs的nameservice为bi，需要和core-site.xml中的保持一致 --&gt;&lt;property&gt;&lt;name&gt;dfs.nameservices&lt;/name&gt;&lt;value&gt;bi&lt;/value&gt;&lt;/property&gt;&lt;!-- bi下面有两个NameNode，分别是nn1，nn2 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.bi&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;!-- nn1的RPC通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.bi.nn1&lt;/name&gt;&lt;value&gt;mini1:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn1的http通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.bi.nn1&lt;/name&gt;&lt;value&gt;mini1:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的RPC通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.bi.nn2&lt;/name&gt;&lt;value&gt;mini2:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的http通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.bi.nn2&lt;/name&gt;&lt;value&gt;mini2:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定NameNode的edits元数据在JournalNode上的存放位置 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;&lt;value&gt;qjournal://mini5:8485;mini6:8485;mini7:8485/bi&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;&lt;property&gt;&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/journaldata&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启NameNode失败自动切换 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置失败自动切换实现方式 --&gt;&lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.bi&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;sshfenceshell(/bin/true)&lt;/value&gt;&lt;/property&gt;&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置sshfence隔离机制超时时间 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 1234567&lt;configuration&gt;&lt;!-- 指定mr框架为yarn方式 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt;&lt;!-- 开启RM高可用 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定RM的cluster id --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;&lt;value&gt;yrc&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定RM的名字 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;&lt;value&gt;rm1,rm2&lt;/value&gt;&lt;/property&gt;&lt;!-- 分别指定RM的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;&lt;value&gt;mini3&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;&lt;value&gt;mini4&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定zk集群地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;&lt;value&gt;mini5:2181,mini6:2181,mini7:2181&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; slave文件 12345# slaves是指定子节点的位置，因为要在hadoop01上启动HDFS、在hadoop03启动yarn，所以hadoop01上的slaves文件指定的是datanode的位置，hadoop03上的slaves文件指定的是nodemanager的位置mini5mini6mini7 拷贝项目到其他节点 配置免密登录 123456789101112131415161718192021222324#首先要配置mini1到mini2、mini3、mini4、mini5、mini6、mini7的免密码登陆#在mini1上生产一对钥匙ssh-keygen -t rsa#将公钥拷贝到其他节点，包括自己ssh-coyp-id mini1ssh-coyp-id mini2ssh-coyp-id mini3ssh-coyp-id mini4ssh-coyp-id mini5ssh-coyp-id mini6ssh-coyp-id mini7#配置mini3到mini4、mini5、mini6、mini7的免密码登陆#在mini3上生产一对钥匙ssh-keygen -t rsa#将公钥拷贝到其他节点ssh-coyp-id mini3 ssh-coyp-id mini4ssh-coyp-id mini5ssh-coyp-id mini6ssh-coyp-id mini7#注意：两个namenode之间要配置ssh免密码登陆，别忘了配置hadoop02到hadoop01的免登陆在mini2上生产一对钥匙ssh-keygen -t rsassh-coyp-id -i mini1 将配置好的hadoop拷贝到其他节点 123456scp -r /home/hadoop/app/hadoop-2.6.4 mini2:/home/hadoop/app/hadoop-2.6.4scp -r /home/hadoop/app/hadoop-2.6.4 mini3:/home/hadoop/app/hadoop-2.6.4scp -r /home/hadoop/app/hadoop-2.6.4 mini4:/home/hadoop/app/hadoop-2.6.4scp -r /home/hadoop/app/hadoop-2.6.4 mini5:/home/hadoop/app/hadoop-2.6.4scp -r /home/hadoop/app/hadoop-2.6.4 mini6:/home/hadoop/app/hadoop-2.6.4scp -r /home/hadoop/app/hadoop-2.6.4 mini7:/home/hadoop/app/hadoop-2.6.4 三.启动Hadoop HA 启动zookeeper集群（分别在mini5、mini6、mini7上启动zk） 1234cd /hadoop/zookeeper-3.4.5/bin/./zkServer.sh start# 查看状态：一个leader，两个follower./zkServer.sh status 启动journalnode（分别在在mini5、mini6、mini7上执行) 123cd /hadoop/hadoop-2.6.4sbin/hadoop-daemon.sh start journalnode# 运行jps命令检验，hadoop05、hadoop06、hadoop07上多了JournalNode进程 格式化HDFS 12345# 在mini1上执行命令:hdfs namenode -format# 格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件，这里我配置的是/hadoop/hadoop-2.6.4/tmp，然后将/hadoop/hadoop-2.6.4/tmp拷贝到hadoop02的/hadoop/hadoop-2.6.4/下。scp -r tmp/ hadoop02:/home/hadoop/app/hadoop-2.6.4/# 也可以这样，建议hdfs namenode -bootstrapStandby 格式化ZKFC(在mini1上执行一次即可) 1hdfs zkfc -formatZK 启动HDFS(在mini1上执行) 1sbin/start-dfs.sh 启动YARN 12# 启动YARN，是在hadoop02上执行start-yarn.sh，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动)sbin/start-yarn.sh 验证集群 到此，hadoop-2.6.4配置并安装完毕 浏览器访问验证 1234http://hadoop00:50070NameNode 'hadoop01:9000' (active)http://hadoop01:50070NameNode 'hadoop02:9000' (standby) 验证HDFS HA 12345678910111213141516# 首先向hdfs上传一个文件hadoop fs -put /etc/profile /profilehadoop fs -ls /# 然后再kill掉active的NameNodekill -9 &lt;pid of NN&gt;# 通过浏览器访问：http://192.168.1.202:50070# NameNode 'hadoop02:9000' (active)# 这个时候hadoop02上的NameNode变成了active# 在执行命令：hadoop fs -ls /# -rw-r--r-- 3 root supergroup 1926 2014-02-06 15:36 /profile# 刚才上传的文件依然存在！！！# 手动启动那个挂掉的NameNodesbin/hadoop-daemon.sh start namenode# 通过浏览器访问：http://192.168.1.201:50070# NameNode 'hadoop01:9000' (standby) 集群工作状态查询查看hdfs的各节点状态信息 1bin/hdfs dfsadmin -report 获取一个namenode节点的HA状态 1bin/hdfs haadmin -getServiceState nn1 单独启动一个namenode进程 12 单独启动一个zkfc进程 1./hadoop-daemon.sh start zkfc 集群运维测试Datanode 动态上下线Datanode动态上下线很简单，步骤如下： 准备一台服务器，设置好环境 部署hadoop的安装包，并同步集群配置 联网上线，新datanode会自动加入集群 如果是一次增加大批datanode，还应该做集群负载重均衡 Namenode 状态切换管理使用的命令 1hdfs haadmin 查看namenode工作状态 1hdfs haadmin -getServiceState nn1 将standby状态namenode切换到active 1hdfs haadmin –transitionToActive nn1 将active状态namenode切换到standby 1hdfs haadmin –transitionToStandby nn2 数据块的balance启动balancer的命令： 1start-balancer.sh -threshold 8 运行之后，会有Balancer进程出现： 上述命令设置了Threshold为8%，那么执行balancer命令的时候，首先统计所有DataNode的磁盘利用率的均值，然后判断如果某一个DataNode的磁盘利用率超过这个均值Threshold，那么将会把这个DataNode的block转移到磁盘利用率低的DataNode，这对于新节点的加入来说十分有用。Threshold的值为1到100之间，不显示的进行参数设置的话，默认是10。 HA 下的hdfs-api变化客户端需要nameservice的配置信息，其他不变 12345678910111213141516171819202122/** * 如果访问的是一个ha机制的集群 * 则一定要把core-site.xml和hdfs-site.xml配置文件放在客户端程序的classpath下 * 以让客户端能够理解hdfs://ns1/中 “ns1”是一个ha机制中的namenode对——nameservice * 以及知道ns1下具体的namenode通信地址 * @author * */public class UploadFile &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://ns1/"); FileSystem fs = FileSystem.get(new URI("hdfs://ns1/"),conf,"hadoop"); fs.copyFromLocalFile(new Path("g:/eclipse-jee-luna-SR1-linux-gtk.tar.gz"), new Path("hdfs://ns1/")); fs.close(); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>HADOOP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAPREDUCE]]></title>
    <url>%2F2020%2F01%2F10%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHADOOP%2FMAPREDUCE%2F</url>
    <content type="text"><![CDATA[MAPREDUCE 介绍MAPREDUCE简介Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架； Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上； 为什么使用MAPREDUCE 海量数据在单机上在单机上处理硬件资源限制，无法胜任 而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度 引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理 可见在程序由单机版扩成分布式时，会引入大量的复杂工作。为了提高开发效率，可以将分布式程序中的公共功能封装成框架，让开发人员可以将精力集中于业务逻辑。 MAPREDUCE框架设计思想 MAPREDUCE结构与运行流程MAPREDUCE设计结构一个完成的MAPREDUCE程序在分布式运行时有三类实例进程: MRAppMaster: 负责整个程序的过程调度及状态协调 mapTask: 负责map阶段的整个数据处理流程 ReduceTask: 负责 reduce 阶段的整个数据处理流程 MAPREDUCE运行流程示意图 流程解析 一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程 maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为： a. 利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对 b. 将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存 c. 将缓存中的KV对按照K分区排序后不断溢写到磁盘文件 MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区） Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储 MRclient提交MR程序给MR框架的流程 MAPREDUCE并行度机制maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度 那么，mapTask并行实例是否越多越好呢？其并行度又是如何决定呢？ mapTask 并行度的决定机制mapTask并行度规则一个job的map阶段并行度由客户端在提交job时决定 而客户端对map阶段并行度的规划的基本逻辑为： 将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理 这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图： FileInputFormat 切片机制切片机制 (TextInputFormat示例) FileInputFormat 用来读取数据，其本身为一个抽象类，继承自 InputFormat 抽象类，针对不同的类型的数据有不同的子类来处理； FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLinelnputFormat、CombineTextInputFormat 和自定义 ImputFormat 等。 切片定义在MRclient的FileInputFormat类中的getSplit()方法中 FileInputFormat中默认的切片机制: a. 简单地按照文件的内容长度进行切片 b. 切片大小，默认等于block大小 c. 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件: 12file1.txt 320Mfile2.txt 10M 经过FileInputFormat的切片机制运算后，行程的切片信息如下: 1234file1.txt.split1-- 0~128file1.txt.split2-- 128~256file1.txt.split3-- 256~320file2.txt.split1-- 0~10M FileInputFormat 中切片的大小参数配置 通过分析源码，在FileInputFormat中，计算切片大小的逻辑： 1Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定： minsize: 默认值：1, 配置参数： mapreduce.input.fileinputformat.split.minsize maxsize：默认值：Long.MAXValue, 配置参数：mapreduce.input.fileinputformat.split.maxsize 因此，默认情况下，切片大小=blocksize maxsize（切片最大值）： 参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值 minsize （切片最小值）： 参数调的比blockSize大，则可以让切片变得比blocksize还大 配置并发数的影响因素: 运算节点的硬件配置 运算任务的类型: CPU密集型还是IO密集型 运算任务的数量 小文件切片优化 如果是小文件，就会产生大量的小切片，造成大量的maptask运行 解决办法: 1. 从源头上解决问题，文件合并后再上传处理 可以用另外一种InputFormat: CombineInputFormat（可以将多个文件划分到一个切片中）, 可以设置每个切片的最小容量和最大容量; MAPTASK 并行度建议设置 如果硬件配置为212core + 64G，恰当的map并行度是大约每个节点20-100个map，*最好每个map的执行时间至少一分钟。** 如果job的每个map或者 reduce task的运行时间都只有30-40秒钟，那么就减少该job的map或者reduce数，每一个task(map|reduce)的setup和加入到调度器中进行调度，这个中间的过程可能都要花费几秒钟，所以如果每个task都非常快就跑完了，就会在task的开始和结束的时候浪费太多的时间。 配置task的JVM重用可以改善该问题：（mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM） 如果input的文件非常的大，比如1TB，可以考虑将hdfs上的每个block size设大，比如设成256MB或者512MB ReduceTask并行度的决定ReduceTask 设置方式reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置： 12//默认值是1，手动设置为4job.setNumReduceTasks(4); 如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜 注意： reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask，尽量不要运行太多的reduce task。 当设置reduceTask数量为0的时候，mapreduce 不会执行shuffle以及后面的操作，会直接将map的结果输出到outputPath目录，此时有多少个mapTask就有多少个文件。 如果不设置reduceTask数量，mapreduce将会默认启动一个reduceTask， 用来将所有map的结果进行整理，并输出到统一文件到输出目录。 MAPREDUCE 编程规范编写规范 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper的输入数据是KV对的形式（KV的类型可自定义） Mapper的输出数据是KV对的形式（KV的类型可自定义） Mapper中的业务逻辑写在map()方法中 map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 用户自定义的Mapper和Reducer都要继承各自的父类 整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 MAPREDUCE示例 需求: 在一堆给定的文本文件中统计输出每一个单词出现的总次数 定义mapper类 1234567891011121314151617181920//首先要定义四个泛型的类型//keyin: LongWritable valuein: Text//keyout: Text valueout:IntWritablepublic class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; //map方法的生命周期： 框架每传一行数据就被调用一次 //key : 这一行的起始点在文件中的偏移量 //value: 这一行的内容 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //拿到一行数据转换为string String line = value.toString(); //将这一行切分出各个单词 String[] words = line.split(" "); //遍历数组，输出&lt;单词，1&gt; for(String word:words)&#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; 定义reduce类 123456789101112//生命周期：框架每传递进来一个kv 组，reduce方法被调用一次 @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //定义一个计数器 int count = 0; //遍历这一组kv的所有v，累加到count中 for(IntWritable value:values)&#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; 定义主类，描述并提交job 123456789101112131415161718192021222324252627public class WordCountRunner &#123; //把业务逻辑相关的信息（哪个是mapper，哪个是reducer，要处理的数据在哪里，输出的结果放哪里……）描述成一个job对象 //把这个描述好的job提交给集群去运行 public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job wcjob = Job.getInstance(conf); //指定我这个job所在的jar包// wcjob.setJar("/home/hadoop/wordcount.jar"); wcjob.setJarByClass(WordCountRunner.class); wcjob.setMapperClass(WordCountMapper.class); wcjob.setReducerClass(WordCountReducer.class); //设置我们的业务逻辑Mapper类的输出key和value的数据类型 wcjob.setMapOutputKeyClass(Text.class); wcjob.setMapOutputValueClass(IntWritable.class); //设置我们的业务逻辑Reducer类的输出key和value的数据类型 wcjob.setOutputKeyClass(Text.class); wcjob.setOutputValueClass(IntWritable.class); //指定要处理的数据所在的位置 FileInputFormat.setInputPaths(wcjob, "hdfs://hdp-server01:9000/wordcount/data/big.txt"); //指定处理完成之后的结果所保存的位置 FileOutputFormat.setOutputPath(wcjob, new Path("hdfs://hdp-server01:9000/wordcount/output/")); //向yarn集群提交这个job boolean res = wcjob.waitForCompletion(true); System.exit(res?0:1); &#125; MAPREDUCE 之YARN概述Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序 YARN的重要概念 yarn并不清楚用户提交的程序的运行机制 yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源） yarn中的主管角色叫ResourceManager yarn中具体提供运算资源的角色叫NodeManager 这样一来，yarn其实就与运行的用户程序完全解耦，就意味着yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序，tez …… 所以，spark、storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可 Yarn就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享 MR YARN集群运行机制 MAPREDUCE程序运行机制本地运行模式 mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行 而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上 怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数） 本地模式非常便于进行业务逻辑的debug，只要在eclipse中打断点即可 如果在windows下想运行本地模式来测试程序逻辑，需要在windows中配置环境变量： ％HADOOP_HOME％ = d:/hadoop-2.6.1 %PATH% = ％HADOOP_HOME％\bin 并且要将d:/hadoop-2.6.1的lib和bin目录替换成windows平台编译的版本 集群运行模式什么是集群模式 将mapreduce程序提交给yarn集群resourcemanager，分发到很多的节点上并发执行 处理的数据和输出结果应该位于hdfs文件系统 提交集群的实现步骤 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动 1hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver inputpath outputpath 直接在linux的eclipse中运行main方法 （项目中要带参数：mapreduce.framework.name=yarn以及yarn的两个基本配置） 如果要在windows的eclipse中提交job给集群，则要修改YarnRunner类 在windows平台上访问hadoop时, 应该带上参数 -DHADOOP_USER_NAME=hadoop MAPREDUCE运行原理MAPREDUCE运行原理图 MAPREDUCE的shuffle机制shuffle机制概述 mapreduce中，map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle(如上图绿色框部分)； shuffle: 洗牌、发牌 ——（核心机制：数据分区，排序，缓存）； 具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区、合并和排序； shuffle主要流程 shuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个mapTask和reduceTask几点上完成的，主要有3个动作: 将结果分区 根据key进行排序 使用Combiner进行局部排序 在整个流程中有几个关键的步骤: 对缓冲区输出的结果进行分区和排序， 数据的分区和排序最开始行程的位置 对碎片数据的合并，包含第一次从缓冲区经过排序分区的结果，以及每个taskMap输出的结果，并且都会经过combine整合 对不同mapTask的分区结果进行整合 shuffle的详细流程 maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序 reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快 缓冲区的大小可以通过参数调整, 参数：io.sort.mb 默认100M MAPREDUCE 各种类Map类和Reduce类作用 MR主要的工作机制就是通过map对数据行进行递归的操作， 通过reduce对处理的结果进行汇总操作 具体实现步骤： 自定义Map 和 Reduce类 并实现map和reduce方法； 在job实例中，引用Map和Reduce的类: job.setJarByClass(FlowCount.class);job.setMapperClass(FlowCountMapper.class);job.setReducerClass(FlowCountReducer.class); Map类示例 12345678910111213141516171819202122232425262728public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; HashMap&lt;String,String&gt; cache = new HashMap&lt;String, String&gt;(); // 是在map任务初始化的时候调用一次，然后再对文本的每一行进行map操作 @Override protected void setup(Context context) &#123; cache.put("some_key", "some_value") &#125; /** * map阶段的业务逻辑就写在自定义的map()方法中 * maptask会对每一行输入数据调用一次我们自定义的map()方法 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //将maptask传给我们的文本内容先转换成String String line = value.toString(); //根据空格将这一行切分成单词 String[] words = line.split(" "); //将单词输出为&lt;单词，1&gt; for(String word:words)&#123; //将单词作为key，将次数1作为value，以便于后续的数据分发，可以根据单词分发，以便于相同单词会到相同的reduce task context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; Reduce类示例 123456789101112131415161718192021222324252627282930public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; HashMap&lt;String,String&gt; cache = new HashMap&lt;String, String&gt;(); // 是在map任务初始化的时候调用一次，然后再对文本的每一行进行map操作 @Override protected void setup(Context context) &#123; cache.put("some_key", "some_value") &#125; /** * &lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt; * &lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt; * &lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt; * 入参key，是一组相同单词kv对的key */ @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count=0; /*Iterator&lt;IntWritable&gt; iterator = values.iterator(); while(iterator.hasNext())&#123; count += iterator.next().get(); &#125;*/ for(IntWritable value:values)&#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; Bean类作用 Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系。。。。），不便于在网络中高效传输； hadoop自己开发了一套序列化机制（Writable），精简，高效，用来进行高效方便的传输 MAPREDUCE就可以借助hadoop内置的或者自定义的Bean对象(继承Writable)， 来作为mapTask和reduceTask的输入输出对象 如果需要将自定义的bean放在key中传输(当需要对key进行排序的时候)，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序； 具体实现步骤： 自定义Writable接口，并重写readFields、write、toString等方法 在map 和 reduce方法中，输出和接受自定义的Bean； 在job实例中，指定keyclass和valueclass: job.setMapOutputKeyClass(Text.class);job.setMapOutputValueClass(FlowBean.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(FlowBean.class); Writable接口方法 12345678910111213141516171819202122232425262728293031323334353637/*** 反序列化的方法，反序列化时，从流中读取到的各个字段的顺序应该与序列化时写出去的顺序保持一致*/@Overridepublic void readFields(DataInput in) throws IOException &#123; upflow = in.readLong(); dflow = in.readLong(); sumflow = in.readLong();&#125;/*** 传输过程中的序列化的方法*/@Overridepublic void write(DataOutput out) throws IOException &#123; out.writeLong(upflow); out.writeLong(dflow); //可以考虑不序列化总流量，因为总流量是可以通过上行流量和下行流量计算出来的 out.writeLong(sumflow);&#125;/*** shuffle过程中对key进行排序的规则*/@Overridepublic int compareTo(FlowBean o) &#123; //实现按照sumflow的大小倒序排序 return sumflow&gt;o.getSumflow()?-1:1;&#125;/*** 最终reduce将bean输出到文本的时候，显示的内容*/@Overridepublic String toString() &#123; return upFlow + "\t" + dFlow + "\t" + sumFlow;&#125; Partitioner类作用 Mapreduce中会将map输出的kv对，按照相同key分组，将数据分为不同的分区, 然后分发给不同的reducetask 默认的分发规则为：根据key的 hashcode%reducetask 数来分发 所以, 如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner 具体实现步骤： 自定义一个CustomPartitioner继承抽象类：Partitioner 然后在job对象中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class) 示例 1234567891011121314151617181920212223242526/** * 定义自己的从map到reduce之间的数据（分组）分发规则 按照手机号所属的省份来分发（分组）ProvincePartitioner * 默认的分组组件是HashPartitioner * * @author * */public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; static HashMap&lt;String, Integer&gt; provinceMap = new HashMap&lt;String, Integer&gt;(); static &#123; provinceMap.put("135", 0); provinceMap.put("136", 1); provinceMap.put("137", 2); provinceMap.put("138", 3); provinceMap.put("139", 4); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; Integer code = provinceMap.get(key.toString().substring(0, 3)); return code == null ? 5 : code; &#125;&#125; Combine类作用 combiner是MR程序中Mapper和Reducer之外的一种组件, combiner组件的父类就是Reducer combiner和reducer的区别在于运行的位置：Combiner是在每一个maptask所在的节点运行, Reducer是接收全局所有Mapper的输出结果； combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量 具体实现步骤： 自定义一个combiner继承Reducer，重写reduce方法 在job中设置： job.setCombinerClass(CustomCombiner.class) combiner能够应用的前提是不能影响最终的业务逻辑。 而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来 示例 1234567891011121314/** * 对象同key的value进行累加合并 */public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count=0; for(IntWritable v: values)&#123; count += v.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; 主类(job对象)作用 是与YARN集群通讯的客户端； 用来指定MR的Map，Reduce类等信息(如Map、Reduce、Partitioner、Combiner)，并设置启动参数等； 并启动客户端来与YARN通讯，并提交jar包等文件，请求启动task等； 示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class FlowCount &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); /*conf.set("mapreduce.framework.name", "yarn"); conf.set("yarn.resoucemanager.hostname", "mini1");*/ Job job = Job.getInstance(conf); /*job.setJar("/home/hadoop/wc.jar");*/ //指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCount.class); //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); //指定我们自定义的数据分区器 job.setPartitionerClass(ProvincePartitioner.class); //同时指定相应“分区”数量的reducetask job.setNumReduceTasks(5); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 指定需要缓存一个文件到所有的maptask运行节点工作目录 /* job.addArchiveToClassPath(archive); */// 缓存jar包到task运行节点的classpath中 /* job.addFileToClassPath(file); */// 缓存普通文件到task运行节点的classpath中 /* job.addCacheArchive(uri); */// 缓存压缩包文件到task运行节点的工作目录 /* job.addCacheFile(uri) */// 缓存普通文件到task运行节点的工作目录 // 将产品表文件缓存到task工作节点的工作目录中去, 在map和reduce运行阶段可以直接从工作目录里读取该文件 // 在Mapper或者Reducer中可以使用context.getLocalCacheFiles() 获取缓存文件,或者直接从工作目录获取 job.addCacheFile(new URI("file:/D:/srcdata/mapjoincache/pdts.txt")); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125;&#125; GroupingComparator 类作用 map 输出的K, V 如果K是自定义Bean对象，那么MAPREDUCE不能识别相同的K的K,V 为一组 这时，如果想要让相同的自定义K作为一组输入到reduce方法，需要设置一个GroupingComparator类，来指定对自定义对象分组的依据 原理是K,V对在传递给REDUCE方法的时候，reduce会先判断下一个K是否与当前K相同，如果相同，继续向下判断，直到获得 使用示例 1234567891011121314151617181920/* * 利用reduce端的GroupingComparator来实现将一组bean看成相同的key*/public class ItemidGroupingComparator extends WritableComparator &#123; //传入作为key的bean的class类型，以及制定需要让框架做反射获取实例对象 protected ItemidGroupingComparator() &#123; super(OrderBean.class, true); &#125; // 通过compare方法来判断key是否属于一组 @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean abean = (OrderBean) a; OrderBean bbean = (OrderBean) b; //比较两个bean时，指定只比较bean中的orderid return abean.getItemid().compareTo(bbean.getItemid()); &#125;&#125;// 在主类中设置GroupingComparatorClass//job.setGroupingComparatorClass(ItemidGroupingComparator.class); OutputFormat类作用MapReduce 只一共了一些标准的OutputFormat，这样当我们想要自定义Reduce结果的保存方式，和保存位置时，标准的OutputFormat就不能满足我们的需求； 比如我们想要在运营商的流量日志中，增加一列标记为访问目标网站的类型（依赖已经存在的网站分类数据库表）, 并且把分类成功统计到一个文件，分类失败的统计到另外一个文件中； 这时候原始的OutputFormat就不能满足需求，因为它只能根据分区结果去生成文件， 因此我们可以用自定义OutputFormat解决； 示例需求 1、 从原始日志文件中读取数据 2、 根据日志中的一个URL字段到外部知识库中获取信息增强到原始日志 3、 如果成功增强，则输出到增强结果目录；如果增强失败，则抽取原始数据中URL字段输出到待爬清单目录 分析 程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现 实现 在mapreduce中访问外部资源 自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write() OutputFormat类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class LogEnhancerOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException &#123; FileSystem fs = FileSystem.get(context.getConfiguration()); Path enhancePath = new Path("hdfs://hdp-node01:9000/flow/enhancelog/enhanced.log"); Path toCrawlPath = new Path("hdfs://hdp-node01:9000/flow/tocrawl/tocrawl.log"); FSDataOutputStream enhanceOut = fs.create(enhancePath); FSDataOutputStream toCrawlOut = fs.create(toCrawlPath); return new MyRecordWriter(enhanceOut,toCrawlOut); &#125; static class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt;&#123; FSDataOutputStream enhanceOut = null; FSDataOutputStream toCrawlOut = null; public MyRecordWriter(FSDataOutputStream enhanceOut, FSDataOutputStream toCrawlOut) &#123; super(); this.enhanceOut = enhanceOut; this.toCrawlOut = toCrawlOut; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; //有了数据，你来负责写到目的地 —— hdfs //判断，进来内容如果是带tocrawl的，就往待爬清单输出流中写 toCrawlOut if(key.toString().contains("tocrawl"))&#123; toCrawlOut.write(key.toString().getBytes()); &#125;else&#123; enhanceOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; if(toCrawlOut!=null)&#123; toCrawlOut.close(); &#125; if(enhanceOut!=null)&#123; enhanceOut.close(); &#125; &#125; &#125;&#125;// 要将自定义的输出格式组件设置到job中//job.setOutputFormatClass(LogEnhancerOutputFormat.class); IutputFormat类作用更改文件的读取方式，或者对读取的内容提前做一些处理等 示例需求 无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案 分析 1、 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS 2、 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并 3、 在mapreduce处理时，可采用combineInputFormat提高效率 实现 实现的是上述第二种方式 程序的核心机制： 自定义一个InputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 再输出时使用SequenceFileOutPutFormat输出合并文件 IutputFormat类 1234567891011121314151617public class WholeFileInputFormat extends FileInputFormat&lt;NullWritable, BytesWritable&gt; &#123; //设置每个小文件不可分片,保证一个小文件生成一个key-value键值对 @Override protected boolean isSplitable(JobContext context, Path file) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader( InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeFileRecordReader reader = new WholeFileRecordReader(); reader.initialize(split, context); return reader; &#125;&#125; RecordReader类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class WholeFileRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt; &#123; private FileSplit fileSplit; private Configuration conf; private BytesWritable value = new BytesWritable(); private boolean processed = false; @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.fileSplit = (FileSplit) split; this.conf = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; byte[] contents = new byte[(int) fileSplit.getLength()]; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); FSDataInputStream in = null; try &#123; in = fs.open(file); IOUtils.readFully(in, contents, 0, contents.length); value.set(contents, 0, contents.length); &#125; finally &#123; IOUtils.closeStream(in); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException &#123; return processed ? 1.0f : 0.0f; &#125; @Override public void close() throws IOException &#123; // do nothing &#125;&#125; mapreduce处理流程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class SmallFilesToSequenceFileConverter extends Configured implements Tool &#123; static class SequenceFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt; &#123; private Text filenameKey; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; InputSplit split = context.getInputSplit(); Path path = ((FileSplit) split).getPath(); filenameKey = new Text(path.toString()); &#125; @Override protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(filenameKey, value); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Configuration conf = new Configuration(); /*System.setProperty("HADOOP_USER_NAME", "hadoop");*/ String[] otherArgs = new GenericOptionsParser(conf, args) .getRemainingArgs(); if (otherArgs.length != 2) &#123; System.err.println("Usage: combinefiles &lt;in&gt; &lt;out&gt;"); System.exit(2); &#125; Job job = Job.getInstance(conf,"combine small files to sequencefile"); job.setJarByClass(SmallFilesToSequenceFileConverter.class); job.setInputFormatClass(WholeFileInputFormat.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); job.setMapperClass(SequenceFileMapper.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); return job.waitForCompletion(true) ? 0 : 1; &#125; public static void main(String[] args) throws Exception &#123; args=new String[]&#123;"c:/wordcount/smallinput","c:/wordcount/smallout"&#125;; int exitCode = ToolRunner.run(new SmallFilesToSequenceFileConverter(), args); System.exit(exitCode); &#125;&#125; 计数器作用 在实际生产代码中，常常需要将数据处理过程中遇到的不合规数据行进行全局计数，类似这种需求可以借助mapreduce框架中提供的全局计数器来实现 示例 12345678910111213141516171819public class MultiOutputs &#123; //通过枚举形式定义自定义计数器 enum MyCounter&#123;MALFORORMED,NORMAL&#125; static class CommaMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] words = value.toString().split(","); for (String word : words) &#123; context.write(new Text(word), new LongWritable(1)); &#125; //对枚举定义的自定义计数器加1 context.getCounter(MyCounter.MALFORORMED).increment(1); //通过动态设置自定义计数器加1 context.getCounter("counterGroupa", "countera").increment(1); &#125; &#125; MAPREDUCE 数据压缩数据压缩配置在配置参数或在代码中都可以设置reduce的输出压缩 Reduce输出数据压缩 在配置参数中设置 123mapreduce.output.fileoutputformat.compress=falsemapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodecmapreduce.output.fileoutputformat.compress.type=RECORD 在代码中配置 123Job job = Job.getInstance(conf);FileOutputFormat.setCompressOutput(job, true);FileOutputFormat.setOutputCompressorClass(job, (Class&lt;? extends CompressionCodec&gt;) Class.forName("")); Mapper数据压缩 在配置参数中设置 12mapreduce.map.output.compress=falsemapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec 在代码中设置 12conf.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class); 压缩文件的读取Hadoop自带的InputFormat类内置支持压缩文件的读取，比如TextInputformat类，在其initialize方法中： 1234567891011121314151617181920212223242526272829303132333435363738394041public void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException &#123; FileSplit split = (FileSplit) genericSplit; Configuration job = context.getConfiguration(); this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE); start = split.getStart(); end = start + split.getLength(); final Path file = split.getPath(); // open the file and seek to the start of the split final FileSystem fs = file.getFileSystem(job); fileIn = fs.open(file); //根据文件后缀名创建相应压缩编码的codec CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file); if (null!=codec) &#123; isCompressedInput = true; decompressor = CodecPool.getDecompressor(codec); //判断是否属于可切片压缩编码类型 if (codec instanceof SplittableCompressionCodec) &#123; final SplitCompressionInputStream cIn = ((SplittableCompressionCodec)codec).createInputStream( fileIn, decompressor, start, end, SplittableCompressionCodec.READ_MODE.BYBLOCK); //如果是可切片压缩编码，则创建一个CompressedSplitLineReader读取压缩数据 in = new CompressedSplitLineReader(cIn, job, this.recordDelimiterBytes); start = cIn.getAdjustedStart(); end = cIn.getAdjustedEnd(); filePosition = cIn; &#125; else &#123; //如果是不可切片压缩编码，则创建一个SplitLineReader读取压缩数据，并将文件输入流转换成解压数据流传递给普通SplitLineReader读取 in = new SplitLineReader(codec.createInputStream(fileIn, decompressor), job, this.recordDelimiterBytes); filePosition = fileIn; &#125; &#125; else &#123; fileIn.seek(start); //如果不是压缩文件，则创建普通SplitLineReader读取数据 in = new SplitLineReader(fileIn, job, this.recordDelimiterBytes); filePosition = fileIn; &#125; MAPREDUCE 参数优化资源相关参数 以下参数是在用户自己的mr应用程序中配置就可以生效 mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。 mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。 mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g：-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “” mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc, 默认值: “” mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1 mapreduce.reduce.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 1 以下参数应该在yarn启动之前就配置在服务器的配置文件中才能生效 yarn.scheduler.minimum-allocation-mb 1024 给应用程序container分配的最小内存 yarn.scheduler.maximum-allocation-mb 8192 给应用程序container分配的最大内存 yarn.scheduler.minimum-allocation-vcores 1 最小cpu yarn.scheduler.maximum-allocation-vcores 32 最大cpu yarn.nodemanager.resource.memory-mb 8192 一台nodemanager 可用的总内存 shuffle性能优化的关键参数，应在yarn启动之前就配置好 mapreduce.task.io.sort.mb 100 //shuffle的环形缓冲区大小，默认100m mapreduce.map.sort.spill.percent 0.8 //环形缓冲区溢出的阈值，默认80% 容错相关参数 mapreduce.map.maxattempts: 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.reduce.maxattempts: 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.map.failures.maxpercent: 当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。 mapreduce.reduce.failures.maxpercent: 当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0. mapreduce.task.timeout: Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 本地运行mapreduce 作业设置以下几个参数: mapreduce.framework.name=local mapreduce.jobtracker.address=local fs.defaultFS=local 效率和稳定性相关参数 mapreduce.map.speculative: 是否为Map Task打开推测执行机制，默认为false mapreduce.reduce.speculative: 是否为Reduce Task打开推测执行机制，默认为false mapreduce.job.user.classpath.first &amp; mapreduce.task.classpath.user.precedence：当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。 mapreduce.input.fileinputformat.split.minsize: FileInputFormat做切片时的最小切片大小，(5)mapreduce.input.fileinputformat.split.maxsize: FileInputFormat做切片时的最大切片大小 (切片的默认大小就等于blocksize，即 134217728)]]></content>
      <categories>
        <category>大数据</category>
        <category>HADOOP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS]]></title>
    <url>%2F2019%2F12%2F30%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHADOOP%2FHDFS%2F</url>
    <content type="text"><![CDATA[HDFS 基本概念HDFS 思想设计思想分而治之: 将大文件、大批量文件， 分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析； 在大数据系统中的应用为各类分布式运算框架(如: mapreduce, spark, tez, …) 提供数据存储服务 重点概念文件切块， 副本存放， 元数据管理 HDFS的概念和特性概念首先，它是一个文件系统，用于存储文件，通过统一的命名空间——目录树来定位文件 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色； 重要特征 DFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data 目录结构及文件分块信息(元数据)的管理由namenode节点承担， namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器） 文件的各个block的存储管理由datanode节点承担， datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication） HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改， 适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高 HDFS的工作机制HDFS 工作机制介绍 HDFS集群分为两大角色：NameNode、DataNode (Secondary Namenode) NameNode负责管理整个文件系统的元数据 DataNode 负责管理用户的文件数据块 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上 每一个文件块可以有多个副本，并存放在不同的datanode上 Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量 HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行 HDFS 写数据的流程 客户端与namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在 namenode返回是否可以上传 client请求第一个 block该传输到哪些datanode服务器上 namenode返回可以上传的节点, 示例3个datanode服务器ABC client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将整个pipeline建立完成，逐级返回客户端 client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位(chunk为校验单位)，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 当一个block传输完成之后(只要有一个节点上传成功，就算成功)，client再次请求namenode上传第二个block的服务器。 HDFS读数据流程 client跟namenode通信查询元数据，找到文件块所在的datanode服务器 cilent挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流 datanode开始发送数据（从磁盘里面读取数据放入流，以packet为传输单位，chunk为校验单位） 客户端以packet为单位接收，先在本地缓存，然后写入目标文件 block、packet与chunk在DFSClient写HDFS的过程中，有三个需要搞清楚的单位：block、packet与chunk； block是最大的一个单位，它是最终存储于DataNode上的数据粒度，由dfs.block.size参数决定，默认是64M；注：这个参数由客户端配置决定； packet是中等的一个单位，它是数据由DFSClient流向DataNode的粒度，以dfs.write.packet.size参数为参考值，默认是64K；注：这个参数为参考值，是指真正在进行数据传输时，会以它为基准进行调整，调整的原因是一个packet有特定的结构，调整的目标是这个packet的大小刚好包含结构中的所有成员，同时也保证写到DataNode后当前block的大小不超过设定值； chunk是最小的一个单位，它是DFSClient到DataNode数据传输中进行数据校验的粒度，由io.bytes.per.checksum参数决定，默认是512B；注：事实上一个chunk还包含4B的校验值，因而chunk写入packet时是516B；数据与检验值的比值为128:1，所以对于一个128M的block会有一个1M的校验文件与之对应； NAMENODE 工作机制NAMENODE 的职责 负责客户端请求的响应 元数据的管理（查询，修改） 元数据管理形式 内存元数据(NameSystem) 磁盘元数据镜像文件(fsimage) 数据操作日志文件（edits文件， 可通过日志运算出元数据） 元数据的存储机制 内存中有一份完整的原数据(内存metadate) 磁盘中有一个”准完整”的原数据镜像(fsimage)文件(在namenode的工作目录中) 用于衔接metadata和持久化元数据镜像的fsimage之间的操作日志(edits文件), 当客户端对hdfs中的文件进行新增或者修改操作，操作记录会首先被记录到edits日志文件中，当客户端操作成功后，相应的原数据会更新到内存meta.data中， 并且每隔一定的间隔hdfs会将当前的metadata同步到fsimage镜像文件中 元数据手动查看hdfs命令 12hdfs oev -i edits -o edits.xmlhdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml 元数据的checkpoint由于在数据备份的时候会占用计算资源，所以为了减轻namenode的负载，通常可以将数据备份的工作交给另外一个专门用来做数据备份的namenode–&gt; sencondary namenode 每隔一段时间，会由secondary namenode 将namenode上积累的所有edits和一个最新的fsimage下载到本地(只有第一次merge才会下载fsimage)，并加载到内存进行merge(这个过程称之为checkpoint) namenode的一些情况namenode如果宕机，hdfs是否还能正常提供服务 不能，secondarynamenode虽然有元数据信息，但是不能更新元数据， 不能充当namenode使用 如果namenode的硬盘损坏，元数据是否能回复，能恢复多少? 可以恢复最后一次merge之前的数据， 只需要将secondarynamenode的数据目录替换成namenode的数据目录 配置namenode的工作目录时，有哪些可以注意的事项 可以将namenode的元数据保存到多块物理磁盘上例如如下的namenode配置 1234&lt;property&gt;&lt;name&gt;dfs.name.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/name1,/home/hadoop/name2&lt;/value&gt;&lt;/property&gt; checkpoint 的触发条件相关配置12345678dfs.namenode.checkpoint.check.period=60 #检查触发条件是否满足的频率，60秒dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary#以上两个参数做checkpoint操作时，secondary namenode的本地工作目录dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125;dfs.namenode.checkpoint.max-retries=3 #最大重试次数dfs.namenode.checkpoint.period=3600 #两次checkpoint之间的时间间隔3600秒dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录 元数据目录说明在第一次部署好Hadoop集群的时候，我们需要在NameNode（NN）节点上格式化磁盘： 1hadoop namenode -format 元数据目录介绍格式化完成之后，将会在$dfs.namenode.name.dir/current目录下如下的文件结构 123456current/|-- VERSION|-- edits_*|-- fsimage_0000000000008547077|-- fsimage_0000000000008547077.md5|-- seen_txid 其中的dfs.name.dir是在hdfs-site.xml文件中配置的，默认值如下： 1234567891011&lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name&lt;/value&gt;&lt;/property&gt;# hadoop.tmp.dir是在core-site.xml中配置的，默认值如下&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt; dfs.namenode.name.dir属性可以配置多个，如/data1/dfs/name,/data2/dfs/name,/data3/dfs/name,….。 各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统Network File System，NFS）之上，即使你这台机器损坏了，元数据也得到保存。 元数据目录文件介绍VERSION文件 VERSION文件是Java属性文件，内容大致如下： 1234567#Fri Nov 15 19:47:46 CST 2013namespaceID=934548976clusterID=CID-cdff7d73-93cd-4783-9399-0a22e6dce196cTime=0storageType=NAME_NODEblockpoolID=BP-893790215-192.168.24.72-1383809616115layoutVersion=-47 namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的； storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）； cTime表示NameNode存储时间的创建时间，由于我的NameNode没有更新过，所以这里的记录值为0，以后对NameNode升级之后，cTime将会记录更新时间戳； layoutVersion表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用 clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它；如下说明 12345# 使用如下命令格式化一个Namenode：选择一个唯一的cluster_id，并且这个cluster_id不能与环境中其他集群有冲突。如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。hadoop namenode -format -clusterId &lt;cluster_id&gt;# 升级集群至最新版本。在升级过程中需要提供一个ClusterID，如果没有提供ClusterID，则会自动生成一个ClusterID。hadoop start namenode --config $HADOOP_CONF_DIR -upgrade -clusterId &lt;cluster_ID&gt; blockpoolID是针对每一个Namespace所对应的blockpool的ID，上面的这个BP-893790215-192.168.24.72-1383809616115就是在我的ns1的namespace下的存储块池的ID，这个ID包括了其对应的NameNode节点的ip地址。 seen_txid文件 是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。 文件中记录的是edits滚动的序号，每次重启namenode时，namenode就知道要将哪些edits进行加载edits fsimage文件和edits文件 fsimage: 元数据的镜像文件 edits: 元数据的滚动日志文件，每次merge之后会对之前的日志文件进行清除 DATANODE工作机制DATANODE 工作职责 存储管理用户的文件块数据 定期向namenode汇报自身所持有的block信息(通过心跳上报)， 当集群中的节点失效，或者block存在丢失的时候，集群可以根据汇报信息恢复block初始副本数量的问题 DATANODE 汇报间隔设置参数12345&lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;3600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&lt;/property&gt; DATANODE掉线判断时限参数datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： ​ timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。 .默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒 dfs配置参数 12345678&lt;property&gt; &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; HDFS客户端操作命令行客户端命令格式 1hadoop fs -ls / 命令行参数 123456789101112131415161718192021222324252627282930313233343536[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;][-cat [-ignoreCrc] &lt;src&gt; ...][-checksum &lt;src&gt; ...][-chgrp [-R] GROUP PATH...][-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...][-chown [-R] [OWNER][:[GROUP]] PATH...][-copyFromLocal [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-count [-q] &lt;path&gt; ...][-cp [-f] [-p] &lt;src&gt; ... &lt;dst&gt;][-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]][-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;][-df [-h] [&lt;path&gt; ...]][-du [-s] [-h] &lt;path&gt; ...][-expunge][-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;][-getfacl [-R] &lt;path&gt;][-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;][-help [cmd ...]][-ls [-d] [-h] [-R] [&lt;path&gt; ...]][-mkdir [-p] &lt;path&gt; ...][-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;][-moveToLocal &lt;src&gt; &lt;localdst&gt;][-mv &lt;src&gt; ... &lt;dst&gt;][-put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;][-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;][-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...][-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...][-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]][-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...][-stat [format] &lt;path&gt; ...][-tail [-f] &lt;file&gt;][-test -[defsz] &lt;path&gt;][-text [-ignoreCrc] &lt;src&gt; ...][-touchz &lt;path&gt; ...][-usage [cmd ...]] 常用命令介绍 -help 功能：输出这个命令参数手册 –ls 功能：显示目录信息 示例：hadoop fs -ls hdfs://hadoop-server01:9000/ 备注：这些参数中，所有的hdfs路径都可以简写, hadoop fs -ls / 等同于上一条命令的效果 -mkdir 功能：在hdfs上创建目录 示例：hadoop fs -mkdir -p /aaa/bbb/cc/dd -moveFromLocal 功能：从本地剪切粘贴到hdfs 示例：hadoop fs - moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd -moveToLocal 功能：从hdfs剪切粘贴到本地 示例：hadoop fs - moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt* –appendToFile 功能：追加一个文件到已经存在的文件末尾 示例：hadoop fs -appendToFile ./hello.txt hdfs://hadoop-server01:9000/hello.txt 可以简写为： hadoop fs -appendToFile ./hello.txt /hello.txt -cat 功能：显示文件内容 示例：hadoop fs -cat /hello.txt -tail 功能：显示一个文件的末尾 示例：hadoop fs -tail /weblog/access_log.1 -text 功能：以字符形式打印一个文件的内容 示例：hadoop fs -text /weblog/access_log.1 -chgrp , -chmod, -chown 功能：linux文件系统中的用法一样，对文件所属权限 示例： hadoop fs -chmod 666 /hello.txt hadoop fs -chown someuser:somegrp /hello.txt -copyFromLocal 功能：从本地文件系统中拷贝文件到hdfs路径去 示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ -copyToLocal 功能：从hdfs拷贝到本地 示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz -cp 功能：从hdfs的一个路径拷贝hdfs的另一个路径 示例：hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -mv 功能：在hdfs目录中移动文件 示例：hadoop fs -mv /aaa/jdk.tar.gz / -get 功能：等同于copyToLocal，就是从hdfs下载文件到本地 示例：hadoop fs -get /aaa/jdk.tar.gz -getmerge 功能：合并下载多个文件 示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,… hadoop fs -getmerge /aaa/log.* ./log.sum -put 功能：等同于copyFromLocal 示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -rm 功能：删除文件或文件夹 示例：hadoop fs -rm -r /aaa/bbb/ -rmdir 功能：删除空目录 示例：hadoop fs -rmdir /aaa/bbb/ccc -df 功能：统计文件系统的可用空间信息* 示例：hadoop fs -df -h / -du 功能：统计文件夹的大小信息 示例：hadoop fs -du -s -h /aaa/* -count 功能：统计一个指定目录下的文件节点数量 示例：hadoop fs -count /aaa/ -setrep 功能：设置hdfs中文件的副本数量 示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz java api 使用引入依赖(maven)12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt;&lt;/dependency&gt; window 下开发需要注意建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境： A、在windows的某个目录下解压一个hadoop的安装包 B、将安装包下的lib和bin目录用对应windows版本平台编译的本地库替换(下载地址: https://github.com/steveloughran/winutils,下载之后直接解压,将bin目录里的内容直接覆盖到hadoop的bin ) C、在window系统中配置HADOOP_HOME指向你解压的安装包 D、在windows系统的path变量中加入hadoop的bin目录 java 客户端的配置在java中操作hdfs，首先要获得一个客户端实例 12Configuration conf = new Configuration() fs = FileSystem.get(new URI("hdfs://master:9000"),conf,"hadoop"); 而我们的操作目标是HDFS，所以获取到的fs对象应该是DistributedFileSystem的实例； get方法是从何处判断具体实例化那种客户端类呢？ —从conf中的一个参数 fs.defaultFS的配置值判断； 如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:///，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象 配置方法 123conf = new Configuration();conf.set("fs.defaultFS", "hdfs://master:9000"); // 或者conf.addResource, 直接添加配置文件fs = FileSystem.get(new URI("hdfs://master:9000"),conf,"hadoop"); java 客户端 HDFS增删改查示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package com.bigdata.utils.hdfs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.junit.Before;import org.junit.Test;import java.net.URI;import java.util.Iterator;import java.util.Map.Entry;/** * * 客户端去操作hdfs时，是有一个用户身份的 * 默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=hadoop * * 也可以在构造客户端fs对象时，通过参数传递进去 * @author * */public class HdfsClientDemo &#123; FileSystem fs = null; Configuration conf = null; @Before public void init() throws Exception&#123; conf = new Configuration();// conf.set("fs.defaultFS", "hdfs://mini1:9000"); conf.set("dfs.replication", "5"); //拿到一个文件系统操作的客户端实例对象 fs = FileSystem.get(conf); //可以直接传入 uri和用户身份 fs = FileSystem.get(new URI("hdfs://mini1:9000"),conf,"hadoop"); &#125; /** * 上传文件 * @throws Exception */ @Test public void testUpload() throws Exception &#123; fs.copyFromLocalFile(new Path("c:/access.log"), new Path("/access.log.copy")); fs.close(); &#125; /** * 下载文件 * @throws Exception */ @Test public void testDownload() throws Exception &#123; fs.copyToLocalFile(new Path("/access.log.copy"), new Path("d:/")); &#125; /** * 打印参数 */ @Test public void testConf()&#123; Iterator&lt;Entry&lt;String, String&gt;&gt; it = conf.iterator(); while(it.hasNext())&#123; Entry&lt;String, String&gt; ent = it.next(); System.out.println(ent.getKey() + " : " + ent.getValue()); &#125; &#125; @Test public void testMkdir() throws Exception &#123; boolean mkdirs = fs.mkdirs(new Path("/testmkdir/aaa/bbb")); System.out.println(mkdirs); &#125; @Test public void testDelete() throws Exception &#123; boolean flag = fs.delete(new Path("/testmkdir/aaa"), true); System.out.println(flag); &#125; /** * 递归列出指定目录下所有子文件夹中的文件 * @throws Exception */ @Test public void testLs() throws Exception &#123; RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true); while(listFiles.hasNext())&#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println("blocksize: " +fileStatus.getBlockSize()); System.out.println("owner: " +fileStatus.getOwner()); System.out.println("Replication: " +fileStatus.getReplication()); System.out.println("Permission: " +fileStatus.getPermission()); System.out.println("Name: " +fileStatus.getPath().getName()); System.out.println("------------------"); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for(BlockLocation b:blockLocations)&#123; System.out.println("块起始偏移量: " +b.getOffset()); System.out.println("块长度:" + b.getLength()); //块所在的datanode节点 String[] datanodes = b.getHosts(); for(String dn:datanodes)&#123; System.out.println("datanode:" + dn); &#125; &#125; &#125; &#125; @Test public void testLs2() throws Exception &#123; FileStatus[] listStatus = fs.listStatus(new Path("/")); for(FileStatus file :listStatus)&#123; System.out.println("name: " + file.getPath().getName()); System.out.println((file.isFile()?"file":"directory")); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://mini1:9000"); //拿到一个文件系统操作的客户端实例对象 FileSystem fs = FileSystem.get(conf); fs.copyFromLocalFile(new Path("c:/access.log"), new Path("/access.log.copy")); fs.close(); &#125;&#125; java 客户端 HDFS通过流的方式上传下载文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package com.bigdata.utils.hdfs;import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.Before;import org.junit.Test;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.net.URI;/** * 用流的方式来操作hdfs上的文件 * 可以实现读取指定偏移量范围的数据 * @author * */public class HdfsStreamAccess &#123; FileSystem fs = null; Configuration conf = null; @Before public void init() throws Exception&#123; conf = new Configuration(); //拿到一个文件系统操作的客户端实例对象// fs = FileSystem.get(conf); //可以直接传入 uri和用户身份 fs = FileSystem.get(new URI("hdfs://mini1:9000"),conf,"hadoop"); &#125; /** * 通过流的方式上传文件到hdfs * @throws Exception */ @Test public void testUpload() throws Exception &#123; FSDataOutputStream outputStream = fs.create(new Path("/angelababy.love"), true); FileInputStream inputStream = new FileInputStream("c:/angelababy.love"); IOUtils.copy(inputStream, outputStream); &#125; @Test public void testDownLoad() throws Exception &#123; FSDataInputStream inputStream = fs.open(new Path("/angelababy.love")); FileOutputStream outputStream = new FileOutputStream("d:/angelababy.love"); IOUtils.copy(inputStream, outputStream); &#125; @Test public void testRandomAccess() throws Exception&#123; FSDataInputStream inputStream = fs.open(new Path("/angelababy.love")); inputStream.seek(12); FileOutputStream outputStream = new FileOutputStream("d:/angelababy.love.part2"); IOUtils.copy(inputStream, outputStream); &#125; /** * 显示hdfs上文件的内容 * @throws IOException * @throws IllegalArgumentException */ @Test public void testCat() throws IllegalArgumentException, IOException&#123; FSDataInputStream in = fs.open(new Path("/angelababy.love")); IOUtils.copy(in, System.out); &#125;&#125; Java客户端获取文件block信息并读取指定block内容1234567891011121314151617181920212223242526272829@Test public void testCat() throws IllegalArgumentException, IOException&#123; FSDataInputStream in = fs.open(new Path("/weblog/input/access.log.10")); //拿到文件信息 FileStatus[] listStatus = fs.listStatus(new Path("/weblog/input/access.log.10")); //获取这个文件的所有block的信息 BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(listStatus[0], 0L, listStatus[0].getLen()); //第一个block的长度 long length = fileBlockLocations[0].getLength(); //第一个block的起始偏移量 long offset = fileBlockLocations[0].getOffset(); System.out.println(length); System.out.println(offset); //获取第一个block写入输出流// IOUtils.copyBytes(in, System.out, (int)length); byte[] b = new byte[4096]; FileOutputStream os = new FileOutputStream(new File("d:/block0")); while(in.read(offset, b, 0, 4096)!=-1)&#123; os.write(b); offset += 4096; if(offset&gt;=length) return; &#125;; os.flush(); os.close(); in.close(); &#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>HADOOP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 编译]]></title>
    <url>%2F2019%2F12%2F29%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHADOOP%2Fhadoop%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[编译hadoop步骤下载依赖 下载编译需要的软件包 apache-ant-1.9.4-bin.tar.gz findbugs-3.0.0.tar.gz protobuf-2.5.0.tar.gz apache-maven-3.0.5-bin.tar.gz 下载并解压缩hadoop源码包 1 tar -zxvf hadoop-2.4.0-src.tar.gz 安装各依赖软件 安装maven 1234tar -zxvf apache-maven-3.0.5-bin.tar.gz -C /opt/vi /etc/profile# 增加maven的bin目录到pathsource /etc/profile 安装ant 1234tar -zxvf apache-ant-1.9.4-bin.tar.gz -C /opt/vi /etc/profile# 增加ant 的bin目录到pathsource /etc/profile 安装findbugs 1234tar -zxvf findbugs-3.0.0.tar.gz -C /opt/vi /etc/profile# 增加findbugs 的bin目录到pathsource /etc/profile 安装protubuf 123456tar -zxvf protobuf-2.5.0.tar.gzcd protobuf-2.5.0./configuremakemake checkmake install 安装linux依赖 1yum install -y cmake openssl-devel ncurses-devel 编译hadoop 123cd hadoop-2.4.0-srcmvn clean install -DskipTestsmvn package -Pdist,native -DskipTests -Dtar # 等待编译完成 编译成功的文件在： /hadoop-2.4.0-src/hadoop-dist/target/]]></content>
      <categories>
        <category>大数据</category>
        <category>HADOOP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 介绍、安装]]></title>
    <url>%2F2019%2F12%2F29%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2FHADOOP%2Fhadoop%E4%BB%8B%E7%BB%8D%E3%80%81%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[hadoop 介绍什么是hadoop HADOOP是apache旗下的一套开源软件平台 HADOOP提供的功能：利用服务器集群，根据用户的自定义业务逻辑，对海量数据进行分布式处理 hadoop 的核心组件 HDFS（分布式文件系统） YARN（运算资源调度系统） MAPREDUCE（分布式运算编程框架） 广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈 hadoop 的产生背景 HADOOP最早起源于Nutch。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，遇到了严重的可扩展性问题——如何解决数十亿网页的存储和索引问题。 2003年、2004年谷歌发表的两篇论文为该问题提供了可行的解决方案。 分布式文件系统（GFS），可用于处理海量网页的存储 分布式计算框架MAPREDUCE(YARN+MAPREDUCE)，可用于处理海量网页的索引计算问题。 Nutch的开发人员完成了相应的开源实现HDFS和MAPREDUCE，并从Nutch中剥离成为独立项目HADOOP，到2008年1月，HADOOP成为Apache顶级项目，迎来了它的快速发展期。 hadoop生态圈组件 HDFS: 分布式文件系统 MAPREDUCE: 分布式运算程序开发框架 HIVE： 基于大数据技术(文件系统/运算框架)的SQL数据仓库工具 HBASE： 基于HADOOP的分布式海量数据库 ZOOKEEPER: 分布式协调服务基础组件 Mahout: 基于mapreduce/spark/flink 等分布式运算框架的机器学习算法库 Oozie: 工作流调度框架 Sqoop：数据导入导出工具 Flume： 日志数据采集框架 离线大数据分析流程介绍流程图分析 1) 数据采集：定制开发采集程序，或使用开源框架FLUME 2) 数据预处理：定制开发mapreduce程序运行于hadoop集群 3) 数据仓库技术：基于hadoop之上的Hive 4) 数据导出：基于hadoop的sqoop数据导入导出工具 5) 数据可视化：定制开发web程序或使用kettle等产品 6) 整个过程的流程调度：hadoop生态圈中的oozie工具或其他类似开源产品 项目技术架构图 推荐系统架构示例 hadoop集群搭建集群简介HADOOP集群具体来说包含两个集群：HDFS集群和YARN集群，两者逻辑上分离，但物理上常在一起，因为YARN需要分配资源对数据进行处理，所以最好的方式也是将YARN和HDFS保持物理地址统一 HDFS集群： 负责海量数据的存储，集群中的角色主要有 NameNode / DataNode YARN集群： 负责海量数据运算时的资源调度，集群中的角色主要有 ResourceManager /NodeManager Mapreduce： 它其实是一个应用程序开发包) 接下来的集群示例，以3节点为例进行搭建: hosts 文件示例 123hdp-node-01 NameNode SecondaryNameNodehdp-node-02 ResourceManager hdp-node-03 DataNode NodeManager 环境准备服务器 Centos 6.5 64bit 网络 网络环境需要互通 同步时间 设置时间同步服务器ntp 或者机器之间同步执行 date -s “2019-06-06 00:00:00” 服务器系统设置 添加hadoop用户, 并给sudo权限 1234useradd hadooppasswd hadoopvi /etc/sudoers # 添加 hadoop ALL=(ALL) ALL 设置主机名 123hdp-node-01hdp-node-02hdp-node-03 配置域名映射, 修改host文件 123192.168.33.101 hdp-node-01192.168.33.102 hdp-node-02192.168.33.103 hdp-node-03 配置ssh免密登录 1234ssh-keygenssh-copy-id hdp-node-01ssh-copy-id hdp-node-02ssh-copy-id hdp-node-03 关闭防火墙 12systemctl stop firewalldsystemctl disable firewalld jdk安装 上传jdk安装包 规划安装目录 /home/hadoop/apps/jdk_1.7.65 解压安装包 配置环境变量 /etc/profile， 添加 1234export JAVA_HOME=/usr/local/java/jdk1.8.0_141export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH hadoop集群安装 上传安装包并解压 上传hadoop安装包, 并解压到一个自定义目录（该安装包通过编译生成，编译方法参考hadoop编译） 1tar -xzvf hadoop-2.6.1.tar.gz -C /home/hadoop/apps 修改配置文件 etc/hadoop/hadoop-env.sh hadoop 环境变量配置 修改JAVA_HOME 的位置, 如果不通过ssh连接执行hadoop命令可以不改 12# The java implementation to use.export JAVA_HOME=/home/hadoop/apps/jdk1.7.0_51 core-site.xml hadoop基础配置 配置使用文件系统，和nameNode的地址，还有hadoop的系统数据目录 12345678910&lt;configuration&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hdp-node-01:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/HADOOP/apps/hadoop-2.6.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml hdfs相关配置 配置hdfs的数据目录, 副本数量, secondaryNameNode 的http地址等 1234567891011121314151617181920&lt;configuration&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/data/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/data/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.secondary.http.address&lt;/name&gt;&lt;value&gt;hdp-node-01:50090&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; mapred-site.xml mapreduce计算框架相关配置 配置使用的资源调度工具配置等 123456&lt;configuration&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; yarn-site.xml yarn资源调度器相关配置 配置yarn的resource Manager和nodeManager 1234567891011&lt;configuration&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop01&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; slaves 集群配置文件 123hdp-node-01hdp-node-02hdp-node-03 启动集群 格式化hdfs 格式化的作用是初始化namenode 的工作目录 1bin/hadoop namenode -format ​ 启动HDFS和YARN ​ 第一种方法: hadoop-daemon 启动 ​ 每次只会启动一个组件，不自动化，比较麻烦 12345sbin/hadoop-daemon.sh start namenode # 在namenode执行sbin/hadoop-daemon.sh start datanode # 在datanode执行sbin/hadoop-daemon.sh start secondarynamenode # 启动secondarynamenodesbin/yarn-daemon.sh start resourcemanager # 在resourcemanager节点启动sbin/yarn-daemon.sh start nodemanager # 在nodemanager节点启动 ​ 第二种方法: start-dfs / start-yarn 启动 12sbin/start-dfs.sh # 会把dfs集群都启动起来sbin/start-yarn.sh # 直接把yarn集群都启动起来 ​ 第三种方法: start-all 启动 1sbin/start-all.sh # 直接把dfs 和yarn 集群都启动起来 HDFS 集群状态查询 查看集群状态 命令 1hdfs dfsadmin –report 示例 ​ 可以看出，集群共有3个datanode可用 也可打开web控制台查看HDFS集群信息 在浏览器打开http://hdp-node-01:50070/ 示例]]></content>
      <categories>
        <category>大数据</category>
        <category>HADOOP</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HADOOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java rpc 开发]]></title>
    <url>%2F2019%2F12%2F25%2F14.Java%2FJava%E5%9F%BA%E7%A1%80%2Fjava%20rpc%20%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[RPC原理学习什么是RPCRPC（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。 RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。 RPC原理 运行时,一次客户机对服务器的RPC调用,其内部操作大致有如下十步： 调用客户端句柄；执行传送参数 调用本地系统内核发送网络消息 消息传送到远程主机 服务器句柄得到消息并取得参数 执行远程过程 执行的过程将结果返回服务器句柄 服务器句柄返回结果，调用远程系统内核 消息传回本地主机 客户句柄由内核接收消息 客户接收句柄返回的数据 NIO 介绍简介nio 是New IO 的简称(同步非阻塞io)，在jdk1.4 里提供的新api 。 Sun 官方标榜的特性如下： 为所有的原始类型提供(Buffer)缓存支持。 字符集编码解码解决方案。 Channel ：一个新的原始I/O 抽象。 支持锁和内存映射文件的文件访问接口。 提供多路(non-bloking) 非阻塞式的高伸缩性网络I/O 传统io原理 传统的I/O 使用实例 使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socket), 如下程序: 123File.read(fileDesc, buf, len);Socket.send(socket, buf, len); 会有较大的性能开销, 主要表现在一下两方面: 上下文切换(context switch), 此处有4次用户态和内核态的切换 Buffer内存开销, 一个是应用程序buffer, 另一个是系统读取buffer以及socket buffer 其运行示意图如下 传统io buffer操作步骤 1) 先将文件内容从磁盘中拷贝到操作系统buffer 2) 再从操作系统buffer拷贝到程序应用buffer 3) 从程序buffer拷贝到socket buffer 4) 从socket buffer拷贝到协议引擎. NIO 原理nio 机制介绍 NIO技术省去了将操作系统的read buffer拷贝到程序的buffer, 以及从程序buffer拷贝到socket buffer的步骤, 直接将 read buffer 拷贝到 socket buffer. java 的 FileChannel.transferTo() 方法就是这样的实现, 这个实现是依赖于操作系统底层的sendFile()实现的. publicvoid transferTo(long position, long count, WritableByteChannel target); 他的底层调用的是系统调用sendFile()方法 sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 运行示意图 netty常用API学习netty简介​ Netty是基于Java NIO的网络应用框架. ​ Netty是一个NIO client-server(客户端服务器)框架，使用Netty可以快速开发网络应用，例如服务器和客户端协议。Netty提供了一种新的方式来使开发网络应用程序，这种新的方式使得它很容易使用和有很强的扩展性。Netty的内部实现时很复杂的，但是Netty提供了简单易用的api从网络处理代码中解耦业务逻辑。Netty是完全基于NIO实现的，所以整个Netty都是异步的。 ​ 网络应用程序通常需要有较高的可扩展性，无论是Netty还是其他的基于Java NIO的框架，都会提供可扩展性的解决方案。Netty中一个关键组成部分是它的异步特性. netty使用 下载netty包 下载netty包，下载地址http://netty.io/ 服务端启动类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * • 配置服务器功能，如线程、端口 • 实现服务器处理程序，它包含业务逻辑，决定当有一个请求连接或接收数据时该做什么 * * @author wilson * */public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //创建ServerBootstrap实例来引导绑定和启动服务器 ServerBootstrap serverBootstrap = new ServerBootstrap(); //创建NioEventLoopGroup对象来处理事件，如接受新连接、接收数据、写数据等等 eventLoopGroup = new NioEventLoopGroup(); //指定通道类型为NioServerSocketChannel，设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 serverBootstrap.group(eventLoopGroup).channel(NioServerSocketChannel.class).localAddress("localhost",port).childHandler(new ChannelInitializer&lt;Channel&gt;() &#123; //设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(new EchoServerHandler()); &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println("开始监听，端口为：" + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; 服务端回调方法 12345678910111213141516171819202122232425262728293031323334353637383940import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.util.Date;public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("server 读取数据……"); //读取数据 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "UTF-8"); System.out.println("接收客户端数据:" + body); //向客户端写数据 System.out.println("server向client发送数据"); String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("server 读取数据完毕.."); ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 客户端启动类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;import java.net.InetSocketAddress;/** * • 连接服务器 • 写数据到服务器 • 等待接受服务器返回相同的数据 • 关闭连接 * * @author wilson * */public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup nioEventLoopGroup = null; try &#123; //创建Bootstrap对象用来引导启动客户端 Bootstrap bootstrap = new Bootstrap(); //创建EventLoopGroup对象并设置到Bootstrap中，EventLoopGroup可以理解为是一个线程池，这个线程池用来处理连接、接受数据、发送数据 nioEventLoopGroup = new NioEventLoopGroup(); //创建InetSocketAddress并设置到Bootstrap中，InetSocketAddress是指定连接的服务器地址 bootstrap.group(nioEventLoopGroup).channel(NioSocketChannel.class).remoteAddress(new InetSocketAddress(host, port)) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //添加一个ChannelHandler，客户端成功连接服务器后就会被执行 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoClientHandler()); &#125; &#125;); // • 调用Bootstrap.connect()来连接服务器 ChannelFuture f = bootstrap.connect().sync(); // • 最后关闭EventLoopGroup来释放资源 f.channel().closeFuture().sync(); &#125; finally &#123; nioEventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoClient("localhost", 20000).start(); &#125;&#125; 客户端回调方法 1234567891011121314151617181920212223242526272829303132333435import io.netty.buffer.ByteBuf;import io.netty.buffer.ByteBufUtil;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler; public class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; //客户端连接服务器后被调用 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("客户端连接服务器，开始发送数据……"); byte[] req = "QUERY TIME ORDER".getBytes(); ByteBuf firstMessage = Unpooled.buffer(req.length); firstMessage.writeBytes(req); ctx.writeAndFlush(firstMessage); &#125; //• 从服务器接收到数据后调用 @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; System.out.println("client 读取server数据.."); //服务端返回消息后 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "UTF-8"); System.out.println("服务端数据为 :" + body);&#125; //• 发生异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught.."); // 释放资源 ctx.close(); &#125; &#125; netty中handler的执行顺序简介Handler在netty中，无疑占据着非常重要的地位。Handler与Servlet中的filter很像，通过Handler可以完成通讯报文的解码编码、拦截指定的报文、统一对日志错误进行处理、统一对请求进行计数、控制Handler执行与否。一句话，没有它做不到的只有你想不到的。 Netty中的所有handler都实现自ChannelHandler接口。按照输出输出来分，分为ChannelInboundHandler、ChannelOutboundHandler两大类。ChannelInboundHandler对从客户端发往服务器的报文进行处理，一般用来执行解码、读取客户端数据、进行业务处理等；ChannelOutboundHandler对从服务器发往客户端的报文进行处理，一般用来进行编码、发送报文到客户端。 Netty中，可以注册多个handler。ChannelInboundHandler按照注册的先后顺序执行；ChannelOutboundHandler按照注册的先后顺序逆序执行，如下图所示，按照注册的先后顺序对Handler进行排序，request进入Netty后的执行顺序为： 总结在使用Handler的过程中，需要注意： ChannelInboundHandler之间的传递，通过调用 ctx.fireChannelRead(msg) 实现；调用ctx.write(msg) 将传递到ChannelOutboundHandler。 ctx.write()方法执行后，需要调用flush()方法才能令它立即执行。 流水线pipeline中outhandler不能放在最后，否则不生效 Handler的消费处理放在最后一个处理。 如何通过netty发送对象netty发送对象Netty中，通讯的双方建立连接后，会把数据按照ByteBuf的方式进行传输，例如http协议中，就是通过HttpRequestDecoder对ByteBuf数据流进行处理，转换成http的对象。基于这个思路，我自定义一种通讯协议：Server和客户端直接传输java对象。 实现的原理是通过Encoder把java对象转换成ByteBuf流进行传输，通过Decoder把ByteBuf转换成java对象进行处理，处理逻辑如下图所示： Spring(IOC/AOP) 注解spring的初始化顺序在spring的配置文件中配置bean，如下 在One类和Two类中，分别实现一个参数的构造如下 加载spring配置文件，初始化bean如下 那么。结果如何呢？ 结论：spring会按照bean的顺序依次初始化xml中配置的所有bean 通过ApplicationContextAware加载Spring上下文环境在One中实现ApplicationContextAware接口会出现如何的变换呢？ 结果 InitializingBean的作用在One中实现InitializingBean接口呢? 结果: 如果使用注解@Component使用@Component注入类，那么它的顺序是如何呢？ 1、 spring先检查注解注入的bean，并将它们实例化 2、 然后spring初始化bean的顺序是按照xml中配置的顺序依次执行构造 3、 如果某个类实现了ApplicationContextAware接口，会在类初始化完成后调用setApplicationContext（）方法进行操作 4、 如果某个类实现了InitializingBean接口，会在类初始化完成后，并在setApplicationContext（）方法执行完毕后，调用afterPropertiesSet（）方法进行操作 注解使用回顾​ 1、在spring中，用注解来向Spring容器注册Bean。需要在applicationContext.xml中注册&lt;context:component-scan base-package=”pagkage1[,pagkage2,…,pagkageN]”/&gt;。 ​ 2、如果某个类的头上带有特定的注解@Component/@Repository/@Service/@Controller，就会将这个对象作为Bean注册进Spring容器 ​ 3、在使用spring管理的bean时，无需在对调用的对象进行new的过程，只需使用@Autowired将需要的bean注入本类即可 自定义注解解释 自定义注解的作用：在反射中获取注解，以取得注解修饰的“类、方法、属性”的相关解释。 java内置注解 ​ @Target 表示该注解用于什么地方，可能的 ElemenetType 参数包括： ​ ElemenetType.CONSTRUCTOR 构造器声明 ​ ElemenetType.FIELD 域声明（包括 enum 实例） ​ ElemenetType.LOCAL_VARIABLE 局部变量声明 ​ ElemenetType.METHOD 方法声明 ​ ElemenetType.PACKAGE 包声明 ​ ElemenetType.PARAMETER 参数声明 ​ ElemenetType.TYPE 类，接口（包括注解类型）或enum声明 ​ @Retention 表示在什么级别保存该注解信息。可选的 RetentionPolicy 参数包括： ​ RetentionPolicy.SOURCE 注解将被编译器丢弃 ​ RetentionPolicy.CLASS 注解在class文件中可用，但会被VM丢弃 ​ RetentionPolicy.RUNTIME JVM将在运行期也保留注释，因此可以通过反射机制读取注解的信息。 实现 定义自定义注解 123456@Target(&#123; ElementType.TYPE &#125;)//注解用在接口上@Retention(RetentionPolicy.RUNTIME)//VM将在运行期也保留注释，因此可以通过反射机制读取注解的信息@Componentpublic @interface RpcService &#123; String value();&#125; 将直接类加到需要使用的类上，我们可以通过获取注解，来得到这个类 123456@RpcService("HelloService")public class HelloServiceImpl implements HelloService &#123; public String hello(String name) &#123; return "Hello! " + name; &#125;&#125; 类实现的接口 123public interface HelloService &#123; String hello(String name);&#125; 通过ApplicationContext获取所有标记这个注解的类 123456789101112131415161718192021@Componentpublic class MyServer implements ApplicationContextAware &#123; @SuppressWarnings("resource") public static void main(String[] args) &#123; new ClassPathXmlApplicationContext("spring2.xml"); &#125; public void setApplicationContext(ApplicationContext ctx) throws BeansException &#123; Map&lt;String, Object&gt; serviceBeanMap = ctx .getBeansWithAnnotation(HelloService.class); for (Object serviceBean : serviceBeanMap.values()) &#123; try &#123; Method method = serviceBean.getClass().getMethod("hello", new Class[]&#123;String.class&#125;); Object invoke = method.invoke(serviceBean, "bbb"); System.out.println(invoke); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 结合spring实现junit测试 123456789101112131415161718192021222324@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = "classpath:spring2.xml")public class MyServer implements ApplicationContextAware &#123; @Test public void helloTest1() &#123; &#125; public void setApplicationContext(ApplicationContext ctx) throws BeansException &#123; Map&lt;String, Object&gt; serviceBeanMap = ctx .getBeansWithAnnotation(HelloService.class); for (Object serviceBean : serviceBeanMap.values()) &#123; try &#123; Method method = serviceBean.getClass().getMethod("hello", new Class[] &#123; String.class &#125;); Object invoke = method.invoke(serviceBean, "bbb"); System.out.println(invoke); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 轻量级RPC框架开发netty实现的RPC的缺点在我们平常使用的RPC中，例如webservice，使用的习惯类似于下图 但是netty的实现过于底层，我们不能够像以前一样只关心方法的调用，而是要关心数据的传输，对于不熟悉netty的开发者，需要了解很多netty的概念和逻辑，才能实现RPC的调用。 应上面的需求，我们需要基于netty实现一个我们熟悉的RPC框架。逻辑如下： zk在框架中的实现在上面的框架中，server端存在着一个问题，就是单点问题，也就是说，当服务端“挂了”之后，框架的使用就造成了单点屏障。 我们可以通过zookeeper来实现服务端的负载均衡]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存模型]]></title>
    <url>%2F2019%2F12%2F25%2F14.Java%2FJava%E5%9F%BA%E7%A1%80%2FJVM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[JVM内存模型内存模型图解Java虚拟机在执行Java程序的过程中，会把它所管理的内存划分为若干个不同的数据区。这些区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有的区域则依赖用户线程的启动和结束而建立和销毁，我们可以将这些区域统称为Java运行时数据区域。 ​ 如下图是一个内存模型的关系图 ​ 如上图所示，Java虚拟机运行时数据区域被分为五个区域：堆(Heap)、栈(Stack)、本地方法栈(Native Stack)、方法区(Method Area)、程序计数器(Program Count Register) 堆(Heap)对于大多数应用来说，Java Heap是Java虚拟机管理的内存的最大一块，这块区域随着虚拟机的启动而创建。在实际的运用中，我们创建的对象和数组就是存放在堆里面。如果你听说线程安全的问题，就会很明确的知道Java Heap是一块共享的区域，操作共享区域的成员就有了锁和同步。 ​ 与Java Heap相关的还有Java的垃圾回收机制（GC）,Java Heap是垃圾回收器管理的主要区域。程序猿所熟悉的新生代、老生代、永久代的概念就是在堆里面，现在大多数的GC基本都采用了分代收集算法。如果再细致一点，Java Heap还有Eden空间，From Survivor空间,To Survivor空间等。 ​ Java Heap可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。 栈（Stack）相对于Java Heap来讲，Java Stack是线程私有的，她的生命周期与线程相同。Java Stack描述的是Java方法执行时的内存模型，每个方法执行时都会创建一个栈帧（Stack Frame）用来存储局部变量表、操作数栈、动态链接、方法出口等信息。从下图从可以看到，每个线程在执行一个方法时，都意味着有一个栈帧在当前线程对应的栈帧中入栈和出栈。 图中可以看到每一个栈帧中都有局部变量表。局部变量表存放了编译期间的各种基本数据类型，对象引用等信息。 本地方法栈（Native Stack）​ 本地方法栈（Native Stack）与Java虚拟机站（Java Stack）所发挥的作用非常相似，他们之间的区别在于虚拟机栈为虚拟机执行java方法（也就是字节码）服务，而本地方法栈则为使用到Native方法服务。 方法区（Method Area）​ 方法区（Method Area）与堆（Java Heap）一样，是各个线程共享的内存区域，它用于存储虚拟机加载的类信息，常量，静态变量，即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是她却有一个别名叫做非堆（Non-Heap）。分析下Java虚拟机规范，之所以把方法区描述为堆的一个逻辑部分，应该觉得她们都是存储数据的角度出发的。一个存储对象数据（堆），一个存储静态信息(方法区)。 ​ 在上文中，我们看到堆中有新生代、老生代、永久代的描述。为什么我们将新生代、老生代、永久代三个概念一起说，那是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。这样HotSpot的垃圾收集器就能想管理Java堆一样管理这部分内存。简单点说就是HotSpot虚拟机中内存模型的分代，其中新生代和老生代在堆中，永久代使用方法区实现。根据官方发布的路线图信息，现在也有放弃永久代并逐步采用Native Memory来实现方法区的规划，在JDK1.7的HotSpot中，已经把原本放在永久代的字符串常量池移出。 数据区域划分 线程私有区域 Java虚拟机栈（Java Stack） 本地方法栈（Native Stack） 线程共有的数据区域 堆（Java Heap） 方法区 (Method Area) JVM GC（垃圾回收机制）什么是GC机制在Java程序中不能显式的分配和注销缓存，因为这些事情JVM都帮我们做了，那就是GC。 JVM GC回收哪的垃圾？需要注意的是，JVM GC只回收堆区和方法区内的对象。而栈区的数据，在超出作用域后会被JVM自动释放掉，所以其不在JVM GC的管理范围内。 JVM GC怎么判断对象可以被回收了？ 对象没有引用 作用域发生未捕获异常 程序在作用域正常执行完毕 程序执行了System.exit() 程序发生意外终止（被杀线程等） 按代的垃圾回收机制 新生代（Young generation）：绝大多数最新被创建的对象都会被分配到这里，由于大部分在创建后很快变得不可达，很多对象被创建在新生代，然后“消失”。对象从这个区域“消失”的过程我们称之为：Minor GC 。 老年代（Old generation）：对象没有变得不可达，并且从新生代周期中存活了下来，会被拷贝到这里。其区域分配的空间要比新生代多。也正由于其相对大的空间，发生在老年代的GC次数要比新生代少得多。对象从老年代中消失的过程，称之为：Major GC 或者 Full GC。 持久代（Permanent generation）也称之为 方法区（Method area）：用于保存类常量以及字符串常量。注意，这个区域不是用于存储那些从老年代存活下来的对象，这个区域也可能发生GC。发生在这个区域的GC事件也被算为 Major GC 。只不过在这个区域发生GC的条件非常严苛，必须符合以下三种条件才会被回收：1、所有实例被回收； 2、加载该类的ClassLoader 被回收； 3、Class 对象无法通过任何途径访问（包括反射） 可能我们会有疑问： 如果老年代的对象需要引用新生代的对象，会发生什么呢？ 为了解决这个问题，老年代中存在一个 card table ，它是一个512byte大小的块。所有老年代的对象指向新生代对象的引用都会被记录在这个表中。当针对新生代执行GC的时候，只需要查询 card table 来决定是否可以被回收，而不用查询整个老年代。这个 card table 由一个write barrier 来管理。write barrier给GC带来了很大的性能提升，虽然由此可能带来一些开销，但完全是值得的。 默认新生代和老年代的比例 默认的新生代（Young generation）、老年代（Old generation）所占空间比例为 1 : 2 。 新生代老年代构成逻辑新生代构成逻辑为了更好的理解GC，我们来学习新生代的构成，它用来保存那些第一次被创建的对象，它被分成三个空间： 一个伊甸园空间（Eden） 两个幸存者空间（Fron Survivor、To Survivor） 默认新生代空间的分配：Eden : Fron : To = 8 : 1 : 1 每个空间的执行顺序如下： 1、绝大多数刚刚被创建的对象会存放在伊甸园空间（Eden）。 2、在伊甸园空间执行第一次GC（Minor GC）之后，存活的对象被移动到其中一个幸存者空间（Survivor）。 3、此后，每次伊甸园空间执行GC后，存活的对象会被堆积在同一个幸存者空间。 4、当一个幸存者空间饱和，还在存活的对象会被移动到另一个幸存者空间。然后会清空已经饱和的哪个幸存者空间。 5、在以上步骤中重复N次（N = MaxTenuringThreshold（年龄阀值设定，默认15））依然存活的对象，就会被移动到老年代。 从上面的步骤可以发现，两个幸存者空间，必须有一个是保持空的。如果两个两个幸存者空间都有数据，或两个空间都是空的，那一定是你的系统出现了某种错误。 我们需要重点记住的是，对象在刚刚被创建之后，是保存在伊甸园空间的（Eden）。那些长期存活的对象会经由幸存者空间（Survivor）转存到老年代空间（Old generation）。 也有例外出现，对于一些比较大的对象（需要分配一块比较大的连续内存空间）则直接进入到老年代。一般在Survivor 空间不足的情况下发生。 老年代构成逻辑老年代空间的构成其实很简单，它不像新生代空间那样划分为几个区域，它只有一个区域，里面存储的对象并不像新生代空间绝大部分都是朝闻道，夕死矣。这里的对象几乎都是从Survivor 空间中熬过来的，它们绝不会轻易的狗带。因此，Full GC（Major GC）发生的次数不会有Minor GC 那么频繁，并且做一次Major GC 的时间比Minor GC 要更长（约10倍）。 JVM GC什么时候执行？eden区空间不够存放新对象的时候，执行Minro GC。升到老年代的对象大于老年代剩余空间的时候执行Full GC，或者小于的时候被HandlePromotionFailure 参数强制Full GC 。调优主要是减少 Full GC 的触发次数，可以通过 NewRatio 控制新生代转老年代的比例，通过MaxTenuringThreshold 设置对象进入老年代的年龄阀值（后面会介绍到）。 JVM GC 算法讲解1、根搜索算法根搜索算法是从离散数学中的图论引入的，程序把所有引用关系看作一张图，从一个节点GC ROOT 开始，寻找对应的引用节点，找到这个节点后，继续寻找这个节点的引用节点。当所有的引用节点寻找完毕后，剩余的节点则被认为是没有被引用到的节点，即无用的节点。 上图红色为无用的节点，可以被回收。 目前Java中可以作为GC ROOT的对象有： 1、虚拟机栈中引用的对象（本地变量表） 2、方法区中静态属性引用的对象 3、方法区中常亮引用的对象 4、本地方法栈中引用的对象（Native对象） 基本所有GC算法都引用根搜索算法这种概念。 2、标记 - 清除算法 标记-清除算法采用从根集合进行扫描，对存活的对象进行标记，标记完毕后，再扫描整个空间中未被标记的对象进行直接回收，如上图。 标记-清除算法不需要进行对象的移动，并且仅对不存活的对象进行处理，在存活的对象比较多的情况下极为高效，但由于标记-清除算法直接回收不存活的对象，并没有对还存活的对象进行整理，因此会导致内存碎片。 3、复制算法 复制算法将内存分为两个区间，使用此算法时，所有动态分配的对象都只能分配在其中一个区间（活动区间），而另外一个区间（空间区间）则是空闲的。 复制算法采用从根集合扫描，将存活的对象复制到空闲区间，当扫描完毕活动区间后，会的将活动区间一次性全部回收。此时原本的空闲区间变成了活动区间。下次GC时候又会重复刚才的操作，以此循环。 复制算法在存活对象比较少的时候，极为高效，但是带来的成本是牺牲一半的内存空间用于进行对象的移动。所以复制算法的使用场景，必须是对象的存活率非常低才行，而且最重要的是，我们需要克服50%内存的浪费。 4、标记 - 整理算法 标记-整理算法采用 标记-清除 算法一样的方式进行对象的标记、清除，但在回收不存活的对象占用的空间后，会将所有存活的对象往左端空闲空间移动，并更新对应的指针。标记-整理 算法是在标记-清除 算法之上，又进行了对象的移动排序整理，因此成本更高，但却解决了内存碎片的问题。 JVM为了优化内存的回收，使用了分代回收的方式，对于新生代内存的回收（Minor GC）主要采用复制算法。而对于老年代的回收（Major GC），大多采用标记-整理算法。 垃圾回收器简介需要注意的是，每一个回收器都存在Stop The World 的问题，只不过各个回收器在Stop The World 时间优化程度、算法的不同，可根据自身需求选择适合的回收器。 1、Serial（-XX:+UseSerialGC）从名字我们可以看出，这是一个串行收集器。 Serial收集器是Java虚拟机中最基本、历史最悠久的收集器。在JDK1.3之前是Java虚拟机新生代收集器的唯一选择。目前也是ClientVM下ServerVM 4核4GB以下机器默认垃圾回收器。Serial收集器并不是只能使用一个CPU进行收集，而是当JVM需要进行垃圾回收的时候，需暂停所有的用户线程，直到回收结束。 使用算法：复制算法 JVM中文名称为Java虚拟机，因此它像一台虚拟的电脑在工作，而其中的每一个线程都被认为是JVM的一个处理器，因此图中的CPU0、CPU1实际上为用户的线程，而不是真正的机器CPU，不要误解哦。 Serial收集器虽然是最老的，但是它对于限定单个CPU的环境来说，由于没有线程交互的开销，专心做垃圾收集，所以它在这种情况下是相对于其他收集器中最高效的。 2、SerialOld（-XX:+UseSerialGC）SerialOld是Serial收集器的老年代收集器版本，它同样是一个单线程收集器，这个收集器目前主要用于Client模式下使用。如果在Server模式下，它主要还有两大用途：一个是在JDK1.5及之前的版本中与Parallel Scavenge收集器搭配使用，另外一个就是作为CMS收集器的后备预案，如果CMS出现Concurrent Mode Failure，则SerialOld将作为后备收集器。 使用算法：标记 - 整理算法 运行示意图与上图一致。 3、ParNew（-XX:+UseParNewGC）ParNew其实就是Serial收集器的多线程版本。除了Serial收集器外，只有它能与CMS收集器配合工作。 使用算法：复制算法 ParNew是许多运行在Server模式下的JVM首选的新生代收集器。但是在单CPU的情况下，它的效率远远低于Serial收集器，所以一定要注意使用场景。 4、ParallelScavenge（-XX:+UseParallelGC）ParallelScavenge又被称为吞吐量优先收集器，和ParNew 收集器类似，是一个新生代收集器。 使用算法：复制算法 ParallelScavenge收集器的目标是达到一个可控件的吞吐量，所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量 = 运行用户代码时间 / （运行用户代码时间 + 垃圾收集时间）。如果虚拟机总共运行了100分钟，其中垃圾收集花了1分钟，那么吞吐量就是99% 。 5、ParallelOld（-XX:+UseParallelOldGC）ParallelOld是并行收集器，和SerialOld一样，ParallelOld是一个老年代收集器，是老年代吞吐量优先的一个收集器。这个收集器在JDK1.6之后才开始提供的，在此之前，ParallelScavenge只能选择SerialOld来作为其老年代的收集器，这严重拖累了ParallelScavenge整体的速度。而ParallelOld的出现后，“吞吐量优先”收集器才名副其实！ 使用算法：标记 - 整理算法 在注重吞吐量与CPU数量大于1的情况下，都可以优先考虑ParallelScavenge + ParalleloOld收集器。 6、CMS （-XX:+UseConcMarkSweepGC）CMS是一个老年代收集器，全称 Concurrent Low Pause Collector，是JDK1.4后期开始引用的新GC收集器，在JDK1.5、1.6中得到了进一步的改进。它是对于响应时间的重要性需求大于吞吐量要求的收集器。对于要求服务器响应速度高的情况下，使用CMS非常合适。 CMS的一大特点，就是用两次短暂的暂停来代替串行或并行标记整理算法时候的长暂停。 使用算法：标记 - 清理 CMS的执行过程如下： 初始标记（STW initial mark） 在这个阶段，需要虚拟机停顿正在执行的应用线程，官方的叫法STW（Stop Tow World）。这个过程从根对象扫描直接关联的对象，并作标记。这个过程会很快的完成。 并发标记（Concurrent marking） 这个阶段紧随初始标记阶段，在“初始标记”的基础上继续向下追溯标记。注意这里是并发标记，表示用户线程可以和GC线程一起并发执行，这个阶段不会暂停用户的线程哦。 并发预清理（Concurrent precleaning） 这个阶段任然是并发的，JVM查找正在执行“并发标记”阶段时候进入老年代的对象（可能这时会有对象从新生代晋升到老年代，或被分配到老年代）。通过重新扫描，减少在一个阶段“重新标记”的工作，因为下一阶段会STW。 重新标记（STW remark） 这个阶段会再次暂停正在执行的应用线程，重新重根对象开始查找并标记并发阶段遗漏的对象（在并发标记阶段结束后对象状态的更新导致），并处理对象关联。这一次耗时会比“初始标记”更长，并且这个阶段可以并行标记。 并发清理（Concurrent sweeping） 这个阶段是并发的，应用线程和GC清除线程可以一起并发执行。 并发重置（Concurrent reset） 这个阶段任然是并发的，重置CMS收集器的数据结构，等待下一次垃圾回收。 CMS的缺点： 内存碎片。由于使用了 标记-清理 算法，导致内存空间中会产生内存碎片。不过CMS收集器做了一些小的优化，就是把未分配的空间汇总成一个列表，当有JVM需要分配内存空间的时候，会搜索这个列表找到符合条件的空间来存储这个对象。但是内存碎片的问题依然存在，如果一个对象需要3块连续的空间来存储，因为内存碎片的原因，寻找不到这样的空间，就会导致Full GC。 需要更多的CPU资源。由于使用了并发处理，很多情况下都是GC线程和应用线程并发执行的，这样就需要占用更多的CPU资源，也是牺牲了一定吞吐量的原因。 需要更大的堆空间。因为CMS标记阶段应用程序的线程还是执行的，那么就会有堆空间继续分配的问题，为了保障CMS在回收堆空间之前还有空间分配给新加入的对象，必须预留一部分空间。CMS默认在老年代空间使用68%时候启动垃圾回收。可以通过-XX:CMSinitiatingOccupancyFraction=n来设置这个阀值。 7、GarbageFirst（G1）这是一个新的垃圾回收器，既可以回收新生代也可以回收老年代，SunHotSpot1.6u14以上EarlyAccess版本加入了这个回收器，Sun公司预期SunHotSpot1.7发布正式版本。通过重新划分内存区域，整合优化CMS，同时注重吞吐量和响应时间。杯具的是Oracle收购这个收集器之后将其用于商用收费版收集器。因此目前暂时没有发现哪个公司使用它，这个放在之后再去研究吧。 收集器整理新生代收集器： Serial （-XX:+UseSerialGC） ParNew（-XX:+UseParNewGC） ParallelScavenge（-XX:+UseParallelGC） G1 收集器 老年代收集器： SerialOld（-XX:+UseSerialOldGC） ParallelOld（-XX:+UseParallelOldGC） CMS（-XX:+UseConcMarkSweepGC） G1 收集器 内存溢出和内存泄露的区别1、内存溢出 内存溢出指的是程序在申请内存的时候，没有足够大的空间可以分配了。 2、内存泄露 内存泄露指的是程序在申请内存之后，没有办法释放掉已经申请到内存，它始终占用着内存，即被分配的对象可达但无用。内存泄露一般都是因为内存中有一块很大的对象，但是无法释放。 从定义上可以看出，内存泄露终将导致内存溢出。 注意，定位虚拟机问题内存问题的时候第一步就是要判断到底是内存溢出还是内存泄露，前者好判断，跟踪堆栈信息就可以了；后者比较复杂一点，一般都是老年代中的大对象没释放掉，要通过各种办法找出老年代中的大对象没有被释放的原因。 并行和并发的区别这两个名词都是并发编程中的概念，在谈论垃圾收集器的上下文语境中，可以这么理解这两个名词： 1、并行Parallel 多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态 2、并发Concurrent 指用户线程与垃圾收集线程同时执行（但并不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上 Minor GC和Full GC的区别1、新生代GC（Minor GC） 指发生在新生代的垃圾收集动作，因为大多数Java对象存活率都不高，所以Minor GC非常频繁，一般回收速度也比较快 2、老年代GC（Major GC/Full GC） 指发生在老年代的垃圾收集动作，出现了Major GC，经常会伴随至少一次的Minor GC（但并不是绝对的）。Major GC的速度一般要比Minor GC慢上10倍以上 JVM 参数列表命令格式1java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0 jvm参数列表内存相关参数 12345678910111213141516171819202122232425-Xmx3550m：最大堆内存为3550M。-Xms3550m：初始堆内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xmn2g：设置年轻代大小为2G。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。-Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000左右。 -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6-XX:MaxPermSize=16m:设置持久代大小为16m。-XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直 接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象 再年轻代的存活时间，增加在年轻代即被回收的概论。 gc相关参数 12345678910111213141516171819202122232425-Xmx3550m：最大堆内存为3550M。-Xms3550m：初始堆内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xmn2g：设置年轻代大小为2G。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。-Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000左右。 -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6-XX:MaxPermSize=16m:设置持久代大小为16m。-XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直 接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象 再年轻代的存活时间，增加在年轻代即被回收的概论。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 代理、反射]]></title>
    <url>%2F2019%2F12%2F24%2F14.Java%2FJava%E5%9F%BA%E7%A1%80%2Fjava%20%E4%BB%A3%E7%90%86%E3%80%81%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[反射什么是反射 在Java里面一个类有两种状态–编译和运行状态，通常我们需要获取这个类的信息都是在编译阶段获得的，也就是直接点出来或者new出来，可是如果需要在类运行的阶段获得Java的类的信息的话， 就需要用到Java的反射。 反射的作用反射被广泛地用于那些需要在运行时检测或修改程序行为的程序中。这是一个相对高级的特性，只有那些语言基础非常扎实的开发者才应该使用它。如果能把这句警示时刻放在心里，那么反射机制就会成为一项强大的技术，可以让应用程序做一些几乎不可能做到的事情。 反射的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;import java.util.ArrayList;import java.util.List;import org.junit.Before;import org.junit.Test;public class MyReflect &#123; public String className = null; @SuppressWarnings("rawtypes") public Class personClass = null; /** * 反射Person类 * @throws Exception */ @Before public void init() throws Exception &#123; className = "cn.java.reflect.Person"; personClass = Class.forName(className); &#125; /** *获取某个class文件对象 */ @Test public void getClassName() throws Exception &#123; System.out.println(personClass); &#125; /** *获取某个class文件对象的另一种方式 */ @Test public void getClassName2() throws Exception &#123; System.out.println(Person.class); &#125; /** *创建一个class文件表示的真实对象，底层会调用空参数的构造方法 */ @Test public void getNewInstance() throws Exception &#123; System.out.println(personClass.newInstance()); &#125; /** *获取非私有的构造函数 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getPublicConstructor() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class,String.class); Person person = (Person)constructor.newInstance(100L,"zhangsan"); System.out.println(person.getId()); System.out.println(person.getName()); &#125; /** *获得私有的构造函数 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getPrivateConstructor() throws Exception &#123; Constructor con = personClass.getDeclaredConstructor(String.class); con.setAccessible(true);//强制取消Java的权限检测 Person person2 = (Person)con.newInstance("zhangsan"); System.out.println(person2.getName()); &#125; /** *获取非私有的成员变量 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getNotPrivateField() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class,String.class); Object obj = constructor.newInstance(100L,"zhangsan"); Field field = personClass.getField("name"); field.set(obj, "lisi"); System.out.println(field.get(obj)); &#125; /** *获取私有的成员变量 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getPrivateField() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class); Object obj = constructor.newInstance(100L); Field field2 = personClass.getDeclaredField("id"); field2.setAccessible(true);//强制取消Java的权限检测 field2.set(obj,10000L); System.out.println(field2.get(obj)); &#125; /** *获取非私有的成员函数 */ @SuppressWarnings(&#123; "unchecked" &#125;) @Test public void getNotPrivateMethod() throws Exception &#123; System.out.println(personClass.getMethod("toString")); Object obj = personClass.newInstance();//获取空参的构造函数 Object object = personClass.getMethod("toString").invoke(obj); System.out.println(object); &#125; /** *获取私有的成员函数 */ @SuppressWarnings("unchecked") @Test public void getPrivateMethod() throws Exception &#123; Object obj = personClass.newInstance();//获取空参的构造函数 Method method = personClass.getDeclaredMethod("getSomeThing"); method.setAccessible(true); Object value = method.invoke(obj); System.out.println(value); &#125; /** * */ @Test public void otherMethod() throws Exception &#123; //当前加载这个class文件的那个类加载器对象 System.out.println(personClass.getClassLoader()); //获取某个类实现的所有接口 Class[] interfaces = personClass.getInterfaces(); for (Class class1 : interfaces) &#123; System.out.println(class1); &#125; //反射当前这个类的直接父类 System.out.println(personClass.getGenericSuperclass()); /** * getResourceAsStream这个方法可以获取到一个输入流，这个输入流会关联到name所表示的那个文件上。 */ //path 不以’/'开头时默认是从此类所在的包下取资源，以’/'开头则是从ClassPath根下获取。其只是通过path构造一个绝对路径，最终还是由ClassLoader获取资源。 System.out.println(personClass.getResourceAsStream("/log4j.properties")); //默认则是从ClassPath根下获取，path不能以’/'开头，最终是由ClassLoader获取资源。 System.out.println(personClass.getResourceAsStream("/log4j.properties")); //判断当前的Class对象表示是否是数组 System.out.println(personClass.isArray()); System.out.println(new String[3].getClass().isArray()); //判断当前的Class对象表示是否是枚举类 System.out.println(personClass.isEnum()); System.out.println(Class.forName("cn.java.reflect.City").isEnum()); //判断当前的Class对象表示是否是接口 System.out.println(personClass.isInterface()); System.out.println(Class.forName("cn.java.reflect.TestInterface").isInterface()); &#125;&#125; 动态代理java 动态代理的设计模式Java动态代理的优势是实现无侵入式的代码扩展，也就是方法的增强；让你可以在不用修改源码的情况下，增强一些方法；在方法的前后你可以做你任何想做的事情（甚至不去执行这个方法就可以）。 动态代理为其它对象提供一种代理以控制对这个对象的访问控制；在某些情况下，客户不想或者不能直接引用另一个对象，这时候代理对象可以在客户端和目标对象之间起到中介的作用。 静态代理 静态代理类：由程序员创建或者由第三方工具生成，再进行编译；在程序运行之前，代理类的.class文件已经存在了。 静态代理类通常只代理一个类。 静态代理事先知道要代理的是什么。 动态代理 动态代理类：在程序运行时，通过反射机制动态生成。 动态代理类通常代理接口下的所有类。 动态代理事先不知道要代理的是什么，只有在运行的时候才能确定。 动态代理的调用处理程序必须实现InvocationHandler接口，及使用Proxy类中的newProxyInstance方法动态的创建代理类。 Java动态代理只能代理接口，要代理类需要使用第三方的CLIGB等类库。 动态代理实现流程 书写代理类和代理方法，在代理方法中实现代理Proxy.newProxyInstance 代理中需要的参数分别为：被代理的类的类加载器soneObjectclass.getClassLoader()，被代理类的所有实现接口 new Class[] { Interface.class }，句柄方法new InvocationHandler() 在句柄方法中复写invoke方法，invoke方法的输入有3个参数Object proxy（代理类对象）, Method method（被代理类的方法）,Object[] args（被代理类方法的传入参数），在这个方法中，我们可以定制化的开发新的业务。 获取代理类，强转成被代理的接口 最后，我们可以像没被代理一样，调用接口的认可方法，方法被调用后，方法名和参数列表将被传入代理类的invoke方法中，进行新业务的逻辑流程。 动态代理代码示例接口类 123public interface IBoss &#123;//接口 int yifu(String size);&#125; 业务代码 1234567public class Boss implements IBoss&#123; public int yifu(String size)&#123; System.err.println("天猫小强旗舰店，老板给客户发快递----衣服型号："+size); //这件衣服的价钱，从数据库读取 return 50; &#125;&#125; 原调用方法调用代码 123456789public class SaleAction &#123; @Test public void saleByBossSelf() throws Exception &#123; IBoss boss = new Boss(); System.out.println("老板自营！"); int money = boss.yifu("xxl"); System.out.println("衣服成交价：" + money); &#125;&#125; 使用代理类进行调用代理类 12345678910111213public static IBoss getProxyBoss(final int discountCoupon) throws Exception &#123; Object proxedObj = Proxy.newProxyInstance(Boss.class.getClassLoader(), new Class[] &#123; IBoss.class &#125;, new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Integer returnValue = (Integer) method.invoke(new Boss(), args);// 调用原始对象以后返回的值 return returnValue - discountCoupon; &#125; &#125;); return (IBoss)proxedObj;&#125;&#125; 通过代理调用代码 123456789public class ProxySaleAction &#123; @Test public void saleByProxy() throws Exception &#123; IBoss boss = ProxyBoss.getProxyBoss(20);// 将代理的方法实例化成接口 System.out.println("代理经营！"); int money = boss.yifu("xxl");// 调用接口的方法，实际上调用方式没有变 System.out.println("衣服成交价：" + money); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java JMS基础]]></title>
    <url>%2F2019%2F12%2F23%2F14.Java%2FJava%E5%9F%BA%E7%A1%80%2FJMS%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[什么是JMS​ JMS即Java消息服务（Java Message Service）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持。​ JMS是一种与厂商无关的 API，用来访问消息收发系统消息。它类似于JDBC(Java Database Connectivity)：这里，JDBC 是可以用来访问许多不同关系数据库的 API，而 JMS 则提供同样与厂商无关的访问方法，以访问消息收发服务。许多厂商都支持 JMS，包括 IBM 的 MQSeries、BEA的 Weblogic JMS service和 Progress 的 SonicMQ，这只是几个例子。 JMS 使您能够通过消息收发服务（有时称为消息中介程序或路由器）从一个 JMS 客户机向另一个 JMS客户机发送消息。消息是 JMS 中的一种类型对象，由两部分组成：报头和消息主体。报头由路由信息以及有关该消息的元数据组成。消息主体则携带着应用程序的数据或有效负载。根据有效负载的类型来划分，可以将消息分为几种类型，它们分别携带：简单文本(TextMessage)、可序列化的对象 (ObjectMessage)、属性集合 (MapMessage)、字节流 (BytesMessage)、原始值流 (StreamMessage)，还有无有效负载的消息 (Message)。 JMS规范 专业技术规范JMS（Java Messaging Service）是Java平台上有关面向消息中间件(MOM)的技术规范，它便于消息系统中的Java应用程序进行消息交换,并且通过提供标准的产生、发送、接收消息的接口简化企业应用的开发，翻译为Java消息服务。 体系架构JMS由以下元素组成。JMS提供者provider：连接面向消息中间件的，JMS接口的一个实现。提供者可以是Java平台的JMS实现，也可以是非Java平台的面向消息中间件的适配器。JMS客户：生产或消费基于消息的Java的应用程序或对象。JMS生产者：创建并发送消息的JMS客户。JMS消费者：接收消息的JMS客户。JMS消息：包括可以在JMS客户之间传递的数据的对象JMS队列：一个容纳那些被发送的等待阅读的消息的区域。与队列名字所暗示的意思不同，消息的接受顺序并不一定要与消息的发送顺序相同。一旦一个消息被阅读，该消息将被从队列中移走。JMS主题：一种支持发送消息给多个订阅者的机制。 Java消息服务应用程序结构支持两种模型1、点对点或队列模型在点对点或队列模型下，一个生产者向一个特定的队列发布消息，一个消费者从该队列中读取消息。这里，生产者知道消费者的队列，并直接将消息发送到消费者的队列。 这种模式被概括为：只有一个消费者将获得消息生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。每一个成功处理的消息都由接收者签收 2、发布者/订阅者模型发布者/订阅者模型支持向一个特定的消息主题发布消息。0或多个订阅者可能对接收来自特定消息主题的消息感兴趣。在这种模型下，发布者和订阅者彼此不知道对方。这种模式好比是匿名公告板。 这种模式被概括为：多个消费者可以获得消息在发布者和订阅者之间存在时间依赖性。发布者需要建立一个订阅（subscription），以便客户能够订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者建立了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。 acitveMQ部署1.下载ActiveMQ去官方网站下载：http://activemq.apache.org/ 2.运行ActiveMQ解压缩apache-activemq-5.5.1-bin.zip， 修改配置文件activeMQ.xml，将0.0.0.0修改为localhost 123456&lt;transportConnectors&gt; &lt;transportConnector name="openwire" uri="tcp://localhost:61616"/&gt; &lt;transportConnector name="ssl" uri="ssl://localhost:61617"/&gt; &lt;transportConnector name="stomp" uri="stomp://localhost:61613"/&gt; &lt;transportConnector uri="http://localhost:8081"/&gt; &lt;transportConnector uri="udp://localhost:61618"/&gt; 然后双击apache-activemq-5.5.1\bin\activemq.bat运行ActiveMQ程序。 启动ActiveMQ以后，登陆：http://localhost:8161/admin/，创建一个Queue，命名为FirstQueue。 activeMQ使用代码示例消费者使用方法consumerTool 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import javax.jms.Connection; import javax.jms.Destination; import javax.jms.ExceptionListener;import javax.jms.JMSException; import javax.jms.MessageConsumer; import javax.jms.Session; import javax.jms.MessageListener; import javax.jms.Message; import javax.jms.TextMessage; import org.apache.activemq.ActiveMQConnection; import org.apache.activemq.ActiveMQConnectionFactory; public class ConsumerTool implements MessageListener,ExceptionListener &#123; private String user = ActiveMQConnection.DEFAULT_USER; private String password = ActiveMQConnection.DEFAULT_PASSWORD; private String url =ActiveMQConnection.DEFAULT_BROKER_URL; private String subject = "mytopic"; private Destination destination = null; private Connection connection = null; private Session session = null; private MessageConsumer consumer = null; public static Boolean isconnection=false; // 初始化 private void initialize() throws JMSException, Exception &#123; ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory( user, password, url); connection = connectionFactory.createConnection(); session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); destination = session.createTopic(subject); consumer = session.createConsumer(destination); &#125; // 消费消息 public void consumeMessage() throws JMSException, Exception &#123; initialize(); connection.start(); consumer.setMessageListener(this); connection.setExceptionListener(this); isconnection=true; System.out.println("Consumer:-&gt;Begin listening..."); // 开始监听 // Message message = consumer.receive(); &#125; // 关闭连接 public void close() throws JMSException &#123; System.out.println("Consumer:-&gt;Closing connection"); if (consumer != null) consumer.close(); if (session != null) session.close(); if (connection != null) connection.close(); &#125; // 消息处理函数 public void onMessage(Message message) &#123; try &#123; if (message instanceof TextMessage) &#123; TextMessage txtMsg = (TextMessage) message; String msg = txtMsg.getText(); System.out.println("Consumer:-&gt;Received: " + msg); &#125; else &#123; System.out.println("Consumer:-&gt;Received: " + message); &#125; &#125; catch (JMSException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public void onException(JMSException arg0) &#123; isconnection=false; &#125; &#125; consumerRun 123456789101112131415161718192021222324252627282930313233343536373839404142434445import javax.jms.JMSException;public class ConsumerTest implements Runnable &#123; static Thread t1 = null; /** * @param args * @throws InterruptedException * @throws InterruptedException * @throws JMSException * @throws InterruptedException */ public static void main(String[] args) throws InterruptedException &#123; t1 = new Thread(new ConsumerTest()); t1.setDaemon(false); t1.start(); /** * 如果发生异常，则重启consumer */ /*while (true) &#123; System.out.println(t1.isAlive()); if (!t1.isAlive()) &#123; t1 = new Thread(new ConsumerTest()); t1.start(); System.out.println("重新启动"); &#125; Thread.sleep(5000); &#125;*/ // 延时500毫秒之后停止接受消息 // Thread.sleep(500); // consumer.close(); &#125; public void run() &#123; try &#123; ConsumerTool consumer = new ConsumerTool(); consumer.consumeMessage(); while (ConsumerTool.isconnection) &#123; &#125; &#125; catch (Exception e) &#123; &#125; &#125;&#125; 发布者使用producerTool 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import javax.jms.Connection; import javax.jms.DeliveryMode; import javax.jms.Destination; import javax.jms.JMSException; import javax.jms.MessageProducer; import javax.jms.Session; import javax.jms.TextMessage; import org.apache.activemq.ActiveMQConnection; import org.apache.activemq.ActiveMQConnectionFactory; public class ProducerTool &#123; private String user = ActiveMQConnection.DEFAULT_USER; private String password = ActiveMQConnection.DEFAULT_PASSWORD; private String url = ActiveMQConnection.DEFAULT_BROKER_URL; private String subject = "mytopic"; private Destination destination = null; private Connection connection = null; private Session session = null; private MessageProducer producer = null; // 初始化 private void initialize() throws JMSException, Exception &#123; ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory( user, password, url); connection = connectionFactory.createConnection(); session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); destination = session.createTopic(subject); producer = session.createProducer(destination); producer.setDeliveryMode(DeliveryMode.NON_PERSISTENT); &#125; // 发送消息 public void produceMessage(String message) throws JMSException, Exception &#123; initialize(); TextMessage msg = session.createTextMessage(message); connection.start(); System.out.println("Producer:-&gt;Sending message: " + message); producer.send(msg); System.out.println("Producer:-&gt;Message sent complete!"); &#125; // 关闭连接 public void close() throws JMSException &#123; System.out.println("Producer:-&gt;Closing connection"); if (producer != null) producer.close(); if (session != null) session.close(); if (connection != null) connection.close(); &#125; &#125; producerRun 12345678910111213141516171819202122import java.util.Random;import javax.jms.JMSException; public class ProducerTest &#123; /** * @param args */ public static void main(String[] args) throws JMSException, Exception &#123; ProducerTool producer = new ProducerTool(); Random random = new Random(); for(int i=0;i&lt;20;i++)&#123; Thread.sleep(random.nextInt(10)*1000); producer.produceMessage("Hello, world!--"+i); producer.close(); &#125; &#125; &#125; JMS 实现要 使用Java消息服务，你必须要有一个JMS提供者，管理会话和队列。既有开源的提供者也有专有的提供者。 开源的提供者包括： Apache ActiveMQ JBoss 社区所研发的 HornetQ Joram Coridan的MantaRay The OpenJMS Group的OpenJMS 专有的提供者包括： BEA的BEA WebLogic Server JMS TIBCO Software的EMS GigaSpaces Technologies的GigaSpaces Softwired 2006的iBus IONA Technologies的IONA JMS SeeBeyond的IQManager（2005年8月被Sun Microsystems并购） webMethods的JMS+ - my-channels的Nirvana Sonic Software的SonicMQ SwiftMQ的SwiftMQ IBM的WebSphere MQ kafka不满足有序的队列，所以只可作为缓存中间件]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 多线程]]></title>
    <url>%2F2019%2F12%2F23%2F14.Java%2FJava%E5%9F%BA%E7%A1%80%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程和线程介绍什么是进程​ 不管是我们开发的应用程序，还是我们运行的其他的应用程序，都需要先把程序安装在本地的硬盘上。然后找到这个程序的启动文件，启动程序的时候，其实是电脑把当前的这个程序加载到内存中，在内存中需要给当前的程序分配一段独立的运行空间。这片空间就专门负责当前这个程序的运行。 ​ 不同的应用程序运行的过程中都需要在内存中分配自己独立的运行空间，彼此之间不会相互的影响。我们把每个独立应用程序在内存的独立空间称为当前应用程序运行的一个进程。 进程：它是内存中的一段独立的空间，可以负责当前应用程序的运行。当前这个进程负责调度当前程序中的所有运行细节。 什么是线程​ 启动的QQ聊天软件，需要和多个人进行聊天。这时多个人之间是不能相互影响，但是它们都位于当前QQ这个软件运行时所分配的内存的独立空间中。 ​ 在一个进程中，每个独立的功能都需要独立的去运行，这时又需要把当前这个进程划分成多个运行区域，每个独立的小区域（小单元）称为一个线程。 线程：它是位于进程中，负责当前进程中的某个具备独立运行资格的空间。 进程是负责整个程序的运行，而线程是程序中具体的某个独立功能的运行。一个进程中至少应该有一个线程。 多线程介绍​ 现在的操作系统基本都是多用户，多任务的操作系统。每个任务就是一个进程。而在这个进程中就会有线程。 ​ 真正可以完成程序运行和功能的实现靠的是进程中的线程。 多线程：在一个进程中，我们同时开启多个线程，让多个线程同时去完成某些任务（功能）。 (比如后台服务系统，就可以用多个线程同时响应多个客户的请求) 多线程的目的：提高程序的运行效率。 多线程运行原理​ cpu在线程中做时间片的切换。 ​ 其实真正电脑中的程序的运行不是同时在运行的。CPU负责程序的运行，而CPU在运行程序的过程中某个时刻点上，它其实只能运行一个程序。而不是多个程序。而CPU它可以在多个程序之间进行高速的切换。而切换频率和速度太快，导致人的肉眼看不到。每个程序就是进程， 而每个进程中会有多个线程，而CPU是在这些线程之间进行切换。了解了CPU对一个任务的执行过程，我们就必须知道，多线程可以提高程序的运行效率，但不能无限制的开线程。 java实现线程的两种方式继承Thread 的方式，并实现run方法代码示例 123456789101112131415161718192021222324252627282930313233343536import java.util.Random;public class MyThreadWithExtends extends Thread &#123; String flag; public MyThreadWithExtends(String flag)&#123; this.flag = flag; &#125; @Override public void run() &#123; String tname = Thread.currentThread().getName(); System.out.println(tname+"线程的run方法被调用……"); Random random = new Random(); for(int i=0;i&lt;20;i++)&#123; try &#123; Thread.sleep(random.nextInt(10)*100); System.out.println(tname+ "...."+ flag); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Thread thread1 = new MyThreadWithExtends("a"); Thread thread2 = new MyThreadWithExtends("b"); thread1.start(); thread2.start(); /** * 如果是调用thread的run方法，则只是一个普通的方法调用，不会开启新的线程 */// thread1.run();// thread2.run(); &#125;&#125; 声明实现Runnable接口的方式代码示例 1234567891011121314151617181920212223242526272829303132public class MyThreadWithImpliment implements Runnable &#123; int x; public MyThreadWithImpliment(int x) &#123; this.x = x; &#125; @Override public void run() &#123; String name = Thread.currentThread().getName(); System.out.println("线程" + name + "的run方法被调用……"); for (int i = 0; i &lt; 10; i++) &#123; System.out.println(x); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Thread thread1 = new Thread(new MyThreadWithImpliment(1), "thread-1"); Thread thread2 = new Thread(new MyThreadWithImpliment(2), "thread-2"); thread1.start(); thread2.start(); // 注意调用run和调用start的区别,直接调用run，则都运行在main线程中// thread1.run();// thread2.run(); &#125;&#125; java数据同步在多线程开发中，不同的线程经常会执行相同的代码逻辑，在线程中如果对同一个变量或者数据进行操作的时候，可能会导致不同线程之间产生数据冲突，java中经常采用如下两种方法来解决数据冲突的问题 一、synchronized 关键字java 关键字 使用格式 1234// 当synchronized 修饰对象的时候是修饰该对象，修饰类方法的时候，修饰的是该对象实例 synchronized( 需要一个任意的对象（锁） )&#123; 代码块中放操作共享数据的代码。&#125; 关键字作用 如果一个代码块被synchronized修饰了，当一个线程获取了对应的锁，并执行该代码块时，其他线程便只能一直等待，等待获取锁的线程释放锁 即保证进程中同时只有一处可以运行syncheoniced修饰的代码块，其他调用阻塞执行 释放锁的情况 获取锁的线程执行完了该代码块，然后线程释放对锁的占有； 线程执行发生异常，此时JVM会让线程自动释放锁。 synchronized 关键字的缺点 不能知道是否成功获取到了锁 不可以中断对锁的等待 使用synchronized关键字代码示例 1234567891011121314151617181920212223242526272829public class MySynchronized &#123; public static void main(String[] args) &#123; final MySynchronized mySynchronized = new MySynchronized(); new Thread("thread1") &#123; public void run() &#123; synchronized (mySynchronized) &#123; try &#123; System.out.println(this.getName()+" start"); // int i =1/0; //如果发生异常，jvm会将锁释放 Thread.sleep(5000); System.out.println(this.getName()+"醒了"); System.out.println(this.getName()+" end"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;.start(); new Thread("thread2") &#123; public void run() &#123; synchronized (mySynchronized) &#123; //争抢同一把锁时，线程1没释放之前，线程2只能等待 System.out.println(this.getName()+" start"); System.out.println(this.getName()+" end"); &#125; &#125; &#125;.start(); &#125;&#125; 二、lock 同步锁Lock 接口lock 同步锁是 实现了 Lock 接口的 java.util.concurrent.locks包下常用的类集合 Lock接口 12345678910public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; void unlock();&#125;// lock()、tryLock()、tryLock(long time, TimeUnit unit)、lockInterruptibly()是用来获取锁的。// unLock()方法是用来释放锁的。 几种获取锁的方法对比 lock()： 方法是平常使用得最多的一个方法，就是用来获取锁。如果锁已被一个线程获取，则其他线程进行等待。使用lock的时候必须主动去释放锁，并且在发生异常时，不会自动释放锁。 tryLock(): 方法是有返回值的，它表示用来尝试获取锁，如果获取成功，则返回true，如果获取失败（即锁已被其他线程获取），则返回false，也就说这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。 tryLock(long time, TimeUnit unit): 方法和tryLock()方法是类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还拿不到锁，就返回false。如果如果一开始拿到锁或者在等待期间内拿到了锁，则返回true。 lockInterruptibly(): 当通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通过lock.lockInterruptibly()想获取某个锁时，假若此时线程A获取到了锁，而线程B只有等待，那么对线程B调用threadB.interrupt()方法能够中断线程B的等待过程。注意，当一个线程获取了锁之后，是不会被interrupt()方法中断的.因此当通过lockInterruptibly()方法获取某个锁时，如果不能获取到，只有进行等待的情况下，是可以响应中断的。 ReentrantLock： ​ 直接使用lock接口的话，我们需要实现很多方法，不太方便，ReentrantLock是唯一实现了Lock接口的类，并且ReentrantLock提供了更多的方法，ReentrantLock，意思是“可重入锁”。使用方法和上面 代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.util.ArrayList;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 观察现象：一个线程获得锁后，另一个线程取不到锁，不会一直等待 * @author * */public class MyTryLock &#123; private static ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); static Lock lock = new ReentrantLock(); // 注意这个地方 public static void main(String[] args) &#123; new Thread() &#123; public void run() &#123; Thread thread = Thread.currentThread(); boolean tryLock = lock.tryLock(); System.out.println(thread.getName()+" "+tryLock); if (tryLock) &#123; try &#123; System.out.println(thread.getName() + "得到了锁"); for (int i = 0; i &lt; 5; i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; System.out.println(thread.getName() + "释放了锁"); lock.unlock(); &#125; &#125; &#125;; &#125;.start(); new Thread() &#123; public void run() &#123; Thread thread = Thread.currentThread(); boolean tryLock = lock.tryLock(); System.out.println(thread.getName()+" "+tryLock); if (tryLock) &#123; try &#123; System.out.println(thread.getName() + "得到了锁"); for (int i = 0; i &lt; 5; i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; System.out.println(thread.getName() + "释放了锁"); lock.unlock(); &#125; &#125; &#125;; &#125;.start(); &#125;&#125; ReadWriteLock接口ReadWriteLock接口 123456789101112131415public interface ReadWriteLock &#123; /** * Returns the lock used for reading. * * @return the lock used for reading. */ Lock readLock(); /** * Returns the lock used for writing. * * @return the lock used for writing. */ Lock writeLock();&#125; ReadWriteLock接口方法 readLock: 获取读锁， 同一时间允许所有线程读取文件，但是不可以进行写入 writeLock: 获取写锁， 同一时间不予许其他线程对文件进行任何操作 ReentrantReadWriteLock ReentrantReadWriteLock实现了ReadWriteLock接口。ReentrantReadWriteLock里面提供了很多丰富的方法，不过最主要的有两个方法：readLock()和writeLock()用来获取读锁和写锁。 代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import java.util.concurrent.locks.ReentrantReadWriteLock;/** * 使用读写锁，可以实现读写分离锁定，读操作并发进行，写操作锁定单个线程 * * 如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁。 * 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程会一直等待释放写锁。 * @author * */public class MyReentrantReadWriteLock &#123; private ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); public static void main(String[] args) &#123; final MyReentrantReadWriteLock test = new MyReentrantReadWriteLock(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); test.write(Thread.currentThread()); &#125;; &#125;.start(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); test.write(Thread.currentThread()); &#125;; &#125;.start(); &#125; /** * 读操作,用读锁来锁定 * @param thread */ public void get(Thread thread) &#123; rwl.readLock().lock(); try &#123; long start = System.currentTimeMillis(); while(System.currentTimeMillis() - start &lt;= 1) &#123; System.out.println(thread.getName()+"正在进行读操作"); &#125; System.out.println(thread.getName()+"读操作完毕"); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125; /** * 写操作，用写锁来锁定 * @param thread */ public void write(Thread thread) &#123; rwl.writeLock().lock();; try &#123; long start = System.currentTimeMillis(); while(System.currentTimeMillis() - start &lt;= 1) &#123; System.out.println(thread.getName()+"正在进行写操作"); &#125; System.out.println(thread.getName()+"写操作完毕"); &#125; finally &#123; rwl.writeLock().unlock(); &#125; &#125;&#125; 三、 syncchronized和lock的区别 1. Lock不是Java语言内置的，synchronized是Java语言的关键字，因此是内置特性。Lock是一个类，通过这个类可以实现同步访问 2. Lock和synchronized有一点非常大的不同，采用synchronized不需要用户去手动释放锁，当synchronized方法或者synchronized代码块执行完之后，系统会自动让线程释放对锁的占用；而Lock则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。 3. Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 4. 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。 5. Lock可以提高多个线程进行读操作的效率。 6. 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 Java 并发包JDK5.0 以后的版本都引入了高级并发特性，大多数的特性在java.util.concurrent 包中，是专门用于多线程发编程的，充分利用了现代多处理器和多核心系统的功能以编写大规模并发应用程序。主要包含原子量、并发集合、同步器、可重入锁，并对线程池的构造提供了强力的支持。 线程池线程池的作用在并发变成中，经常会用线程处理不发任务，但是控制并发数量和控制线程之间的消息协调是一个难点，线程池解决了这部分问题，我们只需要关心线程池的容量和数据返回方式，并不需要关心创建线程，调用线程等步骤 五种线程池 Single Thread Executor : 只有一个线程的线程池，因此所有提交的任务是顺序执行 创建方式 Executors.newSingleThreadExecutor() Cached Thread Pool : 线程池里有很多线程需要同时执行，老的可用线程将被新的任务触发重新执行，如果线程超过60秒内没执行，那么将被终止并从池中删除， 创建方式 Executors.newCachedThreadPool() Fixed Thread Pool : 拥有固定线程数的线程池，如果没有任务执行，那么线程会一直等待 创建方式 Executors.newFixedThreadPool(4) Scheduled Thread Pool : 用来调度即将执行的任务的线程池，可能是不是直接执行, 每隔多久执行一次… 策略型的 创建方式 Executors.newScheduledThreadPool() 其实，这些不同类型的线程池都是通过构建一个ThreadPoolExecutor来完成的，所不同的是corePoolSize,maximumPoolSize,keepAliveTime,unit,workQueue,threadFactory这么几个参数。具体可以参见JDK DOC。 线程池使用方法1. 第一种方式 ​ 使用方法 ​ 提交 Runnable ，任务完成后 Future 对象返回 null ​ 调用excute,提交任务, 匿名Runable重写run方法, run方法里是业务逻辑 ​ 代码示例 1234567891011121314151617181920212223242526272829import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPoolWithRunable &#123; /** * 通过线程池执行线程 * @param args */ public static void main(String[] args) &#123; //创建一个线程池 ExecutorService pool = Executors.newCachedThreadPool(); for(int i = 1; i &lt; 5; i++)&#123; pool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("thread name: " + Thread.currentThread().getName()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; pool.shutdown(); &#125;&#125; 2. 第二种方式 ​ 使用方法 ​ 提交 Callable，该方法返回一个 Future 实例表示任务的状态 ​ 调用submit提交任务, 匿名Callable,重写call方法, 有返回值, 获取返回值会阻塞,一直要等到线程任务返回结果 ​ 代码示例 123456789101112131415161718192021222324252627282930313233343536import java.util.ArrayList;import java.util.List;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;/** * callable 跟runnable的区别： * runnable的run方法不会有任何返回结果，所以主线程无法获得任务线程的返回值 * * callable的call方法可以返回结果，但是主线程在获取时是被阻塞，需要等待任务线程返回才能拿到结果 * @author * */public class ThreadPoolWithcallable &#123; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; ExecutorService pool = Executors.newFixedThreadPool(4); for(int i = 0; i &lt; 10; i++)&#123; Future&lt;String&gt; submit = pool.submit(new Callable&lt;String&gt;()&#123; @Override public String call() throws Exception &#123; //System.out.println("a"); Thread.sleep(5000); return "b--"+Thread.currentThread().getName(); &#125; &#125;); //从Future中get结果，这个方法是会被阻塞的，一直要等到线程任务返回结果 System.out.println(submit.get()); &#125; pool.shutdown(); &#125;&#125; 不用线程池的缺点 新建线程的开销。线程虽然比进程要轻量许多，但对于JVM来说，新建一个线程的代价还是挺大的，决不同于新建一个对象 资源消耗量。没有一个池来限制线程的数量，会导致线程的数量直接取决于应用的并发量，这样有潜在的线程数据巨大的可能，那么资源消耗量将是巨大的 稳定性。当线程数量超过系统资源所能承受的程度，稳定性就会成问题 线程池的饱和策略以上线程池类型可知，除了CachedThreadPool其他线程池都有饱和的可能，当饱和以后就需要相应的策略处理请求线程的任务，比如，达到上限时通过ThreadPoolExecutor.setRejectedExecutionHandler方法设置一个拒绝任务的策略，JDK提供了AbortPolicy、CallerRunsPolicy、DiscardPolicy、DiscardOldestPolicy几种策略，具体差异可见JDK DOC 消息队列java.util.concurrent下的主要用来控制线程同步的工具 消息队列作用用来主线程和线程之间，和各线程之间的消息通讯； 两种消息队列 ArrayBlockingQueue: 一个由数组支持的有界阻塞队列，规定大小的BlockingQueue,其构造函数必须带一个int参数来指明其大小.其所含的对象是以FIFO(先入先出)顺序排序的。 LinkedBlockingQueue：大小不定的BlockingQueue,若其构造函数带一个规定大小的参数,生成的BlockingQueue有大小限制,若不带大小参数,所生成的BlockingQueue的大小由Integer.MAX_VALUE来决定.其所含的对象是以FIFO(先入先出)顺序排序的。 两种消息队列区别： ​ LinkedBlockingQueue和ArrayBlockingQueue比较起来,它们背后所用的数据结构不一样,导致LinkedBlockingQueue的数据吞吐量要大于ArrayBlockingQueue,但在线程数量很大时其性能的可预见性低于ArrayBlockingQueue. 消息队列对象的方法插入 add(anObject): 把anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则抛出异常 offer(anObject): 表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false. put(anObject): 把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续, 有阻塞, 放不进去就等待 读取 1. **poll(time)**: 取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null; 取不到返回null 2. **take()**: 取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到Blocking有新的对象被加入为止; 阻塞, 取不到就一直等其他 int remainingCapacity(): 返回队列剩余的容量，在队列插入和获取的时候，不要瞎搞，数据可能不准, 不能保证数据的准确性 boolean remove(Object o): 从队列移除元素，如果存在，即移除一个或者更多，队列改 变了返回true public boolean contains(Object o): 查看队列是否存在这个元素，存在返回true int drainTo(Collection&lt;? super E&gt; c): 移除此队列中所有可用的元素,并将它们添加到给定 collection 中。取出放到集合中 int drainTo(Collection&lt;? super E&gt; c, int maxElements): 和上面方法的区别在于，指定了移动的数量; 取出指定个数放到集合 代码示例consumer 12345678910111213141516171819import java.util.concurrent.BlockingQueue;public class Consumer implements Runnable&#123; BlockingQueue&lt;String&gt; queue; public Consumer(BlockingQueue&lt;String&gt; queue)&#123; this.queue = queue; &#125; @Override public void run() &#123; try &#123; String consumer = Thread.currentThread().getName(); System.out.println(consumer); String temp = queue.take();//如果队列为空，会阻塞当前线程 System.out.println(consumer+"get a product:"+temp); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; producer 123456789101112131415161718192021import java.util.concurrent.BlockingQueue;public class Producer implements Runnable &#123; BlockingQueue&lt;String&gt; queue; public Producer(BlockingQueue&lt;String&gt; queue) &#123; this.queue = queue; &#125; @Override public void run() &#123; try &#123; System.out.println("I have made a product:" + Thread.currentThread().getName()); String temp = "A Product, 生产线程：" + Thread.currentThread().getName(); queue.put(temp);//如果队列是满的话，会阻塞当前线程 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; volatile关键字在多线程中的作用解决多线程之间的变量可见性问题 可见性问题主要指一个线程修改了共享变量值，而另一个线程却看不到。引起可见性问题的主要原因是每个线程拥有自己的一个高速缓存区——线程工作内存。 volatile就可以解决这个问题, 看下面代码就可以知道其作用: 1234567891011121314151617181920212223242526272829303132333435363738394041public class VolatileTest &#123; int a = 1; int b = 2; public void change()&#123; a = 3; b = a; &#125; public void print()&#123; System.out.println("b="+b+";a="+a); &#125; public static void main(String[] args) &#123; while (true)&#123; final VolatileTest test = new VolatileTest(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; test.change(); &#125; &#125;).start(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; test.print(); &#125; &#125;).start(); &#125; &#125;&#125; 直观上说，这段代码的结果只可能有两种：b=3;a=3 或 b=2;a=1。不过运行上面的代码（可能时间上要长一点），你会发现除了上两种结果之外，还出现了第三种结果： 1234567891011...... b=2;a=1b=2;a=1b=3;a=3b=3;a=3b=3;a=1b=3;a=3b=2;a=1b=3;a=3b=3;a=3...... 为什么会出现b=3;a=1这种结果呢？正常情况下，如果先执行change方法，再执行print方法，输出结果应该为b=3;a=3。相反，如果先执行的print方法，再执行change方法，结果应该是 b=2;a=1。那b=3;a=1的结果是怎么出来的？原因就是第一个线程将值a=3修改后，但是对第二个线程是不可见的，所以才出现这一结果。如果将a和b都改成volatile类型的变量再执行，则再也不会出现b=3;a=1的结果了 volatile 和 锁的区别当多个线程同时请求锁的时候，一些线程将被挂起并且等待其他线程执行完它们的时间片后才能被调度执行。频繁的线程间上下文切换及线程调度是十分耗资源的。另外锁还存在着死锁的风险。 与锁相比，volatile是一种更加轻量级的同步机制，因为在使用这些变量的时候不会发生上下文切换和线程调度等操作。但是volatile同样也存在局限性:当变量依赖于其他变量或旧值时（自增）就不能使用volatile变量， 因为他们不是原子操作。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2F2019%2F12%2F21%2F15.%E5%A4%A7%E6%95%B0%E6%8D%AE%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[Zookeeper 的简介zookeeper的作用Zookeeper是一个分布式协调服务；就是为用户的分布式应用程序提供协调服务 主要用来给分布式节点提供数据保管和节点监听功能 zookeeper特点 zookeeper是为别的分布式程序服务的 Zookeeper本身就是一个分布式程序（只要有半数以上节点存活，zk就能正常服务，一般要部署奇数台，最好大于等于三台） Zookeeper所提供的服务涵盖：主从协调、服务器节点动态上下线、统一配置管理、分布式共享锁、统一名称服务…… 虽然说可以提供各种服务，但是zookeeper在底层其实只提供了两个功能：1. 管理(存储，读取)用户程序提交的数据；2. 为用户程序提供数据节点监听服务； zookeeper集群的角色： Leader follower （Observer） zookeeper集群的选举机制（全新集群）假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动； 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态. 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader. 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了. 服务器5启动,同4一样,当小弟. zookeeper集群的选举机制（运行中集群）运行中集群的选举机制需要加入数据id、leader id和逻辑时钟的概念。 数据id：数据新的id就大，数据每次更新都会更新id。 Leader id：就是我们配置的myid中的值，每个机器一个。 逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说: 如果在同一次选举中,那么这个值应该是一致的 ; 逻辑时钟值越大,说明这一次选举leader的进程更新. 选举的标准就变成： ​ 1、逻辑时钟小的选举结果被忽略，重新投票 ​ 2、统一逻辑时钟后，数据id大的胜出 ​ 3、数据id相同的情况下，leader id大的胜出 zookeeper集群特性 Zookeeper：一个leader，多个follower组成的集群 全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的 分布式读写，更新请求转发，由leader实施 更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行 数据更新原子性，一次数据更新要么成功，要么失败 实时性，在一定时间范围内，client能读到最新数据 zookeeper的监听工作机制 原理: 监听器是一个接口，我们的代码中可以实现Wather这个接口，实现其中的process方法，方法中即我们自己的业务逻辑，当zookeeper检测到有事件发生时，就会进行事件回调，出发process方法 监听器的注册是在获取数据的操作中实现： getData(path,watch?)监听的事件是：节点数据变化事件 getChildren(path,watch?)监听的事件是：节点下的子节点增减变化事件 zookeeper 的部署步骤 前提: 每台服务器安装好jdk, 3台以上的部署机 下载zookeeper的压缩包(每台执行) 1wget ....../zookeeper-3.4.5.tar.gz 解压缩(每台执行) 12tar -zxvf -C /opt/apps zookeeper-3.4.5.tar.gzmv /opt/apps/zookeeper-3.4.5 /opt/apps/zookeeper 修改环境变量(每台执行) 123echo "export ZOOKEEEPER_HOME=/opt/apps/zookeeper"echo "export PATH=$PATH:$ZOOKEEPER_HOME/bin"source /etc/profile 修改集群配置文件(每台执行) 12345678cp /opt/apps/zookeeper/conf/zoo_sample.cfg /opt/apps/zookeeper/conf/zoo.cfgecho "server.1=ip1:2888:3888" # 1.2.3 代表zookeeper集群中的idecho "server.2=ip2:2888:3888"echo "server.3=ip3:2888:3888"vi /opt/apps/zookeeper/conf/zoo.cfgdataDir=/opt/apps/zookeeper/data # 配置zookeeper数据目录dataLogDir=/opt/apps/zookeeper/log # 配置zookeeper log目录 创建数据文件夹(每台执行) 12mkdir -m 755 /opt/apps/zookeeper/datamkdir -m 755 /opt/apps/zookeeper/ 创建myid文件(每台执行) 123cd datavi myid1 # 添加每台机器的id文件，对应cfg文件中的server.x 启动zookeeper 1zkServer.sh start 查看集群的状态 12jps（查看进程）zkServer.sh status（查看集群状态，主从信息） zookeeper数据结构数据结构图zookeeper是按照树状结构对数据进行存储的 数据结构特点 层次化的目录结构，命名符合常规文件系统规范(见下图) 每个节点在zookeeper中叫做znode,并且其有一个唯一的路径标识 节点Znode可以包含数据和子节点（但是EPHEMERAL类型的节点不能有子节点，下一页详细讲解） 客户端应用可以在节点上设置监视器（后续详细讲解） 节点类型znode的两种类型 短暂(ephemeral) : 客户端断开后节点自动删除，节点不能有子节点 持久(persistent)：客户端断开后不删除节点 znode四种目录形式(默认persistent) persistent : 持久节点 persistent_sequential: 持久序列节点，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护(例: /test0000000019)， ephemeral: 短暂节点 ephemera_sequential: 短暂序列节点 序列节点的作用 在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 zookeeper 命令行操作启动客户端 1zkCli.sh -server &lt;ip&gt; # 连接本机可以省略-server ip 查看当前节点包含的所有节点 1[zk: 202.115.36.251:2181(CONNECTED) 1] ls / 创建一个新节点 123# /zk 是节点名称 myData 是节点中保存的数据# -e 创建短暂节点， -s 创建序列节点[zk: 202.115.36.251:2181(CONNECTED) 1] create [-s] [-e] /zk "myData" 获取节点数据 12# 当使用watch后缀时， 监听这个节点的变化,当另外一个客户端改变/zk时,会出发watch事件，客户端它会打印watch时间信息[zk: 202.115.36.251:2181(CONNECTED) 3] get /zk [watch] 更改节点数据 1[zk: 202.115.36.251:2181(CONNECTED) 4] set /zk "zsl“ 删除节点 12[zk: 202.115.36.251:2181(CONNECTED) 5] delete /zk[zk: 202.115.36.251:2181(CONNECTED) 5] rmr /zk # 递归删除节点 zookeeper java api 使用org.apache.zookeeper.Zookeeper 是zookeeper客户端入口的主要类，负责建立与zk server 的会话 主要方法 create 在本地目录树中创建一个节点 delete 删除一个节点 exists 测试本地是否存在目标节点 get/setData 从目标节点上读取 / 写数据 get/setACL 获取 / 设置目标节点访问控制列表信息 getChildren 检索一个子节点上的列表 sync 等待要被传送的数据 zookeeper简单使用代码示例实现简单的增删改查和监听操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package com.bigdata.utils.zookeeper;import org.apache.zookeeper.*;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.data.Stat;import org.junit.Before;import org.junit.Test;import java.util.List;public class SimpleZkClient &#123; private static final String connectString = "mini1:2181,mini2:2181,mini3:2181"; private static final int sessionTimeout = 2000; private ZooKeeper zkClient = null; @Before public void init() throws Exception &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） System.out.println(event.getType() + "---" + event.getPath()); try &#123; zkClient.getChildren("/", true); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); &#125; /** * 数据的增删改查 * * @throws InterruptedException * @throws KeeperException */ // 创建数据节点到zk中 public void testCreate() throws KeeperException, InterruptedException &#123; // 参数1：要创建的节点的路径 参数2：节点大数据 参数3：节点的权限 参数4：节点的类型 String nodeCreated = zkClient.create("/eclipse", "hellozk".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); //上传的数据可以是任何类型，但都要转成byte[] &#125; //判断znode是否存在 @Test public void testExist() throws Exception&#123; Stat stat = zkClient.exists("/eclipse", false); System.out.println(stat==null?"not exist":"exist"); &#125; // 获取子节点 @Test public void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren("/", true); for (String child : children) &#123; System.out.println(child); &#125; Thread.sleep(Long.MAX_VALUE); &#125; //获取znode的数据 @Test public void getData() throws Exception &#123; byte[] data = zkClient.getData("/eclipse", false, null); System.out.println(new String(data)); &#125; //删除znode @Test public void deleteZnode() throws Exception &#123; //参数2：指定要删除的版本，-1表示删除所有版本 zkClient.delete("/eclipse", -1); &#125; //删除znode @Test public void setData() throws Exception &#123; // -1 代表删除所有节点版本 zkClient.setData("/app1", "imissyou angelababy".getBytes(), -1); byte[] data = zkClient.getData("/app1", false, null); System.out.println(new String(data)); &#125;&#125; 分布式系统使用zookeeper进行协调示例实现功能: 分布式系统中，有多台服务器和客户端，服务端可以动态的上下线，要实现客户端可以动态的感知服务节点的变化和最新状态 服务端代码示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.bigdata.utils.zookeeper;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class DistributedServer &#123; private static final String connectString = "mini1:2181,mini2:2181,mini3:2181"; private static final int sessionTimeout = 2000; private static final String parentNode = "/servers"; private ZooKeeper zk = null; /** * 创建到zk的客户端连接 * * @throws Exception */ public void getConnect() throws Exception &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） System.out.println(event.getType() + "---" + event.getPath()); try &#123; zk.getChildren("/", true); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); &#125; /** * 向zk集群注册服务器信息 * * @param hostname * @throws Exception */ public void registerServer(String hostname) throws Exception &#123; String create = zk.create(parentNode + "/server", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname + "is online.." + create); &#125; /** * 业务功能 * * @throws InterruptedException */ public void handleBussiness(String hostname) throws InterruptedException &#123; System.out.println(hostname + "start working....."); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributedServer server = new DistributedServer(); server.getConnect(); // 利用zk连接注册服务器信息 server.registerServer(args[0]); // 启动业务功能 server.handleBussiness(args[0]); &#125;&#125; 客户端代码示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package com.bigdata.utils.zookeeper;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.util.ArrayList;import java.util.List;public class DistributedClient &#123; private static final String connectString = "mini1:2181,mini2:2181,mini3:2181"; private static final int sessionTimeout = 2000; private static final String parentNode = "/servers"; // 注意:加volatile的意义何在？ private volatile List&lt;String&gt; serverList; private ZooKeeper zk = null; /** * 创建到zk的客户端连接 * * @throws Exception */ public void getConnect() throws Exception &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） try &#123; //重新更新服务器列表，并且注册了监听 getServerList(); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); &#125; /** * 获取服务器信息列表 * * @throws Exception */ public void getServerList() throws Exception &#123; // 获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); // 先创建一个局部的list来存服务器信息 List&lt;String&gt; servers = new ArrayList&lt;String&gt;(); for (String child : children) &#123; // child只是子节点的节点名 byte[] data = zk.getData(parentNode + "/" + child, false, null); servers.add(new String(data)); &#125; // 把servers赋值给成员变量serverList，已提供给各业务线程使用 serverList = servers; //打印服务器列表 System.out.println(serverList); &#125; /** * 业务功能 * * @throws InterruptedException */ public void handleBussiness() throws InterruptedException &#123; System.out.println("client start working....."); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributedClient client = new DistributedClient(); client.getConnect(); // 获取servers的子节点信息（并监听），从中获取服务器信息列表 client.getServerList(); // 业务线程启动 client.handleBussiness(); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据工具</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell文本命令]]></title>
    <url>%2F2019%2F12%2F20%2F09.shell%2Fshell%E8%AF%AD%E6%B3%95%2Fshell%E6%96%87%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[cut命令从一个文本文件或者文本流从提取文本列 参数-d: 用什么字符来分隔文本， -f: 取分隔后的第几列，支持格式 : n; n1,n2; n1-n2; n1-; 示例12345text='/sbin:/usr/sbin:/usr/local/bin:/usr/x11R6/bin'echo $test | cut -d '':'' -f 1-3# /bin:/usr/bin:/sbin: sort 命令对文件的行进行排序，并将结果写到标准输出，如果指定多个文件，那么sort命令将这些文件链接起来，并当做一个文件进行排序 参数-f : 忽略大小写的差异 -b : 忽略最见面的空格符部分 -M : 以月份的名字来排序 -n : 使用纯数字来进行排序，默认是以文字形态来排序的 -r : 反向排序 -u : 相同的数据中心，仅出现一行代表 -t ：分隔符，默认是用tab键来分隔 -k : 以哪个区间来进行排序 示例123cat /test.txt | sort -t ":" -k 3rn# 对文件每行进行: 分隔， 并取第三列进行反向排序，并且使用数字来排序 wc 命令统计文本的行数和字符数量等 参数-l ：统计行数 -w : 统计单词水浪 -m : 统计字节数量 sed 命令对文本的内容进行替换, 删除，更新等操作 命令基本格式1sed [选项] [脚本命令] 文件名 命令参数-e : 该选项会将其后跟的脚本命令添加到已有的命令中。 -f ：该选项会将其后文件中的脚本命令添加到已有的命令中 -n : 默认情况下，sed 会在所有的脚本指定执行完毕后，会自动输出处理后的内容，而该选项会屏蔽启动输出，需使用 print 命令来完成输出。 -i : 此选项会直接修改源文件，要慎用。 脚本命令替换脚本命令s命令格式 1[address]s/pattern/replacement/flags 常见flag n 1~512 之间的数字，表示指定要替换的字符串出现第几次时才进行替换，例如，一行中有 3 个 A，但用户只想替换第二个 A，这是就用到这个标记； g 对数据中所有匹配到的内容进行替换，如果没有 g，则只会在第一次匹配成功时做替换操作。例如，一行数据中有 3 个 A，则只会替换第一个 A； p 会打印与替换命令中指定的模式匹配的行。此标记通常与 -n 选项一起使用。 w file 将缓冲区中的内容写到指定的 file 文件中； &amp; 用正则表达式匹配的内容进行替换； \n 匹配第 n 个子串，该子串之前在 pattern 中用 () 指定。 \ 转义（转义替换部分包含：&amp;、\ 等）。 使用示例 12sed 's/test/trial/2' data4.txt# 可以看到，使用数字 2 作为标记的结果就是，sed 编辑器只替换每行中第 2 次出现的匹配模式。 删除脚本命令命令格式 1[address]d 使用示例 123sed '2,3d' data6.txt # 删除第2、3行的数据sed '/1/,/3/d' data6.txt # 删除1~3行的数据sed '3,$d' data6.txt # 删除第3行到最后一行的数据 插入数据行命令命令格式 1[address]a（或 i）\新文本内容 备注: a 命令表示在指定行的后面附加一行，i 命令表示在指定行的前面插入一行 使用示例 12sed '3a\&gt; This is an appended line.' data6.txt # 将一个新行附加到数据流中第三行后 替换脚本命令c命令格式 1[address]c\用于替换的新文本 使用示例 1234567sed '3c\&gt; This is a changed line of text.' data6.txt # sed 编辑器会修改第三行中的文本sed '/number 3/c\&gt; This is a changed line of text.' data6.txt# sed 编辑器会修改包含number 3 的行 转换脚本命令命令格式 1[address]y/inchars/outchars/ 使用示例 1sed 'y/123/789/' data8.txt # 将1=&gt;7, 2=&gt;8, 3=&gt;9]]></content>
      <categories>
        <category>shell</category>
        <category>shell语法</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jdbc 使用]]></title>
    <url>%2F2019%2F12%2F13%2F14.Java%2FJava%E5%9F%BA%E7%A1%80%2Fjdbc%2F</url>
    <content type="text"><![CDATA[JDBC 的介绍JDBC:java database connectivity JDBC定义一套标准接口，即访问数据库的通用API，不同的数据库厂商根据各自数据库的特点去实现这些接口，实现接口、类：驱动：由数据库厂商实现 JDBC是java应用程序和数据库之间的通信桥梁，是java应用程序访问数据库的通道 JDBC标准主要由一组接口组成，其好处是统一了各种数据库访问方式 JDBC接口的实现类称为数据库驱动，由各个数据库厂商提供，使用JDBC必须导入这个驱动！一定知道驱动是什么！ JDBC 进行CURDJDBC使用步骤1、导入JDBC驱动 再maven配置中导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt;&lt;/dependency&gt; 1、注册JDBC驱动 ​ 参数：“驱动程序类名” 1Class.forname("驱动程序类名") 3、获得Connection对象 需要三个参数：url，username，password - 连接到数据库 1connection = DriverManager.getConnection(url, user, password); 4、创建Statement（语句）对象 1234567state = conn.getStatemnent() 方法创建对象state.execute(ddl) 执行任何SQL，常用于执行DDL、DCLstate.executeUpdate(dml） 执行DML语句，如：insert，update，deletestate.executeQuery(dql) 执行DQL语句，如：select 5处理执行结果： 1234567state.execute(ddl) 如果没有异常则成功 booleanstate.executeUpdate(dml） 返回数字，表示更新“行”数量，抛出异常则失败 intstate.executeQuery(dql) 返回ResultSet（结果）对象，代表2维查询结果， ResultSet使用for遍历处理，如果查询失败抛出异常 6、关闭数据连接！！ 1conn.close(); JDBC使用示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;/** * @author Evan */public class JDBCTest &#123; public static void main(String[] args) throws Exception &#123; Connection connection = null; PreparedStatement prepareStatement = null; ResultSet rs = null; try &#123; // 加载驱动 Class.forName("com.mysql.jdbc.Driver"); // 获取连接 String url = "jdbc:mysql://127.0.0.1:3306/ssmdemo"; String user = "root"; String password = "123456"; connection = DriverManager.getConnection(url, user, password); // 获取statement，preparedStatement String sql = "select * from tb_user where id=?"; prepareStatement = connection.prepareStatement(sql); // 设置参数 prepareStatement.setLong(1, 1l); // 执行查询 rs = prepareStatement.executeQuery(); // 处理结果集 while (rs.next()) &#123; System.out.println(rs.getString("userName")); System.out.println(rs.getString("name")); System.out.println(rs.getInt("age")); System.out.println(rs.getDate("birthday")); &#125; &#125; finally &#123; // 关闭连接，释放资源 if (rs != null) &#123; rs.close(); &#125; if (prepareStatement != null) &#123; prepareStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125; &#125;&#125; 解决sql注入问题解决办法Statement主要用于执行静态SQL语句，即内容固定不变的SQL语句； 这样容易引入sql注入的攻击问题； PreparedStatement对象用于执行带参数的预编译执行计划；可以重复使用执行计划，提高DB效率，可以重用执行计划，而且可以执行多次；可以避免注入攻击。 所以使用PreparedStatement 来代替Statement 来解决sql注入的问题 使用示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package demo;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;import org.junit.Test;public class TestLogin &#123; @Test public void testLogin() &#123; try &#123; login("zhangsan' or 'zhangsan", "666"); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125; public void login(String username, String password) throws Exception &#123; Class.forName("com.mysql.jdbc.Driver"); String url = "jdbc:mysql://localhost:3306/mybase"; String usern = "root"; String pwd = "xuyiqing"; Connection conn = DriverManager.getConnection(url, usern, pwd); String sql = "select * from users where username=? and upassword=?"; PreparedStatement pstmt = conn.prepareStatement(sql); pstmt.setString(1, username); pstmt.setString(2, password); ResultSet rs = pstmt.executeQuery(); if (rs.next()) &#123; System.out.println("登录成功"); System.out.println(sql); &#125; else &#123; System.out.println("账号或密码错误！"); &#125; if (rs != null) &#123; rs.close(); &#125; if (pstmt != null) &#123; pstmt.close(); &#125; if (conn != null) &#123; conn.close(); &#125; &#125;&#125; 事务操作怎样进行事务操作数据库提供了事务控制功能，支持ACID特性 JDBC提供了API，方便地调用数据库的事务功能，其方法有： Connection.getAutoCommit()：获得当前事务的提交方式，默认是true Connection.setAutoCommit()：设置事务的提交属性，参数是true：自动提交；false：不自动提交，取消自动提交，后续手动提交 Connection.Commit()：提交事务 Connection.rollback()：回滚事务 使用示例123456789101112131415161718192021222324252627282930public static void dbTest() &#123; Connection conn = null; try &#123; conn = DBUtils.getConnection(); conn.setAutoCommit(false); //业务处理 执行计划使用步骤 String sql = "insert into test_wcx values( ?,?,?)"; // 1、将带参数的SQL发送到数据库创建执行计划 PreparedStatement ps = conn.prepareStatement(sql); //2、替换执行计划中的参数 ps.setInt(1, 2); ps.setString(2, "weicx"); ps.setString(3, "test"); //3、执行执行计划，得到结果 int result = ps.executeUpdate(); System.out.println(result); conn.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); DBUtils.rollback(conn); &#125; finally &#123; DBUtils.close(conn); &#125; &#125; 数据库连接池为什么使用连接池数据库连接池是管理并发访问数据库连接的理想解决方案 解决并发问题;数据库的并发有限,限制连接数，避免数据库崩溃;重用数据库连接 解决办法连接池是创建和管理连接的缓冲池技术，将连接准备好被任何需要他们的应用使用 DriverManager管理数据库连接适合单线程情况，而在多线程并发情况下，为了能够重用数据库连接，同时控制并发连接总数，保护数据库避免连接过载，一定要使用数据库连接池 DBCP的使用数据库连接池的开源实现非常多，DBCP是其中之一。 使用DBCP步骤： 导入连接池依赖，修改maven 的pom文件 12345678910&lt;dependency&gt; &lt;groupId&gt;commons-dbcp&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-pool&lt;/groupId&gt; &lt;artifactId&gt;commons-pool&lt;/artifactId&gt; &lt;version&gt;1.5.4&lt;/version&gt;&lt;/dependency&gt; 创建连接池对象 设置数据库必须的连接参数, 可选的连接参数 从连接池中获得活动的数据库连接 执行sql操作 使用以后关闭数据库连接，这个关闭不再是真的关闭连接，而是将使用过的连接归还给连接池 使用示例普通方式连接 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.java.dbcp;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;import org.apache.commons.dbcp.BasicDataSource;import com.java.jdbc.util.JDBCUtil;public class DBCPDemo &#123; public void testDBCP01() &#123; Connection connection = null; PreparedStatement ps = null; try &#123; //1、构建数据源对象 BasicDataSource dataSource = new BasicDataSource(); dataSource.setDriverClassName("com.mysql.jdbc.Driver"); dataSource.setUrl("jdbc:mysql://localhost/php?useSSL=false"); dataSource.setUsername("root"); dataSource.setPassword("123456"); //2、得到连接对象 connection = dataSource.getConnection(); String sql = "insert into stus values(6,?,?)"; ps = connection.prepareStatement(sql); ps.setString(1, "ling"); ps.setString(2, "666666"); ps.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; JDBCUtil.release(connection, ps); &#125; &#125; public static void main(String[] args) &#123; DBCPDemo dbcp = new DBCPDemo(); dbcp.testDBCP01(); &#125;&#125; 通过配置文件的方式连接 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.java.dbcp;import java.io.FileInputStream;import java.io.InputStream;import java.sql.Connection;import java.sql.PreparedStatement;import java.util.Properties;import javax.sql.DataSource;import org.apache.commons.dbcp.BasicDataSourceFactory;import com.java.jdbc.util.JDBCUtil;public class DBCPDemo02 &#123; public void dbcpdemo() &#123; Connection connection = null; PreparedStatement ps = null; try &#123; BasicDataSourceFactory factory = new BasicDataSourceFactory(); Properties properties = new Properties(); InputStream iStream = new FileInputStream("src/dbcpconfig.properties"); properties.load(iStream); DataSource dataSource = factory.createDataSource(properties); //2、得到连接对象 connection = dataSource.getConnection(); String sql = "insert into stus values(7,?,?)"; ps = connection.prepareStatement(sql); ps.setString(1, "wu"); ps.setString(2, "123456789"); ps.executeUpdate(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; JDBCUtil.release(connection, ps); &#125; &#125; public static void main(String[] args) &#123; DBCPDemo02 demo = new DBCPDemo02(); demo.dbcpdemo(); &#125;&#125; 配置文件示例 1234567891011121314151617181920212223242526272829303132#连接设置driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost/php?useSSL=falseusername=rootpassword=abc123456#&lt;!-- 初始化连接 --&gt;initialSize=10#最大连接数量maxActive=50#&lt;!-- 最大空闲连接 --&gt;maxIdle=20#&lt;!-- 最小空闲连接 --&gt;minIdle=5#&lt;!-- 超时等待时间以毫秒为单位 6000毫秒/1000等于60秒 --&gt;maxWait=60000#JDBC驱动建立连接时附带的连接属性属性的格式必须为这样：[属性名=property;] #注意："user" 与 "password" 两个属性会被明确地传递，因此这里不需要包含他们。connectionProperties=useUnicode=true;characterEncoding=gbk#指定由连接池所创建的连接的自动提交（auto-commit）状态。defaultAutoCommit=true#driver default 指定由连接池所创建的连接的事务级别（TransactionIsolation）。#可用值为下列之一：（详情可见javadoc。）NONE,READ_UNCOMMITTED, READ_COMMITTED, REPEATABLE_READ, SERIALIZABLEdefaultTransactionIsolation=READ_UNCOMMITTED]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven 使用]]></title>
    <url>%2F2019%2F12%2F07%2F14.Java%2FJava%E5%9F%BA%E7%A1%80%2Fmaven%2F</url>
    <content type="text"><![CDATA[什么是Maven构建java项目需要用到很多第三方的类库，需要引入大量的jar包。Jar包之间的关系错综复杂，一个Jar包往往又会引用其他Jar包，缺少任何一个Jar包都会导致项目编译失败。 以往开发项目时，程序员往往需要花较多的精力在引用Jar包搭建项目环境上，而这一项工作尤为艰难。 而Maven就是一款帮助程序员构建项目的工具，我们只需要告诉Maven需要哪些Jar 包，它会帮助我们下载所有的Jar，极大提升开发效率。 maven 下载地址: http://maven.apache.org/download.cgi Maven 目录结构 Maven基本命令 -v: 查询Maven版本 本命令用于检查maven是否安装成功。 Maven安装完成之后，在命令行输入mvn -v，若出现maven信息，则说明安装成功。 compile：编译 将java源文件编译成class文件 test: 测试项目 执行test目录下的测试用例 package: 打包 将项目打成jar包 clean: 删除target文件夹 install: 安装 将当前项目放到Maven的本地仓库中。供其他项目使用 Maven 仓库Maven仓库用来存放Maven管理的所有Jar包。分为：本地仓库 和 中央仓库。 本地仓库：Maven本地的Jar包仓库。 中央仓库： Maven官方提供的远程仓库。 当项目编译时，Maven首先从本地仓库中寻找项目所需的Jar包，若本地仓库没有，再到Maven的中央仓库下载所需Jar包。 Maven 坐标在Maven中，坐标是Jar包的唯一标识，Maven通过坐标在仓库中找到项目所需的Jar包。 如下代码中，groupId和artifactId构成了一个Jar包的坐标。 123456&lt;dependency&gt; &lt;groupId&gt;cn.missbe.web.search&lt;/groupId&gt; &lt;artifactId&gt;resource-search&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; groupId:所需Jar包的项目名 artifactId:所需Jar包的模块名 version:所需Jar包的版本号 传递依赖和排除依赖 传递依赖：如果我们的项目引用了一个Jar包，而该Jar包又引用了其他Jar包，那么在默认情况下项目编译时，Maven会把直接引用和间接引用的Jar包都下载到本地。 排除依赖：如果我们只想下载直接引用的Jar包，那么需要在pom.xml中做如下配置：(将需要排除的Jar包的坐标写在中) 12345678&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;cn.missbe.web.search&lt;/groupId&gt; &lt;artifactId&gt;resource-search&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 依赖范围 scope在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 compile ：默认范围，用于编译 provided：类似于编译，但支持你期待jdk或者容器提供，类似于classpath runtime: 在执行时需要使用 test: 用于test任务时使用 system: 需要外在提供相应的元素。通过systemPath来取得 systemPath: 仅用于范围为system。提供相应的路径 optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用 依赖冲突若项目中多个Jar同时引用了相同的Jar时，会产生依赖冲突，但Maven采用了两种避免冲突的策略，因此在Maven中是不存在依赖冲突的。 短路优先 本项目——&gt;A.jar——&gt;B.jar——&gt;X.jar 本项目——&gt;C.jar——&gt;X.jar 若本项目引用了A.jar，A.jar又引用了B.jar，B.jar又引用了X.jar，并且C.jar也引用了X.jar。 在此时，Maven只会引用引用路径最短的Jar。 声明优先 若引用路径长度相同时，再pom.xml中谁先被生命，就使用谁； 聚合 什么是聚合 将多个项目同时构建就称为聚合 如何实现聚合 再maven项目的pom中作如下配置 1234&lt;modules&gt; &lt;module&gt;web-connection-pool&lt;/module&gt; &lt;module&gt;web-java-crawler&lt;/module&gt;&lt;/modules&gt; 继承 什么是继承 再聚合多个项目时，如果这些被聚合的项目中需要引入相同的jar包，那么可以将这些jar写入父pom中，各个子项目继承该pom即可。 如果实现继承 父pom配置: 将需要继承的jar包的坐标放入标签即可 12345678910&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;cn.missbe.web.search&lt;/groupId&gt; &lt;artifactId&gt;resource-search&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 子pom配置: 1234567891011&lt;parent&gt; &lt;groupId&gt;父pom所在项目的groupId&lt;/groupId&gt; &lt;artifactId&gt;父pom所在项目的artifactId&lt;/artifactId&gt; &lt;version&gt;父pom所在项目的版本号&lt;/version&gt;&lt;/parent&gt; &lt;parent&gt; &lt;artifactId&gt;resource-search&lt;/artifactId&gt; &lt;groupId&gt;cn.missbe.web.search&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/parent&gt; pom配置解析一、基本配置1.配置说明 1234567891011121314151617181920212223242526272829&lt;!-- 模型版本。maven2.0必须是这样写，现在是maven2唯一支持的版本 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 公司或者组织的唯一标志，并且配置时生成的路径也是由此生成， 如com.winner.trade，maven会将该项目打成的jar包放本地路径：/com/winner/trade --&gt; &lt;groupId&gt;com.winner.trade&lt;/groupId&gt;&lt;!-- 本项目的唯一ID，一个groupId下面可能多个项目，就是靠artifactId来区分的 --&gt;&lt;artifactId&gt;...&lt;/artifactId&gt;&lt;!-- 本项目目前所处的版本号 --&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;!-- 打包的机制，如pom,jar, maven-plugin, ejb, war, ear, rar, par，默认为jar --&gt;&lt;packaging&gt;jar&lt;/packaging&gt;&lt;!-- 定义本项目的依赖关系 --&gt; &lt;dependencies&gt;...&lt;/dependencies&gt;&lt;!-- 父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。坐标包括group ID，artifact ID和 version或者还有relativePath --&gt; &lt;parent&gt;...&lt;/parent&gt;&lt;!-- 定义本项目的依赖关系 --&gt; &lt;dependencyManagement&gt;...&lt;/dependencyManagement&gt;&lt;!-- 子模块代码，对modules下的模块进行聚合 --&gt; &lt;modules&gt;...&lt;/modules&gt;&lt;!-- 为pom定义一些常量，在pom中的其它地方可以直接引用 使用方式 如 ：$&#123;file.encoding&#125; --&gt; &lt;properties&gt;...&lt;/properties&gt; 2.dependencies 和 dependecyManagement 的区别 1）dependencies即使在子项目中不写该依赖项，那么子项目仍然会从父项目中继承该依赖项（全部继承） 2）dependencyManagement里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本，才会从父项目中继承该项，并且version和scope都读取自父pom;另外如果子项目中指定了版本号，那么会使用子项目中指定的jar版本 二、构建过程的设置1.配置示例 12345678910111213141516171819202122&lt;build&gt; &lt;finalName&gt;platform-b2c&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.xls&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;!-- xls文件不能filter处理，需区分出来 --&gt; &lt;resource&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/resources&lt;/directory&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;**/*.xls&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt;到时候详细看&lt;/plugins&gt; &lt;pluginManagement&gt;到时候详细看&lt;/pluginManagement&gt;&lt;/build&gt; 2.配置说明 12345678910111213141516171819202122232425262728&lt;!-- 产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt; &lt;finalName&gt;myPorjectName&lt;/finalName&gt; &lt;!-- 构建产生的所有文件存放的目录,默认为$&#123;basedir&#125;/target，即项目根目录下的target --&gt; &lt;directory&gt;$&#123;basedir&#125;/target&lt;/directory&gt; &lt;!--必须跟命令行上的参数相同例如jar:jar，或者与某个阶段（phase）相同例如install、compile等 --&gt; &lt;defaultGoal&gt;install&lt;/defaultGoal&gt; &lt;!--当filtering开关打开时，使用到的过滤器属性文件列表。 --&gt; &lt;filters&gt; &lt;filter&gt;../filter.properties&lt;/filter&gt; &lt;/filters&gt; &lt;!--项目相关的所有资源路径列表，例如和项目相关的配置文件、属性文件，这些资源被包含在最终的打包文件里。 --&gt; &lt;resources&gt; &lt;resource&gt;见3&lt;/resource&gt;&lt;/resources&gt; &lt;!--使用的插件列表 --&gt;&lt;plugins&gt; &lt;plugin&gt;...&lt;/plugin&gt;&lt;/plugins&gt; &lt;!--pluginManagement主要是为了统一管理插件，确保所有子POM使用的插件版本保持一致，类似dependencies和dependencyManagement，只是一个声明，一般用在父pom文件中 --&gt;&lt;pluginManagement&gt; &lt;plugins&gt;...&lt;/plugins&gt; &lt;/pluginManagement&gt; 3.resource 说明 123456789101112131415161718192021222324252627&lt;resources&gt; &lt;resource&gt; &lt;!--描述了资源的目标路径。该路径相对target/classes目录（例如$&#123;project.build.outputDirectory&#125;）。 --&gt; &lt;!--举个例子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为org/apache/maven/messages。 --&gt; &lt;!--然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --&gt; &lt;targetPath&gt;resources&lt;/targetPath&gt; &lt;!--是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;!--描述存放资源的目录，该路径相对POM路径 --&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;!--包含的模式列表 --&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;!--排除的模式列表 如果&lt;include&gt;与&lt;exclude&gt;划定的范围存在冲突，以&lt;exclude&gt;为准 --&gt; &lt;excludes&gt; &lt;exclude&gt;jdbc.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/resources&gt; 4.plugins说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;execution&gt; &lt;!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id&gt;assembly&lt;/id&gt; &lt;!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;!--配置的执行目标 --&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;!--配置是否被传播到子POM --&gt; &lt;inherited&gt;false&lt;/inherited&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--作为DOM对象的配置,配置项因插件而异 --&gt; &lt;configuration&gt; &lt;finalName&gt;$&#123;finalName&#125;&lt;/finalName&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptor&gt;assembly.xml&lt;/descriptor&gt; &lt;/configuration&gt; &lt;!--是否从该插件下载Maven扩展（例如打包和类型处理器）， --&gt; &lt;!--由于性能原因，只有在真需要下载时，该元素才被设置成true。 --&gt; &lt;extensions&gt;false&lt;/extensions&gt; &lt;!--项目引入插件所需要的额外依赖 --&gt; &lt;dependencies&gt; &lt;dependency&gt;...&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--任何配置是否被传播到子项目 --&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;/plugin&gt; &lt;/plugins&gt; 三、偏好设置profile1.配置示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!--在列的项目构建profile，如果被激活，会修改构建处理 --&gt; &lt;profiles&gt; &lt;!--根据环境参数或命令行参数激活某个构建处理 --&gt; &lt;profile&gt; &lt;!-- 一般是环境的id --&gt; &lt;id /&gt; &lt;build /&gt; &lt;modules /&gt; &lt;repositories /&gt; &lt;pluginRepositories /&gt; &lt;dependencies /&gt; &lt;reporting /&gt; &lt;dependencyManagement /&gt; &lt;distributionManagement /&gt; &lt;!-- 环境的属性设置,一般可以给个属性值，比如dev,test,uat,prod， 从而激发某个环境的配置，比如用在build的resource中 --&gt; &lt;properties /&gt; &lt;!--自动触发profile的条件逻辑。Activation是profile的开启钥匙。 --&gt; &lt;activation&gt; &lt;!--profile默认是否激活的标识 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!--activation有一个内建的java版本检测，如果检测到jdk版本与期待的一样，profile被激活。 --&gt; &lt;jdk&gt;1.7&lt;/jdk&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows') --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;名称&#125;引用），其拥有对应的名称和值，Profile就会被激活。 --&gt; &lt;!-- 如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --&gt; &lt;property&gt; &lt;!--激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活profile。 --&gt; &lt;!--另一方面，exists则会检查文件是否存在，如果存在则激活profile。 --&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/&lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profiles&gt; pom.xml 示例pom.xml是Maven的核心，你的项目需要什么Jar包就在pom.xml里面配置。当编译项目时Maven读取该文件，并从仓库中下载相应的Jar包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;!--父项目的坐标。如果项目中没有规定某个元素的值，那么父项目中的对应值即为项目的默认值。 坐标包括group ID，artifact ID和 version。--&gt; &lt;parent&gt; &lt;!--被继承的父项目的构件标识符--&gt; &lt;artifactId/&gt; &lt;!--被继承的父项目的全球唯一标识符--&gt; &lt;groupId/&gt; &lt;!--被继承的父项目的版本--&gt; &lt;version/&gt; &lt;/parent&gt; &lt;!--声明项目描述符遵循哪一个POM模型版本。模型本身的版本很少改变，虽然如此，但它仍然是必不可少的，这是为了当Maven引入了新的特性或者其他模型变更的时候，确保稳定性。--&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!--项目的全球唯一标识符，通常使用全限定的包名区分该项目和其他项目。并且构建时生成的路径也是由此生成， 如com.mycompany.app生成的相对路径为：/com/mycompany/app--&gt; &lt;groupId&gt;cn.missbe.web&lt;/groupId&gt; &lt;!-- 构件的标识符，它和group ID一起唯一标识一个构件。换句话说，你不能有两个不同的项目拥有同样的artifact ID和groupID；在某个 特定的group ID下，artifact ID也必须是唯一的。构件是项目产生的或使用的一个东西，Maven为项目产生的构件包括：JARs，源码，二进制发布和WARs等。--&gt; &lt;artifactId&gt;search-resources&lt;/artifactId&gt; &lt;!--项目产生的构件类型，例如jar、war、ear、pom。插件可以创建他们自己的构件类型，所以前面列的不是全部构件类型--&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;!--项目当前版本，格式为:主版本.次版本.增量版本-限定版本号--&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!--项目的名称, Maven产生的文档用--&gt; &lt;name&gt;search-resources&lt;/name&gt; &lt;!--项目主页的URL, Maven产生的文档用--&gt; &lt;url&gt;http://www.missbe.cn&lt;/url&gt; &lt;!-- 项目的详细描述, Maven 产生的文档用。 当这个元素能够用HTML格式描述时（例如，CDATA中的文本会被解析器忽略，就可以包含HTML标 签）， 不鼓励使用纯文本描述。如果你需要修改产生的web站点的索引页面，你应该修改你自己的索引页文件，而不是调整这里的文档。--&gt; &lt;description&gt;A maven project to study maven.&lt;/description&gt; &lt;!--描述了这个项目构建环境中的前提条件。--&gt; &lt;prerequisites&gt; &lt;!--构建该项目或使用该插件所需要的Maven的最低版本--&gt; &lt;maven/&gt; &lt;/prerequisites&gt; &lt;!--构建项目需要的信息--&gt; &lt;build&gt; &lt;!--该元素设置了项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。--&gt; &lt;sourceDirectory/&gt; &lt;!--该元素设置了项目脚本源码目录，该目录和源码目录不同：绝大多数情况下，该目录下的内容 会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。--&gt; &lt;scriptSourceDirectory/&gt; &lt;!--该元素设置了项目单元测试使用的源码目录，当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。--&gt; &lt;testSourceDirectory/&gt; &lt;!--被编译过的应用程序class文件存放的目录。--&gt; &lt;outputDirectory/&gt; &lt;!--被编译过的测试class文件存放的目录。--&gt; &lt;testOutputDirectory/&gt; &lt;!--使用来自该项目的一系列构建扩展--&gt; &lt;extensions&gt; &lt;!--描述使用到的构建扩展。--&gt; &lt;extension&gt; &lt;!--构建扩展的groupId--&gt; &lt;groupId/&gt; &lt;!--构建扩展的artifactId--&gt; &lt;artifactId/&gt; &lt;!--构建扩展的版本--&gt; &lt;version/&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!--这个元素描述了项目相关的所有资源路径列表，例如和项目相关的属性文件，这些资源被包含在最终的打包文件里。--&gt; &lt;resources&gt; &lt;!--这个元素描述了项目相关或测试相关的所有资源路径--&gt; &lt;resource&gt; &lt;!-- 描述了资源的目标路径。该路径相对target/classes目录（例如$&#123;project.build.outputDirectory&#125;）。举个例 子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为org/apache/maven /messages。然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。--&gt; &lt;targetPath/&gt; &lt;!--是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。--&gt; &lt;filtering/&gt; &lt;!--描述存放资源的目录，该路径相对POM路径--&gt; &lt;directory/&gt; &lt;!--包含的模式列表，例如**/*.xml.--&gt; &lt;includes/&gt; &lt;!--排除的模式列表，例如**/*.xml--&gt; &lt;excludes/&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!--这个元素描述了单元测试相关的所有资源路径，例如和单元测试相关的属性文件。--&gt; &lt;testResources&gt; &lt;!--这个元素描述了测试相关的所有资源路径，参见build/resources/resource元素的说明--&gt; &lt;testResource&gt; &lt;targetPath/&gt;&lt;filtering/&gt;&lt;directory/&gt;&lt;includes/&gt;&lt;excludes/&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!--构建产生的所有文件存放的目录--&gt; &lt;directory/&gt; &lt;!--产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。--&gt; &lt;finalName/&gt; &lt;!--当filtering开关打开时，使用到的过滤器属性文件列表--&gt; &lt;filters/&gt; &lt;!--子项目可以引用的默认插件信息。该插件配置项直到被引用时才会被解析或绑定到生命周期。给定插件的任何本地配置都会覆盖这里的配置--&gt; &lt;pluginManagement&gt; &lt;!--使用的插件列表 。--&gt; &lt;plugins&gt; &lt;!--plugin元素包含描述插件所需要的信息。--&gt; &lt;plugin&gt; &lt;!--插件在仓库里的group ID--&gt; &lt;groupId/&gt; &lt;!--插件在仓库里的artifact ID--&gt; &lt;artifactId/&gt; &lt;!--被使用的插件的版本（或版本范围）--&gt; &lt;version/&gt; &lt;!--是否从该插件下载Maven扩展（例如打包和类型处理器），由于性能原因，只有在真需要下载时，该元素才被设置成enabled。--&gt; &lt;extensions/&gt; &lt;!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。--&gt; &lt;executions&gt; &lt;!--execution元素包含了插件执行需要的信息--&gt; &lt;execution&gt; &lt;!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标--&gt; &lt;id/&gt; &lt;!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段--&gt; &lt;phase/&gt; &lt;!--配置的执行目标--&gt; &lt;goals/&gt; &lt;!--配置是否被传播到子POM--&gt; &lt;inherited/&gt; &lt;!--作为DOM对象的配置--&gt; &lt;configuration/&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--项目引入插件所需要的额外依赖--&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--任何配置是否被传播到子项目--&gt; &lt;inherited/&gt; &lt;!--作为DOM对象的配置--&gt; &lt;configuration/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;!--使用的插件列表--&gt; &lt;plugins&gt; &lt;!--参见build/pluginManagement/plugins/plugin元素--&gt; &lt;plugin&gt; &lt;groupId/&gt;&lt;artifactId/&gt;&lt;version/&gt;&lt;extensions/&gt; &lt;executions&gt; &lt;execution&gt; &lt;id/&gt;&lt;phase/&gt;&lt;goals/&gt;&lt;inherited/&gt;&lt;configuration/&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;goals/&gt;&lt;inherited/&gt;&lt;configuration/&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!--模块（有时称作子项目） 被构建成项目的一部分。列出的每个模块元素是指向该模块的目录的相对路径--&gt; &lt;modules/&gt; &lt;!--发现依赖和扩展的远程仓库列表。--&gt; &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息--&gt; &lt;repository&gt; &lt;!--如何处理远程仓库里发布版本的下载--&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled/&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。--&gt; &lt;updatePolicy/&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做：ignore（忽略），fail（失败），或者warn（警告）。--&gt; &lt;checksumPolicy/&gt; &lt;/releases&gt; &lt;!-- 如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的 策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --&gt; &lt;snapshots&gt; &lt;enabled/&gt;&lt;updatePolicy/&gt;&lt;checksumPolicy/&gt; &lt;/snapshots&gt; &lt;!--远程仓库唯一标识符。可以用来匹配在settings.xml文件里配置的远程仓库--&gt; &lt;id&gt;banseon-repository-proxy&lt;/id&gt; &lt;!--远程仓库名称--&gt; &lt;name&gt;banseon-repository-proxy&lt;/name&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式--&gt; &lt;url&gt;http://192.168.1.169:9999/repository/&lt;/url&gt; &lt;!-- 用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然 而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。--&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表，这些插件用于构建和报表--&gt; &lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见repositories/repository元素--&gt; &lt;pluginRepository&gt; ...... &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!--该元素描述了项目相关的所有依赖。 这些依赖组成了项目构建过程中的一个个环节。它们自动从项目定义的仓库中下载。要获取更多信息，请看项目依赖机制。--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!--依赖的group ID--&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;!--依赖的artifact ID--&gt; &lt;artifactId&gt;maven-artifact&lt;/artifactId&gt; &lt;!--依赖的版本号。 在Maven 2里, 也可以配置成版本号的范围。--&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;!-- 依赖类型，默认类型是jar。它通常表示依赖的文件的扩展名，但也有例外。一个类型可以被映射成另外一个扩展名或分类器。类型经常和使用的打包方式对应， 尽管这也有例外。一些类型的例子：jar，war，ejb-client和test-jar。如果设置extensions为 true，就可以在 plugin里定义新的类型。所以前面的类型的例子不完整。--&gt; &lt;type&gt;jar&lt;/type&gt; &lt;!-- 依赖的分类器。分类器可以区分属于同一个POM，但不同构建方式的构件。分类器名被附加到文件名的版本号后面。例如，如果你想要构建两个单独的构件成 JAR，一个使用Java 1.4编译器，另一个使用Java 6编译器，你就可以使用分类器来生成两个单独的JAR构件。--&gt; &lt;classifier&gt;&lt;/classifier&gt; &lt;!--依赖范围。在项目发布过程中，帮助决定哪些构件被包括进来。欲知详情请参考依赖机制。 - compile ：默认范围，用于编译 - provided：类似于编译，但支持你期待jdk或者容器提供，类似于classpath - runtime: 在执行时需要使用 - test: 用于test任务时使用 - system: 需要外在提供相应的元素。通过systemPath来取得 - systemPath: 仅用于范围为system。提供相应的路径 - optional: 当项目自身被依赖时，标注依赖是否传递。用于连续依赖时使用--&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;!--仅供system范围使用。注意，不鼓励使用这个元素，并且在新的版本中该元素可能被覆盖掉。该元素为依赖规定了文件系统上的路径。需要绝对路径而不是相对路径。推荐使用属性匹配绝对路径，例如$&#123;java.home&#125;。--&gt; &lt;systemPath&gt;&lt;/systemPath&gt; &lt;!--当计算传递依赖时， 从依赖构件列表里，列出被排除的依赖构件集。即告诉maven你只依赖指定的项目，不依赖项目的依赖。此元素主要用于解决版本冲突问题--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;!--可选依赖，如果你在项目B中把C依赖声明为可选，你就需要在依赖于B的项目（例如项目A）中显式的引用对C的依赖。可选依赖阻断依赖的传递性。--&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 继承自该项目的所有子项目的默认依赖信息。这部分的依赖信息不会被立即解析,而是当子项目声明一个依赖（必须描述group ID和 artifact ID信息），如果group ID和artifact ID以外的一些信息没有描述，则通过group ID和artifact ID 匹配到这里的依赖，并使用这里的依赖信息。--&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!--参见dependencies/dependency元素--&gt; &lt;dependency&gt; ...... &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;!--项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件部署到远程仓库。--&gt; &lt;distributionManagement&gt; &lt;!--部署项目产生的构件到远程仓库需要的信息--&gt; &lt;repository&gt; &lt;!--是分配给快照一个唯一的版本号（由时间戳和构建流水号）？还是每次都使用相同的版本号？参见repositories/repository元素--&gt; &lt;uniqueVersion/&gt; &lt;id&gt;banseon-maven2&lt;/id&gt; &lt;name&gt;banseon maven2&lt;/name&gt; &lt;url&gt;file://$&#123;basedir&#125;/target/deploy&lt;/url&gt; &lt;layout/&gt; &lt;/repository&gt; &lt;!--构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库，参见distributionManagement/repository元素--&gt; &lt;snapshotRepository&gt; &lt;uniqueVersion/&gt; &lt;id&gt;banseon-maven2&lt;/id&gt; &lt;name&gt;Banseon-maven2 Snapshot Repository&lt;/name&gt; &lt;url&gt;scp://svn.baidu.com/banseon:/usr/local/maven-snapshot&lt;/url&gt; &lt;layout/&gt; &lt;/snapshotRepository&gt; &lt;!--部署项目的网站需要的信息--&gt; &lt;site&gt; &lt;!--部署位置的唯一标识符，用来匹配站点和settings.xml文件里的配置--&gt; &lt;id&gt;banseon-site&lt;/id&gt; &lt;!--部署位置的名称--&gt; &lt;name&gt;business api website&lt;/name&gt; &lt;!--部署位置的URL，按protocol://hostname/path形式--&gt; &lt;url&gt; scp://svn.baidu.com/banseon:/var/www/localhost/banseon-web &lt;/url&gt; &lt;/site&gt; &lt;!--项目下载页面的URL。如果没有该元素，用户应该参考主页。使用该元素的原因是：帮助定位那些不在仓库里的构件（由于license限制）。--&gt; &lt;downloadUrl/&gt; &lt;!-- 给出该构件在远程仓库的状态。不得在本地项目中设置该元素，因为这是工具自动更新的。有效的值有：none（默认），converted（仓库管理员从 Maven 1 POM转换过来），partner（直接从伙伴Maven 2仓库同步过来），deployed（从Maven 2实例部 署），verified（被核实时正确的和最终的）。--&gt; &lt;status/&gt; &lt;/distributionManagement&gt; &lt;!--以值替代名称，Properties可以在整个POM中使用，也可以作为触发条件（见settings.xml配置文件里activation元素的说明）。格式是&lt;name&gt;value&lt;/name&gt;。--&gt; &lt;properties/&gt; &lt;/project&gt;]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[量化扫盲]]></title>
    <url>%2F2019%2F12%2F01%2F13.%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93%2F%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E6%89%AB%E7%9B%B2%2F</url>
    <content type="text"><![CDATA[量化交易用数学模型，而不是人脑，决定交易的品种、方向、时机、数量。 数学模型可以复杂，也可以简单 可以是自动交易，也可以不自动交易 有价格曲线的品种，都能量化交易 交易玩法众多 交易流程 构建策略(数学模型) 回测(用历史数据去验证策略的效果) 实盘测试(用资金去跑) 实盘 常用策略 跨交易所币币套利(2017-09之前) 程序自动套利(2018-05之前) 高频交易(资金容量小, 需要实时盯盘口数据，需要大量ip) 选币交易(按照一定的策略选择上涨最高，或者下跌最少的币种) 期现套利(利用期货和现货的价差，做空期货，实现无风险套利，机会不容易获得) 跨期套利(近期合约和远期合约之间价差的套利) 多空趋势交易(CTA策略, 策略判断涨跌，买涨买跌合约)]]></content>
      <categories>
        <category>量化交易</category>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>量化交易</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据调度平台后台]]></title>
    <url>%2F2019%2F11%2F30%2F11.Golang%2FGolang%E9%A1%B9%E7%9B%AE%2F%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E5%90%8E%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[欢迎来到 data-platform-admindata-platform-admin 是什么data-platform-admin 是data-platorm的一个后台项目，用来配置数据源的对接，编写数据处理方法等； 安装方法（单独安装/和data-platform一起安装） 单独安装 前提条件：用有python3的运行环境 12pip install -r requirements.txt # 安装依赖python manage.py runserver --insecure 8800 # 启动后台程序， 访问8800端口就可以访问后台系统 docker安装 1bash deploy.sh start 使用方法 平台首页 数据源配置相关界面 数据api配置页面 变量配置界面 快速体验下面体验一下快速的对接数据源并提供http接口的步骤 先按照之前的步骤搭建好数据调度平台，并运行data-platform-admin项目 然后我们有一个数据库里面有这样一条数据，想要通过接口访问的方式拿到 在数据地址管理界面先配置好数据源的地址信息(以mysql数据为例) 增加数据源，在数据源管理界面增加一条mysql数据源信息（mysql为例） 接下来就可以直接通过http请求，请求数据 1curl localhost:3000/api/basic -X POST -d 'hello=world' 这里只演示接入数据的最基本方式，想要对数据进行处理或者做参数加工，返回结果加工和封装，日志日定义，等可以多看一下页面有哪些相关配置，或者读取源码来更深入了解 贡献代码 拉取代码 1git clone https://github.com/KyleAdultHub/data-platform-admin.git 创建非maser分支，更改代码后提交请求]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang项目</category>
      </categories>
      <tags>
        <tag>数据平台</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据调度平台]]></title>
    <url>%2F2019%2F11%2F30%2F11.Golang%2FGolang%E9%A1%B9%E7%9B%AE%2F%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[欢迎来到 data-platformdata-platform 是什么data-platform 是一个快速对接数据源, 可以实现自由的加工(js代码)和整合数据， 并自动提供封装api的一个平台，并具有结构化的log输出和rest的接口风格； 其分为五个服务: 数据源对接服务(data-source)，数据源http代理服务(data-source-api), 数据api封装服务(data-api), 数据变量加工服务(data-derive), 数据平台总代理(data-proxy)； 在使用平台的时候可以根据需求选择需要的服务, 不需要所有服务都使用; 数据接入和编写加工逻辑等, 可以通过 data-platform-admin 项目进行页面可视化编辑; 数据源对接服务数据源对接服务, 可以快速的实现数据对接, 并提供rpc接口供访问 支持数据种类: mysql， oracle，redis, couchdb， tidb，mongo，file，http 数据源http代理服务对数据源对接服务得rpc接口进行封装，进行反向代理，提供http服务供调用，用户可以根据需求选择是否使用 数据api封装服务对数据源接入的数据返回进行加工，返回加工后的结果，并自动提供http接口供访问，支持对访问参数做加工，对返回结果做加工， 对log输出结果做加工； 数据变量加工服务对批量的数据api封装的结果进行整合，并加工和封装，并可以以变量和变量集合的方式作为接口调用的返回，支持对访问参数做加工，对返回结果做加工， 对log输出结果做加工，对变量整合成集合等； 数据平台总代理对调用进行代理，规整化请求方法，清洗接口调用方式 安装方法(提前安装docker，docker-compose) 拉取代码 制作二进制程序(如果已经存在可以跳过) 123456cd docker-deploggo build -o data-source ../services/main-control/main_data_source_plus.go # 编译data-sourcego build -o data-source-api ../apis/data-source/api.go # 编译data-source-apigo build -o data-api ../webs/main-control/main_data_api.go # 编译data-apigo build -o data-derive ../webs/main-control/main_data_derive.go # 编译data-derivego build -o data-proxy ../data-proxy/main.go # 编译data-proxy 修改配置文件 配置文件位置： docker-compose/config 目录 ， 可以根据需求自己更改配置文件的内容 启动数据平台依赖 12bash import-images.shdocker-compose -f docker-compose-db.yml up -d 启动数据平台服务 1docker-compose -f docker-compose-local.yml up -d 使用方法 数据源接入，加工数据等方式 数据源接入，加工数据，参数，log日志等都是通过页面配置的方式进行，改部分参考 data-platform 项目 数据调用 数据调用都是通过http的方式进行，暴露的接口集合如下 数据源相关接口123456789数据源所需参数获取方式url: /ds/query/paramsmethod: POSTparams: code 数据源代码 数据调用方式url: /ds/query/datamethod: POSTparams: code 数据源代码 | data 数据源配置所需要的参数 | cache 0/1 是否需要缓存数据 数据api封装相关接口1234567891011121314api所需参数获取方式url: /data-api/data/paramsmethod: POSTparams: code 数据api代码api数据调用方式url: /data-api/data/querymethod: POSTparams: code api代码 | data api所需要的参数 | cache 0/1 是否需要缓存数据api接口验证方式url: /data-api/data/verifymethod: POSTparams: code api代码 数据变量相关接口 123456789101112131415161718192021222324变量所需参数获取方式url: /data-derive/data/paramsmethod: POSTparams: code 数据变量代码变量调用方式url: /data-derive/data/queryDerivemethod: POSTparams: code 变量代码 | data 变量配置所需要的参数 | cache 0/1 是否需要缓存数据变量数据接口验证方式url: /data-api/data/verifyDerivemethod: POSTparams: code 变量代码 变量集调用方式url: /data-derive/data/queryDeriveSetmethod: POSTparams: code 变量集代码 | data 变量集配置所需要的参数 | cache 0/1 是否需要缓存数据变量集数据接口验证方式url: /data-api/data/verifyDeriveSetmethod: POSTparams: code 变量集代码 consul 服务取消1curl --request PUT http://127.0.0.1:8500/v1/agent/service/deregister/&#123;&#125; 贡献代码 拉取代码 1git clone https://github.com/KyleAdultHub/data-platform.git 创建非maser分支，更改代码后提交请求]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang项目</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>数据平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 共识机制]]></title>
    <url>%2F2019%2F09%2F11%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F7.Fabric%20%E5%85%B1%E8%AF%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[fabric 共识机制由于fabric是分布式的系统，因此需要共识机制来保障各个节点以相同的顺序状态保存账本，达成一致性。 在当前fabric1.4版本中，存在三种共识机制，分别是solo，kafka，etcdraft。交易的共识包括3个阶段的处理：提议阶段、打包阶段和验证阶段。 1.Solo 共识模式Solo共识模式指网络环境中只有一个排序节点，从Peer节点发送来的消息由一个排序节点进行排序和产生区块；由于排序服务只有一个排序节点为所有Peer节点服务，没有高可用性和可扩展性，不适合用于生产环境，通常用于开发和测试环境。 Solo共识模式调用过程说明： Peer节点通过gPRC连接排序服务，连接成功后，发送交易信息。 排序服务通过Recv接口，监听Peer节点发送过来的信息，收到信息后进行数据区块处理。 排序服务根据收到的消息生成数据区块，并将数据区块写入账本（Ledger）中，返回处理信息。 Peer节点通过deliver接口，获取排序服务生成的区块数据。 2.Kafka 共识模式Hyperledger Fabric的核心共识算法通过Kafka集群实现，简单来说，就是通过Kafka对所有交易信息进行排序（如果系统存在多个channel，则对每个channel分别排序）。Kafka是一个分布式的流式信息处理平台，目标是为实时数据提供统一的、高吞吐、低延迟的性能。Kafka由以下几类角色构成： Broker：消息处理节点，主要任务是接收producers发送的消息，然后写入对应的topic的partition中，并将排序后的消息发送给订阅该topic的consumers。 大量的Broker节点提高了数据吞吐量，并互相对partition数据做冗余备份（类似RAID技术）。 Zookeeper：为Brokers提供集群管理服务和共识算法服务（paxos算法），例如，选举leader节点处理消息并将结果同步给其它followers节点，移除故障节点以及加入新节点并将最新的网络拓扑图同步发送给所有Brokers。 Producer：消息生产者，应用程序通过调用Producer API将消息发送给Brokers。 Consumer：消息消费者，应用程序通过Consumer API订阅topic并接收处理后的消息。 Kafka将消息分类保存为多个topic，每个topic中包含多个partition，消息被连续追加写入partition中，形成目录式的结构。一个topic可以被多个consumers订阅。简单来说，partition就是一个FIFO的消息管道，一端由producer写入消息，另一端由consumer取走消息（注意，这里的取走并不会移除消息，而是移动consumer的位置指针）。 在Hyperledger Fabric中的Kafka实际运行逻辑如下： 对于每一条链，都有一个对应的分区 每个链对应一个单一的分区主题 排序节点负责将来自特定链的交易（通过广播RPC接收）中继到对应的分区 排序节点可以读取分区并获得在所有排序节点间达成一致的排序交易列表 一个链中的交易是定时分批处理的，也就是说当一个新的批次的第一个交易进来时，开始计时 当交易达到最大数量时或超时后进行批次切分，生成新的区块 定时交易是另一个交易，由上面描述的定时器生成 每个排序节点为每个链维护一个本地日志，生成的区块保存在本地账本中 交易区块通过分发RPC返回客户端 当发生崩溃时，可以利用不同的排序节点分发区块，因为所有的排序节点都维护有本地日志 3. Etcdraft 共识模式Raft 是 v1.4.1 中引入的，它是一种基于 etcd 的崩溃容错（CFT）排序服务。Raft 遵循 “领导者和追随者” 模型，其中领导者在通道中的orderer节点之间动态选出（这个节点集合称为“consenter set”），该领导者将消息复制到跟随者节点。由于系统可以承受节点（包括领导节点）的丢失，只要剩下大多数排序节点（即所谓的“仲裁”），Raft就被称为“崩溃容错”（CFT）。换句话说，如果一个通道中有三个节点，它可以承受一个节点的丢失（剩下两个节点）。 3.1 raft相关概念 日志条目：Raft排序服务中的主要工作单元是“日志条目”，这些条目的完整序列称为“日志”。如果成员的多数（法定人数，换言之）成员到条目及其顺序达成一致，我们认为日志是一致的。 Consenter设置：排序节点主动参与给定信道的共识机制并接收信道的复制日志。这可以是所有可用节点（在单个群集中或在对系统通道有贡献的多个群集中），或者是这些节点的子集。 有限状态机（FSM）：Raft中的每个排序节点都有一个FSM，它们共同用于确保各个排序节点中的日志序列是确定性的（以相同的顺序编写）。 法定人数：描述需要确认提案的最少数量的同意者，以便可以提交交易。对于每个consenter集，这是 大多数节点。在具有五个节点的群集中，必须有三个节点才能存在仲裁。如果由于任何原因导致法定数量的节点不可用，则orderer将无法用于通道上的读取和写入操作，并且不能提交新日志。 Leader：Leader负责提取新的日志条目，将它们复制到跟随者订购节点，以及管理何时认为条目已提交。这不是特殊类型orderer人。在情况决定的情况下，这只是orderer在某些时候可能拥有的角色，而不是其他角色。 Follower：Follower从Leader那里接收日志并确定性地复制它们，确保日志保持一致。Follower也会收到来自Leader的“心跳”信息。如果Leader停止在可配置的时间内发送这些消息，则追将发起选举，其中一个Follower将被选为新Leader。 3.2 raft在交易中的流程每个通道都在Raft协议的单独实例上运行，这允许每个实例选择不同的leader。还允许集群由不同组织控制的排序节点组成的用例中进一步分散服务。虽然所有Raft节点必须是系统通道的一部分，但它们不一定必须是所有应用程序通道的一部分。通道创建者（和通道管理员）可以选择可用orderer的子集，并根据需要添加或删除orderer（一次只能添加或删除一个节点）。 在Raft中，交易（提案或配置更新的形式）由接收交易的orderer节点自动路由到该信道的当前leader。这意味着peer和应用程序不需要知道leader是谁。只有orderer节点需要知道。 3.3 raft节点选举日志传输raft节点始终处于以下三种状态之一：follower，candidate或leader。所有节点最初都是follower。在这种状态下，他们可以接受来自leader的日志条目（如果已经当选），或者为leader投票。如果在设定的时间内没有收到日志条目或心跳（例如，五秒），则节点会自我提升到candidate状态。在候选状态中，节点请求来自其他节点的投票。如果候选人获得法定数量的选票，则将其提升为leader。leader接受新的日志条目并将其复制给follower。 虽然可以无限期地保留所有日志，但为了节省磁盘空间，Raft使用一个名为“snapshotting”的进程，用户可以在其中定义将在日志中保留多少字节的数据。每个快照将包含一定数量的块）。]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电子合同和电子签名]]></title>
    <url>%2F2019%2F07%2F30%2F20.%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2FCA%2F%E7%94%B5%E5%AD%90%E5%90%88%E5%90%8C%E5%92%8C%E7%94%B5%E5%AD%90%E7%AD%BE%E5%90%8D%2F</url>
    <content type="text"><![CDATA[电子合同的简介什么是电子合同电子合同实现了合同的签约方可以通过互联网进行签订，广泛应用于互联网金融、电子商务、O2O等行业，是互联网中一项相当重要的基础性服务，也是从事互联网行业的产品经理必须着重研究学习的领域。电子合同的有效性来自于电子签名，本篇文章将详细解析电子合同及电子签名的原理以及电子合同的产品设计思路。 依据2013年商务部公布的《电子合同在线订立流程规范》, 电子合同是指”平等主体的自然人、法人、其他组织之间以数据电文为载体, 并利用电子通信手段设立、变更、终止民事权利义务关系的协议”。 而根据《电子签名法》的规定, 数据电文是指”以电子、光学、磁或者类似手段生成、发送、接收或者储存的信息”。据此, 我们理解, 电子合同是以数据电文为载体, 以电子通信为手段的协议, 适用关于数据电文的相关规定。 电子合同的属性根据《合同法》的规定, 当事人订立合同, 有书面形式、口头形式和其他形式。法律、行政法规规定采用书面形式应当采用书面形式。当事人约定采用书面形式的, 应当采用书面形式。同时, 《合同法》也规定, 书面形式是指合同书、信件和数据电文(包括电报、电传、传真、电子数据交换和电子邮件)等可以有形地表现所载内容的形式。据此, 结合上述论述, 电子合同既然以数据电文作为载体, 即能够作为书面形式合同的一种, 得到《合同法》的认可与适用。 电子合同的电子签名考虑到一份成立并生效的电子合同需要合意的达成作为先决条件, 又结合电子合同与传统纸质合同相比采用了新的信息承载方式, 所以电子合同作为一种特殊的数据电文, 除了要满足传统合同所要求的法律要件外, 还需要满足《电子签名法》中关于数据电文的意思表示达成方式的法律要件。 电子合同的组成结合法律法规, 一份完整的数据电文主要包括: 数据电文和电子签名； 其中电子签名又包括 数字证书、可信时间戳 和 其他信息。如下图所列示。 CA 证书法律效益(有效性)电子签名法条例第十一条： 书面形式是指合同书、信件和数据电文（包括电报、电传、传真、电子数据交换和电子邮件）等可以有形地表现所载内容的形式。 该条例确定了电子合同属于书面合同的一种，得到了合同法的任何和适用； 第十三条: 电子签名制作数据用于电子签名时，属于电子签名人专有； 签署时电子签名制作数据仅由电子签名人控制； 签署后对电子签名的任何改动能够被发现； 签署后对数据电文内容和形式的任何改动能够被发现。 该条例规不仅认可了可靠电子签名（电子签章）的法律效力，同时也对可靠电子签名（电子签章）做出了要求：锁定签约主体真实身份、有效防止文件篡改、精确记录签约时间。 第十四条: 可靠的电子签名与手写签名或者盖章具有同等的法律效力 “可靠的电子签名”: 就是采用合法CA签发的数字证书签署产生的数字签名 第十六条: 电子签名法第十六条: 电子签名需要有第三方认证的，由依法设立的电子认证服务提供者提供认证服务。 该条例规定了电子签名的认证来源 总结: 电子签名是由依法设立的电子认证服务提供者提供的法律认可的一种书面形式的合同； 并且有效的电子合同需要满足电子签名法第十三条以下四个属性, 简称为: “真实身份、真实意愿、签名未改、原文未改“ 合法的第三发CA机构具有法律效益的国家CA机构有大约37家左右，任何一家都可以作为电子合同的第三方认证机构； 比较常用的CA机构有: 沃通CA、CFCA 等。。。 电子签名技术原理(确保有效性)电子签名满足电子签名法第十四条的规定，需要满足四个属性，简称为“身份真实、真实意愿、签名未改、原文未改”，是进行产品设计时要考虑的首要因素。 这些属性，电子签名认证提供商需要充分考虑， 只有满足了这些属性，才能确保电子签名具有一定的有效性； 1. 真实身份对签约方真实身份的确定，是通过各种实名认证方式来进行保证，所以实名认证是电子合同产品设计中必不可少的一环。实名认证功能可以由电子合同服务提供商提供，也可以由其他第三方符合要求的服务商提供。 实名认证分为多种方式，可靠性和用户体验各有不同，要根据产品需求灵活选用。 2. 真实意愿意愿认证是指在每一次签署前对用户的身份进行确认，保证是用户本人操作的认证方式。我们常见的用户密码登录其实就是一种意愿认证，但由于目前登录密码丢失、被盗取、被破解的风险很高，所以对于涉及金额较大和较重要的电子合同，产品设计中要考虑加入其他意愿认证方式进行可靠性强化。 常见的意愿认证方式有：指纹登入、短信验证码登入、语音验证码登入、人脸识别登入、UKEY登入等。各种意愿认证方式同样可靠性和用户体验各有不同，根据不同产品要求可以进行选用。 比如在比特币网络中，用户如果想要进行交易，则必须用保存在本地的私钥进行签名才能进行交易，因此也确保了真是意愿性； 3. 签名未改及原文未改签名未改及原文未改是通过数字签名技术+时间戳+CA机构颁发的数字证书一起保证和实现的，数字签名技术是其中的核心部分，时间戳和数字证书的实现方式都使用到数字签名。 对于数字签名和非对称算法等内容，可以参考区块链部分的密码学文章； 下面介绍一下数字签名在电子签名中的应用: 假设一个用户A执行了电子合同签约，则服务端先对电子合同原文进行哈希算法，得到电子合同原文的哈希值。然后使用用户A的私钥对哈希值进行加密，把加密得到的数字签名连同电子合同一起发送出去。 用户B收到带数字签名的电子合同后，先对原文用哈希算法得到哈希值，然后使用用户A的公钥对数字签名进行解密（对于用户A的公钥可信度是由CA机构的数字证书来保证，只要是国家认证的CA机构颁发的数字证书都认为是可信的，在此不展开讲解），解密得到的哈希值与原文用哈希算法得到哈希值进行比对，如果哈希值一致，则证明电子合同确实是A所签署的，并且没有被纂改过，也就达到验证“签名未改、原文未改”的目的。 时间戳在电子合同中的应用： 签订时间同样是电子签名成立和有效性关键环节，电子签名一般使用时间戳技术来对电子合同的签订时间进行有效确认。原理是对电子合同进行一次哈希运算，将哈希值发送给时间戳签发中心，时间戳签发中心使用数字签名技术对哈希值和当前时间进行一次数字签名。 由于对于电子合同哈希值具有唯一性，所以时间戳的数字签名可以确认电子合同签约的时间点，这样就保证了“签名未改”的实现。 如何接入第三方CA机构(CFCA安心签 + 供应链金融 为例)CFCA的证据保全与司法服务CFCA为证据采集、证据准备及证据受理提供全流程服务。 在证据采集阶段，电子证据保全系统通过采集客户业务活动过程中产生的电子数据，进行保全，形成电子证据，并出具证据保全报告。 在证据准备阶段，通过数字签名验证平台为客户业务活动中使用的数字签名进行验证，并出具数字签名验证报告。 在证据受理阶段，司法机关通过法律服务平台对数字签名验证报告和证据保全报告等电子证据进行专业化在线验证。 CFCA 对供应链金融的解决方法业务概述 随着社会化生产方式的不断深入，市场竞争已经从单一客户之间的竞争转变为供应链与供应链之间的竞争，同一供应链内部各方相互依存，同时赊销已成为交易的主流方式，处于供应链中上游的供应商，很难通过传统的信贷方式获得银行的资金支持，而资金短缺又会直接导致后续环节的停滞。 2011年以来，各家商业银行受到信贷规模的限制，可以发放的贷款额度十分有限，但是通过承兑、票据、信用证等延期支付工具，既能够增强企业之间的互相信任，也稳定了一批客户，银行业空前重视供应链金融业务。面对国内互联网金融的飞速发展，传统线下的供应链金融模式已经很难满足企业客户对于高效、便捷、不受时空限制的需求，利用线上供应链金融平台实现线上业务办理已是趋势。 传统供应链金融采用现场面签的方式，而采用互联网模式的供应链金融面临的最大的挑战还是法律风险、交易风险、技术风险，引入CFCA数字证书体系可以有效的解决上述风险。 平台价值​ 解决方案​ 总体架构 ·参与各方（融资企业、核心企业、商业银行）获得由供应链金融平台发放的数字证书，登录平台、签署电子合同时使用证书做身份验证和电子签名； ·供应链金融平台调用RA系统实现证书管理功能，调用签名验签服务器实现签名验签的功能，RA系统连接CFCA的CA系统完成证书签发和管理。 业务流程​ 方案特点用户真实身份 线下面对面并使用纸质证件代表参与各方的身份，线上使用数字证书实现高强度的身份认证，并解决用户身份真实性验证问题，防止假冒用户恶意操作； 电子合同法律效力 在平台签署电子合同时使用电子签名技术，符合电子签名法的要求，从而保证平台签署的电子合同具有法律效力； 电子合同信息做到防篡改和保密 平台各参与方签署电子合同包含敏感业务信息，使用签名和加密技术实现敏感业务信息的防篡改和保密； 平台用户的易操作性 数字证书和签名技术已经是广泛应用的成熟技术，目前在网上银行广泛使用，平台用户已经成熟掌握该技术的操作和使用。 安心签平台接入流程安心签平台介绍安心签是中国金融认证中心（以下简称“CFCA”）旗下的第三方电子缔约服务平台，为持有有效数字证书的用户提供数据电文或电子缔约文件的在线签署、存储和管理服务。 名词介绍客户平台: 与安心签对接的电子合同建设方。 平台用户: 客户平台的用户，通过客户平台方使用安心签电子合同服务。 用户三要素: 用户名、证件类型、证件号码，用以判断用户身份及唯一性，相同的三要素视为同一个用户。 业务流程分类客户平台与安心签对接，实现电子合同签署，根据业务场景，可以分为以下四种方式。 1. 证书托管 + 合同模板方式 平台用户通过客户平台在安心签开户，安心签自动为用户生成数字证书并保存在安心签中。客户平台预先生成合同模板并保存在安心签中。合同签署时，客户平台将关键信息及用户信息传给安心签，安心签自动将关键信息填入相应的合同模板中，并调用用户证书进行签署，生成电子合同。 2.证书托管 + 自定义合同方式 平台用户通过客户平台在安心签开户，安心签自动为用户生成数字证书并保存在安心签中。合同签署时，客户平台将合同文件和用户信息传给安心签，安心签调用用户证书在合同的指定位置进行签署，生成电子合同。 上述两种方式的区别在于，如果每份合同差别不大，则适用合同模板的方式；如果每份合同差别较大，则适用自定义合同的方式。 3.自有证书签署 + 合同模板方式 如果客户平台或平台用户不希望安心签托管证书，平台用户可以通过客户平台在安心签开户，并绑定自有的CFCA证书，即在安心签中使用用户自己保管的CFCA证书（该证书信息必须已经收录在网络身份认证平台数据库中，具体业务中客户若需要绑定自有证书，请技术支持与业务确认该信息）。客户平台预先生成合同模板并保存在安心签中。合同签署时，客户平台将关键信息及用户信息传给安心签，安心签自动将关键信息填入相应的合同模板中，生成合同全文，获取合同摘要（哈希值），客户平台在用户本地调用用户证书对合同摘要进行签署（签名值），并将签名值发送到安心签，生成电子合同。 4.证书托管 + 合同模板方式 如果客户平台或平台用户不希望安心签托管证书，平台用户可以通过客户平台在安心签开户，并绑定自有的CFCA证书，即在安心签中使用用户自己保管的CFCA证书（该证书信息必须已经收录在网络身份认证平台数据库中，具体业务中客户若需要绑定自有证书，请技术支持与业务确认该信息）。合同签署时，客户平台将合同文件和用户信息传给安心签。客户平台在获取合同摘要（哈希值）后，在用户本地调用用户证书对合同摘要进行签署（签名值），并将签名值发送到安心签，生成电子合同。]]></content>
      <categories>
        <category>开发工具</category>
        <category>CA</category>
      </categories>
      <tags>
        <tag>true</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hyperleder explorer 部署]]></title>
    <url>%2F2019%2F07%2F26%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F6.Hyperledger%20explorer%2F</url>
    <content type="text"><![CDATA[该篇文档的前提是已经存在部署好的fabric网络, 并且在peer节点有运行的自定义链码 Explorer 区块链浏览器部署区块链网络部署(前提) 在部署 Explorer项目前提前要有存在的区块链网络，这里说一下区块链网络部署时应该注意点的点 在区块链网络中启动操作服务器 如果要使用区块链监控，在启动网络的时候应该加上如下环境变量 1234- ORDERER_OPERATIONS_LISTENADDRESS=0.0.0.0:8443 # operation RESTful API- ORDERER_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API- CORE_OPERATIONS_LISTENADDRESS=0.0.0.0:9443 # operation RESTful API- CORE_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API 务必开启gossip 开启gossip， 使区块链网络开启发现服务 12- CORE_PEER_GOSSIP_BOOTSTRAP=X.X.X.X:XXXX- CORE_PEER_GOSSIP_EXTERNALENDPOINT=X.X.X.X:XXXX 项目拉取12git clone https://github.com/hyperledger/blockchain-explorer.gitcd blockchain-explorer 安装数据库配置prostgreSQL数据库 第一种配置方法 cd blockchain-explorer/app 修改 explorerconfig.json 1234567"postgreSQL": &#123; "host": "127.0.0.1", "port": "5432", "database": "fabricexplorer", "username": "hppoc", "passwd": "password"&#125; 第二种配置方法 12345export DATABASE_HOST=127.0.0.1export DATABASE_PORT=5432export DATABASE_DATABASE=fabricexplorerexport DATABASE_USERNAME=hppocexport DATABASE_PASSWD=pass12345 给执行sql脚本的目录带执行权限1chomd -R 775 /blockchain-explorer/app/persistence/fabric/postgreSQL/db 执行数据库脚本12cd blockchain-explorer/app/persistence/fabric/postgreSQL/dbsudo -u postgres ./createdb.sh 配置身份认证配置jwt​ cd blockchain-explorer/app 修改explorerconfig.json 文件 1234"jwt": &#123; "secret" : "a secret phrase!!", # 用来签名payload的密钥 "expiresIn": "2 days" # token 过期时间 Eg: 60, "2 days", "10h", "7d".&#125; 配置explorer 链接到区块链网络配置 explorer 链接区块链网络 配置网络文件位置: /blockchain-explorer/app/platform/fabric/config.json 配置链接文件位置: /blockchain-explorer/app/platform/fabric/connection-profile 配置 prometheus 链接到操作服务器​ 1. 配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml 安装并配置 grafana 配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json 配置文件位置: app/platform/fabric/artifacts/operations/grafana_conf/provisioning/dashboards/dashboard.yaml 配置文件位置: app/platform/fabric/artifacts/operations/grafana_conf/provisioning/datasources/datasource.yaml Build Explorer 项目12./main.sh clean./main.sh install 运行Explorer项目1234567891011121314cd blockchain-explorer/app 修改explorerconfig.json 同步频率 主机类型 区块同步时间的详细配置# 普通方式运行和停止cd blockchain-explorer/./start.sh (it will have the backend up)../start.sh debug (it will have the backend in debug mode)../start.sh print (it will print help).Launch the URL http(s)://localhost:8080 on a browser../stop.sh (it will stop the node server).# Sync方式运行和停止cd blockchain-explorer/./syncstart.sh (it will have the sync node up)../syncstop.sh (it will stop the sync node). Explorer Docker 部署 前提: bash Docker Docker Compose 仓库地址: Hyperledger Explorer docker repository https://hub.docker.com/r/hyperledger/explorer/ Hyperledger Explorer PostgreSQL docker repository https://hub.docker.com/r/hyperledger/explorer-db 区块链网络部署(前提) 在部署 Explorer项目前提前要有存在的区块链网络，这里说一下区块链网络部署时应该注意点的点 在区块链网络中启动操作服务器 如果要使用区块链监控，在启动网络的时候应该加上如下环境变量 1234- ORDERER_OPERATIONS_LISTENADDRESS=0.0.0.0:8443 # operation RESTful API- ORDERER_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API- CORE_OPERATIONS_LISTENADDRESS=0.0.0.0:9443 # operation RESTful API- CORE_METRICS_PROVIDER=prometheus # prometheus will pull metrics from orderer via /metrics RESTful API 务必开启gossip 开启gossip， 使区块链网络开启发现服务 12- CORE_PEER_GOSSIP_BOOTSTRAP=X.X.X.X:XXXX- CORE_PEER_GOSSIP_EXTERNALENDPOINT=X.X.X.X:XXXX Explorer项目拉取12git clone https://github.com/hyperledger/blockchain-explorer.gitcd blockchain-explorer 配置Explorer配置explorer和fabric网络的连接参考配置文件位置: examples/net1/config.json 连接的主配置文件 examples/net1/connection-profile 和 fabric 的连接配置 examples/net1/crypto 网络传输加密文件目录(如果有的话) 配置 prometheus参考配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml 配置 grafana参考配置文件位置: app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json 一个dashboard的详细配置 app/platform/fabric/artifacts/operations/grafana_conf/provisioning/dashboards/dashboard.yaml dashboard的存储目录等基本配置 app/platform/fabric/artifacts/operations/grafana_conf/provisioning/datasources/datasource.yaml 数据源的配置 配置docker-compose 文件配置网络配置网络，可以指定已经存在的网络，如果和区块链网络在同一台机器上可以使用区块链网络 如果和区块链网络是分开部署的，可以指定一个其他的任意网络 1234networks: mynetwork.com: external: name: net_byfn 配置数据库1234567environment: # 配置数据库名称，用户密码等 - DATABASE_DATABASE=fabricexplorer - DATABASE_USERNAME=hppoc - DATABASE_PASSWORD=password volumes: # 配置数据库创建监本位置； 配置数据库持久化的位置 - ./app/persistence/fabric/postgreSQL/db/createdb.sh:/docker-entrypoint-initdb.d/createdb.sh - pgdata:/var/lib/postgresql/data 配置 explorer12345678910environment: - DATABASE_HOST=explorerdb.mynetwork.com # 连接的数据库配置 - DATABASE_USERNAME=hppoc - DATABASE_PASSWD=password - DISCOVERY_AS_LOCALHOST=false # 如果是通过桥接的方式连接到区块链浏览器，务必设置为false volumes: # 引用的配置文件位置配置 - ./examples/net1/config.json:/opt/explorer/app/platform/fabric/config.json - ./examples/net1/connection-profile:/opt/explorer/app/platform/fabric/connection-profile - ./examples/net1/crypto:/tmp/crypto - walletstore:/opt/wallet 配置 prometheus123volumes: - ./app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml:/etc/prometheus/prometheus.yml # 引用的配置文件位置 - prometheus-storage:/prometheus # prometheus数据持久化的位置 配置 grafana1234volumes: # 配置引用配置文件的位置，以及数据持久化的文职 - ./app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json:/var/lib/grafana/dashboards/mydashboard.json - ./app/platform/fabric/artifacts/operations/grafana_conf/provisioning:/etc/grafana/provisioning - grafana-storage:/var/lib/grafana docker-compose.yaml 文件的示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970version: '2.1'volumes: pgdata: walletstore: grafana-storage: prometheus-storage:networks: mynetwork.com: external: name: net_byfnservices: explorerdb.mynetwork.com: image: hyperledger/explorer-db:latest container_name: explorerdb.mynetwork.com hostname: explorerdb.mynetwork.com environment: - DATABASE_DATABASE=fabricexplorer - DATABASE_USERNAME=hppoc - DATABASE_PASSWORD=password volumes: - ./app/persistence/fabric/postgreSQL/db/createdb.sh:/docker-entrypoint-initdb.d/createdb.sh - pgdata:/var/lib/postgresql/data networks: - mynetwork.com explorer.mynetwork.com: image: hyperledger/explorer:latest container_name: explorer.mynetwork.com hostname: explorer.mynetwork.com environment: - DATABASE_HOST=explorerdb.mynetwork.com - DATABASE_USERNAME=hppoc - DATABASE_PASSWD=password - DISCOVERY_AS_LOCALHOST=false volumes: - ./examples/net1/config.json:/opt/explorer/app/platform/fabric/config.json - ./examples/net1/connection-profile:/opt/explorer/app/platform/fabric/connection-profile - ./examples/net1/crypto:/tmp/crypto - walletstore:/opt/wallet command: sh -c "sleep 16&amp;&amp; node /opt/explorer/main.js &amp;&amp; tail -f /dev/null" ports: - 8090:8080 networks: - mynetwork.com proms: container_name: proms image: prom/prometheus:latest volumes: - ./app/platform/fabric/artifacts/operations/balance-transfer/prometheus.yml:/etc/prometheus/prometheus.yml - prometheus-storage:/prometheus ports: - '9090:9090' networks: - mynetwork.com grafana: container_name: grafana image: grafana/grafana:latest volumes: - ./app/platform/fabric/artifacts/operations/balance-transfer/balance-transfer-grafana-dashboard.json:/var/lib/grafana/dashboards/mydashboard.json - ./app/platform/fabric/artifacts/operations/grafana_conf/provisioning:/etc/grafana/provisioning - grafana-storage:/var/lib/grafana ports: - '3000:3000' networks: - mynetwork.com 启动网络1docker-compose up -d]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webpack 使用]]></title>
    <url>%2F2019%2F07%2F16%2F03.web%E5%89%8D%E7%AB%AF%2FVUE%2Fwebpack%20%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、webpack介绍1、由来由于前端之前js、css、图片文件需要单独进行压缩和打包，这样团队人员处理很繁琐，然后 Instagram 团队就想让这些工作自动化，然后webpack应运而生。 2、介绍webpack是一个模块打包器（module bundler），webpack视HTML，JS，CSS，图片等文件都是一种 资源 ，每个资源文件都是一个模块（module）文件，webpack就是根据每个模块文件之间的依赖关系将所有的模块打包（bundle）起来。 3、作用 对 CommonJS 、 AMD 、ES6的语法做了兼容 对js、css、图片等资源文件都支持打包（适合团队化开发） ，比方你写一个js文件，另外一个人也写一个js文件，需要合并很麻烦，现在交给webpack合并很简单 有独立的配置文件webpack.config.js 可以将代码切割成不同的chunk，实现按需加载，降低了初始化时间 具有强大的Plugin（插件）接口，大多是内部插件，使用起来比较灵活 4、拓展说明 CommonJS、AMD、CMD是用于JavaScript模块管理的三大规范，CommonJS定义的是模块的同步加载，是一个更偏向于服务器端的规范（也可以在浏览器中使用），主要用于Nodejs，根据CommonJS规范，一个单独的文件就是一个模块，加载模块使用require()方法，该方法读取一个文件并执行，最后返回文件内部的exports对象。 AMD和CMD则是定义模块异步加载适用于浏览器端，都是为了 JavaScript 的模块化开发，（这里说一下为什要有异步加载，因为浏览器如果使用common.js同步加载模块的话，就会导致性能等问题，所以针对这个问题，又出了一个规范，这个规范可以实现异步加载依赖模块） AMD规范会提前加载依赖模块，AMD规范是通过requireJs 在推广过程中对模块定义的规范化产出。 CMD规范会延迟加载依赖模块， CMD 规范是 SeaJs 在推广过程中对模块定义的规范化产出。 AMD规范和CMD规范的区别 对于依赖的模块，AMD 是提前执行，CMD 是延迟执行。不过 RequireJS 从 2.0 开始，也改成可以延迟执行（根据写法不同，处理方式不同）CMD 推崇依赖就近，AMD 推崇依赖前置AMD 的 API 默认是一个当多个用，CMD 的 API 严格区分，推崇职责单一。比如 AMD 里，require 分全局 require 和局部 require，都叫 require。CMD 里，没有全局 require，而是根据模块系统的完备性，提供 seajs.use 来实现模块系统的加载启动。CMD 里，每个 API 都简单纯粹webpack和gulp的区别 gulp是前端自动化构建工具，强调的是前端开发的工作流程，我们可以通过配置一系列的task，定义task处理的事情（代码压缩、合并、编译、浏览器实时更新等），然后定义执行顺序，来让gulp执行这些task，从而构建项目的整个前端开发流程，自动化构建工具并不能把所有模块打包到一起，也不能构建不同模块之间的依赖关系。webpack是 JavaScript 应用程序的模块打包器，强调的是一个前端模块化方案，更侧重模块打包，我们可以把开发中的所有资源（图片、js文件、css文件等）都看成模块，通过loader（加载器）和plugins（插件）对资源进行处理，打包成符合生产环境部署的前端资源。 5、webpack整体认知 (1)、webpack的核心概念分为 入口(Entry)、加载器(Loader)、插件(Plugins)、出口(Output); 入口(Entry)：入口起点告诉 webpack 从哪里开始，并根据依赖关系图确定需要打包的文件内容 加载器(Loader)：webpack 将所有的资源（css, js, image 等）都看做模块，但是 webpack 能处理的只是 JavaScript，因此，需要存在一个能将其他资源转换为模块，让 webpack 能将其加入依赖树中的东西，它就是 loader。loader用于对模块的源代码进行转换。loader 可以使你在 import 或”加载”模块时预处理文件。因此，loader 类似于其他构建工具中“任务(task)”，并提供了处理前端构建步骤的强大方法。 1234567js rules: [ &#123; test: /\.(js|jsx)$/, use: 'babel-loader' &#125; ] 插件(Plugins)：loader 只能针对某种特定类型的文件进行处理，而 plugin 的功能则更为强大。在 plugin 中能够介入到整个 webpack 编译的生命周期，Plugins用于解决 loader 无法实现的其他事情，也就是说loader是预处理文件，那plugin 就是后处理文件。比如对loader打包后的模块文件（bundle.js）进行二次优化处理，例如：代码压缩从而减小文件体积；提供辅助开发的作用：例如：热更新（浏览器实时显示） 1234plugins: [ new webpack.optimize.UglifyJsPlugin(), new HtmlWebpackPlugin(&#123;template: './src/index.html'&#125;)] 二、webpack安装1、安装node使用 node -v 命令检查版本 2、安装cnpm1npm install -g cnpm --registry=https://registry.npm.taobao.org 使用 cnpm -v 命令检查版本 3、安装nrm的两种方法nrm可以帮助我们切换不同的NPM源的快捷开关，可以切换的NPM源包括：npm，cnpm，taobao， rednpm， npmMirror ， edunpm 第一种方法（由于是外网访问进行安装，可能会被墙） 1npm install -g nrm 第二种方法（国内的淘宝镜像，访问稳定，推荐） 1cnpm install -g nrm 使用 nrm - V 命令检查版本(注意这里的 V 是大写的)使用nrm ls 命令可以查看当前可以可以切换的 NPM源使用 npm use cnpm 命令 指定要使用的哪种NPM源 4、安装webpack全局安装 1npm install --global webpack 在项目中安装最新版本或特定版本，分别执行以下命令： 12npm install --save-dev webpack npm install --save-dev webpack@&lt; version &gt; 三、webpack配置0、搭建项目结构12345678Project- dist- src- - js- - - moudle1.js- - - main.js- index.html- webpack.config.js ## 手动创建 1、初始化一个项目（会创建一个package.json文件）1npm init 2、在当前的项目中安装Webpack作为依赖包1npm install --save-dev webpack –save ：将配置信息保存到package.json中，也是项目生产环境，项目发布之后还依赖的东西，保存在dependencies –save-dev ：是项目开发环境依赖的东西，保存在devDependencies中, 例如：写 ES6 代码，如果你想编译成 ES5 发布那么 babel 就是devDependencies 3、当前项目结构 4、实现CSS打包1cnpm install css-loader style-loader --save-dev 在src—&gt;css目录中新建main.css 1234css #first&#123; border: 1px solid red; &#125; 在webpack.config.js中配置相关的loader 123456789101112131415161718192021222324const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径// const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错// 常量存储的是一个不可以变化的变量。// module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, './dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125; ] &#125;&#125; 在main.js中获取css目录中的main.css文件 12345678910111213141516// 1、获取index.html中的dom对象var first = document.getElementById('first');var btn = document.getElementById('btn');var two = document.getElementById('two');var res = document.getElementById('res'); btn.onclick = function()&#123; var firstValue = parseFloat(first.value); var twoValue = parseFloat(two.value); //2、获取 module1.js中的 sum函数 //http://www.ruanyifeng.com/blog/2015/05/require.html var sum = require('./module1.js'); res.value = sum(firstValue,twoValue); &#125;// 3、获取css目录中的main.css文件require('../css/main.css'); 在终端中输入 webpack命令进行css文件打包 5、实现SCSS打包在src目录中新建 sass目录–&gt; scss1.scss 12345// scss1.scss文件 $color:purple; #two&#123; border:1px solid $color; &#125; 安装对应的loader 1cnpm install sass-loader css-loader style-loader node-sass webpack --save-dev 在webpack.config.js中配置相关的loader 1234567891011121314151617181920212223242526272829const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径// const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错// 常量存储的是一个不可以变化的变量。// module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, './dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css\$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125; ] &#125; 在js目录中 main.js里面引入 scss1.scss 1234567891011121314151617// 1、获取index.html中的dom对象var first = document.getElementById('first');var btn = document.getElementById('btn');var two = document.getElementById('two');var res = document.getElementById('res');btn.onclick = function()&#123; var firstValue = parseFloat(first.value); var twoValue = parseFloat(two.value); //2、获取 module1.js中的 sum函数 //http://www.ruanyifeng.com/blog/2015/05/require.html var sum = require('./module1.js'); res.value = sum(firstValue,twoValue);&#125;// 3、获取css目录中的main.css文件require('../css/main.css');// 4、获取sass目录中的scss1.scss文件require('../sass/scss1.scss'); 在终端中输入 webpack命令进行scss文件打包 6、实现打包url资源（图片、gif、图标等）功能在src 目录中 新建imgs目录，放入两张不同大小的图片 在index.html中新增 &lt; div id=”bg1” &gt;&lt; /div &gt; &lt; div id=”bg2” &gt;&lt; /div &gt; 在mian.css中新增 // mian.css文件 12345#bg1&#123; width: 200px; height: 200px; background: url('../imgs/bxg.jpg');&#125; 12345#bg2&#123; width: 200px; height: 200px; background: url('../imgs/web.jpg') no-repeat;&#125; 如果我们希望在页面引入图片（包括img的src和background的url）。当我们基于webpack进行开发时，引入图片会遇到一些问题。 其中一个就是引用路径的问题。拿background样式用url引入背景图来说，我们都知道，webpack最终会将各个模块打包成一个文件，因此我们样式中的url路径是相对入口html页面的，而不是相对于原始css文件所在的路径的。这就会导致图片引入失败。这个问题是用file-loader解决的，file-loader可以解析项目中的url引入（不仅限于css），根据我们的配置，将图片拷贝到相应的路径，再根据我们的配置，修改打包后文件引用路径，使之指向正确的文件。 另外，如果图片较多，会发很多http请求，会降低页面性能。这个问题可以通过url-loader解决。url-loader会将引入的图片编码，生成dataURl。相当于把图片数据翻译成一串字符。再把这串字符打包到文件中，最终只需要引入这个文件就能访问图片了。当然，如果图片较大，编码会消耗性能。因此url-loader提供了一个limit参数，小于limit字节的文件会被转为DataURl，大于limit的还会使用file-loader进行copy。 url-loader和file-loader是什么关系呢？简答地说，url-loader封装了file-loader。url-loader不依赖于file-loader，即使用url-loader时，只需要安装url-loader即可，不需要安装file-loader，因为url-loader内置了file-loader。通过上面的介绍，我们可以看到，url-loader工作分两种情况：1.文件大小小于limit参数，url-loader将会把文件转为DataURL；2.文件大小大于limit，url-loader会调用file-loader进行处理，参数也会直接传给file-loader。因此我们只需要安装url-loader即可。 安装 1cnpm install url-loader file-loader --save-dev 在webpack.config.js中配置相关的loader 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径// const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错// 常量存储的是一个不可以变化的变量。// module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, './dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 publicPath:'dist/' // path:所有输出文件的目标路径; // publicPath:输出解析文件的目录，url 相对于 HTML 页面 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css\$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss\$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less\$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片和字体文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf|eot|woff|woff2|svg)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式（为什么呢？因为一个很小的图片，不值当的去发送一个请求，减少请求次数; 其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为dataurl形式） &#125; &#125; ] //保证输出的图片名称与之前命名的图片名称保持一致(目前只是支持这样的写法，webpack3 没有响应的options进行配置) // use:'url-loader?limit=8192&amp;name=imgs/[name].[ext]' &#125; ] &#125;&#125; 在main.js中引入mui目录中icons-extra.css的文件 12// 6、获取src目录中的mui目录中icons-extra.css的文件require('../mui/css/icons-extra.css'); 7、Webpack-dev-server结合后端服务器的热替换配置webpack-dev-server提供了一个简单的 web 服务器，并且能够实时重新加载(live reloading)，同时把生成好的js和html构建到我们的电脑内存中，这样的话，即使我们的目录中没有了相关js等文件，还能够加载出来，这样能够提高我们页面运行速度。 安装 webpack-dev-server 插件 1cnpm install webpack-dev-server --save-dev 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// webpack.config.js const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径 // const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错 // 常量存储的是一个不可以变化的变量。 module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125; ] &#125; ] &#125;, devServer: &#123; // contentBase: './dist', // 在 localhost:8080(默认) 下建立服务，将 dist 目录下的文件，作为可访问文件contentBase：告诉服务器从哪里提供内容 // 或者通过以下方式配置 contentBase: path.join(__dirname, "dist"), compress: true, // 当它被设置为true的时候对所有的服务器资源采用gzip压缩 // 对JS，CSS资源的压缩率很高，可以极大得提高文件传输的速率，从而提升web性能 port: 9000, // 如果想要改端口，可以通过 port更改 hot: true, // 启用 webpack 的模块热替换特性() inline: true, // 实现实时重载（实现自动刷新功能）默认情况下是 true。 host: "localhost" // 如果你希望服务器外部可访问，指定使用一个 host。默认是 localhost(也就是你可以不写这个host这个配置属性)。 &#125;&#125; 123456789101112131415161718192021222324252627// package.json&#123; "name": "mywebpack", "version": "1.0.0", "description": "", "main": "webpack.config.js", "scripts": &#123; "test": "echo \"Error: no test specified\" &amp;&amp; exit 1", "start": "webpack-dev-server --open" // "start": "webpack-dev-server --open --port 8080 --hot --inline" // 如果在这里配置了，就不用在webpack.config.js中配置devServer属性了。 &#125;, "author": "", "license": "ISC", "devDependencies": &#123; "css-loader": "^0.28.7", "file-loader": "^1.1.5", "html-webpack-plugin": "^2.30.1", "less": "^3.0.0-alpha.3", "less-loader": "^4.0.5", "node-sass": "^4.5.3", "sass-loader": "^6.0.6", "style-loader": "^0.19.0", "url-loader": "^0.6.2", "webpack": "^3.8.1", "webpack-dev-server": "^2.9.3" &#125;&#125; 在命令行中运行 npm start，就会看到浏览器自动加载页面。如果现在修改和保存任意源文件，web 服务器就会自动重新加载编译后的代码，但是打开后发现，打开的是 dist目录，我们想要的是 index.html显示我们的页面，所以这是我们还要借助里另一个 html-webpack-plugin 插件。 8、html-webpack-plugin 插件安装html-webpack-plugin 简单创建 HTML 文件，用于服务器访问，其中包括使用script标签的body中的所有webpack包。 安装 html-webpack-plugin 插件 1cnpm install --save-dev html-webpack-plugin webpack.config.js配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// webpack.config.js const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径 // const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错 // 常量存储的是一个不可以变化的变量。 module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125; ] &#125; ] &#125;, devServer: &#123; // contentBase: './dist', // 在 localhost:8080(默认) 下建立服务，将 dist 目录下的文件，作为可访问文件contentBase：告诉服务器从哪里提供内容 // 或者通过以下方式配置 contentBase: path.join(__dirname, "dist"), compress: true, // 当它被设置为true的时候对所有的服务器资源采用gzip压缩 // 对JS，CSS资源的压缩率很高，可以极大得提高文件传输的速率，从而提升web性能 port: 9000, // 如果想要改端口，可以通过 port更改 hot: true, // 启用 webpack 的模块热替换特性() inline: true, // 实现实时重载（实现自动刷新功能）默认情况下是 true。 host: "localhost" // 如果你希望服务器外部可访问，指定使用一个 host。默认是 localhost(也就是你可以不写这个host这个配置属性)。 &#125; plugins: [ new HtmlWebpackPlugin(&#123; title: '首页', // 用于生成的HTML文档的标题 filename: 'index.html', //写入HTML的文件。默认为index.html。也可以指定一个子目录（例如：）assets/admin.html template: 'index.html' // Webpack需要模板的路径 &#125;), new webpack.HotModuleReplacementPlugin() // 需要结合 启用热替换模块(Hot Module Replacement)，也被称为 HMR ]&#125; 再次使用npm start命令就可以实现浏览器自动更新。 问题来了，HtmlWebpackPlugin中的 title并没有显示出来，原因是需要在定义的template模板中使用ejs语法， 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt; &lt;!-- EJS 语法 /* EJS是一个简单高效的模板语言，通过数据和模板，可以生成HTML标记文本。可以说EJS是一个JavaScript库，EJS可以同时运行在客户端和服务器端，客户端安装直接引入文件即可 */ --&gt; &lt;title&gt;&lt;%= htmlWebpackPlugin.options.title%&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;input type="text" id="first"&gt; &lt;input type="button" id="btn" value="+"&gt; &lt;input type="text" id="two"&gt; &lt;input type="button" id="btn" value="="&gt; &lt;input type="text" id="res"&gt; &lt;div id="bg1"&gt;&lt;/div&gt; &lt;div id="bg2"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 9、ES6转换为ES5语法安装 1cnpm install --save-dev babel-loader babel-core babel-preset-env babel-core 如果某些代码需要调用Babel的API进行转码，就要使用babel-core模块babel-preset-env 通过根据您的目标浏览器或运行时环境自动确定您需要的Babel插件babel 对一些公共方法使用了非常小的辅助代码，比如 _extend。 默认情况下会被添加到每一个需要它的文件中,你可以引入 babel runtime 作为一个独立模块，来避免重复引入。 你必须执行 npm install babel-plugin-transform-runtime –save-dev 来把它包含到你的项目中，也要使用 npm install babel-runtime –save 把 babel-runtime 安装为一个依赖 配置 1234567891011121314151617181920212223242526// 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [&#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125;] &#125;, // 实现 ES6转 ES5 &#123; test: /\.js$/, exclude: /(node_modules)/, // exclude 排除的意思，把 node_modules文件夹排除编译之外 use: &#123; loader: 'babel-loader', options: &#123; // presets 预设列表（一组插件）加载和使用 presets: ['env'], plugins: ['transform-runtime'] // 加载和使用的插件列表 &#125; &#125; &#125; 把一些代码改成ES6 语法的写法 1234567891011// moudule1.jsfunction sum(x,y)&#123; return x + y;&#125;// 导出 sum 函数// module.exports = sum;// 改成ES6 的写法语法export default&#123; sum&#125; 12345678910111213141516171819202122232425262728293031323334353637// main.js// 1、获取index.html中的dom对象var first = document.getElementById('first');var btn1 = document.getElementById('btn1');var two = document.getElementById('two');var res = document.getElementById('res');console.log(1);btn1.onclick = function() &#123; var firstValue = parseFloat(first.value); var twoValue = parseFloat(two.value); //2、获取 module1.js中的 sum函数 //http://www.ruanyifeng.com/blog/2015/05/require.html console.log(2); /* var sum = require('./module1.js'); res.value = sum(firstValue,twoValue);*/ res.value = sumObj.sum(firstValue, twoValue);&#125;// 3、获取css目录中的main.css文件// require('../css/main.css');// 把步骤3 改为 ES6写法,引入css目录中的main.css文件import '../css/main.css';// 4、获取sass目录中的scss1.scss文件require('../sass/scss1.scss');// 5、获取less目录中的less1.less文件require('../less/less1.less');// 6、获取src目录中的mui目录中icons-extra.css的文件require('../mui/css/icons-extra.css');// 把 var sum = require('./module1.js'); 写成 ES6语法import sumObj from './module1.js' 10、防止文件缓存（生成带有20位的hash值的唯一文件）1234567// webpack.config.jsoutput: &#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 // filename: 'bulid.js' // 制定出口文件的名称 filename: '[name].[hash].js' // 将入口文件重命名为带有20位的hash值的唯一文件 &#125; 11、抽取CSS为单独文件安装插件从 build.js文件中提取文本（CSS）到单独的文件 1npm install --save-dev extract-text-webpack-plugin 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// webpack.config.js const path = require('path'); // 首先要引入node.js中path 模块，用于处理文件与目录的路径 // const 命令声明一个只读的常量，一旦声明，值不可以改变，改变会报错；只声明不赋值也会报错 // 常量存储的是一个不可以变化的变量。 module.exports = &#123; entry:'./src/./js/main.js', // 指定入口文件 output:&#123; path: path.resolve(__dirname, 'dist/js'), // 指定出口文件的路径目录 filename: 'bulid.js' // 制定出口文件的名称 &#125;, module:&#123; rules:[ // 在webpack2中，loaders 被替换成了 rules 其实就是loader的规则 // 实现 css 打包 &#123; test: /\.css$/, use: [ 'style-loader', 'css-loader' ] // test 说明了当前 loader 能处理那些类型的文件 // use 则指定了 loader 的类型。 // 注意：数组中的loader不能省略扩展名 &#125;, // 实现 scss 打包 &#123; test: /\.scss$/, // 注意 是sass-loader ，不是 scss-loader use: [ 'style-loader', 'css-loader', 'sass-loader' ] &#125;, // 实现 less 打包 &#123; test: /\.less$/, use: [ 'style-loader', 'css-loader', 'less-loader' ] &#125;, // 实现 url 资源打包 &#123; // 图片文件使用 url-loader 来处理 test: /\.(png|jpg|gif|ttf)$/, use: [ &#123; loader: 'url-loader', // options 为可以配置的选项 options: &#123; limit: 8192 // limit=8192表示将所有小于8kb的图片都转为base64形式 // （其实应该说超过8kb的才使用 url-loader 来映射到文件，否则转为data url形式） &#125; &#125;, &#123; test: /\.css$/, use: ExtractTextPlugin.extract(&#123; fallback: "style-loader", use: "css-loader" &#125;) &#125; ] &#125; ] &#125;, devServer: &#123; // contentBase: './dist', // 在 localhost:8080(默认) 下建立服务，将 dist 目录下的文件，作为可访问文件contentBase：告诉服务器从哪里提供内容 // 或者通过以下方式配置 contentBase: path.join(__dirname, "dist"), compress: true, // 当它被设置为true的时候对所有的服务器资源采用gzip压缩 // 对JS，CSS资源的压缩率很高，可以极大得提高文件传输的速率，从而提升web性能 port: 9000, // 如果想要改端口，可以通过 port更改 hot: true, // 启用 webpack 的模块热替换特性() inline: true, // 实现实时重载（实现自动刷新功能）默认情况下是 true。 host: "localhost" // 如果你希望服务器外部可访问，指定使用一个 host。默认是 localhost(也就是你可以不写这个host这个配置属性)。 &#125; plugins: [ new HtmlWebpackPlugin(&#123; title: '首页', // 用于生成的HTML文档的标题 filename: 'index.html', //写入HTML的文件。默认为index.html。也可以指定一个子目录（例如：）assets/admin.html template: 'index.html' // Webpack需要模板的路径 &#125;), new webpack.HotModuleReplacementPlugin() // 需要结合 启用热替换模块(Hot Module Replacement)，也被称为 HMR ]&#125; 12、开发环境和生产环境的分离（1）开发环境与生产环境分离的原因如下：在开发环境中，我们使用热更新插件帮助我们实现浏览器的自动更新功能，我们的代码没有进行压缩，如果压缩了不方便我们调试代码等等，所以以上这些代码不应出现在生产环境中。生产环境中，我们把项目部署到服务器时，我们会对代码进行各种各样的优化，比如压缩代码等等，这时候我们不应该把这些代码放到开发环境中，不利于代码开发和调试。 总结：针对以上这些说明，我们很有必要把区分开发环境与生产环境分离。 （2）开发环境的配置和生产换环境配置的区别。开发环境有的配置，生产环境不一定有，比如说热更新时使用到的HotModuleReplacementPlugin。 生产环境有的配置，开发环境不一定有，比如说用来压缩js用的UglifyJsPlugin。 （3）如何是开开发和生产分离？ 1&gt; 因为webpack 默认找的是 webpack.config.js配置文件，所以要把开发环境的webpack.config.js配置文件改为webpack.dev.config.js代表开发环境的配置文件。 2&gt; 新建一个webpack.prod.config.js，再把开发环境中的webpack.config.js复制进去（没用的配置文件该删除的删除） 3&gt; 修改package.json文件（在scripts 标签中添加”dev”和”prod” 属性配置） 123456js "scripts": &#123; "test": "echo \"Error: no test specified\" &amp;&amp; exit 1", "dev": "webpack --config webpack.dev.config.js", "prod": "webpack --config webpack.prod.config.js" &#125;, 执行构建命令 1234# 执行开发环境的中配置 npm run dev# 执行生产环境的中配置 npm run prod 13、在生产环境中配置代码压缩功能配置webpack.prod.config.js 文件 12345678910111213// webpack.prod.config.jsvar UglifyJsPlugin = webpack.optimize.UglifyJsPlugin; // …… plugins: [ // …… // js代码 压缩 new UglifyJsPlugin(&#123; compress: &#123; warnings: false &#125; &#125;)] 执行 npm run prod 命令]]></content>
      <categories>
        <category>web前端</category>
        <category>VUE</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>VUE</tag>
        <tag>webpack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue-cli 使用]]></title>
    <url>%2F2019%2F07%2F15%2F03.web%E5%89%8D%E7%AB%AF%2FVUE%2Fvue-cli%20%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载自: https://www.cnblogs.com/m18320364494/p/9560522.html 一、安装vue-cli安装vue-cli的前提是你已经安装了npm，安装npm你可以直接下载node的安装包进行安装。你可以在命令行工具里输入npm -v 检测你是否安装了npm和版本情况。出现版本号说明你已经安装了npm和node，我这里的npm版本为3.10.10。如果该命令不可以使用，需要安装node软件包，根据你的系统版本选择下载安装就可以了。 npm没有问题，接下来我们可以用npm 命令安装vue-cli了，在命令行输入下面的命令： 1npm install vue-cli -g -g :代表全局安装。如果你安装时报错，一般是网络问题，你可以尝试用cnpm来进行安装。安装完成后，可以用vue -V来进行查看 vue-cli的版本号。注意这里的V是大写的。我这里版本号是2.8.1. 如果vue -V的命令管用了，说明已经顺利的把vue-cli安装到我们的计算机里了。 二、初始化项目我们用vue init命令来初始化项目，具体看一下这条命令的使用方法。 1vue init &lt;template-name&gt; &lt;project-name&gt; init：表示我要用vue-cli来初始化项目 template-name：表示模板名称，vue-cli官方为我们提供了5种模板， webpack: 一个全面的webpack+vue-loader的模板，功能包括热加载，linting,检测和CSS扩展。 webpack-simple: 一个简单webpack+vue-loader的模板，不包含其他功能，让你快速的搭建vue的开发环境。 browserify: 一个全面的Browserify+vueify 的模板，功能包括热加载，linting,单元检测。 browserify-simple: 一个简单Browserify+vueify的模板，不包含其他功能，让你快速的搭建vue的开发环境。 simple: 一个最简单的单页应用模板。 project-name: 标识项目名称，这个你可以根据自己的项目来起名字。 在实际开发中，一般我们都会使用webpack这个模板，那我们这里也安装这个模板，在命令行输入以下命令： 1vue init webpack vuecliTest 输入命令后，会询问我们几个简单的选项，我们根据自己的需要进行填写就可以了。 Project name :项目名称 ，如果不需要更改直接回车就可以了。注意：这里不能使用大写，所以我把名称改成了vueclitest Project description:项目描述，默认为A Vue.js project,直接回车，不用编写。 Author：作者，如果你有配置git的作者，他会读取。 Install vue-router? 是否安装vue的路由插件，我们这里需要安装，所以选择Y Use ESLint to lint your code? 是否用ESLint来限制你的代码错误和风格。我们这里不需要输入n，如果你是大型团队开发，最好是进行配置。 setup unit tests with Karma + Mocha? 是否需要安装单元测试工具Karma+Mocha，我们这里不需要，所以输入n。 Setup e2e tests with Nightwatch?是否安装e2e来进行用户行为模拟测试，我们这里不需要，所以输入n。 命令行出现上面的文字，说明我们已经初始化好了第一步。命令行提示我们现在可以作的三件事情。 1、cd vuecliTest 进入我们的vue项目目录。 2、npm install 安装我们的项目依赖包，也就是安装package.json里的包，如果你网速不好，你也可以使用cnpm来安装。 3、npm run dev 开发模式下运行我们的程序（实际上是执行了package.json 中的script脚本）。给我们自动构建了开发用的服务器环境和在浏览器中打开，并实时监视我们的代码更改，即时呈现给我们。 三、Vue-cli项目结构讲解vue-cli脚手架工具就是为我们搭建了开发所需要的环境，为我们省去了很多精力。有必要对这个环境进行熟悉，我们就从项目的结构讲起。 Ps：由于版本实时更新和你选择安装的不同（这里列出的是模板为webpack的目录结构），所以你看到的有可能和下边的有所差别。 123456789101112131415161718192021222324252627282930.|-- build // 项目构建(webpack)相关代码| |-- build.js // 生产环境构建代码| |-- check-version.js // 检查node、npm等版本| |-- dev-client.js // 热重载相关| |-- dev-server.js // 构建本地服务器| |-- utils.js // 构建工具相关| |-- webpack.base.conf.js // webpack基础配置| |-- webpack.dev.conf.js // webpack开发环境配置| |-- webpack.prod.conf.js // webpack生产环境配置|-- config // 项目开发环境配置| |-- dev.env.js // 开发环境变量| |-- index.js // 项目一些配置变量| |-- prod.env.js // 生产环境变量| |-- test.env.js // 测试环境变量|-- src // 源码目录| |-- components // vue公共组件| |-- store // vuex的状态管理| |-- App.vue // 页面入口文件| |-- main.js // 程序入口文件，加载各种公共组件|-- static // 静态文件，比如一些图片，json数据等| |-- data // 群聊分析得到的数据用于数据可视化|-- .babelrc // ES6语法编译配置|-- .editorconfig // 定义代码格式|-- .gitignore // git上传需要忽略的文件格式|-- README.md // 项目说明|-- favicon.ico |-- index.html // 入口页面|-- package.json // 项目基本信息. 重要文件讲解： package.jsonpackage.json文件是项目根目录下的一个文件，定义该项目开发所需要的各种模块以及一些项目配置信息（如项目名称、版本、描述、作者等）。 package.json 里的scripts字段，这个字段定义了你可以用npm运行的命令。在开发环境下，在命令行工具中运行npm run dev 就相当于执行 node build/dev-server.js .也就是开启了一个node写的开发行建议服务器。由此可以看出script字段是用来指定npm相关命令的缩写。 1234"scripts": &#123; "dev": "node build/dev-server.js", "build": "node build/build.js"&#125;, dependencies字段和devDependencies字段 dependencies字段指项目运行时所依赖的模块； devDependencies字段指定了项目开发时所依赖的模块； 在命令行中运行npm install命令，会自动安装dependencies和devDempendencies字段中的模块。package.json还有很多相关配置，如果你想全面了解，可以专门去百度学习一下。 webpack配置相关我们在上面说了运行npm run dev 就相当于执行了node build/dev-server.js,说明这个文件相当重要，先来熟悉一下它。 我贴出代码并给出重要的解释。 dev-server.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 检查 Node 和 npm 版本require('./check-versions')()// 获取 config/index.js 的默认配置var config = require('../config')// 如果 Node 的环境无法判断当前是 dev / product 环境// 使用 config.dev.env.NODE_ENV 作为当前的环境if (!process.env.NODE_ENV) process.env.NODE_ENV = JSON.parse(config.dev.env.NODE_ENV)// 使用 NodeJS 自带的文件路径工具var path = require('path')// 使用 expressvar express = require('express')// 使用 webpackvar webpack = require('webpack')// 一个可以强制打开浏览器并跳转到指定 url 的插件var opn = require('opn')// 使用 proxyTablevar proxyMiddleware = require('http-proxy-middleware')// 使用 dev 环境的 webpack 配置var webpackConfig = require('./webpack.dev.conf')// default port where dev server listens for incoming traffic// 如果没有指定运行端口，使用 config.dev.port 作为运行端口var port = process.env.PORT || config.dev.port// Define HTTP proxies to your custom API backend// https://github.com/chimurai/http-proxy-middleware// 使用 config.dev.proxyTable 的配置作为 proxyTable 的代理配置var proxyTable = config.dev.proxyTable// 使用 express 启动一个服务var app = express()// 启动 webpack 进行编译var compiler = webpack(webpackConfig)// 启动 webpack-dev-middleware，将 编译后的文件暂存到内存中var devMiddleware = require('webpack-dev-middleware')(compiler, &#123; publicPath: webpackConfig.output.publicPath, stats: &#123; colors: true, chunks: false &#125;&#125;)// 启动 webpack-hot-middleware，也就是我们常说的 Hot-reloadvar hotMiddleware = require('webpack-hot-middleware')(compiler)// force page reload when html-webpack-plugin template changescompiler.plugin('compilation', function (compilation) &#123; compilation.plugin('html-webpack-plugin-after-emit', function (data, cb) &#123; hotMiddleware.publish(&#123; action: 'reload' &#125;) cb() &#125;)&#125;)// proxy api requests// 将 proxyTable 中的请求配置挂在到启动的 express 服务上Object.keys(proxyTable).forEach(function (context) &#123; var options = proxyTable[context] if (typeof options === 'string') &#123; options = &#123; target: options &#125; &#125; app.use(proxyMiddleware(context, options))&#125;)// handle fallback for HTML5 history API// 使用 connect-history-api-fallback 匹配资源，如果不匹配就可以重定向到指定地址app.use(require('connect-history-api-fallback')())// serve webpack bundle output// 将暂存到内存中的 webpack 编译后的文件挂在到 express 服务上app.use(devMiddleware)// enable hot-reload and state-preserving// compilation error display// 将 Hot-reload 挂在到 express 服务上app.use(hotMiddleware)// serve pure static assets// 拼接 static 文件夹的静态资源路径var staticPath = path.posix.join(config.dev.assetsPublicPath, config.dev.assetsSubDirectory)// 为静态资源提供响应服务app.use(staticPath, express.static('./static'))// 让我们这个 express 服务监听 port 的请求，并且将此服务作为 dev-server.js 的接口暴露module.exports = app.listen(port, function (err) &#123; if (err) &#123; console.log(err) return &#125; var uri = 'http://localhost:' + port console.log('Listening at ' + uri + '\n') // when env is testing, don't need open it // 如果不是测试环境，自动打开浏览器并跳到我们的开发地址 if (process.env.NODE_ENV !== 'testing') &#123; opn(uri) &#125;&#125;) webpack.base.confg.js webpack的基础配置文件123456789101112131415161718192021222324252627282930......module.export = &#123; // 编译入口文件 entry: &#123;&#125;, // 编译输出路径 output: &#123;&#125;, // 一些解决方案配置 resolve: &#123;&#125;, resolveLoader: &#123;&#125;, module: &#123; // 各种不同类型文件加载器配置 loaders: &#123; ... ... // js文件用babel转码 &#123; test: /\.js$/, loader: 'babel', include: projectRoot, // 哪些文件不需要转码 exclude: /node_modules/ &#125;, ... ... &#125; &#125;, // vue文件一些相关配置 vue: &#123;&#125;&#125; .babelrcBabel解释器的配置文件，存放在根目录下。Babel是一个转码器，项目里需要用它将ES6代码转为ES5代码。如果你想了解更多，可以查看babel的知识。 1234567891011121314151617&#123; //设定转码规则 "presets": [ ["env", &#123; "modules": false &#125;], "stage-2" ], //转码用的插件 "plugins": ["transform-runtime"], "comments": false, //对BABEL_ENV或者NODE_ENV指定的不同的环境变量，进行不同的编译操作 "env": &#123; "test": &#123; "presets": ["env", "stage-2"], "plugins": [ "istanbul" ] &#125; &#125;&#125; .editorconfig该文件定义项目的编码规范，编译器的行为会与.editorconfig文件中定义的一致，并且其优先级比编译器自身的设置要高，这在多人合作开发项目时十分有用而且必要。 123456789root = true[*] // 对所有文件应用下面的规则charset = utf-8 // 编码规则用utf-8indent_style = space // 缩进用空格indent_size = 2 // 缩进数量为2个空格end_of_line = lf // 换行符格式insert_final_newline = true // 是否在文件的最后插入一个空行trim_trailing_whitespace = true // 是否删除行尾的空格 这是比较重要的关于vue-cli的配置文件，当然还有很多文件，我们会在以后的文章中讲解。 四、Vue-cli的模板1、npm run build 命令有小伙伴问我，如何把写好的Vue网页放到服务器上，那我就在这里讲解一下，主要的命令就是要用到npm run build 命令。我们在命令行中输入npm run build命令后，vue-cli会自动进行项目发布打包。你在package.json文件的scripts字段中可以看出，你执行的npm run build命令就相对执行的 node build/build.js 。 package.json的scripts 字段：1234"scripts": &#123; "dev": "node build/dev-server.js", "build": "node build/build.js"&#125;, 在执行完npm run build命令后，在你的项目根目录生成了dist文件夹，这个文件夹里边就是我们要传到服务器上的文件。 dist文件夹下目录包括： index.html 主页文件:因为我们开发的是单页web应用，所以说一般只有一个html文件。 static 静态资源文件夹：里边js、CSS和一些图片。 2、main.js文件解读main.js是整个项目的入口文件,在src文件夹下： 12345678910111213import Vue from 'vue' import App from './App'import router from './router'Vue.config.productionTip = false //生产环境提示，这里设置成了false/* eslint-disable no-new */new Vue(&#123; el: '#app', router, template: '&lt;App/&gt;', components: &#123; App &#125;&#125;) 通过代码可以看出这里引进了App的组件和’&lt; APP /&gt;’的模板，它是通过 import App from ‘./App’这句代码引入的。 我们找到App.vue文件，打开查看。 3、App.vue文件:1234567891011121314151617181920212223&lt;template&gt; &lt;div id=&quot;app&quot;&gt; &lt;img src=&quot;./assets/logo.png&quot;&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &apos;app&apos;&#125;&lt;/script&gt;&lt;style&gt;#app &#123; font-family: &apos;Avenir&apos;, Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; color: #2c3e50; margin-top: 60px;&#125;&lt;/style&gt; app.vue文件我们可以分成三部分解读， &lt; template &gt;&lt; /template &gt;标签包裹的内容：这是模板的HTMLDom结构，里边引入了一张图片和&lt; router-view &gt;&lt; /router-view &gt;标签，&lt; router-view &gt;标签说明使用了路由机制。我们会在以后专门拿出一篇文章讲Vue-router。 &lt; script &gt;&lt; /script &gt;标签包括的js内容：你可以在这里些一些页面的动态效果和Vue的逻辑代码。 &lt; style&gt;&lt; /style &gt;标签包裹的css内容：这里就是你平时写的CSS样式，对页面样子进行装饰用的，需要特别说明的是你可以用&lt; style scoped &gt;&lt; /style &gt;来声明这些css样式只在本模板中起作用。 4、router/index.js 路由文件引文在app.vue中我们看到了路由文件，虽然router的内容比较多，但是我们先简单的看一下。 123456789101112131415import Vue from 'vue'import Router from 'vue-router'import Hello from '@/components/Hello'Vue.use(Router)export default new Router(&#123; routes: [ &#123; path: '/', name: 'Hello', component: Hello &#125; ]&#125;) 我们可以看到 import Hello from ‘@/components/Hello’这句话， 文件引入了/components/Hello.vue文件。这个文件里就配置了一个路由，就是当我们访问网站时给我们显示Hello.vue的内容。 5、Hello.vue文件解读：这个文件就是我们在第一节课看到的页面文件了。也是分为&lt; template &gt;&lt; script &gt;&lt; style &gt;三个部分，以后我们大部分的工作都是写这些.vue结尾的文件。现在我们可以试着改一些内容，然后预览一下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;template&gt; &lt;div class=&quot;hello&quot;&gt; &lt;h1&gt;&#123;&#123; msg &#125;&#125;&lt;/h1&gt; &lt;h2&gt;Essential Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://vuejs.org&quot; target=&quot;_blank&quot;&gt;Core Docs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://forum.vuejs.org&quot; target=&quot;_blank&quot;&gt;Forum&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://gitter.im/vuejs/vue&quot; target=&quot;_blank&quot;&gt;Gitter Chat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://twitter.com/vuejs&quot; target=&quot;_blank&quot;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &lt;br&gt; &lt;li&gt;&lt;a href=&quot;http://vuejs-templates.github.io/webpack/&quot; target=&quot;_blank&quot;&gt;Docs for This Template&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Ecosystem&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://router.vuejs.org/&quot; target=&quot;_blank&quot;&gt;vue-router&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vuex.vuejs.org/&quot; target=&quot;_blank&quot;&gt;vuex&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://vue-loader.vuejs.org/&quot; target=&quot;_blank&quot;&gt;vue-loader&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/vuejs/awesome-vue&quot; target=&quot;_blank&quot;&gt;awesome-vue&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &apos;hello&apos;, data () &#123; return &#123; msg: &apos;Welcome to Your Vue.js App&apos; &#125; &#125;&#125;&lt;/script&gt;&lt;!-- Add &quot;scoped&quot; attribute to limit CSS to this component only --&gt;&lt;style scoped&gt;h1, h2 &#123; font-weight: normal;&#125;ul &#123; list-style-type: none; padding: 0;&#125;li &#123; display: inline-block; margin: 0 10px;&#125;a &#123; color: #42b983;&#125;&lt;/style&gt;]]></content>
      <categories>
        <category>web前端</category>
        <category>VUE</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>VUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue 基础语法]]></title>
    <url>%2F2019%2F07%2F10%2F03.web%E5%89%8D%E7%AB%AF%2FVUE%2Fvue%20%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. vue的基础语法介绍1-1基本数据绑定12345678910&lt;div id="app"&gt; &#123;&#123; msg &#125;&#125;&lt;/div&gt;//scriptnew Vue(&#123; el:"#app",//代表vue的范围 data:&#123; msg:'hello Vue' //数据 &#125;&#125;) 在这个例子中我们可以进行赋值 123var app = new Vue(...);app.msg = '初探vue';//那么页面的值就被改变了 1-2 v-html命令12345678910&lt;div id="app" v-html="msg"&gt; &lt;/div&gt;//scriptnew Vue(&#123; el:"#app",//代表vue的范围 data:&#123; msg:'&lt;h1&gt;你好，世界&lt;/h1&gt;' //这里就不会是文本了 而是会被当成是html标签了 &#125;&#125;) 1-3 v-on:click||@click指令12345678910111213141516171819202122232425262728293031&lt;div id="app"&gt;&lt;button v-on:clikc="clickHead"&gt;事件&lt;/button&gt;&lt;button @click="clickHead"&gt;事件&lt;/button&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", methods:&#123; clickHead:functoin()&#123; alert('vue的事件绑定') &#125; &#125;&#125;)//在es6语法中对象中的函数可以const json=&#123; clickHead()&#123; //do something &#125;&#125;json.clickHead()调用方法//和一样的const json=&#123; clickHead:function()&#123; //do something &#125;&#125; 1-4 v-bind 属性绑定指令 例如绑定class 和id 已经已经存在的属性 和自定义属性 绑定类名 12345678910111213141516171819202122&lt;div id="app"&gt; &lt;p v-bind:class="className"&gt;&#123;&#123;msg&#125;&#125;&lt;/p&gt; &lt;/div&gt;/*v-bind 自定义名字v-bind:id="..." 绑定id名字v-bind:title="..."绑定title属性v-bind:style="..." 绑定样式属性 v-bind:...="..."绑定自定义属性、、、*///jsnew Vue(&#123; el:"#app", data:&#123; msg:'这是v-bind绑定数据', className:'contatiner' &#125;,&#125;)const Name = document.querySelector('.contatiner');console.log(Name) //能正常的获取这个元素 1-5 v-show 根据值的真假 控制元素的display的属性 12345678910111213&lt;div id="app"&gt; &lt;p v-show="msg"&gt; 可以看到啊 &lt;/p&gt; &lt;p v-show="msg1"&gt; 不可以看到啊 &lt;/p&gt; &lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:1+1==2, msg1:1+1!=2 &#125;&#125;) 1-6 v-text 赋予文本值 1234567891011&lt;div id="app"&gt; &lt;p v-text="msg"&gt; 可以看到啊 &lt;/p&gt; &lt;!-- 最终会被替换掉 1+1==2 --&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:'你好哈 v-text' &#125;&#125;) 1-7 v-for 循环 123456789101112131415161718 &lt;div id="app"&gt; &lt;ol&gt; &lt;li v-for="module in modules"&gt;&#123;&#123;module.msg&#125;&#125;&lt;/li&gt; &lt;/ol&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; modules:[ &#123;msg:'第一个'&#125;, &#123;msg:'第二个'&#125;, &#123;msg:'第三个'&#125;, &#123;msg:'第四个'&#125; ] &#125;&#125;) 1-8 v-model 双向数据绑定123456789101112&lt;div id="app"&gt; &lt;input type="text" v-model="msg"&gt; &lt;p&gt;&#123;&#123;msg&#125;&#125;&lt;/p&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:'' &#125;&#125;) 1-9 template属性123456789101112&lt;div id="app"&gt; &lt;span&gt;你好啊&lt;/span&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; msg:'这是数据' &#125;, template:"&lt;div&gt;模板div&lt;/div&gt;"&#125;) 最终的效果是把id为app的div直接替换成template里面的元素 注意在template的值中不能含有兄弟根节点 1234new Vue(&#123; el:"#app", template:"&lt;div&gt;1&lt;div&gt;&lt;div&gt;2&lt;div&gt;"&#125;) 这样是错误的 , 在template 可以把团苏片段放在script标签内 12345678910111213141516171819&lt;div id="app"&gt; &lt;span&gt;你好啊&lt;/span&gt; &lt;/div&gt; &lt;script type="x-template" id="temp"&gt; //这个地方就是模板片段 &lt;div id="tpl"&gt; &lt;p&gt; 这是模板啊,&#123;&#123;msg&#125;&#125; &lt;/p&gt; &lt;input type="text" v-model="msg" /&gt; &lt;/div&gt; &lt;/script&gt; &lt;script src="js/vue.min.js"&gt;&lt;/script&gt; &lt;script&gt; new Vue(&#123; el:"#app", data:&#123; msg:'' &#125;, template:"#temp" &#125;) &lt;/script&gt; 2. vue实例中的属性2-1 el el表示在vue的实例中的作用范围 123new Vue(&#123; el:"#app" //作用在id名为app的div上&#125;) 2-2 data data的属性就是数据绑定 12345678new Vue(&#123; data:&#123; msg:'数据的值' arrayData:[ &#123;title:'这是1'&#125; ] &#125;&#125;) 2-3 methods methods绑定事件 12345678new Vue(&#123; el:"#app", methods:&#123; mouseClick()&#123; alert('绑定事件') &#125; &#125;&#125;) 2-3 template template模板的绑定 1234new Vue(&#123; el:"#app", template:'&lt;div&gt;这是模板属性&lt;/div&gt;'&#125;) 2-4 render render模板的绑定 1234567891011121314151617181920212223242526new Vue(&#123; el:"#app", render(createElement)&#123; return createElement( "ol", &#123; //新创建的元素身上绑定属性 style:&#123; fontSize:'30px', border:'1px solid red', fontWeight:'bold' &#125;, attrs:&#123; title:'你好啊', coundNum:'01', id:'ls', class:'bg' &#125; &#125;, [ createElement("li",'这是第一个文本'), createElement("li",'这是第二个文本'), createElement("li",'这是第三个文本'), ] ) &#125;&#125;) 3. vue的自定义指令 在vue中除了内置的指令如v-for、v-if…,用户可以自定义指令官网 123456789//这里定义v-focusdirectives: &#123; focus: &#123; // 指令的定义 inserted: function (el) &#123; el.focus() &#125; &#125;&#125; 一个指令定义对象可以提供如下几个钩子函数 (均为可选)： bind：只调用一次，指令第一次绑定到元素时调用。在这里可以进行一次性的初始化设置。 inserted：被绑定元素插入父节点时调用 (仅保证父节点存在，但不一定已被插入文档中)。 update：所在组件的 VNode 更新时调用，但是可能发生在其子 VNode 更新之前。指令的值可能发生了改变，也可能没有。但是你可以通过比较更新前后的值来忽略不必要的模板更新 (详细的钩子函数参数见下)。 componentUpdated：指令所在组件的 VNode 及其子 VNode 全部更新后调用。 unbind：只调用一次，指令与元素解绑时调用。 4. vue的扩展4-1 v-bind根据条件绑定类名 案例 比如现在在true的情况下绑定red类名 1234567891011121314&lt;div id="app"&gt; &lt;span :class="&#123;red:addClass&#125;"&gt;&#123;&#123;msg&#125;&#125;&lt;/span&gt; &lt;!--可以利用简单的表达式--&gt;&lt;!--这是v-bind指令可以省略--&gt;&lt;/div&gt;&lt;script src="js/vue.min.js"&gt;&lt;/script&gt;&lt;script&gt; new Vue(&#123; el:"#app", data:&#123; msg:'条件绑定类名', addClass:true &#125; &#125;)&lt;/script&gt; 4-2 v-on || @eventName 事件绑定 有一个事件修饰符12345678910//阻止事件冒泡&lt;div v-on:click.stop="eventHadles"&gt;&lt;/div&gt;//阻止默认事件&lt;div v-on:click.prevent="eventHadles"&gt;&lt;/div&gt;//事件只能触发一次&lt;div v-on:click.once="eventHadles"&gt;&lt;/div&gt;//只能回车触发事件&lt;div @keyup.enter="eventHadles"&gt;&lt;/div&gt;//只能指定keyCode的值触发事件&lt;div @keyup.13="eventHadles"&gt;&lt;/div&gt; 5. vue的计算属性 -computed 例如一个案例需要过滤一些列表 而我们需要利用v-for进行循环出来列表 需要用到我们的实例的属性 computed 说透点就是过滤你的数据根据你的条件进行过滤 1234567891011121314151617181920212223242526272829303132333435363738394041//jsnew Vue(&#123; el:"#app", data:&#123; list:[ &#123; title:'你好啊', isChecked:true &#125;, &#123; title:'你好啊2', isChecked:false &#125; ]， hash:'all' //过滤条件 &#125;, computed:&#123; //重头戏 filterData()&#123; //这个根据你的条件进行过滤 记住这个函数最终返回的是数据需要return 数据出来 不需要调用此函数 let filterDatas = &#123; all(list)&#123; return list &#125;, unfinshed(list)&#123; return list.filter(function(item)&#123; return !item.isChecked &#125;) &#125;, finshed(list)&#123; return list.filter(function(item)&#123; return item.isChecked &#125;) &#125; &#125; return filterDatas[你的条件]?filterDatas[你的条件](this.list):this.list //这里的你的条件可以使hash值 然后很久hash值的不同进行过滤 不需要调用这个函数 &#125; &#125;&#125;)//然后在v-for的指令中&lt;div v-for="item in filterData"&gt;&lt;/div&gt; &lt;!--注意不是list了 而是刚刚的computed中的计算属性的函数名字--&gt; 6. vue的组件6-1 底层学习组件 组件 (Component) 是 Vue.js 最强大的功能之一。组件可以扩展 HTML 元素，封装可重用的代码。在较高层面上，组件是自定义元素，Vue.js 的编译器为它添加特殊功能。在有些情况下，组件也可以表现为用 is 特性进行了扩展的原生 HTML 元素。 12345678910111213//html&lt;div id="app"&gt; &lt;my-test&gt;&lt;/my-test&gt; &lt;!--自定义标签--&gt;&lt;/div&gt;//jsVue.component('my-test',&#123; //注册组件 template:'&lt;div&gt;初学组件&lt;/div&gt;' &#125;);new Vue(&#123; el:"#app"&#125;) 6-2 父子组件通信 利用props进行传值 1234567891011121314151617181920212223242526//局部组件&lt;div id="app"&gt; &lt;my-test msg="你好"&gt;&lt;/my-test&gt; &lt;my-test msg="传值2"&gt;&lt;/my-test&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", components:&#123; 'my-test':&#123; props:['msg'], template:'&lt;div&gt;&#123;&#123;msg&#125;&#125;&lt;/div&gt;' &#125; &#125;&#125;)//全局组件&lt;div id="app"&gt; &lt;my-test msg="你好"&gt;&lt;/my-test&gt; &lt;my-test msg="传值2"&gt;&lt;/my-test&gt;&lt;/div&gt;//jsVue.component('my-test',&#123; props:['msg'], template:'&lt;div&gt;&#123;&#123;msg&#125;&#125;&lt;/div&gt;'&#125;) 如果需要传的值在vue的实例中 12345678910111213141516171819202122232425//html&lt;div id="app"&gt; &lt;my-test v-bind:listData="list"&gt;&lt;/my-test&gt;&lt;/div&gt;//jsnew Vue(&#123; el:"#app", data:&#123; list:[ &#123;title:'这是数据'&#125;, &#123;title:'这是数据22'&#125; ] &#125;, components:&#123; 'my-test':&#123; props:['listData'], template:` &lt;select name="" id=""&gt; &lt;option v-for="item in listData"&gt;&#123;&#123;item.title&#125;&#125;&lt;/option&gt; &lt;/select&gt; ` &#125; &#125;&#125;) 7. vue获取dom元素1234567891011121314在想获取的元素身上&lt;div class="container" rel="getDom"&gt;//jsnew Vue(&#123; el:"#app", methods:&#123; _someDo()&#123; this.dom = this.$refs.getDom; &#125; &#125;, mounted()&#123; this._someDo(); //vue完成挂载 调用函数 &#125;&#125;) 8. vue渲染dom是异步$.nextTick()函数 因为vue渲染dom是异步的所以直接操作dom是会出错的 案例 例如 创建vue实例的时候请求接口数据，然后要进行dom元素操作 1234567891011121314151617new Vue(&#123; data:&#123; result:'' &#125;, created()&#123; axios.get('/data') .then(data=&gt;&#123; this.result = data.data //如果在dom中用到了v-for这些元素 而我们乡操作这些元素 this.$nextTick(()=&gt;&#123; //这个函数的意义就是等待dom渲染结束后执行 &#125;) &#125;) .catch(err=&gt;&#123; //错误处理 &#125;) &#125;&#125;)]]></content>
      <categories>
        <category>web前端</category>
        <category>VUE</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>VUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6 新语法 (二)]]></title>
    <url>%2F2019%2F07%2F09%2F03.web%E5%89%8D%E7%AB%AF%2FJavaScript%2FES6%20%E6%96%B0%E8%AF%AD%E6%B3%95(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[ES6 对象对象字面量属性的简洁表示法 ES6允许对象的属性直接写变量，这时候属性名是变量名，属性值是变量值。 1234567891011const age = 12; const name = "Amy";const person = &#123;age, name&#125;; person //&#123;age: 12, name: "Amy"&#125; //等同于 const person = &#123;age: age, name: name&#125; 方法名也可以简写 12345678910111213const person = &#123; sayHi()&#123; console.log("Hi"); &#125;&#125;person.sayHi(); //"Hi"//等同于const person = &#123; sayHi:function()&#123; console.log("Hi"); &#125;&#125;person.sayHi();//"Hi" 如果是Generator 函数，则要在前面加一个星号: 1234567891011const obj = &#123; * myGenerator() &#123; yield 'hello world'; &#125;&#125;;//等同于const obj = &#123; myGenerator: function* () &#123; yield 'hello world'; &#125;&#125;; 属性名表达式 ES6允许用表达式作为属性名，但是一定要将表达式放在方括号内。 123456const obj = &#123; ["he"+"llo"]()&#123; return "Hi"; &#125;&#125;obj.hello(); //"Hi" 注意点：属性的简洁表示法和属性名表达式不能同时使用，否则会报错。 1234567891011const hello = "Hello";const obj = &#123; [hello]&#125;;obj //SyntaxError: Unexpected token &#125; const hello = "Hello";const obj = &#123; [hello+"2"]:"world"&#125;;obj //&#123;Hello2: "world"&#125; 对象拓展运算符拓展运算符（…）用于取出参数对象所有可遍历属性然后拷贝到当前对象。 基本用法 123let person = &#123;name: "Amy", age: 15&#125;;let someone = &#123; ...person &#125;;someone; //&#123;name: "Amy", age: 15&#125; 可用于合并两个对象 1234let age = &#123;age: 15&#125;;let name = &#123;name: "Amy"&#125;;let person = &#123;...age, ...name&#125;;person; //&#123;age: 15, name: "Amy"&#125; 对象的新方法一、Object.assign 1Object.assign(target, source_1, ···) 用于将源对象的所有可枚举属性复制到目标对象中。 基本用法 123456let target = &#123;a: 1&#125;;let object2 = &#123;b: 2&#125;;let object3 = &#123;c: 3&#125;;Object.assign(target,object2,object3); // 第一个参数是目标对象，后面的参数是源对象target; // &#123;a: 1, b: 2, c: 3 如果目标对象和源对象有同名属性，或者多个源对象有同名属性，则后面的属性会覆盖前面的属性。 如果该函数只有一个参数，当参数为对象时，直接返回该对象；当参数不是对象时，会先将参数转为对象然后返回。 12Object.assign(3); // Number &#123;3&#125;typeof Object.assign(3); // "object" 因为 null 和 undefined 不能转化为对象，所以会报错: 1234567Object.assign(null); // TypeError: Cannot convert undefined or null to objectObject.assign(undefined); // TypeError: Cannot convert undefined or null to object当参数不止一个时，null 和 undefined 不放第一个，即不为目标对象时，会跳过 null 和 undefined ，不报错Object.assign(1,undefined); // Number &#123;1&#125;Object.assign(&#123;a: 1&#125;,null); // &#123;a: 1&#125; Object.assign(undefined,&#123;a: 1&#125;); // TypeError: Cannot convert undefined or null to object 注意点 assign 的属性拷贝是浅拷贝: 12345let sourceObj = &#123; a: &#123; b: 1&#125;&#125;;let targetObj = &#123;c: 3&#125;;Object.assign(targetObj, sourceObj);targetObj.a.b = 2;sourceObj.a.b; // 2 同名属性替换 1234targetObj = &#123; a: &#123; b: 1, c:2&#125;&#125;;sourceObj = &#123; a: &#123; b: "hh"&#125;&#125;;Object.assign(targetObj, sourceObj);targetObj; // &#123;a: &#123;b: "hh"&#125;&#125; 数组的处理 1Object.assign([2,3], [5]); // [5,3] 会将数组处理成对象，所以先将 [2,3] 转为 {0:2,1:3} ，然后再进行属性复制，所以源对象的 0 号属性覆盖了目标对象的 0。 二、Object.is 1Object.is(value1, value2) 用来比较两个值是否严格相等，与（===）基本类似。 基本用法 1234Object.is("q","q"); // trueObject.is(1,1); // trueObject.is([1],[1]); // falseObject.is(&#123;q:1&#125;,&#123;q:1&#125;); // false 与（===）的区别 123456//一是+0不等于-0Object.is(+0,-0); //false+0 === -0 //true//二是NaN等于本身Object.is(NaN,NaN); //trueNaN === NaN //false ES6 数组数组创建Array.of 将参数中所有值作为元素形成数组。 1234567console.log(Array.of(1, 2, 3, 4)); // [1, 2, 3, 4] // 参数值可为不同类型console.log(Array.of(1, '2', true)); // [1, '2', true] // 参数为空时返回空数组console.log(Array.of()); // [] Array.from 将类数组对象或可迭代对象转化为数组。 12345// 参数为数组,返回与原数组一样的数组console.log(Array.from([1, 2])); // [1, 2] // 参数含空位console.log(Array.from([1, , 3])); // [1, undefined, 3] array.from 接收的参数: Array.from(arrayLike[, mapFn[, thisArg]]) ; 返回值为转换后的数组。 arrayLike: 想要转换的类数组对象或可迭代对象。 1console.log(Array.from([1, 2, 3])); // [1, 2, 3] mapFn: 可选，map 函数，用于对每个元素进行处理，放入数组的是处理后的元素。 1console.log(Array.from([1, 2, 3], (n) =&gt; n * 2)); // [2, 4, 6] thisArg: 可选，用于指定 map 函数执行时的 this 对象。 123456789let map = &#123; do: function(n) &#123; return n * 2; &#125;&#125;let arrayLike = [1, 2, 3];console.log(Array.from(arrayLike, function (n)&#123; return this.do(n);&#125;, map)); // [2, 4, 6] 类数组对象 一个类数组对象必须含有 length 属性，且元素属性名必须是数值或者可转换为数值的字符。 1234567891011121314151617181920212223let arr = Array.from(&#123; 0: '1', 1: '2', 2: 3, length: 3&#125;);console.log(); // ['1', '2', 3] // 没有 length 属性,则返回空数组let array = Array.from(&#123; 0: '1', 1: '2', 2: 3,&#125;);console.log(array); // [] // 元素属性名不为数值且无法转换为数值，返回长度为 length 元素值为 undefined 的数组 let array1 = Array.from(&#123; a: 1, b: 2, length: 2&#125;);console.log(array1); // [undefined, undefined] 转换可迭代对象 转换map 12345let map = new Map();map.set('key0', 'value0');map.set('key1', 'value1');console.log(Array.from(map)); // [['key0', 'value0'],['key1',// 'value1']] 转换set 123let arr = [1, 2, 3];let set = new Set(arr);console.log(Array.from(set)); // [1, 2, 3] 转换字符串 12let str = 'abc';console.log(Array.from(str)); // ["a", "b", "c"] 数组拓展方法查找 find() 查找数组中符合条件的元素,若有多个符合条件的元素，则返回第一个元素。 12345let arr = Array.of(1, 2, 3, 4);console.log(arr.find(item =&gt; item &gt; 2)); // 3 // 数组空位处理为 undefinedconsole.log([, 1].find(n =&gt; true)); // undefined findIndex() 查找数组中符合条件的元素索引，若有多个符合条件的元素，则返回第一个元素索引。 1234567let arr = Array.of(1, 2, 1, 3);// 参数1：回调函数// 参数2(可选)：指定回调函数中的 this 值console.log(arr.findIndex(item =&gt; item = 1)); // 0 // 数组空位处理为 undefinedconsole.log([, 1].findIndex(n =&gt; true)); //0 填充 fill() 将一定范围索引的数组元素内容填充为单个指定的值。 12345let arr = Array.of(1, 2, 3, 4);// 参数1：用来填充的值// 参数2：被填充的起始索引// 参数3(可选)：被填充的结束索引，默认为数组末尾console.log(arr.fill(0,1,2)); // [1, 0, 3, 4] copyWithin() 将一定范围索引的数组元素修改为此数组另一指定范围索引的元素。 123456789// 参数1：被修改的起始索引// 参数2：被用来覆盖的数据的起始索引// 参数3(可选)：被用来覆盖的数据的结束索引，默认为数组末尾console.log([1, 2, 3, 4].copyWithin(0,2,4)); // [3, 4, 3, 4] // 参数1为负数表示倒数console.log([1, 2, 3, 4].copyWithin(-2, 0)); // [1, 2, 1, 2] console.log([1, 2, ,4].copyWithin(0, 2, 4)); // [, 4, , 4] 遍历 entries()： 遍历键值对。 keys()： 遍历键名； values(): 遍历键值； 12345678910111213for(let [key, value] of ['a', 'b'].entries())&#123; console.log(key, value);&#125;// 0 "a"// 1 "b" // 不使用 for... of 循环let entries = ['a', 'b'].entries();console.log(entries.next().value); // [0, "a"]console.log(entries.next().value); // [1, "b"] // 数组含空位console.log([...[,'a'].entries()]); // [[0, undefined], [1, "a"]] 包含 includes() 数组是否包含指定值。 注意：与 Set 和 Map 的 has 方法区分；Set 的 has 方法用于查找值；Map 的 has 方法用于查找键名。 12345678// 参数1：包含的指定值[1, 2, 3].includes(1); // true // 参数2：可选，搜索的起始索引，默认为0[1, 2, 3].includes(1, 2); // false // NaN 的包含判断[1, NaN, 3].includes(NaN); // true 嵌套数组转一维数组 flat() 12345678910console.log([1 ,[2, 3]].flat()); // [1, 2, 3] // 指定转换的嵌套层数console.log([1, [2, [3, [4, 5]]]].flat(2)); // [1, 2, 3, [4, 5]] // 不管嵌套多少层console.log([1, [2, [3, [4, 5]]]].flat(Infinity)); // [1, 2, 3, 4, 5] // 自动跳过空位console.log([1, [2, , 3]].flat()); // [1, 2, 3] flatMap() 先对数组中每个元素进行了的处理，再对数组执行 flat() 方法。 123// 参数1：遍历函数，该遍历函数可接受3个参数：当前元素、当前元素索引、原数组// 参数2：指定遍历函数中 this 的指向console.log([1, 2, 3].flatMap(n =&gt; [n * 2])); // [2, 4, 6] 数组缓冲区数组缓冲区是内存中的一段地址。 定型数组的基础。 实际字节数在创建时确定，之后只可修改其中的数据，不可修改大小。 创建数组缓冲区 通过构造函数创建: 123456let buffer = new ArrayBuffer(10);console.log(buffer.byteLength); // 10分割已有数组缓冲区let buffer = new ArrayBuffer(10);let buffer1 = buffer.slice(1, 3);console.log(buffer1.byteLength); // 2 视图 视图是用来操作内存的接口。 视图可以操作数组缓冲区或缓冲区字节的子集,并按照其中一种数值数据类型来读取和写入数据。 DataView 类型是一种通用的数组缓冲区视图,其支持所有8种数值型数据类型。 12345678910// 默认 DataView 可操作数组缓冲区全部内容let buffer = new ArrayBuffer(10); dataView = new DataView(buffer); dataView.setInt8(0,1);console.log(dataView.getInt8(0)); // 1 // 通过设定偏移量(参数2)与长度(参数3)指定 DataView 可操作的字节范围let buffer1 = new ArrayBuffer(10); dataView1 = new DataView(buffer1, 0, 3);dataView1.setInt8(5,1); // RangeError 定型数组数组缓冲区的特定类型的视图。 可以强制使用特定的数据类型，而不是使用通用的 DataView 对象来操作数组缓冲区。 创建 通过数组缓冲区生成 123let buffer = new ArrayBuffer(10), view = new Int8Array(buffer);console.log(view.byteLength); // 10 通过构造函数 1234567891011121314151617181920212223242526let view = new Int32Array(10);console.log(view.byteLength); // 40console.log(view.length); // 10 // 不传参则默认长度为0// 在这种情况下数组缓冲区分配不到空间，创建的定型数组不能用来保存数据let view1 = new Int32Array();console.log(view1.byteLength); // 0console.log(view1.length); // 0 // 可接受参数包括定型数组、可迭代对象、数组、类数组对象let arr = Array.from(&#123; 0: '1', 1: '2', 2: 3, length: 3&#125;);let view2 = new Int16Array([1, 2]), view3 = new Int32Array(view2), view4 = new Int16Array(new Set([1, 2, 3])), view5 = new Int16Array([1, 2, 3]), view6 = new Int16Array(arr);console.log(view2 .buffer === view3.buffer); // falseconsole.log(view4.byteLength); // 6console.log(view5.byteLength); // 6console.log(view6.byteLength); // 6 注意要点 length 属性不可写，如果尝试修改这个值，在非严格模式下会直接忽略该操作，在严格模式下会抛出错误。 123let view = new Int16Array([1, 2]);view.length = 3;console.log(view.length); // 2 定型数组可使用 entries()、keys()、values()进行迭代。 123456let view = new Int16Array([1, 2]);for(let [k, v] of view.entries())&#123; console.log(k, v);&#125;// 0 1// 1 2 find() 等方法也可用于定型数组，但是定型数组中的方法会额外检查数值类型是否安全,也会通过 Symbol.species 确认方法的返回值是定型数组而非普通数组。concat() 方法由于两个定型数组合并结果不确定，故不能用于定型数组；另外，由于定型数组的尺寸不可更改,可以改变数组的尺寸的方法，例如 splice() ，不适用于定型数组。 12let view = new Int16Array([1, 2]);view.find((n) &gt; 1); // 2 所有定型数组都含有静态 of() 方法和 from() 方法,运行效果分别与 Array.of() 方法和 Array.from() 方法相似,区别是定型数组的方法返回定型数组,而普通数组的方法返回普通数组。 12let view = Int16Array.of(1, 2);console.log(view instanceof Int16Array); // true 定型数组不是普通数组，不继承自 Array 。 12let view = new Int16Array([1, 2]);console.log(Array.isArray(view)); // false 定型数组中增加了 set() 与 subarray() 方法。 set() 方法用于将其他数组复制到已有定型数组, subarray() 用于提取已有定型数组的一部分形成新的定型数组。 123456789101112131415161718// set 方法// 参数1：一个定型数组或普通数组// 参数2：可选，偏移量，开始插入数据的位置，默认为0let view= new Int16Array(4);view.set([1, 2]);view.set([3, 4], 2);console.log(view); // [1, 2, 3, 4] // subarray 方法// 参数1：可选，开始位置// 参数2：可选，结束位置(不包含结束位置)let view= new Int16Array([1, 2, 3, 4]), subview1 = view.subarray(), subview2 = view.subarray(1), subview3 = view.subarray(1, 3);console.log(subview1); // [1, 2, 3, 4]console.log(subview2); // [2, 3, 4]console.log(subview3); // [2, 3] 拓展运算符复制数组 12345678let arr = [1, 2], arr1 = [...arr];console.log(arr1); // [1, 2] // 数组含空位let arr2 = [1, , 3], arr3 = [...arr2];console.log(arr3); [1, undefined, 3] ES6 迭代器IteratorIterator 是 ES6 引入的一种新的遍历机制，迭代器有两个核心概念： 迭代器是一个统一的接口，它的作用是使各种数据结构可被便捷的访问，它是通过一个键为Symbol.iterator 的方法来实现。 迭代器是用于遍历数据结构元素的指针（如数据库中的游标）。 迭代过程迭代原理 通过 Symbol.iterator 创建一个迭代器，指向当前数据结构的起始位置 随后通过 next 方法进行向下迭代指向下一个位置， next 方法会返回当前位置的对象，对象包含了 value 和 done 两个属性， value 是当前属性的值， done 用于判断是否遍历结束 当 done 为 true 时则遍历结束 下面通过一个简单的例子进行说明： 1234567891011const items = ["zero", "one", "two"];const it = items[Symbol.iterator](); it.next();&gt;&#123;value: "zero", done: false&#125;it.next();&gt;&#123;value: "one", done: false&#125;it.next();&gt;&#123;value: "two", done: false&#125;it.next();&gt;&#123;value: undefined, done: true&#125; 上面的例子，首先创建一个数组，然后通过 Symbol.iterator 方法创建一个迭代器，之后不断的调用 next 方法对数组内部项进行访问，当属性 done 为 true 时访问结束。 迭代器是协议（使用它们的规则）的一部分，用于迭代。该协议的一个关键特性就是它是顺序的：迭代器一次返回一个值。这意味着如果可迭代数据结构是非线性的（例如树），迭代将会使其线性化。 可迭代的数据结构可迭代结构 Array String Map Set Dom元素（正在进行中） for … of 循环for…of 是 ES6 新引入的循环，用于替代 for..in 和 forEach() ，并且支持新的迭代协议。它可用于迭代常规的数据类型，如 Array 、 String 、 Map 和 Set 等等。 循环方法 12345678910111213141516171819202122let myMap = new Map();myMap.set(0, "zero");myMap.set(1, "one");myMap.set(2, "two"); // 遍历 key 和 valuefor (let [key, value] of myMap) &#123; console.log(key + " = " + value);&#125;for (let [key, value] of myMap.entries()) &#123; console.log(key + " = " + value);&#125; // 只遍历 keyfor (let key of myMap.keys()) &#123; console.log(key);&#125; // 只遍历 valuefor (let value of myMap.values()) &#123; console.log(value);&#125; 可迭代数据结构of 操作数必须是可迭代，这意味着如果是普通对象则无法进行迭代。如果数据结构类似于数组的形式，则可以借助 Array.from() 方法进行转换迭代。 1234567891011121314const arrayLink = &#123;length: 2, 0: "zero", 1: "one"&#125;// 报 TypeError 异常for (let item of arrayLink) &#123; console.log(item);&#125; // 正常运行// output:// zero// onefor (let item of Array.from(arrayLink)) &#123; console.log(item);&#125; let 、const 和 var 用于 for..of 如果使用 let 和 const ，每次迭代将会创建一个新的存储空间，这可以保证作用域在迭代的内部。 1234567const nums = ["zero", "one", "two"]; for (const num of nums) &#123; console.log(num);&#125;// 报 ReferenceErrorconsole.log(num); 从上面的例子我们看到，最后一句会报异常，原因 num 的作用域只在循环体内部，外部无效，具体可查阅 let 与 const 章节。使用 var 则不会出现上述情况，因为 var 会作用于全局，迭代将不会每次都创建一个新的存储空间。 1234567const nums = ["zero", "one", "two"]; forv (var num of nums) &#123; console.log(num);&#125;// output: twoconsole.log(num); ES6 Class 类概述在ES6中，class (类)作为对象的模板被引入，可以通过 class 关键字定义类。 class 的本质是 function。 它可以看作一个语法糖，让对象原型的写法更加清晰、更像面向对象编程的语法。 基础用法类定义类表达式可以为匿名或命名。 123456789101112// 匿名类let Example = class &#123; constructor(a) &#123; this.a = a; &#125;&#125;// 命名类let Example = class Example &#123; constructor(a) &#123; this.a = a; &#125;&#125; 类声明12345class Example &#123; constructor(a) &#123; this.a = a; &#125;&#125; 注意要点：不可重复声明。 123456789class Example&#123;&#125;class Example&#123;&#125;// Uncaught SyntaxError: Identifier 'Example' has already been // declared let Example1 = class&#123;&#125;class Example&#123;&#125;// Uncaught SyntaxError: Identifier 'Example' has already been // declared 注意要点类定义不会被提升，这意味着，必须在访问前对类进行定义，否则就会报错。 类中方法不需要 function 关键字。 方法间不能加分号。 类的主体属性 prototype ES6 中，prototype 仍旧存在，虽然可以直接自类中定义方法，但是其实方法还是定义在 prototype 上的。 覆盖方法 / 初始化时添加方法 123Example.prototype=&#123; //methods&#125; 添加方法 123Object.assign(Example.prototype,&#123; //methods&#125;) 静态属性 静态属性：class 本身的属性，即直接定义在类内部的属性（ Class.propname ），不需要实例化。 ES6 中规定，Class 内部只有静态方法，没有静态属性。 123456class Example &#123;// 新提案 static a = 2;&#125;// 目前可行写法Example.b = 2; 公共属性 12class Example&#123;&#125;Example.prototype.a = 2; 实例属性 实例属性：定义在实例对象（ this ）上的属性。 123456class Example &#123; a = 2; constructor () &#123; console.log(this.a); &#125;&#125; name属性 返回跟在 class 后的类名(存在时)。 12345678910111213let Example=class Exam &#123; constructor(a) &#123; this.a = a; &#125;&#125;console.log(Example.name); // Exam let Example=class &#123; constructor(a) &#123; this.a = a; &#125;&#125;console.log(Example.name); // Example 方法 constructor 方法 constructor 方法是类的默认方法，创建类的实例化对象时被调用。 123456class Example&#123; constructor()&#123; console.log('我是constructor'); &#125;&#125;new Example(); // 我是constructor 返回对象 1234567891011121314class Test &#123; constructor()&#123; // 默认返回实例对象 this &#125;&#125;console.log(new Test() instanceof Test); // true class Example &#123; constructor()&#123; // 指定返回对象 return new Test(); &#125;&#125;console.log(new Example() instanceof Example); // false 静态方法 123456class Example&#123; static sum(a, b) &#123; console.log(a+b); &#125;&#125;Example.sum(1, 2); // 3 原型方法 1234567class Example &#123; sum(a, b) &#123; console.log(a + b); &#125;&#125;let exam = new Example();exam.sum(1, 2); // 3 实例方法 1234567class Example &#123; constructor() &#123; this.sum = (a, b) =&gt; &#123; console.log(a + b); &#125; &#125;&#125; 类的实例化new class 的实例化必须通过 new 关键字。 1234class Example &#123;&#125; let exam1 = Example(); // Class constructor Example cannot be invoked without 'new' 实例化对象 共享原型对象 12345678910111213141516171819class Example &#123; constructor(a, b) &#123; this.a = a; this.b = b; console.log('Example'); &#125; sum() &#123; return this.a + this.b; &#125;&#125;let exam1 = new Example(2, 1);let exam2 = new Example(3, 1);console.log(exam1._proto_ == exam2._proto_); // true exam1._proto_.sub = function() &#123; return this.a - this.b;&#125;console.log(exam1.sub()); // 1console.log(exam2.sub()); // 2 decoratordecorator 是一个函数，用来修改类的行为，在代码编译时产生作用。 类修饰 一个参数 第一个参数 target，指向类本身。 123456function testable(target) &#123; target.isTestable = true;&#125;@testableclass Example &#123;&#125;Example.isTestable; // true 多个参数——嵌套实现 12345678function testable(isTestable) &#123; return function(target) &#123; target.isTestable=isTestable; &#125;&#125;@testable(true)class Example &#123;&#125;Example.isTestable; // true 实例属性 上面两个例子添加的是静态属性，若要添加实例属性，在类的 prototype 上操作即可。 方法修饰 3个参数：target（类的原型对象）、name（修饰的属性名）、descriptor（该属性的描述对象） 12345678910class Example &#123; @writable sum(a, b) &#123; return a + b; &#125;&#125;function writable(target, name, descriptor) &#123; descriptor.writable = false; return descriptor; // 必须返回&#125; 修饰器执行顺序 由外向内进入，由内向外执行。 123456789101112131415class Example &#123; @logMethod(1) @logMthod(2) sum(a, b)&#123; return a + b; &#125;&#125;function logMethod(id) &#123; console.log('evaluated logMethod'+id); return (target, name, desctiptor) =&gt; console.log('excuted logMethod '+id);&#125;// evaluated logMethod 1// evaluated logMethod 2// excuted logMethod 2// excuted logMethod 1 封装与继承getter / setter 定义 12345678910111213141516171819202122232425262728293031class Example&#123; constructor(a, b) &#123; this.a = a; // 实例化时调用 set 方法 this.b = b; &#125; get a()&#123; console.log('getter'); return this.a; &#125; set a(a)&#123; console.log('setter'); this.a = a; // 自身递归调用 &#125;&#125;let exam = new Example(1,2); // 不断输出 setter ，最终导致 RangeErrorclass Example1&#123; constructor(a, b) &#123; this.a = a; this.b = b; &#125; get a()&#123; console.log('getter'); return this._a; &#125; set a(a)&#123; console.log('setter'); this._a = a; &#125;&#125;let exam1 = new Example1(1,2); // 只输出 setter , 不会调用 getter 方法console.log(exam._a); // 1, 可以直接访问 getter 不可单独出现 123456789class Example &#123; constructor(a) &#123; this.a = a; &#125; get a() &#123; return this.a; &#125;&#125;let exam = new Example(1); // Uncaught TypeError: Cannot set property // a of #&lt;Example&gt; which has only a getter getter 与 setter 必须同级出现 123456789101112131415161718192021222324252627282930313233343536class Father &#123; constructor()&#123;&#125; get a() &#123; return this._a; &#125;&#125;class Child extends Father &#123; constructor()&#123; super(); &#125; set a(a) &#123; this._a = a; &#125;&#125;let test = new Child();test.a = 2;console.log(test.a); // undefined class Father1 &#123; constructor()&#123;&#125; // 或者都放在子类中 get a() &#123; return this._a; &#125; set a(a) &#123; this._a = a; &#125;&#125;class Child1 extends Father1 &#123; constructor()&#123; super(); &#125;&#125;let test1 = new Child1();test1.a = 2;console.log(test1.a); // 2 extends 通过 extends 实现类的继承。 1class Child extends Father &#123; ... &#125; super 子类 constructor 方法中必须有 super ，且必须出现在 this 之前。 1234567891011121314class Father &#123; constructor() &#123;&#125;&#125;class Child extends Father &#123; constructor() &#123;&#125; // or // constructor(a) &#123; // this.a = a; // super(); // &#125;&#125;let test = new Child(); // Uncaught ReferenceError: Must call super // constructor in derived class before accessing 'this' or returning // from derived constructor 调用父类构造函数,只能出现在子类的构造函数。 12345678910111213141516171819class Father &#123; test()&#123; return 0; &#125; static test1()&#123; return 1; &#125;&#125;class Child extends Father &#123; constructor()&#123; super(); &#125;&#125;class Child1 extends Father &#123; test2() &#123; super(); // Uncaught SyntaxError: 'super' keyword unexpected // here &#125;&#125; 调用父类方法, super 作为对象，在普通方法中，指向父类的原型对象，在静态方法中，指向父类 123456789101112class Child2 extends Father &#123; constructor()&#123; super(); // 调用父类普通方法 console.log(super.test()); // 0 &#125; static test3()&#123; // 调用父类静态方法 return super.test1+2; &#125;&#125;Child2.test3(); // 3 注意要点 不可继承常规对象 12345678910var Father = &#123; // ...&#125;class Child extends Father &#123; // ...&#125;// Uncaught TypeError: Class extends value #&lt;Object&gt; is not a constructor or null // 解决方案Object.setPrototypeOf(Child.prototype, Father); ES6 模块概述在 ES6 前， 实现模块化使用的是 RequireJS 或者 seaJS（分别是基于 AMD 规范的模块化库， 和基于 CMD 规范的模块化库）。 ES6 引入了模块化，其设计思想是在编译时就能确定模块的依赖关系，以及输入和输出的变量。 ES6 的模块化分为导出（export） @与导入（import）两个模块。 ES6 模块的特点ES6 的模块自动开启严格模式，不管你有没有在模块头部加上 use strict;。 模块中可以导入和导出各种类型的变量，如函数，对象，字符串，数字，布尔值，类等。 每个模块都有自己的上下文，每一个模块内声明的变量都是局部变量，不会污染全局作用域。 每一个模块只加载一次（是单例的）， 若再去加载同目录下同文件，直接从内存中读取。 export 与 import基本 用法 模块导入导出各种类型的变量，如字符串，数值，函数，类。 导出的函数声明与类声明必须要有名称（export default 命令另外考虑）。 不仅能导出声明还能导出引用（例如函数）。 export 命令可以出现在模块的任何位置，但必需处于模块顶层。 import 命令会提升到整个模块的头部，首先执行。 1234567891011121314151617/*-----export [test.js]-----*/let myName = "Tom";let myAge = 20;let myfn = function()&#123; return "My name is" + myName + "! I'm '" + myAge + "years old."&#125;let myClass = class myClass &#123; static a = "yeah!";&#125;export &#123; myName, myAge, myfn, myClass &#125; /*-----import [xxx.js]-----*/import &#123; myName, myAge, myfn, myClass &#125; from "./test.js";console.log(myfn());// My name is Tom! I'm 20 years old.console.log(myAge);// 20console.log(myName);// Tomconsole.log(myClass.a );// yeah! 建议使用大括号指定所要输出的一组变量写在文档尾部，明确导出的接口。 函数与类都需要有对应的名称，导出文档尾部也避免了无对应名称。 as 的用法 export 命令导出的接口名称，须和模块内部的变量有一一对应关系。 导入的变量名，须和导出的接口名称相同，即顺序可以不一致。 12345678910111213141516171819/*-----export [test.js]-----*/let myName = "Tom";export &#123; myName as exportName &#125; /*-----import [xxx.js]-----*/import &#123; exportName &#125; from "./test.js";console.log(exportName);// Tom使用 as 重新定义导出的接口名称，隐藏模块内部的变量/*-----export [test1.js]-----*/let myName = "Tom";export &#123; myName &#125;/*-----export [test2.js]-----*/let myName = "Jerry";export &#123; myName &#125;/*-----import [xxx.js]-----*/import &#123; myName as name1 &#125; from "./test1.js";import &#123; myName as name2 &#125; from "./test2.js";console.log(name1);// Tomconsole.log(name2);// Jerry 不同模块导出接口名称命名重复， 使用 as 重新定义变量名。 import 命令特点 只读属性：不允许在加载模块的脚本里面，改写接口的引用指向，即可以改写 import 变量类型为对象的属性值，不能改写 import 变量类型为基本类型的值。 12345import &#123;a&#125; from "./xxx.js"a = &#123;&#125;; // error import &#123;a&#125; from "./xxx.js"a.foo = "hello"; // a = &#123; foo : 'hello' &#125; 单例模式：多次重复执行同一句 import 语句，那么只会执行一次，而不会执行多次。import 同一模块，声明不同接口引用，会声明对应变量，但只执行一次 import 。 1234567import &#123; a &#125; "./xxx.js";import &#123; a &#125; "./xxx.js";// 相当于 import &#123; a &#125; "./xxx.js"; import &#123; a &#125; from "./xxx.js";import &#123; b &#125; from "./xxx.js";// 相当于 import &#123; a, b &#125; from "./xxx.js"; 静态执行特性：import 是静态执行，所以不能使用表达式和变量。 1234567891011import &#123; "f" + "oo" &#125; from "methods";// errorlet module = "methods";import &#123; foo &#125; from module;// errorif (true) &#123; import &#123; foo &#125; from "method1";&#125; else &#123; import &#123; foo &#125; from "method2";&#125;// error export default 命令基本属性 在一个文件或模块中，export、import 可以有多个，export default 仅有一个。 export default 中的 default 是对应的导出接口变量。 通过 export 方式导出，在导入时要加{ }，export default 则不需要。 export default 向外暴露的成员，可以使用任意变量来接收。 123456var a = "My name is Tom!";export default a; // 仅有一个export default var c = "error"; // error，default 已经是对应的导出变量，不能跟着变量声明语句 import b from "./xxx.js"; // 不需要加&#123;&#125;， 使用任意变量接收 复合使用export 和 import 复合使用 export 与 import 可以在同一模块使用，使用特点： 可以将导出接口改名，包括 default。 复合使用 export 与 import ，也可以导出全部，当前模块导出的接口会覆盖继承导出的。 ES6 Promise对象概述是异步编程的一种解决方案。 从语法上说，Promise 是一个对象，从它可以获取异步操作的消息。 Promise 状态状态的特点 Promise 异步操作有三种状态：pending（进行中）、fulfilled（已成功）和 rejected（已失败）。除了异步操作的结果，任何其他操作都无法改变这个状态。 Promise 对象只有：从 pending 变为 fulfilled 和从 pending 变为 rejected 的状态改变。只要处于 fulfilled 和 rejected ，状态就不会再变了即 resolved（已定型）。 12345678910111213141516const p1 = new Promise(function(resolve,reject)&#123; resolve('success1'); resolve('success2');&#125;); const p2 = new Promise(function()&#123; resolve('success3'); reject('reject');&#125;); p1.then(function(value)&#123; console.log(value); // success1&#125;);p2.then(function(value)&#123; console.log(value); // success3&#125;); 状态的缺点 无法取消 Promise ，一旦新建它就会立即执行，无法中途取消。 如果不设置回调函数，Promise 内部抛出的错误，不会反应到外部。 当处于 pending 状态时，无法得知目前进展到哪一个阶段（刚刚开始还是即将完成）。 then方法在 JavaScript 事件队列的当前运行完成之前，回调函数永远不会被调用。 1234567891011const p = new Promise(function(resolve,reject)&#123; resolve('success');&#125;); p.then(function(value)&#123; console.log(value);&#125;); console.log('first');// first// success 通过 .then 形式添加的回调函数，不论什么时候，都会被调用。 通过多次调用 .then 可以添加多个回调函数， 他们会按照插入顺序并且独立运行 123456789101112131415161718const p = new Promise(function(resolve,reject)&#123; resolve(1);&#125;).then(function(value)&#123; // 第一个then // 1 console.log(value); return value * 2;&#125;).then(function(value)&#123; // 第二个then // 2 console.log(value);&#125;).then(function(value)&#123; // 第三个then // undefined console.log(value); return Promise.resolve('resolve'); &#125;).then(function(value)&#123; // 第四个then // resolve console.log(value); return Promise.reject('reject'); &#125;).then(function(value)&#123; // 第五个then //reject:reject console.log('resolve:' + value);&#125;, function(err) &#123; console.log('reject:' + err);&#125;); then 方法将返回一个 resolved 或 rejected 状态的 Promise 对象用于链式调用，且 Promise 对象的值就是这个返回值。 then 方法注意点 简便的 Promise 链式编程最好保持扁平化，不要嵌套 Promise。 注意总是返回或终止 Promise 链。 12345const p1 = new Promise(function(resolve,reject)&#123; resolve(1);&#125;).then(function(result) &#123; p2(result).then(newResult =&gt; p3(newResult));&#125;).then(() =&gt; p4()); 创建新 Promise 但忘记返回它时，对应链条被打破，导致 p4 会与 p2 和 p3 同时进行。 大多数浏览器中不能终止的 Promise 链里的 rejection，建议后面都跟上 .catch(error =&gt; console.log(error)); ES6 Generator 函数ES6 新引入了 Generator 函数，可以通过 yield 关键字，把函数的执行流挂起，为改变执行流程提供了可能，从而为异步编程提供解决方案。 基本用法 Generator 函数组成Generator 有两个区分于普通函数的部分： 一是在 function 后面，函数名之前有个 * ； 函数内部有 yield 表达式。 其中 * 用来表示函数为 Generator 函数，yield 用来定义函数内部的状态。 12345678function* func()&#123; console.log("one"); yield '1'; console.log("two"); yield '2'; console.log("three"); return '3';&#125; 执行机制调用 Generator 函数和调用普通函数一样，在函数名后面加上()即可，但是 Generator 函数不会像普通函数一样立即执行，而是返回一个指向内部状态对象的指针，所以要调用遍历器对象Iterator 的 next 方法，指针就会从函数头部或者上一次停下来的地方开始执行。 1234567891011121314f.next();// one// &#123;value: "1", done: false&#125; f.next();// two// &#123;value: "2", done: false&#125; f.next();// three// &#123;value: "3", done: true&#125; f.next();// &#123;value: undefined, done: true&#125; 第一次调用 next 方法时，从 Generator 函数的头部开始执行，先是打印了 one ,执行到 yield 就停下来，并将yield 后边表达式的值 ‘1’，作为返回对象的 value 属性值，此时函数还没有执行完， 返回对象的 done 属性值是 false。 第二次调用 next 方法时，同上步 。 第三次调用 next 方法时，先是打印了 three ，然后执行了函数的返回操作，并将 return 后面的表达式的值，作为返回对象的 value 属性值，此时函数已经结束，多以 done 属性值为true 。 第四次调用 next 方法时， 此时函数已经执行完了，所以返回 value 属性值是 undefined ，done 属性值是 true 。如果执行第三步时，没有 return 语句的话，就直接返回 {value: undefined, done: true}。 函数返回的遍历器对象的方法next 方法 一般情况下，next 方法不传入参数的时候，yield 表达式的返回值是 undefined 。当 next 传入参数的时候，该参数会作为上一步yield的返回值。 12345678function* sendParameter()&#123; console.log("strat"); var x = yield '2'; console.log("one:" + x); var y = yield '3'; console.log("two:" + y); console.log("total:" + (x + y));&#125; next不传参 1234567891011121314151617181920212223var sendp1 = sendParameter();sendp1.next();// strat// &#123;value: "2", done: false&#125;sendp1.next();// one:undefined// &#123;value: "3", done: false&#125;sendp1.next();// two:undefined// total:NaN// &#123;value: undefined, done: true&#125;next传参var sendp2 = sendParameter();sendp2.next(10);// strat// &#123;value: "2", done: false&#125;sendp2.next(20);// one:20// &#123;value: "3", done: false&#125;sendp2.next(30);// two:30// total:50// &#123;value: undefined, done: true&#125; 除了使用 next ，还可以使用 for… of 循环遍历 Generator 函数生产的 Iterator 对象。 return 方法 return 方法返回给定值，并结束遍历 Generator 函数。 return 方法提供参数时，返回该参数；不提供参数时，返回 undefined 。 123456789101112131415161718192021222324252627282930313233function* foo()&#123; yield 1; yield 2; yield 3;&#125;var f = foo();f.next();// &#123;value: 1, done: false&#125;f.return("foo");// &#123;value: "foo", done: true&#125;f.next();// &#123;value: undefined, done: true&#125;throw 方法throw 方法可以再 Generator 函数体外面抛出异常，再函数体内部捕获。var g = function* () &#123; try &#123; yield; &#125; catch (e) &#123; console.log('catch inner', e); &#125;&#125;; var i = g();i.next(); try &#123; i.throw('a'); i.throw('b');&#125; catch (e) &#123; console.log('catch outside', e);&#125;// catch inner a// catch outside b 遍历器对象抛出了两个错误，第一个被 Generator 函数内部捕获，第二个因为函数体内部的catch 函数已经执行过了，不会再捕获这个错误，所以这个错误就抛出 Generator 函数体，被函数体外的 catch 捕获。 yield*表达式 yield* 表达式表示 yield 返回一个遍历器对象，用于在 Generator 函数内部，调用另一个 Generator 函数。 1234567891011121314151617181920212223242526function* callee() &#123; console.log('callee: ' + (yield));&#125;function* caller() &#123; while (true) &#123; yield* callee(); &#125;&#125;const callerObj = caller();callerObj.next();// &#123;value: undefined, done: false&#125;callerObj.next("a");// callee: a// &#123;value: undefined, done: false&#125;callerObj.next("b");// callee: b// &#123;value: undefined, done: false&#125; // 等同于function* caller() &#123; while (true) &#123; for (var value of callee) &#123; yield value; &#125; &#125;&#125; 使用场景实现Iterator 为不具备 Iterator 接口的对象提供遍历方法。 12345678910111213function* objectEntries(obj) &#123; const propKeys = Reflect.ownKeys(obj); for (const propKey of propKeys) &#123; yield [propKey, obj[propKey]]; &#125;&#125; const jane = &#123; first: 'Jane', last: 'Doe' &#125;;for (const [key,value] of objectEntries(jane)) &#123; console.log(`$&#123;key&#125;: $&#123;value&#125;`);&#125;// first: Jane// last: Doe Reflect.ownKeys() 返回对象所有的属性，不管属性是否可枚举，包括 Symbol。 jane 原生是不具备 Iterator 接口无法通过 for… of遍历。这边用了 Generator 函数加上了 Iterator 接口，所以就可以遍历 jane 对象了。 ES6 async 函数asyncasync 是 ES7 才有的与异步操作有关的关键字，和 Promise ， Generator 有很大关联的。 语法1async function name([param[, param[, ... param]]]) &#123; statements &#125; name: 函数名称。 param: 要传递给函数的参数的名称。 statements: 函数体语句。 返回值async 函数返回一个 Promise 对象，可以使用 then 方法添加回调函数。 123456789async function helloAsync()&#123; return "helloAsync"; &#125; console.log(helloAsync()) // Promise &#123;&lt;resolved&gt;: "helloAsync"&#125; helloAsync().then(v=&gt;&#123; console.log(v); // helloAsync&#125;) async 函数中可能会有 await 表达式，async 函数执行时，如果遇到 await 就会先暂停执行 ，等到触发的异步操作完成后，恢复 async 函数的执行并返回解析值。 await 关键字仅在 async function 中有效。如果在 async function 函数体外使用 await ，你只会得到一个语法错误。 12345678910111213141516function testAwait()&#123; return new Promise((resolve) =&gt; &#123; setTimeout(function()&#123; console.log("testAwait"); resolve(); &#125;, 1000); &#125;);&#125; async function helloAsync()&#123; await testAwait(); console.log("helloAsync"); &#125;helloAsync();// testAwait// helloAsync awaitawait 操作符用于等待一个 Promise 对象, 它只能在异步函数 async function 内部使用。 语法1[return_value] = await expression; expression: 一个 Promise 对象或者任何要等待的值。 返回值返回 Promise 对象的处理结果。如果等待的不是 Promise 对象，则返回该值本身。 如果一个 Promise 被传递给一个 await 操作符，await 将等待 Promise 正常处理完成并返回其处理结果。 1234567891011121314function testAwait (x) &#123; return new Promise(resolve =&gt; &#123; setTimeout(() =&gt; &#123; resolve(x); &#125;, 2000); &#125;);&#125; async function helloAsync() &#123; var x = await testAwait ("hello world"); console.log(x); &#125;helloAsync ();// hello world 正常情况下，await 命令后面是一个 Promise 对象，它也可以跟其他值，如字符串，布尔值，数值以及普通函数。 12345678910function testAwait()&#123; console.log("testAwait");&#125;async function helloAsync()&#123; await testAwait(); console.log("helloAsync");&#125;helloAsync();// testAwait// helloAsync await针对所跟不同表达式的处理方式： Promise 对象：await 会暂停执行，等待 Promise 对象 resolve，然后恢复 async 函数的执行并返回解析值。 非 Promise 对象：直接返回对应的值。]]></content>
      <categories>
        <category>web前端</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6 新语法 (一)]]></title>
    <url>%2F2019%2F07%2F09%2F03.web%E5%89%8D%E7%AB%AF%2FJavaScript%2FES6%20%E6%96%B0%E8%AF%AD%E6%B3%95(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[ECMAScript简介介绍ES6， 全称 ECMAScript 6.0 ，是 JavaScript 的下一个版本标准，2015.06 发版。 ES6 主要是为了解决 ES5 的先天不足，比如 JavaScript 里并没有类的概念，但是目前浏览器的 JavaScript 是 ES5 版本，大多数高版本的浏览器也支持 ES6，不过只实现了 ES6 的部分特性和功能。 ECMAScript 的背景JavaScript 是大家所了解的语言名称，但是这个语言名称是商标（ Oracle 公司注册的商标）。因此，JavaScript 的正式名称是 ECMAScript 。1996年11月，JavaScript 的创造者网景公司将 JS 提交给国际化标准组织 ECMA（European computer manufactures association，欧洲计算机制造联合会），希望这种语言能够成为国际标准，随后 ECMA 发布了规定浏览器脚本语言的标准，即 ECMAScript。这也有利于这门语言的开放和中立。 ECMAScript的历史ES6 是 ECMAScript 标准十余年来变动最大的一个版本，为其添加了许多新的语法特性。 1997 年 ECMAScript 1.0 诞生。 1998 年 6 月 ECMAScript 2.0 诞生，包含一些小的更改，用于同步独立的 ISO 国际标准。 1999 年 12 月 ECMAScript 3.0诞生，它是一个巨大的成功，在业界得到了广泛的支持，它奠定了 JS 的基本语法，被其后版本完全继承。直到今天，我们一开始学习 JS ，其实就是在学 3.0 版的语法。 2000 年的 ECMAScript 4.0 是当下 ES6 的前身，但由于这个版本太过激烈，对 ES 3 做了彻底升级，所以暂时被”和谐”了。 2009 年 12 月，ECMAScript 5.0 版正式发布。ECMA 专家组预计 ECMAScript 的第五个版本会在 2013 年中期到 2018 年作为主流的开发标准。2011年6月，ES 5.1 版发布，并且成为 ISO 国际标准。 2013 年，ES6 草案冻结，不再添加新的功能，新的功能将被放到 ES7 中；2015年6月， ES6 正式通过，成为国际标准。 webpackwebpack介绍webpack 是一个现代 JavaScript 应用程序的静态模块打包器 (module bundler) 。当 webpack 处理应用程序时，它会递归地构建一个依赖关系图 (dependency graph) ，其中包含应用程序需要的每个模块，然后将所有这些模块打包成一个或多个 bundle 。 web的是个核心概念 入口 (entry) 输出 (output) loader 插件 (plugins) 入口(entry)入口会指示 webpack 应该使用哪个模块，来作为构建其内部依赖图的开始。进入入口起点后，webpack 会找出有哪些模块和库是入口起点（直接和间接）依赖的。在 webpack 中入口有多种方式来定义，如下面例子： 12345678910// 单个入口（简写）语法:const config = &#123; entry: "./src/main.js"&#125;// 对象语法:const config = &#123; app: "./src/main.js", vendors: "./src/vendors.js"&#125; 输出(output)output 属性会告诉 webpack 在哪里输出它创建的 bundles ，以及如何命名这些文件，默认值为 ./dist: 1234567const config = &#123; entry: "./src/main.js", output: &#123; filename: "bundle.js", path: path.resolve(__dirname, 'dist') &#125;&#125; loaderloader 让 webpack 可以去处理那些非 JavaScript 文件（ webpack 自身只理解 JavaScript ）。loader 可以将所有类型的文件转换为 webpack 能够有效处理的模块，例如，开发的时候使用 ES6 ，通过 loader 将 ES6 的语法转为 ES5 ，如下配置： 12345678910111213141516171819const config = &#123; entry: "./src/main.js", output: &#123; filename: "bundle.js", path: path.resolve(__dirname, 'dist') &#125;, module: &#123; rules: [ &#123; test: /\.js$/, exclude: /node_modules/, loader: "babel-loader", options: [ presets: ["env"] ] &#125; ] &#125;&#125; 插件(plugins)loader 被用于转换某些类型的模块，而插件则可以做更多的事情。包括打包优化、压缩、定义环境变量等等。插件的功能强大，是 webpack 扩展非常重要的利器，可以用来处理各种各样的任务。使用一个插件也非常容易，只需要 require() ，然后添加到 plugins 数组中。 12345678910111213141516171819// 通过 npm 安装const HtmlWebpackPlugin = require('html-webpack-plugin');// 用于访问内置插件 const webpack = require('webpack'); const config = &#123; module: &#123; rules: [ &#123; test: /\.js$/, exclude: /node_modules/, loader: "babel-loader" &#125; ] &#125;, plugins: [ new HtmlWebpackPlugin(&#123;template: './src/index.html'&#125;) ]&#125;; 利用 webpack 搭建应用1234567891011121314151617181920212223242526const path = require('path'); module.exports = &#123; mode: "development", // "production" | "development" // 选择 development 为开发模式， production 为生产模式 entry: "./src/main.js", output: &#123; filename: "bundle.js", path: path.resolve(__dirname, 'dist') &#125;, module: &#123; rules: [ &#123; test: /\.js$/, exclude: /node_modules/, loader: "babel-loader", options: [ presets: ["env"] ] &#125; ] &#125;, plugins: [ ... ]&#125; ES6 新语法特性let 与 constlet 的使用方法 let 生命的是块级的变量，声明的变量只在 let 命令所在的代码块内有效。 1234567891011121314151617181920212223242526&#123; let a = 0; a // 0&#125;a // 报错 ReferenceError: a is not defined&#123; let a = 0; var b = 1;&#125;a // ReferenceError: a is not definedb // 1// for 循环计数器很适合用 letfor (var i = 0; i &lt; 10; i++) &#123; setTimeout(function()&#123; console.log(i); &#125;)&#125;// 输出十个 10for (let j = 0; j &lt; 10; j++) &#123; setTimeout(function()&#123; console.log(j); &#125;)&#125;// 输出 0123456789 变量 i 是用 var 声明的，在全局范围内有效，所以全局中只有一个变量 i, 每次循环时，setTimeout 定时器里面的 i 指的是全局变量 i ，而循环里的十个 setTimeout 是在循环结束后才执行，所以此时的 i 都是 10。 变量 j 是用 let 声明的，当前的 i 只在本轮循环中有效，每次循环的 j 其实都是一个新的变量，所以 setTimeout 定时器里面的 j 其实是不同的变量，即最后输出12345。（若每次循环的变量 j 都是重新声明的，如何知道前一个循环的值？这是因为 JavaScript 引擎内部会记住前一个循环的值）。 let的特点 let 声明的变量代码块内有效； 不能重复声明； 不存在变量提升，在声明之前饮用会报错； const 命令const 声明一个只读变量，声明之后不允许改变。意味着，一旦声明必须初始化，否则会报错。 基本用法 1234const PI = "3.1415926";PI // 3.1415926const MY_AGE; // SyntaxError: Missing initializer in const declaration 暂时性死区 12345var PI = "a";if(true)&#123; console.log(PI); // ReferenceError: PI is not defined const PI = "3.1415926";&#125; ES6 明确规定，代码块内如果存在 let 或者 const，代码块会对这些命令声明的变量从块的开始就形成一个封闭作用域。代码块内，在声明变量 PI 之前使用它会报错。 解构赋值概念解构赋值是对赋值运算符的扩展。 他是一种针对数组或者对象进行模式匹配，然后对其中的变量进行赋值。 在代码书写上简洁且易读，语义更加清晰明了；也方便了复杂对象中数据字段获取。 解构模型在解构中，有下面两部分参与： 解构的源，解构赋值表达式的右边部分。解构的目标，解构赋值表达式的左边部分。 数组模型的解构(Array)基本 1let [a, b, c] = [1, 2, 3]; // a = 1 // b = 2 // c = 3 可嵌套 1let [a, [[b], c]] = [1, [[2], 3]]; // a = 1 // b = 2 // c = 3 可忽略 1let [a, , b] = [1, 2, 3]; // a = 1 // b = 3 不完全解构 1let [a = 1, b] = []; // a = 1, b = undefined 剩余运算符 1let [a, ...b] = [1, 2, 3]; //a = 1 //b = [2, 3] 字符串等 在数组的解构中，解构的目标若为可遍历对象，皆可进行解构赋值。可遍历对象即实现 Iterator 接口的数据。 1let [a, b, c, d, e] = 'hello'; // a = 'h' // b = 'e' // c = 'l' // d = 'l' // e = 'o' 解构默认值 123let [a = 2] = [undefined]; // a = 2// 当解构模式有匹配结果，且匹配结果是 undefined 时，会触发默认值作为返回结果。 12345678let [a = 3, b = a] = []; // a = 3, b = 3 let [a = 3, b = a] = [1]; // a = 1, b = 1 let [a = 3, b = a] = [1, 2]; // a = 1, b = 2// a 与 b 匹配结果为 undefined ，触发默认值：a = 3; b = a =3// a 正常解构赋值，匹配结果：a = 1，b 匹配结果 undefined ，触发默认值：b = a =1// a 与 b 正常解构赋值，匹配结果：a = 1，b = 2 对象模型的解构（Object）基本 123456789let &#123; foo, bar &#125; = &#123; foo: 'aaa', bar: 'bbb' &#125;; // foo = 'aaa' // bar = 'bbb' let &#123; baz : foo &#125; = &#123; baz : 'ddd' &#125;; // foo = 'ddd' 可嵌套可忽略 12345678910111213let obj = &#123;p: ['hello', &#123;y: 'world'&#125;] &#125;; let &#123;p: [x, &#123; y &#125;] &#125; = obj;// x = 'hello' // y = 'world' let obj = &#123;p: ['hello', &#123;y: 'world'&#125;] &#125;; let &#123;p: [x, &#123; &#125;] &#125; = obj; // x = 'hello' 不完全解构 1234567let obj = &#123;p: [&#123;y: 'world'&#125;] &#125;; let &#123;p: [&#123; y &#125;, x ] &#125; = obj; // x = undefined // y = 'world' 剩余运算符 1234567let &#123;a, b, ...rest&#125; = &#123;a: 10, b: 20, c: 30, d: 40&#125;; // a = 10 // b = 20 // rest = &#123;c: 30, d: 40&#125; 解构默认值 1234567let &#123;a = 10, b = 5&#125; = &#123;a: 3&#125;; // a = 3; b = 5; let &#123;a: aa = 10, b: bb = 5&#125; = &#123;a: 3&#125;; // aa = 3; bb = 5; ES6 Symbol概述ES6 引入了一种新的原始数据类型 Symbol ，表示独一无二的值，最大的用法是用来定义对象的唯一属性名。 ES6 数据类型除了 Number 、 String 、 Boolean 、 Objec t、 null 和 undefined ，还新增了 Symbol 。 基本用法Symbol 函数栈不能用 new 命令，因为 Symbol 是原始数据类型，不是对象。可以接受一个字符串作为参数，为新创建的 Symbol 提供描述，用来显示在控制台或者作为字符串的时候使用，便于区分。 1234567let sy = Symbol("KK");console.log(sy); // Symbol(KK)typeof(sy); // "symbol" // 相同参数 Symbol() 返回的值不相等let sy1 = Symbol("kk"); sy === sy1; // false 使用场景作为属性名 由于每一个 Symbol 的值都是不相等的，所以 Symbol 作为对象的属性名，可以保证属性不重名。 1234567891011121314151617let sy = Symbol("key1"); // 写法1let syObject = &#123;&#125;;syObject[sy] = "kk";console.log(syObject); // &#123;Symbol(key1): "kk"&#125; // 写法2let syObject = &#123; [sy]: "kk"&#125;;console.log(syObject); // &#123;Symbol(key1): "kk"&#125; // 写法3let syObject = &#123;&#125;;Object.defineProperty(syObject, sy, &#123;value: "kk"&#125;);console.log(syObject); // &#123;Symbol(key1): "kk"&#125; Symbol 作为对象属性名时不能用.运算符，要用方括号。因为.运算符后面是字符串，所以取到的是字符串 sy 属性，而不是 Symbol 值 sy 属性。 12345let syObject = &#123;&#125;;syObject[sy] = "kk"; syObject[sy]; // "kk"syObject.sy; // undefined 注意点 Symbol 值作为属性名时，该属性是公有属性不是私有属性，可以在类的外部访问。但是不会出现在 for…in 、 for…of 的循环中，也不会被 Object.keys() 、 Object.getOwnPropertyNames() 返回。如果要读取到一个对象的 Symbol 属性，可以通过 Object.getOwnPropertySymbols() 和 Reflect.ownKeys() 取到。 1234567891011let syObject = &#123;&#125;;syObject[sy] = "kk";console.log(syObject); for (let i in syObject) &#123; console.log(i);&#125; // 无输出 Object.keys(syObject); // []Object.getOwnPropertySymbols(syObject); // [Symbol(key1)]Reflect.ownKeys(syObject); // [Symbol(key1)] 定义常量 在 ES5 使用字符串表示常量。例如： 123const COLOR_RED = "red";const COLOR_YELLOW = "yellow";const COLOR_BLUE = "blue"; 但是用字符串不能保证常量是独特的，这样会引起一些问题： 12345678910111213141516171819const COLOR_RED = "red";const COLOR_YELLOW = "yellow";const COLOR_BLUE = "blue";const MY_BLUE = "blue"； function getConstantName(color) &#123; switch (color) &#123; case COLOR_RED : return "COLOR_RED"; case COLOR_YELLOW : return "COLOR_YELLOW "; case COLOR_BLUE: return "COLOR_BLUE"; case MY_BLUE: return "MY_BLUE"; default: throw new Exception('Can't find this color'); &#125;&#125; 但是使用 Symbol 定义常量，这样就可以保证这一组常量的值都不相等。用 Symbol 来修改上面的例子。 12345678910111213141516const COLOR_RED = Symbol("red");const COLOR_YELLOW = Symbol("yellow");const COLOR_BLUE = Symbol("blue"); function getConstantName(color) &#123; switch (color) &#123; case COLOR_RED : return "COLOR_RED"; case COLOR_YELLOW : return "COLOR_YELLOW "; case COLOR_BLUE: return "COLOR_BLUE"; default: throw new Exception('Can't find this color'); &#125;&#125; Symbol 的值是唯一的，所以不会出现相同值得常量，即可以保证 switch 按照代码预想的方式执行。 *Symbol.for() * Symbol.for() 类似单例模式，首先会在全局搜索被登记的 Symbol 中是否有该字符串参数作为名称的 Symbol 值，如果有即返回该 Symbol 值，若没有则新建并返回一个以该字符串参数为名称的 Symbol 值，并登记在全局环境中供搜索。 123456let yellow = Symbol("Yellow");let yellow1 = Symbol.for("Yellow");yellow === yellow1; // false let yellow2 = Symbol.for("Yellow");yellow1 === yellow2; // true Symbol.keyFor() 12let yellow1 = Symbol.for("Yellow");Symbol.keyFor(yellow1); // "Yellow" ES6 Map 与 SetMap 对象Map 对象保存键值对。任何值(对象或者原始值) 都可以作为一个键或一个值。 Maps 与Objects 的区别 一个 Object 的键只能是字符串或者 Symbols，但一个 Map 的键可以是任意值。 Map 中的键值是有序的（FIFO 原则），而添加到对象中的键则不是。 Map 的键值对个数可以从 size 属性获取，而 Object 的键值对个数只能手动计算。 Object 都有自己的原型，原型链上的键名有可能和你自己在对象上的设置的键名产生冲突。 Map 中的keykey 是字符串 123456789var myMap = new Map(); var keyString = "a string"; myMap.set(keyString, "和键'a string'关联的值"); myMap.get(keyString); // "和键'a string'关联的值" myMap.get("a string"); // "和键'a string'关联的值" ，因为 keyString === 'a string' key 是对象 123456789var myMap = new Map(); var keyObj = &#123;&#125; myMap.set(keyObj, "和键 keyObj 关联的值"); myMap.get(keyObj); // "和键 keyObj 关联的值" myMap.get(&#123;&#125;); // undefined, 因为 keyObj !== &#123;&#125; key 是函数 123456789var myMap = new Map(); var keyFunc = function () &#123;&#125;, // 函数 myMap.set(keyFunc, "和键 keyFunc 关联的值"); myMap.get(keyFunc); // "和键 keyFunc 关联的值" myMap.get(function() &#123;&#125;) // undefined, 因为 keyFunc !== function () &#123;&#125; key 是 NaN 12345678910var myMap = new Map(); myMap.set(NaN, "not a number"); myMap.get(NaN); // "not a number" var otherNaN = Number("foo"); myMap.get(otherNaN); // "not a number"// 虽然 NaN 和任何值甚至和自己都不相等(NaN !== NaN 返回true)，NaN作为Map的键来说是没有区别的。 Map 的迭代for…of 123456789101112131415161718192021222324var myMap = new Map();myMap.set(0, "zero");myMap.set(1, "one"); // 将会显示两个 log。 一个是 "0 = zero" 另一个是 "1 = one"for (var [key, value] of myMap) &#123; console.log(key + " = " + value);&#125;for (var [key, value] of myMap.entries()) &#123; console.log(key + " = " + value);&#125;/* 这个 entries 方法返回一个新的 Iterator 对象，它按插入顺序包含了 Map 对象中每个元素的 [key, value] 数组。 */ // 将会显示两个log。 一个是 "0" 另一个是 "1"for (var key of myMap.keys()) &#123; console.log(key);&#125;/* 这个 keys 方法返回一个新的 Iterator 对象， 它按插入顺序包含了 Map 对象中每个元素的键。 */ // 将会显示两个log。 一个是 "zero" 另一个是 "one"for (var value of myMap.values()) &#123; console.log(value);&#125;/* 这个 values 方法返回一个新的 Iterator 对象，它按插入顺序包含了 Map 对象中每个元素的值。 */ forEach 12345678var myMap = new Map();myMap.set(0, "zero");myMap.set(1, "one"); // 将会显示两个 logs。 一个是 "0 = zero" 另一个是 "1 = one"myMap.forEach(function(value, key) &#123; console.log(key + " = " + value);&#125;, myMap) Map对象的操作Map 与 Array的转换 1234567var kvArray = [["key1", "value1"], ["key2", "value2"]]; // Map 构造函数可以将一个 二维 键值对数组转换成一个 Map 对象var myMap = new Map(kvArray); // 使用 Array.from 函数可以将一个 Map 对象转换成一个二维键值对数组var outArray = Array.from(myMap); Map 的克隆 12345var myMap1 = new Map([["key1", "value1"], ["key2", "value2"]]);var myMap2 = new Map(myMap1); console.log(original === clone); // 打印 false。 Map 对象构造函数生成实例，迭代出新的对象。 Map 的合并 12345var first = new Map([[1, 'one'], [2, 'two'], [3, 'three'],]);var second = new Map([[1, 'uno'], [2, 'dos']]); // 合并两个 Map 对象时，如果有重复的键值，则后面的会覆盖前面的，对应值即 uno，dos， threevar merged = new Map([...first, ...second]); Set 对象Set 对象允许你存储任何类型的唯一值，无论是原始值或者是对象引用。 Set 中的特殊值 Set 对象存储的值总是唯一的，所以需要判断两个值是否恒等。有几个特殊值需要特殊对待： +0 与 -0 在存储判断唯一性的时候是恒等的，所以不重复； undefined 与 undefined 是恒等的，所以不重复； NaN 与 NaN 是不恒等的，但是在 Set 中只能存一个，不重复。 基本使用 123456789101112let mySet = new Set(); mySet.add(1); // Set(1) &#123;1&#125;mySet.add(5); // Set(2) &#123;1, 5&#125;mySet.add(5); // Set(2) &#123;1, 5&#125; 这里体现了值的唯一性mySet.add("some text"); // Set(3) &#123;1, 5, "some text"&#125; 这里体现了类型的多样性var o = &#123;a: 1, b: 2&#125;; mySet.add(o);mySet.add(&#123;a: 1, b: 2&#125;); // Set(5) &#123;1, 5, "some text", &#123;…&#125;, &#123;…&#125;&#125; // 这里体现了对象之间引用不同不恒等，即使值相同，Set 也能存储 Set 类型转换 12345678// Array 转 Setvar mySet = new Set(["value1", "value2", "value3"]);// 用...操作符，将 Set 转 Arrayvar myArray = [...mySet];String// String 转 Setvar mySet = new Set('hello'); // Set(4) &#123;"h", "e", "l", "o"&#125;// 注：Set 中 toString 方法是不能将 Set 转换成 String Set 对象的作用数组去重 123var mySet = new Set([1, 2, 3, 4, 4]); [...mySet]; // [1, 2, 3, 4] 并集 12345var a = new Set([1, 2, 3]); var b = new Set([4, 3, 2]); var union = new Set([...a, ...b]); // &#123;1, 2, 3, 4&#125; 交集 12345var a = new Set([1, 2, 3]); var b = new Set([4, 3, 2]); var intersect = new Set([...a].filter(x =&gt; b.has(x))); // &#123;2, 3&#125; 差集 12345var a = new Set([1, 2, 3]); var b = new Set([4, 3, 2]); var difference = new Set([...a].filter(x =&gt; !b.has(x))); // &#123;1&#125; ES6 Reflect 与 Proxy概述Proxy 与 Reflect 是 ES6 为了操作对象引入的 API 。 Proxy 可以对目标对象的读取、函数调用等操作进行拦截，然后进行操作处理。它不直接操作对象，而是像代理模式，通过对象的代理对象进行操作，在进行这些操作时，可以添加一些需要的额外操作。 Reflect 可以用于获取目标对象的行为，它与 Object 类似，但是更易读，为操作对象提供了一种更优雅的方式。它的方法与 Proxy 是对应的。 Proxy基本用法一个 Proxy 对象由两个部分组成： target 、 handler 。 在通过 Proxy 构造函数生成实例对象时，需要提供这两个参数。 target 即目标对象， handler 是一个对象，声明了代理 target 的指定行为。 1234567891011121314151617181920212223242526272829303132333435363738394041424344let target = &#123; name: 'Tom', age: 24&#125;let handler = &#123; get: function(target, key) &#123; console.log('getting '+key); return target[key]; // 不是target.key &#125;, set: function(target, key, value) &#123; console.log('setting '+key); target[key] = value; &#125;&#125;let proxy = new Proxy(target, handler)proxy.name // 实际执行 handler.getproxy.age = 25 // 实际执行 handler.set// getting name// setting age// 25 // target 可以为空对象let targetEpt = &#123;&#125;let proxyEpt = new Proxy(targetEpt, handler)// 调用 get 方法，此时目标对象为空，没有 name 属性proxyEpt.name // getting name// 调用 set 方法，向目标对象中添加了 name 属性proxyEpt.name = 'Tom'// setting name// "Tom"// 再次调用 get ，此时已经存在 name 属性proxyEpt.name// getting name// "Tom" // 通过构造函数新建实例时其实是对目标对象进行了浅拷贝，因此目标对象与代理对象会互相影响targetEpt)// &#123;name: "Tom"&#125; // handler 对象也可以为空，相当于不设置拦截操作，直接访问目标对象let targetEmpty = &#123;&#125;let proxyEmpty = new Proxy(targetEmpty,&#123;&#125;)proxyEmpty.name = "Tom"targetEmpty) // &#123;name: "Tom"&#125; Proxy实例方法get 方法 1get(target, propKey, receiver) 用于 target 对象上 propKey 的读取操作。 12345678910111213let exam =&#123; name: "Tom", age: 24&#125;let proxy = new Proxy(exam, &#123; get(target, propKey, receiver) &#123; console.log('Getting ' + propKey); return target[propKey]; &#125;&#125;)proxy.name // Getting name// "Tom" get() 方法可以继承。 1234567891011121314let proxy = new Proxy(&#123;&#125;, &#123; get(target, propKey, receiver) &#123; // 实现私有属性读取保护 if(propKey[0] === '_')&#123; throw new Erro(`Invalid attempt to get private "$&#123;propKey&#125;"`); &#125; console.log('Getting ' + propKey); return target[propKey]; &#125;&#125;); let obj = Object.create(proxy);obj.name// Getting name set 方法 1set(target, propKey, value, receiver) 用于拦截 target 对象上的 propKey 的赋值操作。如果目标对象自身的某个属性，不可写且不可配置，那么set方法将不起作用。 12345678910111213141516171819let validator = &#123; set: function(obj, prop, value) &#123; if (prop === 'age') &#123; if (!Number.isInteger(value)) &#123; throw new TypeError('The age is not an integer'); &#125; if (value &gt; 200) &#123; throw new RangeError('The age seems invalid'); &#125; &#125; // 对于满足条件的 age 属性以及其他属性，直接保存 obj[prop] = value; &#125;&#125;;let proxy= new Proxy(&#123;&#125;, validator)proxy.age = 100;proxy.age // 100proxy.age = 'oppps' // 报错proxy.age = 300 // 报错 第四个参数 receiver 表示原始操作行为所在对象，一般是 Proxy 实例本身。 12345678910111213const handler = &#123; set: function(obj, prop, value, receiver) &#123; obj[prop] = receiver; &#125;&#125;;const proxy = new Proxy(&#123;&#125;, handler);proxy.name= 'Tom';proxy.name=== proxy // true const exam = &#123;&#125;Object.setPrototypeOf(exam, proxy)exam.name = "Tom"exam.name === exam // true 注意，严格模式下，set代理如果没有返回true，就会报错。 apply 方法 1apply(target, ctx, args) 用于拦截函数的调用、call 和 reply 操作。target 表示目标对象，ctx 表示目标对象上下文，args 表示目标对象的参数数组。 12345678910111213function sub(a, b)&#123; return a - b;&#125;let handler = &#123; apply: function(target, ctx, args)&#123; console.log('handle apply'); return Reflect.apply(...arguments); &#125;&#125;let proxy = new Proxy(sub, handler)proxy(2, 1) // handle apply// 1 has 方法 1has(target, propKey) 用于拦截 HasProperty 操作，即在判断 target 对象是否存在 propKey 属性时，会被这个方法拦截。此方法不判断一个属性是对象自身的属性，还是继承的属性。 1234567891011let handler = &#123; has: function(target, propKey)&#123; console.log("handle has"); return propKey in target; &#125;&#125;let exam = &#123;name: "Tom"&#125;let proxy = new Proxy(exam, handler)'name' in proxy// handle has// true 注意：此方法不拦截 for … in 循环。 construct 方法 1construct(target, args) 用于拦截 new 命令。返回值必须为对象。 1234567891011121314let handler = &#123; construct: function(target, args, newTarget)&#123; console.log("handle construct"); return Reflect.construct(target, args, newTarget); &#125; &#125;class exam = &#123; constructor(name)&#123; this.name = name; &#125;&#125;let proxy = new Proxy(exam,handler)new proxy("Tom")// handle construct// exam &#123;name: "Tom"&#125; deleteProperty 方法 1deleteProperty(target, propKey) 用于拦截 delete 操作，如果这个方法抛出错误或者返回 false ，propKey 属性就无法被 delete 命令删除。 defineProperty 方法 1defineProperty(target, propKey, propDesc) 用于拦截 Object.definePro若目标对象不可扩展，增加目标对象上不存在的属性会报错； 若属性不可写或不可配置，则不能改变这些属性。 123456789101112131415161718192021222324let handler = &#123; defineProperty: function(target, propKey, propDesc)&#123; console.log("handle defineProperty"); return true; &#125;&#125;plet target = &#123;&#125;let proxy = new Proxy(target, handler)proxy.name = "Tom"// handle definePropertytarget// &#123;name: "Tom"&#125; // defineProperty 返回值为false，添加属性操作无效let handler1 = &#123; defineProperty: function(target, propKey, propDesc)&#123; console.log("handle defineProperty"); return false; &#125;&#125;let target1 = &#123;&#125;let proxy1 = new Proxy(target1, handler1)proxy1.name = "Jerry"target1// &#123;&#125; getOwnPropertyDescriptor方法 1getOwnPropertyDescriptor(target, propKey) 用于拦截 Object.getOwnPropertyD() 返回值为属性描述对象或者 undefined 。 123456789let handler = &#123; getOwnPropertyDescriptor: function(target, propKey)&#123; return Object.getOwnPropertyDescriptor(target, propKey); &#125;&#125;ilet target = &#123;name: "Tom"&#125;let proxy = new Proxy(target, handler)Object.getOwnPropertyDescriptor(proxy, 'name')// &#123;value: "Tom", writable: true, enumerable: true, configurable: // true&#125; getPrototypeOf 方法 1getPrototypeOf(target) 主要用于拦截获取对象原型的操作。包括以下操作： Object.prototype.proto Object.prototype.isPrototypeOf() Object.getPrototypeOf() Reflect.getPrototypeOf() instanceof 1234567let exam = &#123;&#125;let proxy = new Proxy(&#123;&#125;,&#123; getPrototypeOf: function(target)&#123; return exam; &#125;&#125;)Object.getPrototypeOf(proxy) // &#123;&#125; 注意，返回值必须是对象或者 null ，否则报错。另外，如果目标对象不可扩展（non-extensible），getPrototypeOf 方法必须返回目标对象的原型对象。 1234567let proxy = new Proxy(&#123;&#125;,&#123; getPrototypeOf: function(target)&#123; return true; &#125;&#125;)Object.getPrototypeOf(proxy)// TypeError: 'getPrototypeOf' on proxy: trap returned neither object // nor null isExtensible 方法 1isExtensible(target) 用于拦截 Object.isExtensible 操作。 该方法只能返回布尔值，否则返回值会被自动转为布尔值。 123456let proxy = new Proxy(&#123;&#125;,&#123; isExtensible:function(target)&#123; return true; &#125;&#125;)Object.isExtensible(proxy) // true 注意：它的返回值必须与目标对象的isExtensible属性保持一致，否则会抛出错误。 12345678let proxy = new Proxy(&#123;&#125;,&#123; isExtensible:function(target)&#123; return false; &#125;&#125;)Object.isExtensible(proxy)// TypeError: 'isExtensible' on proxy: trap result does not reflect // extensibility of proxy target (which is 'true') ownKeys 方法 1ownKeys(target) 用于拦截对象自身属性的读取操作。主要包括以下操作： Object.getOwnPropertyNames() Object.getOwnPropertySymbols() Object.keys() or…in 方法返回的数组成员，只能是字符串或 Symbol 值，否则会报错。 若目标对象中含有不可配置的属性，则必须将这些属性在结果中返回，否则就会报错。 若目标对象不可扩展，则必须全部返回且只能返回目标对象包含的所有属性，不能包含不存在的属性，否则也会报错。 123456789101112131415161718192021222324252627282930313233let proxy = new Proxy( &#123; name: "Tom", age: 24&#125;, &#123; ownKeys(target) &#123; return ['name']; &#125;&#125;);Object.keys(proxy)// [ 'name' ]f返回结果中，三类属性会被过滤：// - 目标对象上没有的属性// - 属性名为 Symbol 值的属性// - 不可遍历的属性 let target = &#123; name: "Tom", [Symbol.for('age')]: 24,&#125;;// 添加不可遍历属性 'gender'Object.defineProperty(target, 'gender', &#123; enumerable: false, configurable: true, writable: true, value: 'male'&#125;);let handler = &#123; ownKeys(target) &#123; return ['name', 'parent', Symbol.for('age'), 'gender']; &#125;&#125;;let proxy = new Proxy(target, handler);Object.keys(proxy)// ['name'] preventExtensions 方法 1preventExtensions(target) 拦截 Object.preventExtensions 操作。 该方法必须返回一个布尔值，否则会自动转为布尔值。 1234567891011121314151617181920// 只有目标对象不可扩展时（即 Object.isExtensible(proxy) 为 false ），// proxy.preventExtensions 才能返回 true ，否则会报错var proxy = new Proxy(&#123;&#125;, &#123; preventExtensions: function(target) &#123; return true; &#125;&#125;);// 由于 proxy.preventExtensions 返回 true，此处也会返回 true，因此会报错Object.preventExtensions(proxy) 被// TypeError: 'preventExtensions' on proxy: trap returned truish but // the proxy target is extensible // 解决方案 var proxy = new Proxy(&#123;&#125;, &#123; preventExtensions: function(target) &#123; // 返回前先调用 Object.preventExtensions Object.preventExtensions(target); return true; &#125;&#125;);Object.preventExtensions(proxy)// Proxy &#123;&#125; setPrototypeOf 方法 1setPrototypeOf 主要用来拦截 Object.setPrototypeOf 方法。 返回值必须为布尔值，否则会被自动转为布尔值。 若目标对象不可扩展，setPrototypeOf 方法不得改变目标对象的原型。 12345678910let proto = &#123;&#125;let proxy = new Proxy(function () &#123;&#125;, &#123; setPrototypeOf: function(target, proto) &#123; console.log("setPrototypeOf"); return true; &#125;&#125;);Object.setPrototypeOf(proxy, proto);// setPrototypeOf revocable 方法 用于返回一个可取消的 Proxy 实例。 12345let &#123;proxy, revoke&#125; = Proxy.revocable(&#123;&#125;, &#123;&#125;);proxy.name = "Tom";revoke();proxy.name // TypeError: Cannot perform 'get' on a proxy that has been revoked Reflect介绍ES6 中将 Object 的一些明显属于语言内部的方法移植到了 Reflect 对象上（当前某些方法会同时存在于 Object 和 Reflect 对象上），未来的新方法会只部署在 Reflect 对象上。 Reflect 对象对某些方法的返回结果进行了修改，使其更合理。 Reflect 对象使用函数的方式实现了 Object 的命令式操作。 Reflect静态方法Reflect.get 1Reflect.get(target, name, receiver) 查找并返回 target 对象的 name 属性。 123456789101112131415161718192021let exam = &#123; name: "Tom", age: 24, get info()&#123; return this.name + this.age; &#125;&#125;Reflect.get(exam, 'name'); // "Tom" // 当 target 对象中存在 name 属性的 getter 方法， getter 方法的 this 会绑定 // receiverlet receiver = &#123; name: "Jerry", age: 20&#125;Reflect.get(exam, 'info', receiver); // Jerry20 // 当 name 为不存在于 target 对象的属性时，返回 undefinedReflect.get(exam, 'birth'); // undefined // 当 target 不是对象时，会报错Reflect.get(1, 'name'); // TypeError Reflect.set 1Reflect.set(target, name, value, receiver) 将 target 的 name 属性设置为 value。返回值为 boolean ，true 表示修改成功，false 表示失败。当 target 为不存在的对象时，会报错。 123456789101112131415161718192021222324252627let exam = &#123; name: "Tom", age: 24, set info(value)&#123; return this.age = value; &#125;&#125;exam.age; // 24Reflect.set(exam, 'age', 25); // trueexam.age; // 25 // value 为空时会将 name 属性清除Reflect.set(exam, 'age', ); // trueexam.age; // undefined // 当 target 对象中存在 name 属性 setter 方法时，setter 方法中的 this 会绑定 // receiver , 所以修改的实际上是 receiver 的属性,let receiver = &#123; age: 18&#125;Reflect.set(exam, 'info', 1, receiver); // truereceiver.age; // 1 let receiver1 = &#123; name: 'oppps'&#125;Reflect.set(exam, 'info', 1, receiver1);receiver1.age; // 1 Reflect.has 1Reflect.has(obj, name) 是 name in obj 指令的函数化，用于查找 name 属性在 obj 对象中是否存在。返回值为 boolean。如果 obj 不是对象则会报错 TypeError。 12345let exam = &#123; name: "Tom", age: 24&#125;Reflect.has(exam, 'name'); // true Reflect.deleteProperty 1Reflect.deleteProperty(obj, property) 是 delete obj[property] 的函数化，用于删除 obj 对象的 property 属性，返回值为 boolean。如果 obj 不是对象则会报错 TypeError。 12345678let exam = &#123; name: "Tom", age: 24&#125;Reflect.deleteProperty(exam , 'name'); // trueexam // &#123;age: 24&#125; // property 不存在时，也会返回 trueReflect.deleteProperty(exam , 'name'); // true Reflect.construct 1Reflect.construct(obj, args) 等同于 new target(…args)。 1234function exam(name)&#123; this.name = name;&#125;Reflect.construct(exam, ['Tom']); // exam &#123;name: "Tom"&#125; Reflect.getPrototypeOf 1Reflect.getPrototypeOf(obj) 用于读取 obj 的 _proto_ 属性。在 obj 不是对象时不会像 Object 一样把 obj 转为对象，而是会报错。 123class Exam&#123;&#125;let obj = new Exam()Reflect.getPrototypeOf(obj) === Exam.prototype // true Reflect.setPrototypeOf 1Reflect.setPrototypeOf(obj, newProto) 用于设置目标对象的 prototype。 12let obj =&#123;&#125;Reflect.setPrototypeOf(obj, Array.prototype); // true Reflect.apply 1Reflect.apply(func, thisArg, args) 等同于 Function.prototype.apply.call(func, thisArg, args) 。func 表示目标函数；thisArg 表示目标函数绑定的 this 对象；args 表示目标函数调用时传入的参数列表，可以是数组或类似数组的对象。若目标函数无法调用，会抛出 TypeError 。 1Reflect.apply(Math.max, Math, [1, 3, 5, 3, 1]); // 5 Reflect.defineProperty 1Reflect.defineProperty(target, propertyKey, attributes) 用于为目标对象定义属性。如果 target 不是对象，会抛出错误。 1234let myDate= &#123;&#125;Reflect.defineProperty(MyDate, 'now', &#123; value: () =&gt; Date.now()&#125;); // true Reflect.getOwnPropertyDescriptor 1Reflect.getOwnPropertyDescriptor(target, propertyKey) 用于得到 target 对象的 propertyKey 属性的描述对象。在 target 不是对象时，会抛出错误表示参数非法，不会将非对象转换为对象。 123456789101112var exam = &#123;&#125;Reflect.defineProperty(exam, 'name', &#123; value: true, enumerable: false,&#125;)Reflect.getOwnPropertyDescriptor(exam, 'name')// &#123; configurable: false, enumerable: false, value: true, writable:// false&#125; // propertyKey 属性在 target 对象中不存在时，返回 undefinedReflect.getOwnPropertyDescriptor(exam, 'age') // undefined Reflect.isExtensible 1Reflect.isExtensible(target) 用于判断 target 对象是否可扩展。返回值为 boolean 。如果 target 参数不是对象，会抛出错误。 12let exam = &#123;&#125;Reflect.isExtensible(exam) // true Reflect.preventExtensions 1Reflect.preventExtensions(target) 用于让 target 对象变为不可扩展。如果 target 参数不是对象，会抛出错误。 12let exam = &#123;&#125;Reflect.preventExtensions(exam) // true Reflect.ownKeys 1Reflect.ownKeys(target) 用于返回 target 对象的所有属性，等同于 Object.getOwnPropertyNames 与Object.getOwnPropertySymbols 之和。 12345var exam = &#123; name: 1, [Symbol.for('age')]: 4&#125;Reflect.ownKeys(exam) // ["name", Symbol(age)] Proxy 和 Reflect组合使用Reflect 对象的方法与 Proxy 对象的方法是一一对应的。所以 Proxy 对象的方法可以通过调用 Reflect 对象的方法获取默认行为，然后进行额外操作。 1234567891011121314151617181920let exam = &#123; name: "Tom", age: 24&#125;let handler = &#123; get: function(target, key)&#123; console.log("getting "+key); return Reflect.get(target, key); &#125;, set: function(target, key, value)&#123; console.log("setting "+key+" to "+value) Reflect.set(target, key, value); &#125;&#125;let proxy = new Proxy(exam, handler)proxy.name = "Jerry"proxy.name// setting name to Jerry// getting name// "Jerry" 使用场景拓展 1234567891011121314// 定义 Set 集合const queuedObservers = new Set();// 把观察者函数都放入 Set 集合中const observe = fn =&gt; queuedObservers.add(fn);// observable 返回原始对象的代理，拦截赋值操作const observable = obj =&gt; new Proxy(obj, &#123;set&#125;);function set(target, key, value, receiver) &#123; // 获取对象的赋值操作 const result = Reflect.set(target, key, value, receiver); // 执行所有观察者 queuedObservers.forEach(observer =&gt; observer()); // 执行赋值操作 return result;&#125;]]></content>
      <categories>
        <category>web前端</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GlusterFS 原理及使用]]></title>
    <url>%2F2019%2F07%2F01%2F10.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2FGlusterFS%2FGlusterFS%2F</url>
    <content type="text"><![CDATA[Gluster 简介Gluster 概述 Glusterfs是一个开源的分布式文件系统； 是Scale存储的核心,能够处理千数量级的客户端.在传统的解决方案中Glusterfs能够灵活的结合物理的,虚拟的和云资源去体现高可用和企业级的性能存储. Glusterfs通过TCP/IP或InfiniBand RDMA网络链接将客户端的存储资块源聚集在一起； 使用单一的全局命名空间来管理数据,磁盘和内存资源. Glusterfs基于堆叠的用户空间设计,可以为不同的工作负载提供高优的性能. Glusterfs支持运行在任何标准IP网络上标准应用程序的标准客户端，用户可以在全局统一的命名空间中使用NFS/CIFS等标准协议来访问应用数据. Gluster 主要特征扩展性和高性能GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（Elastic Hash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。 高可用性GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT3、ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问。 全局统一命名空间全局统一命名空间将磁盘和内存资源聚集成一个单一的虚拟存储池，对上层用户和应用屏蔽了底层的物理硬件。存储资源可以根据需要在虚拟存储池中进行弹性扩展，比如扩容或收缩。当存储虚拟机映像时，存储的虚拟映像文件没有数量限制，成千虚拟机均通过单一挂载点进行数据共享。虚拟机I/O可在命名空间内的所有服务器上自动进行负载均衡，消除了SAN环境中经常发生的访问热点和性能瓶颈问题。 弹性哈希算法GlusterFS采用弹性哈希算法在存储池中定位数据，而不是采用集中式或分布式元数据服务器索引。在其他的Scale-Out存储系统中，元数据服务器通常会导致I/O性能瓶颈和单点故障问题。GlusterFS中，所有在Scale-Out存储配置中的存储系统都可以智能地定位任意数据分片，不需要查看索引或者向其他服务器查询。这种设计机制完全并行化了数据访问，实现了真正的线性性能扩展。 基于标准协议Gluster存储服务支持NFS, CIFS, HTTP, FTP以及Gluster原生协议，完全与POSIX标准兼容。现有应用程序不需要作任何修改或使用专用API，就可以对Gluster中的数据进行访问。这在公有云环境中部署Gluster时非常有用，Gluster对云服务提供商专用API进行抽象，然后提供标准POSIX接口。 显著优点 开源，且网上的文档基本上足够你维护一套简单的存储系统。 支持多客户端处理，当前已知的支持千级别及以上规模客户。 支持POSIX&lt;简单来说，软件跨平台，也可以看找的其他博客说明，见‘参考文件’&gt;。 支持各种低端硬件，当然并不推崇，但是可以实现。我在线上用过8核，32G，依然跑的很溜。 支持NFS/SMB/CIFS/GLUSTERFS等行业标准协议访问。 提供各种优秀的功能，如磁盘配额、复制式、分布式、快照、性能检测命令等。 支持大容量存储，当前已知的支持PB及以上规模存储。 可以使用任何支持扩展属性的ondisk文件系统，比如xattr. 强大且简单的扩展能力，你可以在任何时候快速扩容。 自动配置故障转移，在故障发生时，无需任何操作即可恢复数据，且最新副本依然从仍在运行的节点获取。 术语介绍Brick: GFS中的存储单元，通常是一个受信存储池中的服务器的一个导出目录。可以通过主机名和目录名来标识，如’SERVER:EXPORT’ Client: 挂载了GFS卷的设备 Extended Attributes: xattr是一个文件系统的特性，其支持用户或程序关联文件/目录和元数据。 FUSE: Filesystem Userspace是一个可加载的内核模块，其支持非特权用户创建自己的文件系统而不需要修改内核代码。通过在用户空间运行文件系统的代码通过FUSE代码与内核进行桥接。 GFID: GFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode Namespace: 每个Gluster卷都导出单个ns作为POSIX的挂载点 Node: 一个拥有若干brick的设备 RDMA: 远程直接内存访问，支持不通过双方的OS进行直接内存访问。 RRDNS: round robin DNS是一种通过DNS轮转返回不同的设备以进行负载均衡的方法 Self-heal: 用于后台运行检测复本卷中文件和目录的不一致性并解决这些不一致。 Split-brain: 脑裂 Volfile: glusterfs进程的配置文件，通常位于/var/lib/glusterd/vols/volname Volume: 一组bricks的逻辑集合 工作原理 首先是在客户端， 用户通过glusterfs的mount point 来读写数据， 对于用户来说，集群系统的存在对用户是完全透明的，用户感觉不到是操作本地系统还是远端的集群系统。 用户的这个操作被递交给 本地linux系统的VFS来处理。 VFS 将数据递交给FUSE 内核文件系统:在启动 glusterfs 客户端以前，需要想系统注册一个实际的文件系统FUSE,如上图所示，该文件系统与ext3在同一个层次上面， ext3 是对实际的磁盘进行处理， 而fuse 文件系统则是将数据通过/dev/fuse 这个设备文件递交给了glusterfs client端。所以， 我们可以将 fuse文件系统理解为一个代理。 数据被fuse 递交给Glusterfs client 后， client 对数据进行一些指定的处理（所谓的指定，是按照client 配置文件据来进行的一系列处理， 我们在启动glusterfs client 时需要指定这个文件。 在glusterfs client的处理末端，通过网络将数据递交给 Glusterfs Server，并且将数据写入到服务器所控制的存储设备上。 GlusterFS 整体工作流 1，只要安装了glusterFS，会创建一个gluster管理守护进程(glusterd)二进制文件，该守护进程应该在集群中的所有设备上运行。 2，启动glusterd后，可以创建一个由所有存储服务器节点组成的受信任的服务器池(TSP可以包含单个节点) 3，作为基本存储单元的brick可以再这些服务器中作为导出目录创建。 4，从这个TSP的任何数量的brick都可以被联合起来形成一个整体。 5，一旦创建了volume，glusterfsd进程就会开始在每个参与的brick中运行，除此之外，将在/var/lib/glusterd/vols/生成为vol文件的配置文件。 6，volume中将会有与每个brick对应的配置文件，文件中包含关于特定brick的所有内容。 7，客户端进程所需的配置文件也将被创建。 8，文件系统完成，挂载使用。 9，挂载的IP/主机名可以是受信任服务器池中创建所需volume的任何节点的IP/主机名。 10，当我们在客户端安装volume时，客户端glusterfs进程与服务器的glusterd进程进行通信。 11，服务器glusterd进程发送一个配置文件(vol)文件，其中包括客户端转换器列表，另一个包含volume中每个brick的信息。 12，借助于该文件，客户端glusterfs进程可以与每个brick的glusterfsd进行通信。 13，当挂载的文件系统中，客户端发出系统调用&lt;文件操作或Fop或打开文件&gt;时， 14，VFS识别文件系统类型为glusterfs，会将请求发送到FUSE内核模块。 15，FUSE内核模块将通过/dev/fuse将其发送到客户机节点的用户空间中的glusterFS。 16，客户端上的glusterFS进程由一堆成为客户端翻译器的翻译器组成。这些翻译器在存储服务器glusterd进程发送的配置文件(vol文件)中定义。 17，这些翻译器中的第一个由FUSE库(libfuse)组成的FUSE翻译器。 18，每个翻译器都具有与每个文件操作对应的功能或glusterfs支持的fop。 19，该请求将在每个翻译器中发挥相应的功能。 常用volume 卷类型分布（distributed）默认模式，既DHT, 也叫 分布卷: 将文件已hash算法随机分布到 一台服务器节点中存储。 复制（replicate）复制模式，既AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。 条带（striped）条带模式，既Striped, 创建volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。 分布式条带卷(distribute stripe volume)分布式条带模式（组合型），最少需要4台服务器才能创建。 创建volume 时 stripe 2 server = 4 个节点： 是DHT 与 Striped 的组合型。 分布式复制卷(distribute replica volume)分布式复制模式（组合型）, 最少需要4台服务器才能创建。 创建volume 时 replica 2 server = 4 个节点：是DHT 与 AFR 的组合型。 条带复制卷(stripe replica volume)条带复制卷模式（组合型）, 最少需要4台服务器才能创建。 创建volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。 分布式条带复制卷(distribute stripe replicavolume) 三种模式混合, 至少需要8台 服务器才能创建。 stripe 2 replica 2 , 每4个节点 组成一个 组。 环境准备前置条件 至少需要两台设备/节点。 具备正常的网络连接 至少需要两个磁盘，一个用于OS安装，一个用于服务gluster存储； 环境配置关闭防火墙 12systemctl stop firewalld.service #停止firewalldsystemctl disable firewalld.service #禁止firewalld开机自启 关闭SELinux 123sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config #关闭SELinuxsetenforce 0getenforce 配置host文件 123410.0.0.101 node0110.0.0.102 node0210.0.0.103 node0310.0.0.104 node04 同步时间 1ntpdate time.windows.com #同步时间 *安装 gluster 源 * 1yum -y install centos-release-gluster312.noarch 安装glusterfs 1yum -y --enablerepo=centos-gluster*-test install glusterfs-server glusterfs-cli glusterfs-geo-replication 启动gluster， 并设置开机自启动 12345glusterfs -vsystemctl start glusterd.servicesystemctl enable glusterd.servicesystemctl status glusterd.servicenetstat -lntup | grep gluster 格式化磁盘(全部gluster ) 在每台主机上创建几块硬盘，做接下来的分布式存储使用 注：创建的硬盘要用xfs格式来格式化硬盘，如果用ext4来格式化硬盘的话，对于大于16TB空间格式化就无法实现了。所以这里要用xfs格式化磁盘(centos7默认的文件格式就是xfs)，并且xfs的文件格式支持PB级的数据量 123456789fdisk -l # 查看所有磁盘设备mkfs.xfs -i size=512 /dev/sdb # 格式化磁盘, 一块磁盘相当于一个brickmkfs.xfs -i size=512 /dev/sdcmkfs.xfs -i size=512 /dev/sddmkdir -p /data/brick&#123;1..3&#125; # 创建挂载块设备的目录echo '/dev/sdb /data/brick1 xfs defaults 0 0' &gt;&gt; /etc/fstabecho '/dev/sdc /data/brick2 xfs defaults 0 0' &gt;&gt; /etc/fstabecho '/dev/sdd /data/brick3 xfs defaults 0 0' &gt;&gt; /etc/fstabmount -a 将server 主机加入到信任池 随便在一个开启glusterfs服务的主机上将其他主机加入到一个信任的主机池里，这里选择node01 1234567gluster peer probe node01gluster peer probe node02gluster peer probe node03gluster peer probe node04gluster peer detach xxx # 用来将节点从信任池中删除gluster peer status # 查看信任主机池状态# 当客户端需要挂载卷的时候，也需要加入到信任主机池中 gluster 卷 操作GlusterFS 五种卷的创建注意 Distributed：分布式卷，文件通过 hash 算法随机分布到由 bricks 组成的卷上。 Replicated: 复制式卷，类似 RAID 1，replica 数必须等于 volume 中 brick 所包含的存储服务器数，可用性高。 Striped: 条带式卷，类似 RAID 0，stripe 数必须等于 volume 中 brick 所包含的存储服务器数，文件被分成数据块，以 Round Robin 的方式存储在 bricks 中，并发粒度是数据块，大文件性能好。 Distributed Striped: 分布式的条带卷，volume中 brick 所包含的存储服务器数必须是 stripe 的倍数（&gt;=2倍），兼顾分布式和条带式的功能。 Distributed Replicated: 分布式的复制卷，volume 中 brick 所包含的存储服务器数必须是 replica 的倍数（&gt;=2倍），兼顾分布式和复制式的功能。 分布式复制卷的brick顺序决定了文件分布的位置，一般来说，先是两个brick形成一个复制关系，然后两个复制关系形成分布。 企业一般用后两种，大部分会用分布式复制（可用容量为 总容量/复制份数），通过网络传输的话最好用万兆交换机，万兆网卡来做。这样就会优化一部分性能。它们的数据都是通过网络来传输的。 配置分布式卷123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#在信任的主机池中任意一台设备上创建卷都可以，而且创建好后可在任意设备挂载后都可以查看[root@node01 ~] gluster volume create gv1 node01:/data/brick1 node02:/data/brick1 force #创建分布式卷volume create: gv1: success: please start the volume to access data[root@node01 ~] gluster volume start gv1 #启动卷gv1volume start: gv1: success[root@node01 ~] gluster volume info gv1 #查看gv1的配置信息 Volume Name: gv1Type: Distribute #分布式卷Volume ID: 85622964-4b48-47d5-b767-d6c6f1e684ccStatus: StartedSnapshot Count: 0Number of Bricks: 2Transport-type: tcpBricks:Brick1: node01:/data/brick1Brick2: node02:/data/brick1Options Reconfigured:transport.address-family: inetnfs.disable: on[root@node01 ~] mount -t glusterfs 127.0.0.1:/gv1 /opt #挂载gv1卷[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sdc 5.0G 33M 5.0G 1% /data/brick2/dev/sdd 5.0G 33M 5.0G 1% /data/brick3/dev/sdb 5.0G 33M 5.0G 1% /data/brick1127.0.0.1:/gv1 10G 65M 10G 1% /opt # 两个设备容量之和[root@node01 ~] cd /opt/[root@node01 opt] touch &#123;a..f&#125; #创建测试文件[root@node01 opt] ll总用量 0-rw-r--r--. 1 root root 0 2月 2 23:59 a-rw-r--r--. 1 root root 0 2月 2 23:59 b-rw-r--r--. 1 root root 0 2月 2 23:59 c-rw-r--r--. 1 root root 0 2月 2 23:59 d-rw-r--r--. 1 root root 0 2月 2 23:59 e-rw-r--r--. 1 root root 0 2月 2 23:59 f# 在node04也可看到新创建的文件，信任存储池中的每一台主机挂载这个卷后都可以看到[root@node04 ~] mount -t glusterfs 127.0.0.1:/gv1 /opt[root@node04 ~] ll /opt/总用量 0-rw-r--r--. 1 root root 0 2月 2 2018 a-rw-r--r--. 1 root root 0 2月 2 2018 b-rw-r--r--. 1 root root 0 2月 2 2018 c-rw-r--r--. 1 root root 0 2月 2 2018 d-rw-r--r--. 1 root root 0 2月 2 2018 e-rw-r--r--. 1 root root 0 2月 2 2018 f[root@node01 opt] ll /data/brick1/总用量 0-rw-r--r--. 2 root root 0 2月 2 23:59 a-rw-r--r--. 2 root root 0 2月 2 23:59 b-rw-r--r--. 2 root root 0 2月 2 23:59 c-rw-r--r--. 2 root root 0 2月 2 23:59 e[root@node02 ~] ll /data/brick1总用量 0-rw-r--r--. 2 root root 0 2月 2 23:59 d-rw-r--r--. 2 root root 0 2月 2 23:59 f#文件实际存在位置node01和node02上的/data/brick1目录下,通过hash分别存到node01和node02上的分布式磁盘上 配置复制卷复制模式，既AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 这条命令的意思是使用Replicated的方式，建立一个名为gv2的卷(Volume)，存储块(Brick)为2个，分别为node01:/data/brick2 和 node02:/data/brick2；# fore为强制创建：因为复制卷在双方主机通信有故障再恢复通信时容易发生脑裂。本次为实验环境，生产环境不建议使用。[root@node01 ~] gluster volume create gv2 replica 2 node01:/data/brick2 node02:/data/brick2 force volume create: gv2: success: please start the volume to access data[root@node01 ~] gluster volume start gv2 #启动gv2卷volume start: gv2: success[root@node01 ~] gluster volume info gv2 #查看gv2信息Volume Name: gv2Type: Replicate #复制卷Volume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off[root@node01 ~] mount -t glusterfs 127.0.0.1:/gv2 /mnt[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sdc 5.0G 33M 5.0G 1% /data/brick2/dev/sdd 5.0G 33M 5.0G 1% /data/brick3/dev/sdb 5.0G 33M 5.0G 1% /data/brick1127.0.0.1:/gv1 10G 65M 10G 1% /opt127.0.0.1:/gv2 5.0G 33M 5.0G 1% /mnt #容量是总容量的一半[root@node01 ~] cd /mnt/[root@node01 mnt] touch &#123;1..6&#125;[root@node01 mnt] ll /data/brick2总用量 0-rw-r--r--. 2 root root 0 2月 3 01:06 1-rw-r--r--. 2 root root 0 2月 3 01:06 2-rw-r--r--. 2 root root 0 2月 3 01:06 3-rw-r--r--. 2 root root 0 2月 3 01:06 4-rw-r--r--. 2 root root 0 2月 3 01:06 5-rw-r--r--. 2 root root 0 2月 3 01:06 6[root@node02 ~] ll /data/brick2总用量 0-rw-r--r--. 2 root root 0 2月 3 01:06 1-rw-r--r--. 2 root root 0 2月 3 01:06 2-rw-r--r--. 2 root root 0 2月 3 01:06 3-rw-r--r--. 2 root root 0 2月 3 01:06 4-rw-r--r--. 2 root root 0 2月 3 01:06 5-rw-r--r--. 2 root root 0 2月 3 01:06 6# 创建文件的实际存在位置为node01和node02上的/data/brick2目录下，因为是复制卷，这两个目录下的内容是完全一致的。 配置条带卷12345678910111213141516171819202122232425262728293031323334353637383940414243[root@node01 ~] gluster volume create gv3 stripe 2 node01:/data/brick3 node02:/data/brick3 forcevolume create: gv3: success: please start the volume to access data[root@node01 ~] gluster volume start gv3volume start: gv3: success[root@node01 ~] gluster volume info gv3 Volume Name: gv3Type: StripeVolume ID: 54c16832-6bdf-42e2-81a9-6b8d7b547c1aStatus: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick3Brick2: node02:/data/brick3Options Reconfigured:transport.address-family: inetnfs.disable: on[root@node01 ~] mkdir /data01[root@node01 ~] mount -t glusterfs 127.0.0.1:/gv3 /data01[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点127.0.0.1:/gv3 10G 65M 10G 1% /data01[root@node01 ~] dd if=/dev/zero bs=1024 count=10000 of=/data01/10M.file[root@node01 ~] dd if=/dev/zero bs=1024 count=20000 of=/data01/20M.file[root@node01 ~] ll /data01/ -h总用量 30M-rw-r--r--. 1 root root 9.8M 2月 3 02:03 10M.file-rw-r--r--. 1 root root 20M 2月 3 02:04 20M.file*************************************************************************************#文件的实际存放位置：[root@node01 ~] ll -h /data/brick3总用量 15M-rw-r--r--. 2 root root 4.9M 2月 3 02:03 10M.file-rw-r--r--. 2 root root 9.8M 2月 3 02:03 20M.file[root@node02 ~] ll -h /data/brick3总用量 15M-rw-r--r--. 2 root root 4.9M 2月 3 02:03 10M.file-rw-r--r--. 2 root root 9.8M 2月 3 02:04 20M.file# 上面可以看到 10M 20M 的文件分别分成了 2 块（这是条带的特点），写入的时候是循环地一点一点在node01和node02的磁盘上.#上面配置的条带卷在生产环境是很少使用的，因为它会将文件破坏，比如一个图片，它会将图片一份一份地分别存到条带卷中的brick上。 配置分布式复制卷(拓展卷)注意: 块服务器的数量必须是复制的倍数 将按块服务器的排列顺序指定相邻的块服务器成为彼此的复制； 例如，8台服务器： 当复制副本为2时，按照服务器列表的顺序，服务器1和2作为一个复制,3和4作为一个复制,5和6作为一个复制,7和8作为一个复制 当复制副本为4时，按照服务器列表的顺序，服务器1/2/3/4作为一个复制,5/6/7/8作为一个复制 1234567891011121314151617181920212223242526272829#将原有的复制卷gv2进行扩容，使其成为分布式复制卷；#要扩容前需停掉gv2[root@node01 ~] gluster volume stop gv2[root@node01 ~] gluster volume add-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 force #添加brick到gv2中volume add-brick: success[root@node01 ~] gluster volume start gv2volume start: gv2: success[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: Distributed-Replicate # 这里显示是分布式复制卷，是在 gv2 复制卷的基础上增加 2 块 brick 形成的Volume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StartedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Brick3: node03:/data/brick1Brick4: node04:/data/brick1Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off注意：当你给分布式复制卷和分布式条带卷增加 bricks 时，你增加的 bricks 数目必须是复制或条带数目的倍数，例如：你给一个分布式复制卷的 replica 为 2，你在增加 bricks 的时候数量必须为2、4、6、8等。扩容后进行测试，发现文件都分布在扩容前的卷中。 配置分布式条带卷(拓展卷)1234567891011121314151617181920212223#将原有的复制卷gv3进行扩容，使其成为分布式条带卷#要扩容前需停掉gv3[root@node01 ~] gluster volume stop gv3[root@node01 ~] gluster volume add-brick gv3 stripe 2 node03:/data/brick2 node04:/data/brick2 force #添加brick到gv3中[root@node01 ~] gluster volume start gv3volume start: gv3: success[root@node01 ~] gluster volume info gv3 Volume Name: gv3Type: Distributed-Stripe # 这里显示是分布式条带卷，是在 gv3 条带卷的基础上增加 2 块 brick 形成的Volume ID: 54c16832-6bdf-42e2-81a9-6b8d7b547c1aStatus: StartedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: node01:/data/brick3Brick2: node02:/data/brick3Brick3: node03:/data/brick2Brick4: node04:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: on 磁盘存储平衡(拓展卷后操作)平衡布局是很有必要的，因为布局结构是静态的，当新的 bricks 加入现有卷，新创建的文件会分布到旧的 bricks 中，所以需要平衡布局结构，使新加入的 bricks 生效。布局平衡只是使新布局生效，并不会在新的布局中移动老的数据，如果你想在新布局生效后，重新平衡卷中的数据，还需要对卷中的数据进行平衡。 12345678910111213141516171819202122232425262728293031#在gv2的分布式复制卷的挂载目录中创建测试文件如下[root@node01 ~] df -h文件系统 容量 已用 可用 已用% 挂载点127.0.0.1:/gv2 10G 65M 10G 1% /mnt[root@node01 ~] cd /mnt/[root@node01 mnt] touch &#123;x..z&#125;#新创建的文件只在老的brick中有，在新加入的brick中是没有的[root@node01 mnt]# ls /data/brick21 2 3 4 5 6 x y z[root@node02 ~] ls /data/brick21 2 3 4 5 6 x y z[root@node03 ~] ll -h /data/brick1总用量 0[root@node04 ~] ll -h /data/brick1总用量 0# 从上面可以看到，新创建的文件还是在之前的 bricks 中，并没有分布中新加的 bricks 中# 下面进行磁盘存储平衡[root@node01 ~] gluster volume rebalance gv2 start[root@node01 ~] gluster volume rebalance gv2 status #查看平衡存储状态# 查看磁盘存储平衡后文件在 bricks 中的分布情况[root@node01 ~] ls /data/brick21 5 y[root@node02 ~] ls /data/brick21 5 y[root@node03 ~] ls /data/brick12 3 4 6 x z[root@node04 ~] ls /data/brick12 3 4 6 x z# 从上面可以看出部分文件已经平衡到新加入的brick中了# 每做一次扩容后都需要做一次磁盘平衡。 磁盘平衡是在万不得已的情况下再做的，一般再创建一个卷就可以了。 移除brick(收缩卷)你可能想在线缩小卷的大小，例如：当硬件损坏或网络故障的时候，你可能想在卷中移除相关的 bricks。 注意：当你移除 bricks 的时候，你在 gluster 的挂载点将不能继续访问数据，只有配置文件中的信息移除后你才能继续访问 bricks 中的数据。当移除分布式复制卷或者分布式条带卷的时候，移除的 bricks 数目必须是 replica 或者 stripe 的倍数。 但是移除brick在生产环境中基本上不做的，如果是硬盘坏掉的话，直接换个好的硬盘即可，然后再对新的硬盘设置卷标识就可以使用了，后面会演示硬件故障或系统故障的解决办法。 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@node01 ~] gluster volume stop gv2# 先将数据迁移到其它可用的Brick，迁移结束后才将该Brick移除：[root@node01 ~] gluster volume remove-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 start# 在执行了start之后，可以使用status命令查看移除进度[root@node01 ~] gluster volume remove-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 status# 不进行数据迁移，直接删除该Brick[root@node01 ~] gluster volume remove-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 commit[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: ReplicateVolume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StoppedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off# 如果误操作删除了后，其实文件还在 /storage/brick1 里面的，加回来就可以了[root@node01 ~] gluster volume add-brick gv2 replica 2 node03:/data/brick1 node04:/data/brick1 force volume add-brick: success[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: Distributed-ReplicateVolume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StoppedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Brick3: node03:/data/brick1Brick4: node04:/data/brick1Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 迁移卷(迁移卷)12345678910111213141516171819202122[root@node01 ~] gluster volume stop gv2# 先将数据迁移到其它可用的Brick：[root@node01 ~] gluster volume replace-brick gv2 node03:/data/brick1 node06:/data/brick1 start# 在执行了start之后，可以使用status命令查看迁移进度[root@node01 ~] gluster volume replace-brick gv2 node03:/data/brick1 node06:/data/brick1 status# 在数据迁移结束后，执行commit命令来进行Brick替换，如果不进行迁移直接commit会造成数据丢失[root@node01 ~] gluster volume replace-brick gv2 node03:/data/brick1 node06:/data/brick1 commit[root@node01 ~] gluster volume info gv2 Volume Name: gv2Type: ReplicateVolume ID: 9f33bd9a-7096-4749-8d91-1e6de3b50053Status: StoppedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node01:/data/brick2Brick2: node02:/data/brick2Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 删除卷一般会用在命名不规范的时候才会删除 12[root@node01 ~] gluster volume stop gv1[root@node01 ~] gluster volume delete gv 卷误删解决办法12345678910[root@node01 ~] ls /var/lib/glusterd/vols/gv2 gv3[root@node01 ~] rm -rf /var/lib/glusterd/vols/gv3 #删除卷gv3的卷信息[root@node01 ~] ls /var/lib/glusterd/vols/ #再查看卷信息情况如下：gv3卷信息被删除了gv2[root@node01 ~] gluster volume sync node02 #因为其他节点服务器上的卷信息是完整的，比如从node02上同步所有卷信息如下：Sync volume may make data inaccessible while the sync is in progress. Do you want to continue? (y/n) yvolume sync: success[root@node01 ~] ls /var/lib/glusterd/vols/ #验证卷信息是否同步过来gv2 gv3 卷数据不一致解决办法1234567891011[root@node01 ~] ls /data/brick2 #复制卷的存储位置的数据1 5 y[root@node01 ~] rm -f /data/brick2/y [root@node01 ~] ls /data/brick21 5[root@node02 ~] ls /data/brick21 5 y[root@node01 ~] gluster start gv2 #因为之前关闭了，如果未关闭可以忽略此步。[root@node01 ~] cat /mnt/y #通过访问这个复制卷的挂载点的数据来同步数据[root@node01 ~] ls /data/brick2/ #这时候再看复制卷的数据是否同步成功1 5 y glusterfs 分布式存储优化优化参数 1234567891011121314Auth_allow #IP访问授权；缺省值（*.allow all）；合法值：Ip地址Cluster.min-free-disk #剩余磁盘空间阀值；缺省值（10%）；合法值：百分比Cluster.stripe-block-size #条带大小；缺省值（128KB）；合法值：字节Network.frame-timeout #请求等待时间；缺省值（1800s）；合法值：1-1800Network.ping-timeout #客户端等待时间；缺省值（42s）；合法值：0-42Nfs.disabled #关闭NFS服务；缺省值（Off）；合法值：Off|onPerformance.io-thread-count #IO线程数；缺省值（16）；合法值：0-65Performance.cache-refresh-timeout #缓存校验时间；缺省值（1s）；合法值：0-61Performance.cache-size #读缓存大小；缺省值（32MB）；合法值：字节Performance.quick-read: #优化读取小文件的性能Performance.read-ahead: #用预读的方式提高读取的性能，有利于应用频繁持续性的访问文件，当应用完成当前数据块读取的时候，下一个数据块就已经准备好了。Performance.write-behind:先写入缓存内，在写入硬盘，以提高写入的性能。Performance.io-cache:缓存已经被读过的、 优化方式 123456789命令格式：gluster.volume set &lt;卷&gt;&lt;参数&gt;例如：#打开预读方式访问存储[root@node01 ~]# gluster volume set gv2 performance.read-ahead on#调整读取缓存的大小[root@mystorage gv2]# gluster volume set gv2 performance.cache-size 256M glusterfs 监控及维护1234567891011121314151617181920212223242526272829303132333435363738394041424344使用zabbix自带的模板即可，CPU、内存、磁盘空间、主机运行时间、系统load。日常情况要查看服务器监控值，遇到报警要及时处理。#看下节点有没有在线gluster volume status nfsp#启动完全修复gluster volume heal gv2 full#查看需要修复的文件gluster volume heal gv2 info#查看修复成功的文件gluster volume heal gv2 info healed#查看修复失败的文件gluster volume heal gv2 heal-failed#查看主机的状态gluster peer status#查看脑裂的文件gluster volume heal gv2 info split-brain#激活quota功能gluster volume quota gv2 enable#关闭quota功能gulster volume quota gv2 disable#目录限制（卷中文件夹的大小）gluster volume quota limit-usage /data/30MB --/gv2/data#quota信息列表gluster volume quota gv2 list#限制目录的quota信息gluster volume quota gv2 list /data#设置信息的超时时间gluster volume set gv2 features.quota-timeout 5#删除某个目录的quota设置gluster volume quota gv2 remove /data备注：quota功能，主要是对挂载点下的某个目录进行空间限额。如：/mnt/gulster/data目录，而不是对组成卷组的空间进行限制。 故障处理一台主机故障一台节点故障的情况包含以下情况： 物理故障 同时有多块硬盘故障，造成数据丢失 系统损坏不可修复 解决方法： ​ 找一台完全一样的机器，至少要保证硬盘数量和大小一致，安装系统，配置和故障机同样的 IP，安装 gluster 软件，保证配置一样，在其他健康节点上执行命令 gluster peer status，查看故障服务器的 uuid 1234567891011121314151617181920212223242526[root@mystorage2 ~] gluster peer statusNumber of Peers: 3Hostname: mystorage3Uuid: 36e4c45c-466f-47b0-b829-dcd4a69ca2e7State: Peer in Cluster (Connected)Hostname: mystorage4Uuid: c607f6c2-bdcb-4768-bc82-4bc2243b1b7aState: Peer in Cluster (Connected)Hostname: mystorage1Uuid: 6e6a84af-ac7a-44eb-85c9-50f1f46acef1State: Peer in Cluster (Disconnected)复制代码修改新加机器的 /var/lib/glusterd/glusterd.info 和 故障机器一样[root@mystorage1 ~] cat /var/lib/glusterd/glusterd.infoUUID=6e6a84af-ac7a-44eb-85c9-50f1f46acef1operating-version=30712在信任存储池中任意节点执行gluster volume heal gv2 full就会自动开始同步，但在同步的时候会影响整个系统的性能。可以查看状态gluster volume heal gv2 info]]></content>
      <categories>
        <category>数据存储</category>
        <category>GlusterFS</category>
      </categories>
      <tags>
        <tag>glusterfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric架构]]></title>
    <url>%2F2019%2F06%2F21%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F5.Fabric%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Fabric的交易流程Fabric 节点的主要分类记账节点、背书节点、排序节点 交易七步曲 Propose 客户端提交交易提案，及调用了链码的请求 Execute endorse节点运行仿真结果，产生读写集，并签名返回给客户端 Proposal 客户端接收到了endorse节点的运行结果，根据背书策略，如果背书通过，产生proposal给order节点打包区块 Order Trasantion Order节点根据一定规则将交易打包进区块，此处会应用一定的共识策略 Deliver 排序节点将打包好的区块分发给commiter节点 Validate Commiter节点收到区块后，对每一个交易进行验证，如果验证通过(会进行背书策略，当前key的状态等验证)，就运行读写集，并将区块上链 Notify 客户端收到区块上链的Event，确认了交易成功 Fabric 怎么解决双花的在第一步双花的情况会运行成功，但是当第一次花费成功的情况下，后面节点再验证交易的读写集的时候，会因为key的状态变化而验证失败，所以双花只会在第一次的交易中成功； 交易流程图 Fabric 的channelchannel的分类系统channel: 在网络启动的时候系统自动创建的channel 自定义channel: 根据业务逻辑在后面开发的业务channel 创建channel的过程在创建channel的时候， 会想系统channel 发送一个创建channel的请求， 产生创建channel的交易 不同channel的数据是隔离的，即一个channel会维护同一套账本 在channel上部署智能合约同一个channel中的智能合约可以互相调用 不同channel的智能合约指定查询但是不能更改账本的内容 Fabric peerPeer 节点介绍peer 节点的角色 代表联盟链中每个组织的节点 是区块链网络的基础，是账本和智能合约的载体 红色框为peer节点在网络中的地位 peer节点的功能 连接了一个或者多个channel 维护了一个或者多个账本 提供智能合约的运行环境 提供身份认证，加密签名，背书等功能 背书策略的示例 Ledger 的介绍Ledger包含的内容leder包含: blockchain(由区块组成的链，代表区块链账本) 、Word state(记录账本的全局状态) 区块存放的信息 区块头存放信息 交易存放的信息 全局数据库维护账本的当前信息， 代表区块链的全局状态 智能合约的介绍智能合约的作用 定义了不同组织记账的规则 生成交易并更改账本的状态 fabric通过chaincode来实现智能合约 智能合约是怎么更改账本的 chaincode 生命周期 系统chaincode LSCC: 处理链码打包 install instance等 CSCC: 组件channel 对channel 进行join 配置等操作 QSCC: 提供账本query 等api ESCC: 对背书节点对提案运行的结果进行签名 VSCC: 验证交易的背书策略， ESCC + VSCC 主要解决了fabric 的共识问题 Gossip协议 用来管理peer节点的发现 和channel的成员管理， 不断发现新的peer peer节点的账本数据传播 新加入的peer，可以通过gossip 进行点对点快速更新数据 peer节点的分类Leader peer 连接 order 节点，并接受新区块 将区块信息分发给组织中其他committing peers 一个组织中可以有一个或者多个leader peer leader peer 的选举规则可以是: static(静态指定), Dynamic（动态选举） Anchor Peer 通过 gossip 协议， 让不同组织之间的 peer 节点互相认识， 帮助不同组织之间的peer节点建立连接 leader peer 节点的配置方法 Orderer 节点Order节点的性质 参与排序的工作，对所有节点提交的交易进行排序成块 每个order节点产生块的hash值必须一致 要有容错机制，即一个order节点异常后，其他的节点要可以继续执行排序工作(一般会有容错的机制) 强一致性，和pow不同，fabric提交的区块即是确定的区块，交易是不能被复写的，所以一致性是必须要求的 不允许有背叛的order节点， order是CFT的排序算法； Order 出块规则BatchSize: MaxMessageCount: AbsoluteMaxBytes PreferredMaxBytes BatchTimeout: Timeout: Orer节点怎么创建Channel的system Channel: 管理其他用户的链， 创建链的时候通过systemchannel创建一个新的channel的 genesis block Order 节点类型SOLO只有一个节点， 只进行简单的打包 hash 和出块，只可以用于测试环境 Kafka通过Kafka 和 zookeeper进行排序 通过 Kafka的 TTC message 来保证所有的order节点，每次打包的区块包含的交易相同，出块的hash值一样； Raft 基于 Etcd/raft 的library 进行的共识 不需要zookeeper等外部依赖，运维简单 实现了order之间的通讯层 一个channel可以在所有order节点的子集中运行 所有节点之间必须使用TLS通讯 与kafka区别是，kafka是基于交易做的共识， raft是对块做的共识 MSP与CAFabric ca功能 注册创建用户实例 将用户证书签名并下载 证书的更新和撤销 fabric ca 是典型的CS结构 fabric server 的参数 fabric ca init 后产生的文件 fabric ca client端的命令参数 fabric ca 支持Identity类型 peer、order、client、 user 生成证书的示例 Identity lifecycle PKI-X.509PKI介绍公钥基础设施(PKI)是一个利用非对称加密算法原理和技术实现并提供安全服务的具有通用性的技术规范和标准。是管理非对称加密算法的密钥和确认信息。整合数字证书、公钥加密技术和CA的系统。其结合了软件、加密技术和组织需要进行非对称加密算法的服务。 公钥基础设施(PKI)是一种遵循既定标准的密钥管理平台，它通过“信息加密”和“数字签名”等密码服务及所必需的密钥和证书管理体系，为实现网络通信保密性、完整性和不可否认性的一套完整、成熟可靠的解决方案。简单来说，PKI就是利用公钥理论和技术建立的提供安全服务的基础设施。PKI技术是信息安全技术的核心。也是电子商务的关键和基础技术。 MSP介绍 MSP结构 Fabric 应用开发开发流程 部署Fabric网络 开发并部署智能合约 通过fabric-ca 和fabric-sdk 进行application层的业务逻辑开发 Fabric 网络部署流程 安装ordering service 安装peer节点 并配置 记账和背书节点 创建channel 将组织加入到channel 安装并实例化chaincode 调用链码 开发人员主要关心的步骤通过fabric进行开发，我们主要需要开发的有两部分： 即上图橘黄色的部分: 1. chaincode开发； 2. Application开发]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 智能合约编写]]></title>
    <url>%2F2019%2F06%2F11%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F4.Fabric%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E7%BC%96%E5%86%99%2F</url>
    <content type="text"><![CDATA[Chaincode 介绍chaincode作用Fabric的Chaincode是一段运行在容器中的程序。Chaincode是客户端程序和Fabric之间的桥梁。 通过Chaincode客户端程序可以发起交易，查询交易。 chaincode 运行环境Chaincode是运行在Dokcer容器中，因此相对来说安全。 支持语言目前支持 java,node，go,go是最稳定的。其他还在完善。 chaincode 运行和使用步骤 创建代码目录 mkdir -p {path} 存放编写好的chaincode源代码 部署chaincode 1peer chaincode install -n &#123;chaincodeId&#125; -v &#123;version&#125; -p &#123;path&#125; 实例化chaincode 1peer chaincode instantiate -o &#123;order_address&#125; -C &#123;channel&#125; -n &#123;chaincodeId&#125; -v &#123;version&#125; -c '&#123;"Args": ["init", "a", "100", "b", "200"]&#125;' -P "OR('Org1MSP.member', 'Org2MSP.member')" 调用chaincode 1peer chaincode invoke -o &#123;order_address&#125; -C &#123;chainnel&#125; -n &#123;chaincodeId&#125; -v &#123;version&#125; -c '&#123;"Args":["invoke", "1", "a", "b"]&#125;' Golang 版Chaincode 编写规范Chaincode 代码结构包名一个chaincode通常是一个 Goalng源文件，包名必须是main —— package main shim包“github.com/hyperledger/fabric/core/shaincode/shim” pb “github.com/hyperledger/fabric/protos/peer” shim提供了Fabric系统的上下文环境，包含了Chaincode和Fabic交互的接口。 在Chaincode中，执行赋值，查询，等功能都是需要通过shim. 定义结构体1type chainCodeExample struct &#123;&#125; chaincode必须定义一个结构体，结构体的名称可以是任意符合Golang命名规范的字符串，并且必须实现Init 和 Invoke 接口方法； 实现Init方法123func (t *chainCodeExample) Init(stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters()&#125; Init方法是系统初始化方法，当执行命令 peer chaincode instantiate 实例化chaincode的时候调用该方法； 实现Invoke方法123func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters()&#125; Invoke方法主要是写入数据等对链进行查询和插入修改等操作的主要方法； shim包核心方法Success1234func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters() return shim.Success([]byte("success invoke"))&#125; Success 方法负责将正确的消息返回给调用chaincode 的客户端 Error1234func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; Func, args := stub.GetFunvtionAndParameters() return shim.Success([]byte("success invoke"))&#125; Error方法将错误的信息返回给调用Chaincode的客户端。 LogLevel12345func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; logLevel, _ := shim.LogLevel("DEBUG") shim.SetLoggingLevel(logLevel) return shim.Success([]byte("success invoke and not opter!"))&#125; LogLevel 方法负责修改Chaincode 中运行日志级别 Chaincode StubInterface 接口方法shim包提供了一个接口 ChaincodeStubInte， 在Invoke方法和Init 方法中该接口作为参数传入方法中； ChaincodeStubInte 主要方法 GetFunctionAndParameters 获取调用链码的方法名和参数列表 PutState 存储数据到账本中 DelState 删除账本中的数据 GetState 从账本中获取指定数据 CreateCompositeKey 创建符合键 GetStateByPartialCompositeKey 通过符合键取值 SplitCompositeKey 拆分复合键 GetStateByRange 查询指定key 指定范围的历史记录 GetHistoryForKey 获取指定key的历史记录 GetTxID 获取交易编号 GetTxTimestamp 获取交易的时间 GetCreator 获取交易的创建者 InvokeChaincode 调用其他的链码 1、 PutState方法将调用者的数据存储到Fabric链上 1234func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; stub.PutState("a", []byte("test")) return shim.Success([]byte("success invoke"))&#125; 2、 GetState方法将调用者的数据存储到Fabric链上 123456789func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; var keyValue []byte var err error keyValue, err = stub.GetState("getKey") if err != nil &#123; retutn shim.Error("find error") &#125; reutrn shim.Success(keyValue)&#125; 3、GetStateByRange方法根据Key的范围来查询相关数据 12345678910111213141516171819202122232425func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; startKey := "startKey" enKey := "endKey" keysIter, err := stub.GetStateByRange(startKey, endKey) if err != nil &#123; return shim.Error(fmt.Sprintf("GetStateByRange find err: %s", err)) &#125; defer keysIter.Close() var keyValues map[string]string for keysIter.HasNext() &#123; response, iterError := keysIter.Next() if iterError != nil &#123; return shim.Error(fmt.Sprintf("find and error %s", iterErr)) &#125; keyValues[response.Key] = response.Value &#125; for key, value := range keyValues &#123; fmt.Printf("key %d contains %s\n", key, value) &#125; jsonKeys, err := json.Marshal(keys) if err != nil &#123; return shim.Error(fmt.Sprintf("find error on marshaling json: %s", err)) &#125; return shim.Success(jsonKeys)&#125; 4、GetHistoryForKey方法查询某个键的历史记录 12345678910111213141516171819202122232425262728func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; keyName := "key" keysIter, err := stub.GetHistoryForKey(keyName) if err != nil &#123; return shim.Error(fmt.Sprintf("GetHistoryForKey failed err: %s", err)) &#125; defer keysIter.Close() var keys []string for keysIter.HasNext() &#123; response, iterError := keysIter.Next() if iterError != nil &#123; return shim.Error(fmt.Sprintf("iter occur error %s", iterErr)) &#125; txid := response.TxId txvalue := response.Value txstatus := response.IsDelete txtimesamp := response.Timestamp tm := time.Unix(txtimesamp.Seconds, 0) datestr := tm.Format("2006-01-02 03:04:05 PM") fmt.Printf("Tx info - txid: %s value: %s if delete: %t datetime: %s \n", txid, string(txvalue), txstatus, datestr) keys = append(keys, txid) &#125; jsonKeys, err := json.Marshal(keys) if err != nil &#123; return shim.Error(fmt.Sprintf("find error on marshaling json: %s", err)) &#125; return shim.Success(jsonKeys)&#125; 5、DelState 方法12345678func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; delKey := "testKey" err := stub.DelState(delKey) if err != nil &#123; return shim.Error("delete error !") &#125; reutrn shim.Success([]byte("delete success !"))&#125; 6、CreateCompositeKey 方法负责创建组合键 1234567891011func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; params := []string("a", "b", "c") cparams := "test_params" keyName := "testKey" cKey, _ := stub.CreateCompositeKey(keyName, params) err := stub.PutState(ckey, []byte(c_params)) if err != nil &#123; fmt.Println("find errors %s", err) &#125; return shim.Success([]byte(cKey))&#125; 7、GetStateByPartialCompositeKey 和 SplitCompositeKey 方法用来查询复合键的值 12345678910111213141516171819202122232425func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; params := []string("a", "b") keyName := "testKey" response, err := stub.GetStateByPartialCompositeKey(keyName, params) if err != nil &#123; error_str := fmt.Sprintf("find error: %s", err) return shim.Error(error_str) &#125; defer response.Close() var i int var tlist []string for i = 0; response.HasNext; i++ &#123; responseRange, err := response.Next() if err != nil &#123; err_str := fmt.Springf("find error: %s", err) fmt.Println(err_str) return shim.Error(error_str) &#125; value1, compositeKeyParts, _ := stub.SplitCompositeKey(responseRange.Key) value2 := compositeKeyParts[0] value3 := compositeKeyParts[1] fmt.Printf("find value v1:%s v2:%s v3:%s\n", value1, value2, value3) &#125; return shim.Success("success")&#125; 8、GetTxTimestamp方法负责好偶去当前客户端发送交易的时间戳 12345678910func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; txTime, err := stub.GetTxTimestamp() if err != nil &#123; fmt.Printf("Error getting transaction timestamp: %s", err) return shim.Error(fmt.Springf("Error getting transaction timestamp: %s', err)) &#125; tm := time.Unix(txTime.Seconds, 0) fmt.Printf("Transaction Time: %v\n", time.Format("2006-01-02 03:04:05 PM")) return shim.Success([]byte(fmt.Springf("time is : %s", tm.Format("2006-01-02 03:04:05 PM"))))&#125; 9、InvokeChaincode方法用来在chaincode中调用其他的chaincode链码 123456789101112131415161718func (t *chainCodeExample) Invoke (stub shim.ChaincodeStubInterface) pb.Response &#123; chaincodeId := "chaincode2" channelId := "channel2" params1 := []string&#123;"query", "a"&#125; queryArgs := make([][]byte, len(params1)) for i, arg := range params1 &#123; queryArgs[i] = []byte(arg) &#125; response := stub.InvokeChaincode(chaincodeId, params1, channelId) if response.Status != shim.OD &#123; errStr := fmt.Sprintf("Failed to query chaincode, got error: %s", response.Payload) fmt.Printf(errStr) return shim.Error(errStr) &#125; result := string(response.Payload) fmt.Printf("invoke chaincode %s", result) return shim.Success([]byte("success invoke chaincode and not opter!"))&#125; Chaincode 操作命令123456789101112131415161718Available Commands: install instantiate invoke list package query signpackage upgradeFlags: --cafile -o, --orderer --tls --transientGlobal Flags: --logging-level --test.coverprofile -v, --version Chaincode背书规则指定背书介绍Fabaric 中对数据参与方对数据的确认是真实通过Chaincode来进行的。 什么是背书呢？ 背书就是仪表交易被确认的过程。大概意思就是交易你必须背会一本书才能操作。 背书策略被用来指示对相关的参与方如何对交易进行确认。当一个节点接收到一个交易请求的时候，会调用vscc系统（系统Chaincode，专门负责处理背书相关的操作）与交易的Chaincode共同来验证交易的合法性。在vscc和交易的 Chaincode共同对交易的确认中，通常会做一下的校验。 所有的背书是否有效 参与背书的数量是否满足要求 所有背书参与方是否满足要求 指定背书规则的方法背书策略的设置是通过Chaincode部署时instantiate命令中的-p参数来设置的。 1peer chaincode instantiate -o &#123;order_address&#125; -C &#123;channel&#125; -n &#123;chaincodeId&#125; -v &#123;version&#125; -c '&#123;"Args": ["init", "a", "100", "b", "200"]&#125;' -P "OR('Org1MSP.member', 'Org2MSP.member')" 这个参数包说明是当前Chaincode发起的交易，需要组织编号为 Org1MSP的组织编号为Org2MSP的组织中的任何一个用户共同参与交易的确认并且同意。这样交易才能生效并且记录到 区块链中。 通过上述背书策略的示例我们可以知道背书策略是通过一定的关键字和系统的属性组成的。 背书编写示例 逻辑与关系 1AND('Org1MSP.member', 'Org2MSP.member', 'Org3MSP.member') 逻辑或关系 1OR('Org1MSP.member', 'Org2MSP.member', 'Org3MSP.member') 逻辑与或关系并存 1OR('Org1MSP.member', AND('Org2MSP.member', 'Org3MSP.member'))]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oauth2原理及应用]]></title>
    <url>%2F2019%2F05%2F27%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Foauth2%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[OAuth2 的简单解释OAuth2 是目前比较流行的授权机制，用来授权第三方应用来使用用户的一些数据 通过OAuth2可以比较简单的实现跨平台的资源共享，下面简单介绍一下OAuth2具体解决了哪方面的问题； 一个关于小区送快递的场景在一个大型的居民小区，小区入口处的门有密码锁； 进出门的时候，需要业主输入密码下能进入； 但是小区经常会有外卖和快递员出入，快递员需要在有快件派送的时候有权限进入小区，现在必须有一种办法可以让快递员在没有账号密码的情况下通过门禁系统，进入小区 如果业主将密码告诉了快递员， 那快递员就拥有了和业主一样的权限，快递员可以用密码反反复复出入小区, 好像并不能这样设计。 小区授权机制设计对于以上问题，可以设计一套如下的授权方案，授权的流程如下 门禁系统的密码输入下方，提供一个获取授权的按钮。快递员需要首先按这个按钮，去申请授权。 按下按钮以后， 业主的手机就会弹出对话框， 有人正在要求授权。系统还会显示出该快递员的姓名，工号和所属的快递公司，业主确认快递员的请求属实，就会点击按钮，告诉门禁系统，我同意给与他进入小区的权限； 门禁系统得到业主的确认以后，向快递员显示一个进入小区的令牌(一个临时密码, 一般会有一个期限)。 快递员输入临时密码后，便可以进入小区； 互联网授权机制OAuth关于快递的例子，解决了一个不属于本小区用户，通过业主(资源所有者)的授权，有了临时进入小区，执行一些业主权限的问题； 把该例子应用到互联网应用的场景，就是OAuth的作用了； 一、居民小区就相当于拥有用户数据资源的资源服务器。比如，支付宝存储了我们的头像和信用分等信息，想要获取这些数据，用户一定要先成功登陆支付宝； 二、快递员相当于第三方应用，想要穿过门禁系统，进入小区。比如，某贷款机构想要通过支付宝，获取用户的芝麻饮用分来判断贷款金额； 三、就是业主本人相当于互联网应用的资源所有者，同意快递员第三方进入小区，执行业主本人的一些权限。 总结: OAuth就是一种授权机制， 数据所有者告诉系统，同意并授权第三方进入系统，获取一些指定的数据的权限。系统从而会提供一个短期的令牌给第三方，在一定时间内，第三方都可以通过该令牌获取用户已经授权获取的数据；OAuth解决了第三方应用获取平台用户数据的问题； OAuth2的一些基本概念什么是OAuth2.0OAuth 2.0, 允许第三方应用程序来代表资源所有者获得对HTTP服务的有限访问权限以自己的名义获取访问权限。 在传统的客户端 - 服务器身份验证模型中，客户端通过使用资源所有者的凭据向服务器进行身份验证来请求服务器上的访问受限资源（受保护资源）。为了向第三方应用程序提供对受限资源的访问，资源所有者与第三方共享其凭据。这会产生一些问题和限制。 OAuth通过引入授权层并将客户端的角色与资源所有者的角色分开来解决这些问题。在OAuth中，客户端请求访问由资源所有者控制并由资源服务器托管的资源，并发出与资源所有者不同的凭据集。 客户端不是使用资源所有者的凭证来访问受保护资源，而是获取访问令牌 - 表示特定范围，生命周期和其他访问属性的字符串。授权服务器在资源所有者的批准下向第三方客户端颁发访问令牌。客户端使用访问令牌来访问资源服务器托管的受保护资源 什么是OpenID Connect 1.0OpenID Connect 1.0是OAuth 2.0协议之上的简单身份层。它使客户端能够根据授权服务器执行的身份验证来验证最终用户的身份，以及以可互操作和类似REST的方式获取有关最终用户的基本配置文件信息。 作为背景，OAuth 2.0授权框架和OAuth 2.0承载令牌使用规范为第三方应用程序提供了一个通用框架，以获取和使用对HTTP资源的有限访问。它们定义了获取和使用访问令牌来访问资源的机制，但没有定义标准方法来提供身份信息。值得注意的是，如果没有分析OAuth 2.0，它就无法提供有关最终用户身份验证的信息。 OpenID Connect实现身份验证，作为OAuth 2.0授权过程的扩展。 OpenID Connect允许所有类型的客户端（包括基于Web，移动和JavaScript客户端）请求和接收有关经过身份验证的会话和最终用户的信息。规范套件是可扩展的，允许参与者在对它们有意义时使用可选功能，例如身份数据加密，OpenID提供程序的发现和会话管理。 OAuth2 的角色定义 资源所有者 资源所有者是 OAuth 2 四大基本角色之一，在 OAuth 2 标准中，资源所有者即代表授权客户端访问本身资源信息的用户（User），也就是应用场景中的“开发者A”。客户端访问用户帐户的权限仅限于用户授权的“范围”（aka. scope，例如读取或写入权限）。 如果没有特别说明，下文中出现的”用户”将统一代表资源所有者。 用户认证中心 验证用户身份的地方，比如网站的登录系统（密码验证/ session验证）； 资源服务器(resource server) 资源服务器托管了所有的受保护的用户资源等, 存储服务资源的地方就是资源服务器； 我们使用授权的目的，就是获取在资源服务器上和用户相关资源的资格； 授权中心(oauth2 server) 资源服务器托管了受保护的用户账号信息，而授权服务器验证用户身份然后为客户端派发资源访问令牌。 在上述应用场景中，Github 既是授权服务器也是资源服务器，个人信息和仓库信息即为资源（Resource）。而在实际工程中，不同的服务器应用往往独立部署，协同保护用户账户信息资源。 客户端 ( oauth2 client ) 执行授权服务流程的后台程序, 该部分一般要由第三方独立开发； 有的资源提供商，会提供sdk等供用户进行客户端的开发； 用户终端(浏览器…) 调用客户端执行授权过程的地方，一般为浏览器。。。 令牌与密码令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。 （1）令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。 （2）令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。 （3）令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。 上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。 注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。这也是为什么令牌的有效期，一般都设置得很短的原因。 OAuth2.0 的授权模式与使用OAuth 2.0 规定了四种获得令牌的流程。下面就是这四种授权方式。 Auth2.0的四种授权模式 ​ 授权码模式（Authorization Code）(支持refresh token) ​ 隐藏模式（Implicit）(不支持refresh token) ​ 密码模式（Resource Owner Password Credentials） (支持refresh token) ​ 客户端模式（Client Credentials） (不支持refresh token) 注意，不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。 授权码模式授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。 这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。 第一步，A 网站提供一个链接，用户点击后就会跳转到 B 网站，授权用户数据给 A 网站使用。 第二步，用户跳转后，B 网站会要求用户登录，然后询问是否同意给予 A 网站授权。用户表示同意，这时 B 网站就会跳回redirect_uri参数指定的网址。跳转时，会传回一个授权码。 第三步，A 网站拿到授权码以后，就可以在后端，向 B 网站请求令牌。 第四步，B 网站收到请求以后，就会颁发令牌。具体做法是向redirect_uri指定的网址，发送一段 JSON 数据包含了令牌的详细内容。 隐藏模式有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。 第一步，A 网站提供一个链接，要求用户跳转到 B 网站，授权用户数据给 A 网站使用。 第二步，用户跳转到 B 网站，登录后同意给予 A 网站授权。这时，B 网站就会跳回redirect_uri参数指定的跳转网址，并且把令牌作为 URL 参数，传给 A 网站。 这种方式把令牌直接传给前端，是很不安全的。因此，只能用于一些安全要求不高的场景，并且令牌的有效期必须非常短，通常就是会话期间（session）有效，浏览器关掉，令牌就失效了。 密码模式如果你高度信任某个应用，也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为”密码式”（password）。 第一步，A 网站要求用户提供 B 网站的用户名和密码。拿到以后，A 就直接向 B 请求令牌。 第二步，B 网站验证身份通过后，直接给出令牌。注意，这时不需要跳转，而是把令牌放在 JSON 数据里面，作为 HTTP 回应，A 因此拿到令牌。 这种方式需要用户给出自己的用户名/密码，显然风险很大，因此只适用于其他授权方式都无法采用的情况，而且必须是用户高度信任的应用。 客户端模式最后一种方式是凭证式（client credentials），适用于没有前端的命令行应用，即在命令行下请求令牌。 第一步，A 应用在命令行向 B 发出请求。 第二步，B 网站验证通过以后，直接返回令牌。 这种方式给出的令牌，是针对第三方应用的，而不是针对用户的，即有可能多个用户共享同一个令牌。 令牌的使用A 网站拿到令牌以后，就可以向 B 网站的 API 请求数据了。 此时，每个发到 API 的请求，都必须带有令牌。具体做法是在请求的头信息，加上一个Authorization字段，令牌就放在这个字段里面。 123&gt; curl -H "Authorization: Bearer ACCESS_TOKEN" \&gt; "https://api.b.com"&gt; 上面命令中，ACCESS_TOKEN就是拿到的令牌。 更新令牌令牌的有效期到了，如果让用户重新走一遍上面的流程，再申请一个新的令牌，很可能体验不好，而且也没有必要。OAuth 2.0 允许用户自动更新令牌。 具体方法是，B 网站颁发令牌的时候，一次性颁发两个令牌，一个用于获取数据，另一个用于获取新的令牌（refresh token 字段）。令牌到期前，用户使用 refresh token 发一个请求，去更新令牌。 123456&gt; https://b.com/oauth/token?&gt; grant_type=refresh_token&amp;&gt; client_id=CLIENT_ID&amp;&gt; client_secret=CLIENT_SECRET&amp;&gt; refresh_token=REFRESH_TOKEN&gt; 上面 URL 中，grant_type参数为refresh_token表示要求更新令牌，client_id参数和client_secret参数用于确认身份，refresh_token参数就是用于更新令牌的令牌。 第三方授权例子一、场景举例举例来说，A 网站允许使用B网站账号登录，背后就是下面的流程。 A 网站让用户跳转到B网站。 B要求用户登录，然后询问”A 网站要求获得 xx 权限，你是否同意？” 用户同意，B就会重定向回 A 网站，同时发回一个授权码。 A 网站使用授权码，向 B 请求令牌。 B网站返回令牌. A 网站使用令牌，向B资源服务器请求用户数据。 二、授权流程 Authrization Request客户端向用户请求对资源服务器的authorization grant。 Authorization Grant（Get）如果用户授权该次请求，客户端将收到一个authorization grant。 Authorization Grant（Post）客户端向授权服务器发送它自己的客户端身份标识和上一步中的authorization grant，请求访问令牌。 Access Token（Get）如果客户端身份被认证，并且authorization grant也被验证通过，授权服务器将为客户端派发access token。授权阶段至此全部结束。 Access Token（Post &amp;&amp; Validate）客户端向资源服务器发送access token用于验证并请求资源信息。 Protected Resource（Get）如果access token验证通过，资源服务器将向客户端返回资源信息。 三、豆瓣授权流程 User Authorization Request 首先，客户端构造了一个用于请求authorization code的URL并引导User-agent跳转访问。 123456https://authorization-server.com/auth ?response_type=code &amp;client_id=29352915982374239857 &amp;redirect_uri=https%3A%2F%2Fexample-client.com%2Fcallback &amp;scope=create+delete &amp;state=xcoiv98y2kd22vusuye3kch response_type=code此参数和参数值用于提示授权服务器当前客户端正在进行Authorization Code授权流程。 client_id客户端身份标识。 redirect_uri标识授权服务器接收客户端请求后返回给User-agent的跳转访问地址。 scope指定客户端请求的访问级别。 state由客户端生成的随机字符串，步骤2中用户进行授权客户端的请求时也会携带此字符串用于比较，这是为了防止CSRF攻击。 User Authorizes Applcation 当用户点击上文中的示例链接时，用户必须已经在授权服务中进行登录（否则将会跳转到登录界面，不过 OAuth 2 并不关心认证过程），然后授权服务会提示用户授权或拒绝应用程序访问其帐户。以下是授权应用程序的示例： Authorization Code Grant 如果用户确认授权，授权服务器将重定向User-agent至之前客户端提供的指向客户端的redirect_uri地址，并附带code和state参数（由之前客户端提供），于是客户端便能直接读取到authorization code值。 123https://example-client.com/redirect ?code=g0ZGZmNjVmOWIjNTk2NTk4ZTYyZGI3 &amp;state=xcoiv98y2kd22vusuye3kch state值将与客户端在请求中最初设置的值相同。客户端将检查重定向中的状态值是否与最初设置的状态值相匹配。这可以防止CSRF和其他相关攻击。 code是授权服务器生成的authorization code值。code相对较短，通常持续1到10分钟，具体取决于授权服务器设置。 Access Token Request 现在客户端已经拥有了服务器派发的authorization code，接下来便可以使用authorization code和其他参数向服务器请求access token（POST方式）。其他相关参数如下： grant_type=authorization_code - 这告诉服务器当前客户端正在使用Authorization Code授权流程。 code - 应用程序包含它在重定向中给出的授权码。 redirect_uri - 与请求authorization code时使用的redirect_uri相同。某些资源（API）不需要此参数。 client_id - 客户端标识。 client_secret - 应用程序的客户端密钥。这确保了获取access token的请求只能从客户端发出，而不能从可能截获authorization code的攻击者发出。 Access Token Grant 服务器将会验证第4步中的请求参数，当验证通过后（校验authorization code是否过期，client id和client secret是否匹配等），服务器将向客户端返回access token。 1234567&#123; &quot;access_token&quot;:&quot;MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3&quot;, &quot;token_type&quot;:&quot;bearer&quot;, &quot;expires_in&quot;:3600, &quot;refresh_token&quot;:&quot;IwOGYzYTlmM2YxOTQ5MGE3YmNmMDFkNTVk&quot;, &quot;scope&quot;:&quot;create delete&quot;&#125; 至此，授权流程全部结束。直到access token 过期或失效之前，客户端可以通过资源服务器API访问用户的帐户，并具备scope中给定的操作权限。 Ory Hydra OAuth2.0 框架ORY Hydra解决了身份验证和授权问题，是OAuth 2.0和OpenID Connect提供商。 什么是ORY HydraORY Hydra是OAuth 2.0和OpenID Connect Provider。因此，它能够发出访问，刷新和ID令牌。与其他项目相反，ORY Hydra不提供用户管理（登录，注销，配置文件管理，注册），而是使用基于重定向的流和REST API将用户身份验证（登录）委派给您实现的服务，控制。这允许您构建适合您的用户管理，使用您喜欢的前端技术，以及您的用例所需的身份验证机制（例如基于令牌的2FA，SMS 2FA）。 因此，ORY Hydra是最灵活的OAuth 2.0和OpenID Connect提供商，为您提供了实现业务逻辑的极大自由，并且仍然可以从OAuth 2.0和OpenID Connect中获得所有好处。 除了OAuth 2.0功能之外，ORY Hydra还为加密密钥提供安全存储（例如，用于签署JSON Web令牌），并且能够管理OAuth 2.0客户端。 ORY Hydra是OpenID Connect认证（待定），并实现了OpenID Foundation规定的所有要求。因此，它正确地实现了IETF和OpenID Foundation所预期的不同OAuth 2.0和OpenID Connect流程。 ORY Hydra 介绍Hydra是OAuth 2.0授权框架和OpenID Connect Core 1.0的服务器实现。现有的OAuth2实现通常作为库或SDK。 在不了解整个规范的情况下实现和使用OAuth2具有挑战性，并且即使在使用SDK时也容易出错。Hydra的主要目标是使OAuth 2.0和OpenID Connect 1.0的设置更轻松，更易于使用。 Hydra实现OAuth2和OpenID Connect 1.0中描述的流程，而不强制您使用“Hydra用户管理”或某些模板引擎或预定义的前端。相反，它依赖于HTTP重定向和加密方法来验证用户同意，允许您将Hydra与任何身份验证端点一起使用。 ORY Hydra 不管理用户要理解的第一个重要概念是ORY Hydra是OAuth 2.0授权和OpenID Connect服务器。有些人将这些功能误认为存储用户数据并将您登录的系统。事实并非如此。相反，此类服务器负责将用户凭据（通常是用户名和密码）“转换”为OAuth 2.0访问和刷新令牌以及OpenID Connect ID令牌。它基本上就像您使用会话数据存储cookie，但更灵活，它也适用于第三方应用程序。 ORY Hydra不存储用户配置文件，用户名，密码。此功能取决于您。ORY Hydra使用我们称之为用户登录和同意流的东西。此流使用HTTP重定向将任何传入的授权请求（“请给我一个访问令牌。”）转发给登录提供者和同意提供者。这些应用程序是您实现的。它可以是新应用程序或您现有的登录系统。从较高的层面来看，这些提供商可归纳为： 登录提供者负责通过验证他或她的凭证（例如用户名+密码）来验证用户（“登录”）。 同意提供商负责允许OAuth 2.0应用程序代表用户获取令牌（“您是否希望允许foobar-app访问您的所有个人消息和图像？”。 Ory Hydra OAuth 2.0授权流程： 开发人员在授权服务器（ORY Hydra）上注册OAuth 2.0客户端，目的是代表用户获取信息。 应用程序UI要求用户授权应用程序代表他/她访问信息/数据。 用户被重定向到授权服务器。 授权服务器确认用户的身份，并要求用户授予OAuth 2.0客户端某些权限。 授权服务器发出OAuth 2.0客户端用于代表用户访问资源的令牌。 Ory Hydra OAuth 2.0 登录认证网络流程图​ 我们的OAuth2 实现 角色介绍 三方平台 (任何想要拿到其他平台授权的应用) 资源服务器 (我们数据提供端) 授权服务器 (OAuth2.0 + IDP), 统一授权中心和身份认证中心； Account Service (基于普通用户的注册，登录，和修改等接口； 另外提供一个公共的OAuth 客户端，三方平台可通过client_id 和 secret 来使用该公共客户端，用来获取授权) 资源所有者(终端用户) 三方平台接入方法 先申请 client_id, 和 secret ， 之后才可以使用授权服务； 开发跳转接口， 该接口用来接收终端用户收到的code码， 用来获取access_token等 三方平台授权流程(参考上图) 调用Account Service 获取授权平台地址并请求； 用户验证身份通过，并同意授权； 通过跳转(需提前开发好跳转接口)，拿到授权得到的code； 调用Account Service (使用授权code)， 获取access_token 等； 之后便可使用access_token, 调用资源服务器授权范围内的数据；]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Orm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-micro 框架使用]]></title>
    <url>%2F2019%2F05%2F22%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fgo-micro%20%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Micro介绍什么是MicroMicro是一个着眼于分布式系统开发的微服务生态系统。 Micro由开源的库与工具组成，旨在辅助微服务开发。 Micro开源库 go-micro - 基于Go语言的可插拔RPC微服务开发框架；包含服务发现、RPC客户/服务端、广播/订阅机制等等。 go-plugins - go-micro的插件, 其包含etcd、kubernetes、nats、rabbitmq、grpc等等。 micro - 微服务工具集包含传统的入口点（entry point）；API 网关、CLI、Slack Bot、代理及Web UI。 文档位置: https://micro.mu/docs/ 下面主要介绍使用go-micro 对service 的编写注册和启动流程，关于micro的工具箱的使用，可以参考上面提供的文档地址 使用GO-Micro编写服务端环境准备1234brew install protobuf # 下载protoc工具go get github.com/micro/go-micro # 安装go-microgo get github.com/golang/protobuf/&#123;proto,protoc-gen-go&#125; go get github.com/micro/&#123;protoc-gen-micro,micro&#125; go-micro Service接口先看一下go-micro的service interface，是构建micro服务所需的主要组件。它把所有Go-Micror的基础包打包成单一组件接口。 接下来对于服务的开发都将主要围绕service 接口来进行 12345678type Service interface &#123; Init(...Option) Options() Options Client() client.Client Server() server.Server Run() error String() string&#125; 编写.proto文件，定义Service 的Api我们使用protobuf文件来定义服务的API接口。使用protobuf可以非常方便去严格定义API，提供服务端与客户端双边具体一致的类型。 下面是定义的示例 greeter.proto 12345678910111213syntax = "proto3";service Greeter &#123; rpc Hello(HelloRequest) returns (HelloResponse) &#123;&#125;&#125;message HelloRequest &#123; string name = 1;&#125;message HelloResponse &#123; string greeting = 2;&#125; 我们定义了一个服务叫做Greeter的处理器，它有一个接收HelloRequest并返回HelloResponse的Hello方法。 .proto文件的详细定义方法可以参考: https://www.jianshu.com/p/ea656dc9b037 生成API接口我们需要下面这个工具来生成protobuf代码文件，它们负责生成定义的go代码实现。 1protoc --proto_path=$GOPATH/src:. --micro_out=. --go_out=. greeter.proto 生成的类现在可以引入handler中，在服务或客户端来创建请求了。 下面是部分生成的代码 handler的开发，将会直接饮用生成的Request 和Response 等对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051type HelloRequest struct &#123; Name string `protobuf:"bytes,1,opt,name=name" json:"name,omitempty"`&#125;type HelloResponse struct &#123; Greeting string `protobuf:"bytes,2,opt,name=greeting" json:"greeting,omitempty"`&#125;// Greeter service 客户端的APItype GreeterClient interface &#123; Hello(ctx context.Context, in *HelloRequest, opts ...client.CallOption) (*HelloResponse, error)&#125;type greeterClient struct &#123; c client.Client serviceName string&#125;func NewGreeterClient(serviceName string, c client.Client) GreeterClient &#123; if c == nil &#123; c = client.NewClient() &#125; if len(serviceName) == 0 &#123; serviceName = "greeter" &#125; return &amp;greeterClient&#123; c: c, serviceName: serviceName, &#125;&#125;func (c *greeterClient) Hello(ctx context.Context, in *HelloRequest, opts ...client.CallOption) (*HelloResponse, error) &#123; req := c.c.NewRequest(c.serviceName, "Greeter.Hello", in) out := new(HelloResponse) err := c.c.Call(ctx, req, out, opts...) if err != nil &#123; return nil, err &#125; return out, nil&#125;// Greeter service 服务端type GreeterHandler interface &#123; Hello(context.Context, *HelloRequest, *HelloResponse) error&#125;func RegisterGreeterHandler(s server.Server, hdlr GreeterHandler) &#123; s.Handle(s.NewHandler(&amp;Greeter&#123;hdlr&#125;))&#125; 实现handler处理器服务端需要注册handlers，这样才能提供服务并接收请求。处理器相当于是一个拥有公共方法的公共类，它需要符合签名func(ctx context.Context, req interface{}, rsp interface{}) error 通过上面的内容，我们看到，Greeter interface的签名的看上去就是这样： 123type GreeterHandler interface &#123; Hello(context.Context, *HelloRequest, *HelloResponse) error&#125; Greeter处理器实现： 12345678import proto "github.com/micro/examples/service/proto"type Greeter struct&#123;&#125;func (g *Greeter) Hello(ctx context.Context, req *proto.HelloRequest, rsp *proto.HelloResponse) error &#123; rsp.Greeting = "Hello " + req.Name return nil&#125; 服务启动 创建Service服务 可以使用micro.NewService创建服务 12import "github.com/micro/go-micro"service := micro.NewService() 初始化时，也可以传入相关选项 1234service := micro.NewService( micro.Name("greeter"), micro.Version("latest"),) 所有的可选参数参考：配置项 Go Micro也提供通过命令行参数micro.Flags传递配置参数： 12345678910111213import ( "github.com/micro/cli" "github.com/micro/go-micro")service := micro.NewService( micro.Flags( cli.StringFlag&#123; Name: "environment", Usage: "The environment", &#125;, )) 初始化服务 初始化服务时候，可以解析命令行标识参数，增加标识参数可以使用micro.Action选项： 12345678service.Init( micro.Action(func(c *cli.Context) &#123; env := c.StringFlag("environment") if len(env) &gt; 0 &#123; fmt.Println("Environment set to", env) &#125; &#125;),) Go Micro提供预置的标识，service.Init执行时就会设置并解析这些参数。所有的标识参考. 注册服务 处理器会与服务一起被注册，就像http处理器一样。 12345service := micro.NewService( micro.Name("greeter"),)proto.RegisterGreeterHandler(service.Server(), new(Greeter)) 运行服务 Service 服务可以调用server.Run运行起来。这一步会让服务绑到配置中的地址（默认遵循RFC1918，分配随机的端口）接收请求。 另外，这一步会在服务启动时向注册中心注册，并在服务接收到关闭信号时卸载。 123if err := service.Run(); err != nil &#123; log.Fatal(err)&#125; 之后可以通过 go run greeter.go 注: 如果需要更改register为 consul， 更改环境变量 export MICRO_REGISTRY=consul 完成的服务greeter.go 123456789101112131415161718192021222324252627282930313233package mainimport ( "log" "github.com/micro/go-micro" proto "github.com/micro/examples/service/proto" "golang.org/x/net/context")type Greeter struct&#123;&#125;func (g *Greeter) Hello(ctx context.Context, req *proto.HelloRequest, rsp *proto.HelloResponse) error &#123; rsp.Greeting = "Hello " + req.Name return nil&#125;func main() &#123; service := micro.NewService( micro.Name("greeter"), micro.Version("latest"), ) service.Init() proto.RegisterGreeterHandler(service.Server(), new(Greeter)) if err := service.Run(); err != nil &#123; log.Fatal(err) &#125;&#125; 需要注意的是，要保证服务发现机制运行起来，这样服务才能注册，其它服务或客户端才能发现它。 对服务端进行测试使用命令 1micro call SrvName funcName args 使用示例 1micro call go.micro.srv.demo Demo.Call "&#123;\"name\": \"John\"&#125;" 使用GO-Micro编写客户端Client包用于查询服务，当创建服务时，也包含了一个客户端，这个客户端匹配服务所使用的初始化包。 查询上面的服务很简单： 12345678910111213// 创建greate客户端，这需要传入服务名与服务的客户端方法构建的客户端对象greeter := proto.NewGreeterClient("greeter", service.Client())// 在Greeter handler上请求调用Hello方法rsp, err := greeter.Hello(context.TODO(), &amp;proto.HelloRequest&#123; Name: "John",&#125;)if err != nil &#123; fmt.Println(err) return&#125;fmt.Println(rsp.Greeter) proto.NewGreeterClient 需要服务名与客户端实例来请求服务。]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-xorm 使用]]></title>
    <url>%2F2019%2F05%2F20%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fxorm%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Xorm 使用创建引擎 创建引擎 driverName, dataSourceName和database/sql接口相同 第一种方法 1234567891011import ( _ "github.com/go-sql-driver/mysql" "github.com/go-xorm/xorm")var engine *xorm.Enginefunc main() &#123; var err error engine, err = xorm.NewEngine("mysql", "root:123@/test?charset=utf8")&#125; 第二种方法 1234567891011import ( _ "github.com/mattn/go-sqlite3" "github.com/go-xorm/xorm")var engine *xorm.Enginefunc main() &#123; var err error engine, err = xorm.NewEngine("sqlite3", "./test.db")&#125; 创建Engine组(操作集群) 123456dataSourceNameSlice := []string&#123;masterDataSourceName, slave1DataSourceName, slave2DataSourceName&#125;engineGroup, err := xorm.NewEngineGroup(driverName, dataSourceNameSlice)masterEngine, err := xorm.NewEngine(driverName, masterDataSourceName)slave1Engine, err := xorm.NewEngine(driverName, slave1DataSourceName)slave2Engine, err := xorm.NewEngine(driverName, slave2DataSourceName)engineGroup, err := xorm.NewEngineGroup(masterEngine, []*Engine&#123;slave1Engine, slave2Engine&#125;) 所有使用 engine 都可以简单的用 engineGroup 来替换。 同步结构体到数据库表 定义一个和表同步的结构体，并且自动同步结构体到数据库 1234567891011type User struct &#123; Id int64 Name string Salt string Age int Passwd string `xorm:"varchar(200)"` Created time.Time `xorm:"created"` Updated time.Time `xorm:"updated"`&#125;err := engine.Sync2(new(User)) CURD操作Insert 操作 Insert 插入一条或者多条记录 12345678910111213affected, err := engine.Insert(&amp;user)// INSERT INTO struct () values ()affected, err := engine.Insert(&amp;user1, &amp;user2)// INSERT INTO struct1 () values ()// INSERT INTO struct2 () values ()affected, err := engine.Insert(&amp;users)// INSERT INTO struct () values (),(),()affected, err := engine.Insert(&amp;user1, &amp;users)// INSERT INTO struct1 () values ()// INSERT INTO struct2 () values (),(),() Delete操作 Delete 删除记录，需要注意，删除必须至少有一个条件，否则会报错。要清空数据库可以用EmptyTable 12345affected, err := engine.Where(...).Delete(&amp;user)// DELETE FROM user Where ...affected, err := engine.ID(2).Delete(&amp;user)// DELETE FROM user Where id = ? update 更新操作 Update 更新数据，除非使用Cols,AllCols函数指明，默认只更新非空和非0的字段 1234567891011121314151617181920affected, err := engine.ID(1).Update(&amp;user)// UPDATE user SET ... Where id = ?affected, err := engine.Update(&amp;user, &amp;User&#123;Name:name&#125;)// UPDATE user SET ... Where name = ?var ids = []int64&#123;1, 2, 3&#125;affected, err := engine.In(ids).Update(&amp;user)// UPDATE user SET ... Where id IN (?, ?, ?)// force update indicated columns by Colsaffected, err := engine.ID(1).Cols("age").Update(&amp;User&#123;Name:name, Age: 12&#125;)// UPDATE user SET age = ?, updated=? Where id = ?// force NOT update indicated columns by Omitaffected, err := engine.ID(1).Omit("name").Update(&amp;User&#123;Name:name, Age: 12&#125;)// UPDATE user SET age = ?, updated=? Where id = ?affected, err := engine.ID(1).AllCols().Update(&amp;user)// UPDATE user SET name=?,age=?,salt=?,passwd=?,updated=? Where id = ? query操作 Query 最原始的也支持SQL语句查询，返回的结果类型为 []map[string]byte。QueryString 返回 []map[string]string, QueryInterface 返回 []map[string]interface{}. 12345678results, err := engine.Query("select * from user")results, err := engine.Where("a = 1").Query()results, err := engine.QueryString("select * from user")results, err := engine.Where("a = 1").QueryString()results, err := engine.QueryInterface("select * from user")results, err := engine.Where("a = 1").QueryInterface() Get 操作 Get 查询单条记录 12345678910111213141516171819202122has, err := engine.Get(&amp;user)// SELECT * FROM user LIMIT 1has, err := engine.Where(&quot;name = ?&quot;, name).Desc(&quot;id&quot;).Get(&amp;user)// SELECT * FROM user WHERE name = ? ORDER BY id DESC LIMIT 1var name stringhas, err := engine.Table(&amp;user).Where(&quot;id = ?&quot;, id).Cols(&quot;name&quot;).Get(&amp;name)// SELECT name FROM user WHERE id = ?var id int64has, err := engine.Table(&amp;user).Where(&quot;name = ?&quot;, name).Cols(&quot;id&quot;).Get(&amp;id)has, err := engine.SQL(&quot;select id from user&quot;).Get(&amp;id)// SELECT id FROM user WHERE name = ?var valuesMap = make(map[string]string)has, err := engine.Table(&amp;user).Where(&quot;id = ?&quot;, id).Get(&amp;valuesMap)// SELECT * FROM user WHERE id = ?var valuesSlice = make([]interface&#123;&#125;, len(cols))has, err := engine.Table(&amp;user).Where(&quot;id = ?&quot;, id).Cols(cols...).Get(&amp;valuesSlice)// SELECT col1, col2, col3 FROM user WHERE id = ? Find操作(查询多条记录) Find 查询多条记录，当然可以使用Join和extends来组合使用 1234567891011121314151617181920var users []Usererr := engine.Where("name = ?", name).And("age &gt; 10").Limit(10, 0).Find(&amp;users)// SELECT * FROM user WHERE name = ? AND age &gt; 10 limit 10 offset 0type Detail struct &#123; Id int64 UserId int64 `xorm:"index"`&#125;type UserDetail struct &#123; User `xorm:"extends"` Detail `xorm:"extends"`&#125;var users []UserDetailerr := engine.Table("user").Select("user.*, detail.*") Join("INNER", "detail", "detail.user_id = user.id"). Where("user.name = ?", name).Limit(10, 0). Find(&amp;users)// SELECT user.*, detail.* FROM user INNER JOIN detail WHERE user.name = ? limit 10 offset 0 记录Exist 判断 Exist 检测记录是否存在 12345678910111213141516171819has, err := testEngine.Exist(new(RecordExist))// SELECT * FROM record_exist LIMIT 1has, err = testEngine.Exist(&amp;RecordExist&#123; Name: &quot;test1&quot;, &#125;)// SELECT * FROM record_exist WHERE name = ? LIMIT 1has, err = testEngine.Where(&quot;name = ?&quot;, &quot;test1&quot;).Exist(&amp;RecordExist&#123;&#125;)// SELECT * FROM record_exist WHERE name = ? LIMIT 1has, err = testEngine.SQL(&quot;select * from record_exist where name = ?&quot;, &quot;test1&quot;).Exist()// select * from record_exist where name = ?has, err = testEngine.Table(&quot;record_exist&quot;).Exist()// SELECT * FROM record_exist LIMIT 1has, err = testEngine.Table(&quot;record_exist&quot;).Where(&quot;name = ?&quot;, &quot;test1&quot;).Exist()// SELECT * FROM record_exist WHERE name = ? LIMIT 1 执行原生sql语句 Exec 执行一个SQL语句 1affected, err := engine.Exec("update user set age = ? where name = ?", age, name) 数据库遍历 Iterate 和 Rows 根据条件遍历数据库，可以有两种方式: Iterate and Rows 1234567891011121314151617181920err := engine.Iterate(&amp;User&#123;Name:name&#125;, func(idx int, bean interface&#123;&#125;) error &#123; user := bean.(*User) return nil&#125;)// SELECT * FROM usererr := engine.BufferSize(100).Iterate(&amp;User&#123;Name:name&#125;, func(idx int, bean interface&#123;&#125;) error &#123; user := bean.(*User) return nil&#125;)// SELECT * FROM user Limit 0, 100// SELECT * FROM user Limit 101, 100rows, err := engine.Rows(&amp;User&#123;Name:name&#125;)// SELECT * FROM userdefer rows.Close()bean := new(Struct)for rows.Next() &#123; err = rows.Scan(bean)&#125; 统计操作 Count 获取记录条数 12counts, err := engine.Count(&amp;user)// SELECT count(*) AS total FROM user Sum 求和函数 1234567891011agesFloat64, err := engine.Sum(&amp;user, "age")// SELECT sum(age) AS total FROM useragesInt64, err := engine.SumInt(&amp;user, "age")// SELECT sum(age) AS total FROM usersumFloat64Slice, err := engine.Sums(&amp;user, "age", "score")// SELECT sum(age), sum(score) FROM usersumInt64Slice, err := engine.SumsInt(&amp;user, "age", "score")// SELECT sum(age), sum(score) FROM user 条件编辑器 条件编辑器 12err := engine.Where(builder.NotIn("a", 1, 2).And(builder.In("b", "c", "d", "e"))).Find(&amp;users)// SELECT id, name ... FROM user WHERE a NOT IN (?, ?) AND b IN (?, ?, ?) 事务操作 在一个Go程中多次操作数据库，但没有事务 123456789101112131415161718session := engine.NewSession()defer session.Close()user1 := Userinfo&#123;Username: "xiaoxiao", Departname: "dev", Alias: "lunny", Created: time.Now()&#125;if _, err := session.Insert(&amp;user1); err != nil &#123; return err&#125;user2 := Userinfo&#123;Username: "yyy"&#125;if _, err := session.Where("id = ?", 2).Update(&amp;user2); err != nil &#123; return err&#125;if _, err := session.Exec("delete from userinfo where username = ?", user2.Username); err != nil &#123; return err&#125;return nil 在一个Go程中有事务 12345678910111213141516171819202122232425session := engine.NewSession()defer session.Close()// add Begin() before any actionif err := session.Begin(); err != nil &#123; // if returned then will rollback automatically return err&#125;user1 := Userinfo&#123;Username: "xiaoxiao", Departname: "dev", Alias: "lunny", Created: time.Now()&#125;if _, err := session.Insert(&amp;user1); err != nil &#123; return err&#125;user2 := Userinfo&#123;Username: "yyy"&#125;if _, err := session.Where("id = ?", 2).Update(&amp;user2); err != nil &#123; return err&#125;if _, err := session.Exec("delete from userinfo where username = ?", user2.Username); err != nil &#123; return err&#125;// add Commit() after all actionsreturn session.Commit() 事物的简写方法 12345678910111213141516res, err := engine.Transaction(func(session *xorm.Session) (interface&#123;&#125;, error) &#123; user1 := Userinfo&#123;Username: "xiaoxiao", Departname: "dev", Alias: "lunny", Created: time.Now()&#125; if _, err := session.Insert(&amp;user1); err != nil &#123; return nil, err &#125; user2 := Userinfo&#123;Username: "yyy"&#125; if _, err := session.Where("id = ?", 2).Update(&amp;user2); err != nil &#123; return nil, err &#125; if _, err := session.Exec("delete from userinfo where username = ?", user2.Username); err != nil &#123; return nil, err &#125; return nil, nil&#125;) 上下文缓存 上下文缓存，如果启用，那么针对单个对象的查询将会被缓存到系统中，可以被下一个查询使用。 123456789101112131415161718192021222324sess := engine.NewSession()defer sess.Close()var context = xorm.NewMemoryContextCache()var c2 ContextGetStructhas, err := sess.ID(1).ContextCache(context).Get(&amp;c2)assert.NoError(t, err)assert.True(t, has)assert.EqualValues(t, 1, c2.Id)assert.EqualValues(t, "1", c2.Name)sql, args := sess.LastSQL()assert.True(t, len(sql) &gt; 0)assert.True(t, len(args) &gt; 0)var c3 ContextGetStructhas, err = sess.ID(1).ContextCache(context).Get(&amp;c3)assert.NoError(t, err)assert.True(t, has)assert.EqualValues(t, 1, c3.Id)assert.EqualValues(t, "1", c3.Name)sql, args = sess.LastSQL()assert.True(t, len(sql) == 0)assert.True(t, len(args) == 0)]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Orm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swagger从源码生成spec]]></title>
    <url>%2F2019%2F04%2F29%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fswagger%E4%BB%8E%E6%BA%90%E7%A0%81%E7%94%9F%E6%88%90spec%2F</url>
    <content type="text"><![CDATA[从源码生成spec文档spec 生成命令12345678910111213141516171819Usage: swagger [OPTIONS] generate spec [spec-OPTIONS]generate a swagger spec document from a go applicationApplication Options: -q, --quiet silence logs -o, --output=LOG-FILE redirect logs to fileHelp Options: -h, --help Show this help message[spec command options] -b, --base-path= the base path to use (default: .) -t, --tags= build tags -m, --scan-models includes models that were annotated with &apos;swagger:model&apos; --compact when present, doesn&apos;t prettify the json -o, --output= the file to write to -i, --input= the file to use as input spec生成方法生成包的spec1swagger generate spec -o ./swagger.json 如果不提供一个mian文件给swagger， swagger 将遍历包中的左右文件以及文件的依赖并生成spec 如果想要提供给swagger一个main文件，可以在main文件中添加如下注释: 1//go:generate swagger generate spec 它使用go工具加载器加载应用程序，然后扫描代码库使用的所有软件包。这意味着对于可被发现的东西，它需要通过主包触发的代码路径来访问。 合并yml文件定义的spec1swagger generate spec -i ./swagger.yml -o ./swagger.json 生成yaml格式的spec文件1swagger generate spec -o ./swagger.yml Spec生成规则swagger:meta配置 包spec文件的一些原数据， 语法 1swagger:meta 可配置的属性如下 Annotation Format Terms Of Service allows for either a url or a free text definition describing the terms of services for the API Consumes a list of default (global) mime type values, one per line, for the content the API receives. List of supported mime types Produces a list of default (global) mime type values, one per line, for the content the API sends. List of supported mime types Schemes a list of default schemes the API accept (possible values: http, https, ws, wss) https is preferred as default when configured Version the current version of the API Host the host from where the spec is served Base path the default base path for this API Contact the name of for the person to contact concerning the API eg. John Doe &#106;&#111;&#x68;&#110;&#64;&#98;&#x6c;&#x6f;&#x67;&#115;&#x2e;&#99;&#111;&#x6d; http://john.blogs.com License the name of the license followed by the URL of the license eg. MIT http://opensource.org/license/MIT Security a dictionary of key: []string{scopes} SecurityDefinitions list of supported authorization types https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#securityDefinitionsObject Extensions list of extensions to Swagger Schema. The field name MUST begin with x-, for example, x-internal-id. The value can be null, a primitive, an array or an object. 示例: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Package classification Petstore API.//// the purpose of this application is to provide an application// that is using plain go code to define an API//// This should demonstrate all the possible comment annotations// that are available to turn go code into a fully compliant swagger 2.0 spec//// Terms Of Service://// there are no TOS at this moment, use at your own risk we take no responsibility//// Schemes: http, https// Host: localhost// BasePath: /v2// Version: 0.0.1// License: MIT http://opensource.org/licenses/MIT// Contact: John Doe&lt;john.doe@example.com&gt; http://john.doe.com//// Consumes:// - application/json// - application/xml//// Produces:// - application/json// - application/xml//// Security:// - api_key://// SecurityDefinitions:// api_key:// type: apiKey// name: KEY// in: header// oauth2:// type: oauth2// authorizationUrl: /oauth2/auth// tokenUrl: /oauth2/token// in: header// scopes:// bar: foo// flow: accessCode//// Extensions:// x-meta-value: value// x-meta-array:// - value1// - value2// x-meta-array-obj:// - name: obj// value: field//// swagger:metapackage classification swagger:route路径配置的方法， 此操作获取一个唯一ID，用于后面该方法的名称 语法 1swagger:route [method] [path pattern] [?tag1 tag2 tag3] [operation id] 属性 Annotation Format Consumes a list of operation specific mime type values, one per line, for the content the API receives Produces a list of operation specific mime type values, one per line, for the content the API sends Schemes a list of operation specific schemes the API accept (possible values: http, https, ws, wss) https is preferred as default when configured Security a dictionary of key: []string{scopes} Responses a dictionary of status code to named response 示例 123456789101112131415161718192021222324252627282930// ServeAPI serves the API for this record storefunc ServeAPI(host, basePath string, schemes []string) error &#123; // swagger:route GET /pets pets users listPets // // Lists pets filtered by some parameters. // // This will show all available pets by default. // You can get the pets that are out of stock // // Consumes: // - application/json // - application/x-protobuf // // Produces: // - application/json // - application/x-protobuf // // Schemes: http, https, ws, wss // // Security: // api_key: // oauth: read, write // // Responses: // default: genericError // 200: someResponse // 422: validationError mountItem(&quot;GET&quot;, basePath+&quot;/pets&quot;, nil)&#125; swagger:parameters参数注释结构连接到一个或多个操作。 生成的swagger spec中的参数可以由多个结构组成。 语法 1swagger:parameters [operationid1 operationid2] 参数 Annotation Format In where to find the parameter Collection Format when a slice the formatter for the collection when serialized on the request Maximum specifies the maximum a number or integer value can have Minimum specifies the minimum a number or integer value can have Multiple of specifies a value a number or integer value must be a multiple of Minimum length the minimum length for a string value Maximum length the maximum length for a string value Pattern a regular expression a string value needs to match Minimum items the minimum number of items a slice needs to have Maximum items the maximum number of items a slice can have Unique when set to true the slice can only contain unique items Required when set to true this value needs to be present in the request Example an example value, parsed as the field’s type (objects and slices are parsed as JSON) 对于slice属性，还需要定义一些项。这可能是一个嵌套集合，用于指示嵌套级别，值是一个基于0的索引 Annotation Format Items.n.Maximum specifies the maximum a number or integer value can have at the level n Items.n.Minimum specifies the minimum a number or integer value can have at the level n Items.n.Multiple of specifies a value a number or integer value must be a multiple of Items.n.Minimum length the minimum length for a string value at the level n Items.n.Maximum length the maximum length for a string value at the level n Items.n.Pattern a regular expression a string value needs to match at the level n Items.n.Minimum items the minimum number of items a slice needs to have at the level n Items.n.Maximum items the maximum number of items a slice can have at the level n Items.n.Unique when set to true the slice can only contain unique items at the level n 示例： 12345678910111213141516171819// swagger:parameters listBars addBarstype BarSliceParam struct &#123; // a BarSlice has bars which are strings // // min items: 3 // max items: 10 // unique: true // items.minItems: 4 // items.maxItems: 9 // items.items.minItems: 5 // items.items.maxItems: 8 // items.items.items.minLength: 3 // items.items.items.maxLength: 10 // items.items.items.pattern: \w+ // collection format: pipe // in: query // example: [[[&quot;bar_000&quot;]]] BarSlice [][][]string `json:&quot;bar_slice&quot;`&#125; swagger:operation操作注释将路径链接到方法 语法 1swagger:operation [method] [path pattern] [?tag1 tag2 tag3] [operation id] 参数 Field Name Type Description tags [string] A list of tags for API documentation control. Tags can be used for logical grouping of operations by resources or any other qualifier. summary string A short summary of what the operation does. For maximum readability in the swagger-ui, this field SHOULD be less than 120 characters. description string A verbose explanation of the operation behavior. GFM syntax can be used for rich text representation. externalDocs External Documentation Object Additional external documentation for this operation. operationId string Unique string used to identify the operation. The id MUST be unique among all operations described in the API. Tools and libraries MAY use the operationId to uniquely identify an operation, therefore, it is recommended to follow common programming naming conventions. consumes [string] A list of MIME types the operation can consume. This overrides the consumesdefinition at the Swagger Object. An empty value MAY be used to clear the global definition. Value MUST be as described under Mime Types. produces [string] A list of MIME types the operation can produce. This overrides the producesdefinition at the Swagger Object. An empty value MAY be used to clear the global definition. Value MUST be as described under Mime Types. parameters [Parameter Object | Reference Object] A list of parameters that are applicable for this operation. If a parameter is already defined at the Path Item, the new definition will override it, but can never remove it. The list MUST NOT include duplicated parameters. A unique parameter is defined by a combination of a name and location. The list can use the Reference Object to link to parameters that are defined at the Swagger Object’s parameters. There can be one “body” parameter at most. responses Responses Object Required. The list of possible responses as they are returned from executing this operation. schemes [string] The transfer protocol for the operation. Values MUST be from the list: &quot;http&quot;, &quot;https&quot;, &quot;ws&quot;, &quot;wss&quot;. The value overrides the Swagger Object schemesdefinition. deprecated boolean Declares this operation to be deprecated. Usage of the declared operation should be refrained. Default value is false. security [Security Requirement Object] A declaration of which security schemes are applied for this operation. The list of values describes alternative security schemes that can be used (that is, there is a logical OR between the security requirements). This definition overrides any declared top-level security. To remove a top-level security declaration, an empty array can be used. 示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344// ServeAPI serves the API for this record storefunc ServeAPI(host, basePath string, schemes []string) (err error) &#123; // swagger:operation GET /pets getPet // // Returns all pets from the system that the user has access to // // Could be any pet // // --- // produces: // - application/json // - application/xml // - text/xml // - text/html // parameters: // - name: tags // in: query // description: tags to filter by // required: false // type: array // items: // type: string // collectionFormat: csv // - name: limit // in: query // description: maximum number of results to return // required: false // type: integer // format: int32 // responses: // '200': // description: pet response // schema: // type: array // items: // "$ref": "#/definitions/pet" // default: // description: unexpected error // schema: // "$ref": "#/definitions/errorModel" mountItem("GET", basePath+"/pets", nil) return&#125; swagger:response读取用swagger:response修饰的结构， 并使用该信息填充相应的标题和模式 swagger：route可以指定状态代码的响应名称，然后匹配的响应将用于swagger定义中的该操作。 语法 1swagger:response [?response name] 属性 Annotation Description In where to find the field Collection Format when a slice the formatter for the collection when serialized on the request Maximum specifies the maximum a number or integer value can have Minimum specifies the minimum a number or integer value can have Multiple of specifies a value a number or integer value must be a multiple of Minimum length the minimum length for a string value Maximum length the maximum length for a string value Pattern a regular expression a string value needs to match Minimum items the minimum number of items a slice needs to have Maximum items the maximum number of items a slice can have Unique when set to true the slice can only contain unique items Example an example value, parsed as the field’s type (objects and slices are parsed as JSON) 对于切片属性，还有要定义的项目。这可能是嵌套集合，用于指示嵌套级别，该值是基于0的索引，因此items.minLength与items.0.minLength相同 Annotation Format Items.n.Maximum specifies the maximum a number or integer value can have at the level n Items.n.Minimum specifies the minimum a number or integer value can have at the level n Items.n.Multiple of specifies a value a number or integer value must be a multiple of Items.n.Minimum length the minimum length for a string value at the level n Items.n.Maximum length the maximum length for a string value at the level n Items.n.Pattern a regular expression a string value needs to match at the level n Items.n.Minimum items the minimum number of items a slice needs to have at the level n Items.n.Maximum items the maximum number of items a slice can have at the level n Items.n.Unique when set to true the slice can only contain unique items at the level n 示例 123456789101112131415// A ValidationError is an error that is used when the required input fails validation.// swagger:response validationErrortype ValidationError struct &#123; // The error message // in: body Body struct &#123; // The validation message // // Required: true // Example: Expected type int Message string // An optional field name to which this validation applies FieldName string &#125;&#125; swagger:momdel定义结构数据 语法 1swagger:model [?model name] 属性 Annotation Description Maximum specifies the maximum a number or integer value can have Minimum specifies the minimum a number or integer value can have Multiple of specifies a value a number or integer value must be a multiple of Minimum length the minimum length for a string value Maximum length the maximum length for a string value Pattern a regular expression a string value needs to match Minimum items the minimum number of items a slice needs to have Maximum items the maximum number of items a slice can have Unique when set to true the slice can only contain unique items Required when set to true this value needs to be set on the schema Read Only when set to true this value will be marked as read-only and is not required in request bodies Example an example value, parsed as the field’s type (objects and slices are parsed as JSON) 对于切片属性，还有要定义的项目。这可能是嵌套集合，用于指示嵌套级别，该值是基于0的索引，因此items.minLength与items.0.minLength相同 Annotation Format Items.n.Maximum specifies the maximum a number or integer value can have at the level n Items.n.Minimum specifies the minimum a number or integer value can have at the level n Items.n.Multiple of specifies a value a number or integer value must be a multiple of Items.n.Minimum length the minimum length for a string value at the level n Items.n.Maximum length the maximum length for a string value at the level n Items.n.Pattern a regular expression a string value needs to match at the level n Items.n.Minimum items the minimum number of items a slice needs to have at the level n Items.n.Maximum items the maximum number of items a slice can have at the level n Items.n.Unique when set to true the slice can only contain unique items at the level n 示例 1234567891011121314151617181920212223242526272829// User represents the user for this application//// A user is the security principal for this application.// It&apos;s also used as one of main axes for reporting.//// A user can have friends with whom they can share what they like.//// swagger:modeltype User struct &#123; // the id for this user // // required: true // min: 1 ID int64 `json:&quot;id&quot;` // the name for this user // required: true // min length: 3 Name string `json:&quot;name&quot;` // the email address for this user // // required: true // example: user@provider.net Email strfmt.Email `json:&quot;login&quot;` // the friends for this user Friends []User `json:&quot;friends&quot;`&#125; swagger:allOf将嵌入类型标记为allOf的成员 语法 1swagger:allOf 示例 1234567891011121314151617181920212223242526272829303132333435// A SimpleOne is a model with a few simple fieldstype SimpleOne struct &#123; ID int64 `json:"id"` Name string `json:"name"` Age int32 `json:"age"`&#125;// A Something struct is used by other structstype Something struct &#123; DID int64 `json:"did"` Cat string `json:"cat"`&#125;// Notable is a model in a transitive package.// it's used for embedding in another model//// swagger:model withNotestype Notable struct &#123; Notes string `json:"notes"` Extra string `json:"extra"`&#125;// An AllOfModel is composed out of embedded structs but it should build// an allOf propertytype AllOfModel struct &#123; // swagger:allOf SimpleOne // swagger:allOf mods.Notable Something // not annotated with anything, so should be included CreatedAt strfmt.DateTime `json:"createdAt"`&#125; swagger:strfmtstrfmt标注名称的类型为字符串格式。该名称是必需的，将用作此特定字符串格式的格式名称。 语法 1swagger:strfmt [name] 字符串格式包含 uuid, uuid3, uuid4, uuid5 email uri (absolute) hostname ipv4 ipv6 credit card isbn, isbn10, isbn13 social security number hexcolor rgbcolor date date-time duration password custom string formats 示例 12345678910111213141516171819202122232425262728293031323334353637func init() &#123; eml := Email("") Default.Add("email", &amp;eml, govalidator.IsEmail)&#125;// Email represents the email string format as specified by the json schema spec//// swagger:strfmt emailtype Email string// MarshalText turns this instance into textfunc (e Email) MarshalText() ([]byte, error) &#123; return []byte(string(e)), nil&#125;// UnmarshalText hydrates this instance from textfunc (e *Email) UnmarshalText(data []byte) error &#123; // validation is performed later on *e = Email(string(data)) return nil&#125;func (b *Email) Scan(raw interface&#123;&#125;) error &#123; switch v := raw.(type) &#123; case []byte: *b = Email(string(v)) case string: *b = Email(v) default: return fmt.Errorf("cannot sql.Scan() strfmt.Email from: %#v", v) &#125; return nil&#125;func (b Email) Value() (driver.Value, error) &#123; return driver.Value(string(b)), nil&#125; swagger:discriminated将嵌入类型标记为allOf的成员并设置x-class值。在接口定义上，对允许swagger：name的方法有另一个注释 语法 1swagger:allOf org.example.something.TypeName 示例 12345678910111213141516171819202122232425262728293031323334// TeslaCar is a tesla car//// swagger:modeltype TeslaCar interface &#123; // The model of tesla car // // discriminator: true // swagger:name model Model() string // AutoPilot returns true when it supports autopilot // swagger:name autoPilot AutoPilot() bool&#125;// The ModelS version of the tesla car//// swagger:model modelStype ModelS struct &#123; // swagger:allOf com.tesla.models.ModelS TeslaCar // The edition of this Model S Edition string `json:"edition"`&#125;// The ModelX version of the tesla car//// swagger:model modelXtype ModelX struct &#123; // swagger:allOf com.tesla.models.ModelX TeslaCar // The number of doors on this Model X Doors int32 `json:"doors"`&#125; swagger:ignore将结构标记为从Swagger规范输出中显式忽略 语法 1swagger:ignore 在线swagger editor 文档编辑器启动方式 镜像拉取 docker pull swaggerapi/swagger-editor 镜像运行 docker run –rm -p 80:8080 swaggerapi/swagger-editor]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 godoc 生成文档]]></title>
    <url>%2F2019%2F04%2F29%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2F%E4%BD%BF%E7%94%A8%20godoc%20%E7%94%9F%E6%88%90%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[代码注释规范godoc 支持package、const、 var 和func 这些代码生成文档，而且只会对首字母大写自动生成，而小写的私有方法不会被生成到文档，下面介绍下各种注释的使用方式 Package 在package name 代码上面，紧挨着代码进行注释 注释内容中间不能有空行 如果一个包中有多个包注释，就会把多个包的注释放在一起，并按照文件名的首字母顺序排序 注释的格式为 package name [summery] （summery可以是多行） 注释最前面一句话会模块的summary会出现在package index中，第一句话以及之后的内容会出现在OverView中 注释代码 1234/*A web framework includes app server, logger, panicer, util and so on. */ package document 文档生成效果 常量、变量和函数 紧挨着定义代码，在常量、变量和函数上面进行注释 注释格式为: FunctionName Summary （Summary可以是多行） 想圈起来说明参数可以加缩进， 进行预格式化 注释最前面一句话会出现在package index中，第一句话以及之后的内容会出现在OverView中 常量的summary会放到[ Constants ] 里， 变量的summary会放到[ Variables ] 里， 函数的summary会放到 [ func ] 中 注释代码 1234567// Marshaler is the interface implemented by objects that/*can marshal themselves into valid JSON.*/ type Marshaler interface &#123; MarshalJSON() ([]byte, error)&#125; 文档生成效果 BUG godoc会先查找:[空格]BUG 然后显示在Package说明文档最下面 如果代码中有bug，可以用BUG注释, 它会被识别为一个 bug，可以在文档中的「Bugs」中看到。 注释代码 1BUG(who): xxx Deprecated 通过Deprecated注释的内容将不会体现在godoc中，但是还是挺有用的，Goland可以识别它并作出提示。 注释代码1// Deprecated: xxx 返回值Output标签 在函数体中如果定义了Output标签，会在文档页面上展示输出内容 如果没有定义将不会展示( 非必须 ) 一般为测试代码使用，用来展示方法的输出结果 注释代码 123456func ExamplePeel() &#123; fmt.Println("Hello Banana") // Output: // Hello Banana&#125; 使用doc.go书写注释 如果包注释超过3行，可以把注释都迁移到doc.go文件中（可以在当前目录新建一个doc.go文件）。 多行注释自然需要支持一些复杂的格式，如果单行中首字母是大写，并且结尾没有标点符号是标题(标题字体会加粗变蓝，中文加粗变蓝需要加上一个大写字母) 首字母是小写，或者结果又标点符号的是段落 有缩进是预格式化， 在有预格式化的注释段中，不会有标题特征 example_PackageName_test.go 的注释规则 包示例代码注释非常重要，项目的使用方法就是通过每个包例子搭建起来的, 文件必须放在当前包下 示例文件需要创建一个新文件，名称格式为 example_Packagename_test.go. 不加example前缀也是可以的，但是不加前缀通常是单元测试的文件命名规则， 包名的格式为 当前包名 + _test. 包中函数名称的格式为ExampleFuncName[_tag]。不加函数名的话是包级别的示例。加函数名的话是函数级别的示例。 函数的注释会展示在页面上 函数结果加上 // Output: 注释，可以说明函数的返回值，并展示在文档上 包级别的示例函数名称规则 Example 代码示例 123func Example() &#123; logger.Info(&quot;hello, world.&quot;)&#125; 文档展示结果 函数级别示例函数名称规则 ExampleFuncName 代码示例 123456func ExampleNewLogger() &#123; w := os.Stdout flag := log.Llongfile l := logger.NewWriterLogger(w, flag, 3) l.Info(&quot;hello, world&quot;)&#125; 文档展示结果 Output输出注释格式 OutPut: xxxxxxx 代码示例 1234567891011121314// 此函数将被展示在OverView区域, 并展示noOutput标签func Example_noOutput() &#123; fmt.Println("Hello OverView") // (Output: )非必须, 存在时将会展示输出结果&#125;// 此函数将被展示在Function区域// Peel必须是banana包实现的方法func ExamplePeel() &#123; fmt.Println("Hello Banana") // Output: // Hello Banana&#125; 本地运行godoc 文档1godoc -http=:6060]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swagger spec 的编写规范]]></title>
    <url>%2F2019%2F04%2F29%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fswagger%20spec%20%E7%9A%84%E7%BC%96%E5%86%99%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[一、Swagger简介什么是swaggerSwagger是一个简单但功能强大的API表达工具。它具有地球上最大的API工具生态系统，数以千计的开发人员，使用几乎所有的现代编程语言，都在支持和使用Swagger。使用Swagger生成API，我们可以得到交互式文档，自动生成代码的SDK以及API的发现特性等。 Go swagger文档配置文件示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748swagger: "2.0"info: description: From the todo list tutorial on goswagger.io title: A Todo list application version: 1.0.0consumes: - application/io.goswagger.examples.todo-list.v1+jsonproduces:- application/io.goswagger.examples.todo-list.v1+jsonschemes:- httpdefinitions: item: type: object required: - description properties: id: type: integer format: int64 readOnly: true description: type: string minLength: 1 completed: type: booleanpaths: /: get: tags: - todos parameters: - name: since in: query type: integer format: int64 - name: limit in: query type: integer format: int32 default: 20 responses: 200: description: list the todo operations schema: type: array items: $ref: "#/definitions/item" Go-swagger 环境安装 安装依赖 1234567891011121314151617go get github.com/go-openapi/errorsgo get github.com/go-openapi/loadsgo get github.com/go-openapi/runtimego get github.com/go-openapi/specgo get github.com/go-openapi/strfmtgo get github.com/go-openapi/swaggo get github.com/go-openapi/validatego get github.com/jessevdk/go-flagsgo get golang.org/x/net/context 安装swagger 1go get -u github.com/go-swagger/go-swagger/cmd/swagger 语言选择: JSON vs YAML我们可以选择使用JSON或者YAML的语言格式来编写API文档。但是个人建议使用YAML来写，原因是它更简单。一图胜千言，先看用JSON写的文档： 123456789101112131415161718192021222324252627282930313233343536373839404142&#123; "swagger": "2.0", "info": &#123; "version": "1.0.0", "title": "Simple API", "description": "A simple API to learn how to write OpenAPI Specification" &#125;, "schemes": [ "https" ], "host": "simple.api", "basePath": "/openapi101", "paths": &#123; "/persons": &#123; "get": &#123; "summary": "Gets some persons", "description": "Returns a list containing all persons.", "responses": &#123; "200": &#123; "description": "A list of Person", "schema": &#123; "type": "array", "items": &#123; "properties": &#123; "firstName": &#123; "type": "string" &#125;, "lastName": &#123; "type": "string" &#125;, "username": &#123; "type": "string" &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 再来看看同一份API文档的YAML实现： 1234567891011121314151617181920212223242526272829303132swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 对于普通人来说，似乎用YAML更能够简化书写和阅读。这里我们并没有非此即彼的选择问题，因为： 几乎所用支持OpenAPI规范的工具都支持YAML 有很多的工具可以实现YAML-JSON之间的转换 所以，用自己喜欢的方式书写即可。（后面的示例文档也都是用YAML来写的。强烈推荐使用YAML。） 二、基本的swagger api定义方法2.1 最简单的例子我们从一个最简单（几乎没有东西）的API文档开始： 12345678910111213swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: &#123;&#125; 这个文档的内容分成四部分，下面分别来说明。 2.1.1 OpenAPI规范的版本号首先我们要通过一个swagger属性来声明OpenAPI规范的版本。 1swagger: "2.0" 你没看错，是swagger，上面已经介绍了，OpenAPI规范是基于Swagger的，在未来的版本中，这个属性可能会换成别的。 目前这个属性的值，暂时只能填写为2.0。 2.1.2 API描述信息然后我们需要说明一下API文档的相关信息，比如API文档版本（注意不同于上面的规范版本）、API文档名称已经可选的描述信息。 1234info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specification 2.1.3 API的URL作为web API，一个很重要的信息就是用来给消费者使用的根URL，可以用协议（http或者https）、主机名、根路径来描述： 1234schemes: - httpshost: simple.apibasePath: /openapi101 这这个例子中，消费者把https://simple.api/open101作为根节点来访问各种API。因为和具体环境有关，不涉及API描述的根本内容，所以这部分信息是可选的。 2.1.4 API的操作（operation）这个例子中，我们没有写API的操作，用一个YAML的空对象{}先占个位置。 2.2 定义一个API操作如果我们要展示一组用户信息，可以这样描述： 1234567891011121314151617181920212223242526272829303132swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.2.1 添加一个路径（path）我们添加一个/persons的路径，用来访问一组用户信息： 12paths: /persons: 2.2.2 在路径中添加一个HTTP方法在每个路径中，我们可以添加任意的HTTP动词来操作所需要的资源。 比如需要展示一组用户信息，我们可以在/persons路径中添加get方法，同时还可以填写一些简单的描述信息（summary）或者说明该方法的一段长篇大论（description）。 123get: summary: Gets some persons description: Returns a list containing all persons. 这样一来，我们调 get https://simple.api/open101/persons方法就能获取一个用户信息列表了。 2.2.3 定义响应（response）类型对于每个方法（或操作），我们都可以在响应(responses)中添加任意的HTTP状态码（比如200 OK 或者 404 Not Found等）。这个例子中我们添加上200的响应： 123responses: 200: description: A list of Person 2.2.4 定义响应内容get /persons这个接口返回一组用户信息，我们通过响应消息中的模式（schema）属性来描述清楚具体的返回内容。 一组用户信息就是一个用户信息对象的数组（array），每一个数组元素则是一个用户信息对象（object），该对象包含三个string类型的属性：姓氏、名字、用户名，其中用户名必须提供（required）。 123456789101112schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.3 定义请求参数（query parameters）用户太多，我们不想一股脑全部输出出来。这个时候，分页输出是个不错的选择，我们可以通过添加请求参数来实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################ responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.3.1 在get方法中增加请求参数首先我们在 get 方法中增加一个参数属性： 12345678paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters:# END ############################################################################ 2.3.2 添加分页参数在参数列表中，我们添加两个名字（name）分别叫做pageSize和pageNumber的整型（integer）参数，并作简单描述： 123456789101112 parameters:#START############################################################################ - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################## responses: 这样一来，消费者就可以通过 get /persons?pageSize=20&amp;pageNumber=2 来访问第2页的用户信息（不超过20条）了。 2.4 定义路径参数（path parameter）有时候我们想要根据用户名来查找用户信息，这时我们需要增加一个接口操作，比如可以添加一个类似 /persons/{username} 的操作来获取用户信息。注意，{username} 是在请求路径中的参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string#START############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 404: description: The Person does not exists.# END ############################################################################ 2.4.1 添加一个 get /persons/{username} 操作首先我们在 /persons 路径后面，增加一个 /persons/{username} 的路径，并定义一个 get （操作）方法。 12345678910111213141516171819202122swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: username: type: string#START############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username# END ############################################################################ 2.4.2 定义路径参数 username因为 {username} 是路径参数，我们需要先像请求参数一样将它添加到 parameters 属性中，注意名称应该同上面大括号（ { } ） 里面的名称一致。并通过 in 这个属性，来表示它是一个路径（path）参数。 123456parameters: - name: username in: path required: true description: The person's username type: string 定义路径参数时很容易出现的问题就是忘记：required: true，Swagger的自动完成功能中没有包含这个属性定义。 如果没有写 require 属性，默认值是 false，也就是说 username 参数时可选的。可事实上，作为路径参数，它是必需的。 2.4.3 定义响应消息别忘了获取单个用户信息也需要填写 200 响应消息，响应消息体的内容就是之前描述过的用户信息（用户信息列表中的一个元素）： 12345678910111213responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 当然，API的提供者会对 username 进行校验，如果查无此人，应该返回 404 的异常状态。所以我们再加上 404 状态的响应： 12404: description: The Person does not exists. 2.5 定义消息体参数（body parameter）当我们需要添加一个用户信息时，我们需要一个能够提供 post /persons 的API操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string username: type: string#START############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: required: - username properties: firstName: type: string lastName: type: string username: type: string responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.# END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 404: description: The Person does not exists. 2.5.1 添加一个 post /persons 操作首先在 /persons 路径下廷加一个 post 操作： 12345paths: /persons: post: summary: Creates a person description: Adds a new person to the persons list. 2.5.2 定义消息体参数接下来我们给 post 方法添加参数，通过 in 属性显式说明参数是在 body 中的。参数的定义参考 get /persons/{username} 的 200 响应消息体参数，也就是包含用户的姓氏、名字、用户名。 1234567891011121314parameters: - name: person in: body description: The person to create. schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 2.5.3 定义响应消息最后不要忘记定义 post 操作的响应消息。 12345responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. 三、文档瘦身现在我们已经学会了编写API文档的基本方法。不过上面的例子中存在一些重复，这对于程序员的嗅觉来说，就是代码的“坏味道”。这一章我们一起学习如何通过抽取可重用的定义（definitions）来简化API文档。 3.1 简化数据模型我们认真观察第2章最后输出的API文档，很容易发现 Person 的定义出现了三次，非常的不 DRY☹。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: type: array items:#START 第1次定义################################################################### required: - username properties: firstName: type: string lastName: type: string username: type: string# END 第1次定义################################################################### post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema:#START 第2次定义################################################################### required: - username properties: firstName: type: string lastName: type: string username: type: string# END 第2次定义################################################################### responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema:#START 第3次定义################################################################### required: - username properties: firstName: type: string lastName: type: string username: type: string# END 第3次定义################################################################### 404: description: The Person does not exists. 现在，我们通过可重用的定义 （definition）来重构这个文档： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema:#START############################################################################ $ref: "#/definitions/Persons"# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema:#START############################################################################ $ref: "#/definitions/Person"# END ############################################################################ responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema:#START############################################################################ $ref: "#/definitions/Person"# END ############################################################################ 404: description: The Person does not exists.#START 新增定义####################################################################definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person"# END 新增定义#################################################################### 文档简化了很多。这得益于OpenAPI规范中关于定义（definition）的章节中允许我们“一处定义，处处使用”。 3.1.1 添加定义 （definitions）项我们首先在API文档的尾部添加一个定义 （definitions）项（其实它也可以放在文档的任意位置，只不过大家习惯放在文档末尾）： 12345 404: description: The Person does not exists.#START############################################################################definitions:# END ############################################################################ 3.1.2 增加一个可重用的（对象）定义然后我们增加一个 Person 对象的定义： 12345678910111213definitions:#START############################################################################ Person: required: - username properties: firstName: type: string lastName: type: string username: type: string# END ############################################################################ 3.1.3 引用一个定义来增加另一个定义在定义项中，我们可以立即引用刚才定义好的 Person 来增加另一个定义，Persons。Persons 是一个 Person 对象的数组。与之前直接定义的不同之处是，我们增加了一个引用（reference）属性，也就是 $ref 来引用 Person 。 1234Persons: type: array items: $ref: "#/definitions/Person" 3.1.4 在响应消息中使用定义一旦定义好了 Person ，我们可以把原来在响应消息中相应的定义字段替换掉。 3.1.4.1 get/persons原来： 12345678910111213responses: 200: description: A list of Person schema: type: array items: required: - username properties: firstName: type: string lastName: type: string 现在： 12345responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons" 3.1.4.2 get/persons/{username}原来： 12345678910111213responses: 200: description: A Person schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 现在： 12345responses: 200: description: A Person schema: $ref: "#/definitions/Person" 3.1.5 在参数中使用定义不仅仅在消息中可以使用定义，在参数中也可以使用。 3.1.5.1 post /persons原来： 1234567891011121314151617post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: required: - username properties: firstName: type: string lastName: type: string username: type: string 现在： 123456789post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" 3.2 简化响应消息我们看到了引用 （$ref）的作用，接下来我们再把它用到响应消息的定义中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons"#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person"#START############################################################################ Error: properties: code: type: string message: type: stringresponses: Standard500ErrorResponse: description: An unexpected error occured. schema: $ref: "#/definitions/Error"# END ############################################################################ 3.2.1 定义可重用的HTTP 500 响应发生HTTP 500错误时，假如我们希望每一个API操作都返回一个带有错误码（error code）和描述信息（message）的响应，我们可以这样做： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons"#START############################################################################ 500: description: An unexpected error occured. schema: properties: code: type: string message: type: string# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.#START############################################################################ 500: description: An unexpected error occured. schema: properties: code: type: string message: type: string# END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.#START############################################################################ 500: description: An unexpected error occured. schema: properties: code: type: string message: type: string# END ############################################################################ 3.2.2 增加一个Error定义按照“一处定义、处处引用”的原则，我们可以在定义项中增加 Error 的定义： 1234567891011121314151617181920212223definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person"#START############################################################################ Error: properties: code: type: string message: type: string# END ############################################################################ 而且我们也学会了使用引用（$ref），所以我们可以这样写： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons" 500: description: An unexpected error occured. schema:#START############################################################################ $ref: "#/definitions/Error"# END ############################################################################ post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created. 500: description: An unexpected error occured. schema:#START############################################################################ $ref: "#/definitions/Error" # END ############################################################################ /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists. 500: description: An unexpected error occured. schema:#START############################################################################ $ref: "#/definitions/Error"# END ############################################################################ 3.2.3 定义一个可重用的响应消息上面的文档中，还是有一些重复的内容。我们可以根据OpenAPI规范中的responses章节的描述，通过定义一个可重用的响应消息，来进一步简化文档。 12345678910111213141516171819202122232425262728definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person" Error: properties: code: type: string message: type: string#START############################################################################responses: Standard500ErrorResponse: description: An unexpected error occured. schema: $ref: "#/definitions/Error"# END ############################################################################ 注意：响应消息中引用了 Error 的定义。 3.2.4 使用已定义的响应消息我们还是通过引用（$ref）来使用一个已经定义好的响应消息，比如： 3.2.4.1 get /users123456789 responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons"#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ 3.2.4.2 post/users123456789 responses: 204: description: Persons succesfully created. 400: description: Persons couldn't have been created.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ 3.2.4.3 get/users/{username}1234567891011 responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.#START############################################################################ 500: $ref: "#/responses/Standard500ErrorResponse"# END ############################################################################ 3.3 简化参数定义类似数据模型、响应消息的简化，参数定义的简化也很容易。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149swagger: "2.0"info: version: 1.0.0 title: Simple API description: A simple API to learn how to write OpenAPI Specificationschemes: - httpshost: simple.apibasePath: /openapi101paths: /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters: - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ responses: 200: description: A list of Person schema: $ref: "#/definitions/Persons" 500: $ref: "#/responses/Standard500ErrorResponse" post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body description: The person to create. schema: $ref: "#/definitions/Person" responses: 204: description: Person succesfully created. 400: description: Person couldn't have been created. 500: $ref: "#/responses/Standard500ErrorResponse" /persons/&#123;username&#125;:#START############################################################################ parameters: - $ref: "#/parameters/username"# END ############################################################################ get: summary: Gets a person description: Returns a single person for its username. responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" delete: summary: Deletes a person description: Delete a single person identified via its username responses: 204: description: Person successfully deleted. 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" /persons/&#123;username&#125;/friends:#START############################################################################ parameters: - $ref: "#/parameters/username"# END ############################################################################ get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging.#START############################################################################ parameters: - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ responses: 200: description: A person's friends list schema: $ref: "#/definitions/Persons" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse"definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person" Error: required: - code - message properties: code: type: string message: type: stringresponses: Standard500ErrorResponse: description: An unexpected error occured. schema: $ref: "#/definitions/Error" PersonDoesNotExistResponse: description: Person does not exist.#START############################################################################parameters: username: name: username in: path required: true description: The person's username type: string pageSize: name: pageSize in: query description: Number of persons returned type: integer pageNumber: name: pageNumber in: query description: Page number type: integer# END ############################################################################ 3.3.1 路径参数只定义一次如果我们现在想要删除一个用户的信息，就需要增加一个 delete /persons/{username} 的操作，可以这样： 123456789101112131415161718192021222324252627282930313233343536373839 /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" delete: summary: Deletes a person description: Delete a single person identified via its username parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ responses: 204: description: Person successfully deleted. 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" 但是上面两次对参数 username 的定义，却让人有点难受。好消息是我们可以定义路径级别的参数（之前都是定义在操作级别。） 12345678910111213141516171819202122232425262728293031#START############################################################################ /persons/&#123;username&#125;: parameters: - name: username in: path required: true description: The person's username type: string# END ############################################################################ get: summary: Gets a person description: Returns a single person for its username. responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" delete: summary: Deletes a person description: Delete a single person identified via its username responses: 204: description: Person successfully deleted. 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" 3.3.2 定义可重用的参数如果我们想根据用户名查找该用户的朋友圈，可以添加一个 get /persons/{username}/friends 的操作。根据前面所学的内容，第一反应应该这样写： 12345678910111213141516171819202122232425262728/persons/&#123;username&#125;/friends: parameters: - name: username in: path required: true description: The person's username type: string get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging. parameters: - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer responses: 200: description: A person's friends list schema: $ref: "#/definitions/Persons" 404: $ref: "#/responses/PersonDoesNotExistResponse" 500: $ref: "#/responses/Standard500ErrorResponse" 可以看到，关于 username 、pageSize 、pageNumber 的定义跟前面的 /person/{username} 、 get /persons 中的定义重复。如何消除重复呢？ 3.3.2.1 定义可重用的参数根据3.1和3.2中的内容，我们可以参考OpenAPI规范，融汇贯通。 1234567891011121314151617parameters: username: name: username in: path required: true description: The person's username type: string pageSize: name: pageSize in: query description: Number of persons returned type: integer pageNumber: name: pageNumber in: query description: Page number type: integer 3.3.2.2 使用定义参数借助万能的引用（$ref），这都是小菜一碟。比如： 3.3.2.2.1 get /persons原来： 123456789101112131415 /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################ 现在： 123456789 /persons: get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ 3.3.2.2.2 get 和 delete /persons/{username}原来： 123456789 /persons/&#123;username&#125;: parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ 现在： 12345 /persons/&#123;username&#125;: parameters:#START############################################################################ - $ref: "#/parameters/username"# END ############################################################################ 3.3.2.2.3 get /persons/{username}/friends原来： 1234567891011121314151617181920212223 /persons/&#123;username&#125;/friends: parameters:#START############################################################################ - name: username in: path required: true description: The person's username type: string# END ############################################################################ get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - name: pageSize in: query description: Number of persons returned type: integer - name: pageNumber in: query description: Page number type: integer# END ############################################################################ 现在： 12345678910111213 /persons/&#123;username&#125;/friends: parameters:#START############################################################################ - $ref: "#/parameters/username"# END ############################################################################ get: summary: Gets a person's friends description: Returns a list containing all persons. The list supports paging. parameters:#START############################################################################ - $ref: "#/parameters/pageSize" - $ref: "#/parameters/pageNumber"# END ############################################################################ 四、深入了解一下通过前面的练习，我们可以写出一篇结构清晰、内容精炼的API文档了。可是OpenAPI规范还给我们提供了更多的便利和惊喜，等着我们去了解和掌握。这一章主要介绍用于定义属性和数据模型的高级方法。 4.1 私人定制 Primitive data types in the Swagger Specification are based on the types supported by the JSON-Schema Draft 4. Models are described using the Schema Object which is a subset of JSON Schema Draft 4. 4.1.1 字符串（Strings）长度和格式当定义个字符串属性时，我们可以定制它的长度及格式： 属性 类型 描述 minLength number 字符串最小长度 maxLength number 字符串最大长度 pattern string 正则表达式 ，如果你暂时还不熟悉正则表达式 如果我们规定用户名是长度介于8~64，而且只能由小写字母和数字来构成，那么我们可以这样写： 12345username: type: string pattern: "[a-z0-9]&#123;8,64&#125;" minLength: 8 maxLength: 64 4.1.2 日期和时间日期和时间的处理参考 RFC 3339，我们唯一要做的就是写对格式： 格式 属性包含内容 属性示例 date ISO8601 full-date 2016-04-01 dateTime ISO8601 date-time 2016-04-16T16:06:05Z 如果我们在 Person 的定义中增加 生日 和 上次登录时间 时间戳，我们可以这样写： 123456dateOfBirth: type: string format: datelastTimeOnline: type: string format: dateTime 4.1.3 数字类型和范围当我们定义一个数字类型的属性时，我们可以[规定]它是一个整型、长型、浮点型或者双浮点型。 名称 类型 格式 integer integer int32 long integer int64 float number float double number double 和字符串一样，我们也可以定义数字属性的范围，比如： 属性 类型 描述 minimum number 最小值 maximum number 最大值 exclusiveMinimum boolean 数值必须 &gt; 最小值 exclusiveMaximum boolean 数值必须 &lt; 最大值 multipleOf number 数值必须是multipleOf的整数倍 如果我们规定 pageSize 必须是整数，必须 &gt; 0 且 &lt;=100，还必须是 10 的整数倍，可以这样写： 1234567891011pageSize: name: pageSize in: query description: Number of persons returned type: integer format: int32 minimum: 0 exclusiveMinimum: true maximum: 100 exclusiveMaximum: false multipleOf: 10 4.1.4 枚举类型我们还可以定义枚举类型，比如定义 Error 时，我们可以这样写： 123456code: type: string enum: - DBERR - NTERR - UNERR code 的值只能从三个枚举值中选择。 4.1.5 数值的大小和唯一性数字的大小和唯一性通过下面这些属性来定义： 属性 类型 描述 minItems number 数值中的最小元素个数 maxItem number 数值中的最大元素个数 uniqueItems boolean 标示数组中的元素是否唯一 比如我们定义一个用户数组 Persons，希望返回的用户信息条数介于10~100之间，而且不能有重复的用户信息，我们可以这样写： 123456789Persons: properties: items: type: array minItems: 10 maxItems: 100 uniqueItems: true items: $ref: "#/definitions/Person" 4.1.6 二进制数据可以用 string 类型来表示二进制数据： 格式 属性包含 byte Base64编码字符 binary 任意十进制的数据序列 比如我们需要在用户信息中增加一个头像属性（avatarBase64PNG）用base64编码的PNG图片来表示，可以这样写： 123avatarBase64PNG: type: string format: byte 4.2 高级数据定义4.2.1 读写操作同一定义的数据有时候我们读取资源信息的内容会比我们写入资源信息的内容（属性）更多，这很常见。是不是意味着我们必须专门为读取资源和写入资源分别定义不同的数据模型呢？幸运的是，OpenAPI规范中提供了 readOnly字段来帮我们解决整问题。比如： 1234lastTimeOnline: type: string format: dateTime readOnly: true 上面这个例子中，上次在线时间（lastTimeOnline ）是 Person 的一个属性，我们获取用户信息时需要这个属性。但是很明显，在创建用户时，我们不能把这个属性 post 到服务器。于是我们可以把它标记为 readOnly。 4.2.2 组合定义确保一致性一致性设计是在编写API文档时需要重点考虑的问题。比如我们在获取一组用户信息时，需要同时获取页面信息（ totalItems, totalPage, pageSize, currentPage）等，而且这些信息必须在根节点上。 怎么办呢？首先想到的做法就是： 1234567891011121314PagedPersonsV1: properties: items: type: array items: $ref: "#/definitions/Person" totalItems: type: integer totalPages: type: integer pageSize: type: integer currentPage: type: integer 如果其他API操作也需要这些页面信息，那就意味着这些属性必须一遍又一遍的定义。不仅重复体力劳动，而且还很危险：比如忘记了其中的一两个属性，或者需要添加一个新的属性进来，那就是霰弹式的修改，想想都很悲壮。 稍微好一点的做法，就是根据前面学习的内容，把这几个属性抽取出来，建立一个 Paging 模型，“一处定义、处处使用”： 12345678910111213141516171819PagedPersonsV2: properties: items: type: array items: $ref: "#/definitions/Person" paging: $ref: "#/definitions/Paging"Paging: properties: totalItems: type: integer totalPages: type: integer pageSize: type: integer currentPage: type: integer 但是，页面属性都不再位于 根节点！与我们前面设定的要求不一样了。怎么破？ JSON Schema v4 property中定义的allOf，能帮我们解围： 1234PagedPersons: allOf: - $ref: "#/definitions/Persons" - $ref: "#/definitions/Paging" 上面这个例子表示，PagedPersons 根节点下，具有将 Persons 和 Paging 展开 后的全部属性。 allOf同样可以使用行内的数据定义，比如： 1234567891011PagedCollectingItems: allOf: - properties: items: type: array minItems: 10 maxItems: 100 uniqueItems: true items: $ref: "#/definitions/CollectingItem" - $ref: "#/definitions/Paging" 4.2.3 数据模型的继承（TODO）目前各工具支持程度不高，待续 五、 输入输出模型这一章主要介绍如何定义高度精确化的参数和响应消息等。 5.1 高级参数定义5.1.1 必带参数和可选参数我们已经知道使用关键字required来定义一个必带参数。 5.1.1.1 定义必带参数和可选参数在一个参数中，required是一个 boolean 型的可选值。它的默认值是 false 。 比如在某个操作中，username 是必填参数： 12345678 username: name: username in: path#START############################################################################ required: true# END ############################################################################ description: The person's username type: string 5.1.1.2 定义必带属性和可选属性根据定义，required是一个字符串列表，列表中包含各必带参数名。如果某个参数在这张列表中找不到，那就说明它不是必带参数。如果没有定义required，就说明所有参数都是可选。如果required定义在一个HTTP请求上，这说明所有的请求参数都是必填。 在 POST 、persons 中有 Person 的定义，在这里 username 这个属性是必带的，我们可以指定它为required，其他非必带字段则不指定： 12345678910111213141516171819202122232425262728 Person:#START############################################################################ required: - username# END ############################################################################ properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time readOnly: true avatarBase64PNG: type: string format: byte default: data:image/png;base64,i…… spokenLanguages: $ref: '#/definitions/SpokenLanguages' 5.1.2 带默认值的参数通过关键字default，我们可以定义一个参数的默认值。当这个参数不可得（请求未带或者服务器未返回）时，这个参数就取默认值。因此设定了某个参数的默认值后，它是否required就没意义了。 5.1.2.1 定义参数的默认值我们定义参数 pageSize 的默认值为 20 ，那么如果请求时没有填写 pageSize ，服务器也会默认返回 20 个元素。 1234567891011121314 pageSize: name: pageSize in: query description: Number of persons returned type: integer format: int32 minimum: 0 exclusiveMinimum: true maximum: 100 exclusiveMaximum: false multipleOf: 10#START############################################################################ default: 20# END ############################################################################ 5.1.2.2 定义属性的默认值我们在定义 Person 对象时，希望给每个用户一个默认头像，也就是要给 avatarBase64PNG 属性一个默认值。 默认头像: … 12345678910111213141516171819202122232425262728 Person: required: - username properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time readOnly: true avatarBase64PNG: type: string format: byte#START############################################################################ default: data:image/png;base64,iVBORw0KGgoAAAANSUhEUgA……rkJggg==# END ############################################################################ spokenLanguages: $ref: '#/definitions/SpokenLanguages' 5.1.3 带空值的参数在 GET /persons 时，如果我们想添加一个参数来过滤“是否通过实名认证”的用户，应该怎么做呢？首先想到的是这样：GET /persons?page=2&amp;includeVerifiedUsers=true ，问题是 includeVerifiedUsers 语义已经如此清晰，而让 “=true”显得很多余。我们能不能直接用：GET /persons?page=2&amp;includeVerifiedUsers 呢？ 要做到这种写法，我们需要一个关键字 allowEmptyValue。我们定义 includeVerifiedUsers 时允许它为空。那么如果我们请求 GET /persons?page=2&amp;includeVerifiedUsers 则表示需要过滤“实名认证”用户，如果我们直接请求 GET /persons?page=2 则表示不过滤： 12345678 includeNonVerifiedUsers: name: includeNonVerifiedUsers in: query type: boolean default: false#START############################################################################ allowEmptyValue: true# END ############################################################################ 5.1.4 参数组设计API的时候，我们经常会遇到在 GET 请求中需要携带一组请求参数的情况。如何在API文档章呈现呢？很简单，我们只需要设定参数类型（type） 为array，并选择合适的 组合格式（collectionFormat）就行了。 COLLECTIONFORMAT 描述 csv (default value) Comma separated values（逗号分隔） foo,bar ssv Space separated values（空格分隔） foo bar tsv Tab separated values（反斜杠分隔） foo\tbar pipes Pipes separated values（竖线分隔） `foo\ bar` multi 单属性可以取多个值，比如 foo=bar&amp;foo=baz. 只适用于查询参数和表单参数。 比如我们想根据多种参数（username ， firstname ， lastname ， lastTimeOnline ）等来对 Person 进行带排序的查询。我们需要一个这样的API请求： GET /persons?sort=-lastTimeOnline|+firtname|+lastname 。用于排序的参数是 sort ，+表示升序，-表示降序。 相应的API文档，可以这样写： 1234567891011sortPersons: name: sort in: query type: array uniqueItems: true minItems: 1 maxItems: 3 collectionFormat: pipes items: type: string pattern: '[-+](username|lastTimeOnline|firstname|lastname)' 现在我们就能搞定 GET /persons?sort=-lastTimeOnline|+firtname|+lastname 这种请求了。当然，我们还可以指定排序的默认值，锦上添花。 12345678910111213141516 sortPersons: name: sort in: query type: array uniqueItems: true minItems: 1 maxItems: 3 collectionFormat: pipes items: type: string pattern: '[-+](username|lastTimeOnline|firstname|lastname)'#START############################################################################ default: - -lastTimeOnline - +username# END ############################################################################ 5.1.5 消息头（Header）参数参数，按照位置来分，不仅仅包含路径参数、请求参数和消息体参数，还包括消息头参数和表单参数等。比如我们可以在HTTP请求的消息头上加一个 User-Agent （用于跟踪、调试或者其他），可以这样定义它： 12345userAgent: name: User-Agent type: string in: header required: true 然后像使用其他参数一样使用它： 1234paths: /persons: parameters: - $ref: '#/parameters/userAgent' 5.1.6 表单参数有些 js-less-browser 的老浏览器不支持 POST JSON数据，比如在创建用户时，只能够以这样个格式请求： 123POST /js-less-personsusername=apihandyman&amp;firstname=API&amp;lastname=Handyman 没有问题，丝袜哥可以搞定。我们只需要把各个属性的in关键字定义为formData，然后设置consumes的媒体类型为application/x-www-form-urlencoded即可。 123456789101112131415161718192021222324252627282930313233343536 post: summary: Creates a person description: For JS-less partners#START############################################################################ consumes: - application/x-www-form-urlencoded# END ############################################################################ produces: - text/html parameters: - name: username#START############################################################################ in: formData# END ############################################################################ required: true pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 type: string - name: firstname#START############################################################################ in: formData# END ############################################################################ type: string - name: lastname in: formData type: string - name: dateOfBirth#START############################################################################ in: formData# END ############################################################################ type: string format: date responses: '204': description: Person succesfully created. 5.1.7 文件参数当我们要处理一个请求，输入类型是 文件 时，我们需要： 使用 multipart/form-data 媒体类型； 设置参数的 in关键字为 formData； 设置参数的类型（type）为 file。 比如： 123456789101112131415161718/images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image consumes: - multipart/form-data parameters: - name: image in: formData type: file responses: '200': description: Image's ID schema: properties: imageId: type: string 有时候我们想限定输入文件的类型（后缀），很不幸的是，根据现在V2.0的规范暂时还做不到☹ The spec doesn’t allow specifying a content type for specific form data parameters. It’s a limitation of the spec. Ron Ratovsky comment in Swagger UI 609 issue 5.1.8 参数的媒体类型一个API可以消费各种不同的媒体类型，比如说最常见的是 application/json 类型的数据，当然这不是API唯一支持的类型。我们可以在文档的根节点 或者一个操作的根节点 下添加关键字 consumes，来定义这个操作能够消费的媒体类型。 比如我们的API全部都接受JSON和YAML的数据，那我们可以在文档的根节点下添加： 123consumes: - application/json - application/x-yaml 如果某个操作（比如上传图片的操作）很特殊，它可以通过自己添加 consumes来覆盖全局设置： 1234567891011121314151617181920 /images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image#START############################################################################ consumes: - multipart/form-data# END ############################################################################ parameters: - name: image in: formData type: file responses: '200': description: Image's ID schema: properties: imageId: type: string 5.2 高级响应消息定义5.2.1 不带消息体的响应消息不带消息体的响应很常见，比如HTTP 204 状态响应本身就表示服务器返回不带任何消息内容的成功消息。 要定义一个不带消息体的响应很简单，我们只需要写响应状态和描述就行了： 123456789101112131415 post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body required: true description: The person to create. schema: $ref: '#/definitions/Person' responses:#START############################################################################ '204': description: Person succesfully created.# END ############################################################################ 5.2.2 响应消息中的必带参数和可选参数与请求消息中类似，我们使用required参数来表示，比如请求一个用户信息时， 服务器必须返回username，可以这样写： 12345678910111213141516171819202122 Person:#START############################################################################ required: - username# END ############################################################################ properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time readOnly: true 5.2.3 响应消息头API的返回结果不仅仅体现下HTTP状态和响应消息体，还可以在响应消息头上做文章。比如我们可以限定一个API的使用次数和使用时间段，在响应消息头中，增加一个属性X-Rate-Limit-Remaining 来表示API可调用的剩余次数，增加另一个属性 X-Rate-Limit-Reset 来表示API的有效截止时间。 123456789101112131415161718192021 post: summary: Creates a person description: Adds a new person to the persons list. parameters: - name: person in: body required: true description: The person to create. schema: $ref: '#/definitions/Person' responses: '204': description: Person succesfully created. headers:#START############################################################################ X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time# END ############################################################################ 美中不足的是，对于这种响应消息头的修改，目前2.0规范暂时还不支持“一次定义、处处使用”☹ 5.2.4 默认响应消息我们在定义响应消息时，通常会列举不同的HTTP状态结果。如果有些状态不在我们API文档的定义范围（比如服务器需要返回 993 的状态），该怎么处理呢？这时需要通过关键字default来定义一个默认响应消息，用于各种定义之外的状态响应，比如： 1234567891011121314151617181920 delete: summary: Deletes a person description: Delete a single person identified via its username responses: '204': description: Person successfully deleted. headers: X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time '404': $ref: '#/responses/PersonDoesNotExistResponse' '500': $ref: '#/responses/Standard500ErrorResponse'#START############################################################################ default: $ref: '#/responses/TotallyUnexpectedResponse'# END ############################################################################ 目前这个配置也不支持“一次定义，处处使用” 。☹ 5.2.5 响应消息的媒体类型与请求消息一样，我们也可以定义响应消息所支持的媒体类型，不同的是我们要用到关键字 produces（与请求消息中的consumes相对，由此可见，API文档描述的主体是服务提供者）。 比如，我们可以在文档的根路径下全局设置： 123produces: - application/json - application/x-yaml 也可以在某个操作的根路径下覆盖设置： 123456789101112131415161718192021222324252627282930313233343536373839 /images/&#123;imageId&#125;: parameters: - $ref: '#/parameters/userAgent' get: summary: Gets an image parameters: - name: imageId in: path required: true type: string#START############################################################################ produces: - image/png - image/gif - image/jpeg - application/json - application/x-yaml# END ############################################################################ responses: '200': description: The image headers: X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time '404': description: Image do not exists headers: X-Rate-Limit-Remaining: type: integer X-Rate-Limit-Reset: type: string format: date-time '500': $ref: '#/responses/Standard500ErrorResponse' default: $ref: '#/responses/TotallyUnexpectedResponse' 5.3 定义某个参数只存在于响应消息中如前章节4.2.1中已经提到的，定义一个对象，其中某个属性我们只希望在响应消息中携带，而不希望在请求消息中携带，应该用readOnly关键字来表示。考虑到内容的完整性，这里再介绍一下。 比如 Person 对象中的 lastTimeOnline 这个属性，注册用户时我们不需要填写，但是在获取用户信息时，需要提供给服务消费者： 12345678910111213141516171819202122 Person: required: - username properties: firstName: type: string lastName: type: string username: type: string pattern: '[a-z0-9]&#123;8,64&#125;' minLength: 8 maxLength: 64 dateOfBirth: type: string format: date lastTimeOnline: type: string format: date-time#START############################################################################ readOnly: true# END ############################################################################ 六、 不要让API裸奔这一章主要介绍API文档中如何描述安全相关的内容。 6.1 定义安全安全相关内容的定义一般放在API文档根目录下的securityDefinition 中，它包括一组具体的命名安全项，每一个命名安全定义可能包括下面三种安全类型之一：basic，apiKey ，oauth2。 6.1.1 基础鉴权（Basic Authentication）要定义一个基础（basic） 鉴权，我们只需要将type设置为 basic 即可： 1234567securityDefinitions: UserSecurity: type: basic AdminSecurity: type: basic MediaSecurity: type: basic 这个例子中，我们定义了三种安全说明（UserSecurity，AdminSecurity， MediaSecurity），都属于基础鉴权。 6.1.2 API秘钥鉴权（API Key）要定义一个API秘钥鉴权，我们需要： 设置type为 apiKey 通过关键字in指示api秘钥所在位置。通常api秘钥会放在消息头、请求参数或者消息体中 给安全项命个名字 12345678910111213securityDefinitions: UserSecurity: type: apiKey in: header name: SIMPLE-API-KEY AdminSecurity: type: apiKey in: header name: ADMIN-API-KEY MediaSecurity: type: apiKey in: query name: MEDIA-API-KEY 在这个例子中,我们定义了三个apiKey类型的安全项： UserSecurity 定义了一个名为SIMPLE-API-KEY 的参数在消息头（header） AdminSecurity 定义了一个名为 ADMIN-API-KEY 的参数在消息头 （header） MediaSecurity 定义了一个名为MEDIA-API-KEY的参数在请求参数中 6.1.3 Oauth2鉴权6.1.3.1 流程（Flow）和URL当我们定义个 Oauth2 类型的安全项上，我们通常会定义Oauth2 的流程（flow）和并根据选定的流程配置相应的鉴权地址（authorizationUrl）和/或令牌地址（tokenUrl）。 流程 所需要的URL implicit authorizationUrl（鉴权地址） password tokenUrl（令牌地址） application tokenUrl accessCode authorizationUrl and tokenUrl 比如： 123456securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' 在这个例子中，我们定义了一个 Oauth2 的安全项，配置的流程是 accessCode 方式，同时配置了鉴权地址和令牌地址。 6.1.3.2 作用范围（scope）我们借助关键字scopes并通过哈希键值对来还可以配置 Oauth2 安全项的作用范围（scope），键值对的键表示作用范围名称；值是它的相关描述，比如： 12345678910securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope media: Media scope 在这个例子中，我们给 OauthSecurity 安全项添加了三个作用范围（admin ，user ，media）。 6.2 使用安全定义现在我们已经在securityDefinition 中定义好了安全项，现在我们可以将将它们应用到文档中了。使用的时候，我们通过security关键字，把安全项添加进去。 6.2.1 基础鉴权6.2.1.1 API级别12345678910111213securityDefinitions: UserSecurity: type: basic AdminSecurity: type: basic MediaSecurity: type: basic#START############################################################################security: - UserSecurity: [] # END ############################################################################paths: /persons: 在这个例子中，我们在API文档的根路径下直接使用了安全项 UserSecurity，它的作用范围是整个API文档。 6.2.1.2 操作级别比如我们在添加或者修改用户信息时，需要进行管理员鉴权，可以在 POST /persons 操作中增加安全项： 1234567 post: summary: Creates a person description: Adds a new person to the persons list.#START############################################################################ security: - AdminSecurity: []# END ############################################################################ 而在上传图片时，需要进行媒体操作鉴权，可以在 POST /images 操作中增加安全项： 123456789 /images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image#START############################################################################ security: - MediaSecurity: []# END ############################################################################ 6.2.2 API秘钥鉴权使用方式和基础鉴权一样，可以在API级别和操作级别使用： 123456789101112131415161718192021222324securityDefinitions: UserSecurity: type: apiKey in: header name: SIMPLE-API-KEY AdminSecurity: type: apiKey in: header name: ADMIN-API-KEY MediaSecurity: type: apiKey in: query name: media-api-key#START############################################################################security: - UserSecurity: [] # END ############################################################################paths: /persons: post: summary: Creates a person description: Adds a new person to the persons list. security: - AdminSecurity: [] 6.2.3 Oauth2鉴权Oauth2 鉴权的使用和上面的两种鉴权方式基本相同，不同之处在于我们可以指定它的哪一个作用范围（scope）。 比如API级别的鉴权： 1234567891011121314151617securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope media: Media scope#START############################################################################security: - OauthSecurity: - user# END ############################################################################paths: /persons: 操作级别的鉴权： 123456post: summary: Creates a person description: Adds a new person to the persons list. security: - OauthSecurity: - admin 在这个例子中，作用范围 admin 将覆盖全局配置的作用范围 user 。 6.3 使用多种安全配置OpenAPI规范并没有限定我们只能使用一种安全项。下面的例子将展示如何使用多种安全配置。 6.3.1 安全定义123456789101112131415securityDefinitions: OauthSecurity: type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope MediaSecurity: type: apiKey in: query name: media-api-key LegacySecurity: type: basic 这个例子中，我们定义了三种鉴权方式。 6.3.2 全局安全配置1234security: - OauthSecurity: - user - LegacySecurity: [] 这个配置的意思是用户可以通过两种方式中的任意一种来访问我们提供的API接口。 6.3.3 覆盖全局配置1234567post: summary: Creates a person description: Adds a new person to the persons list. security: - OauthSecurity: - admin - LegacySecurity: [] 在 POST /persons 操作中，OauthSecurity 的作用范围被覆写为admin。此时用户可以通过admin 的Oauth2*或者 *legacySecurity 来鉴权使用这个操作。 1234567/images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image security: - MediaSecurity: [] 在 POST/images 操作中，用MediaSecurity 整体覆写了全局安全项，用户只能通过 MediaSecurity 鉴权使用这个操作。 七、 让文档的可读性更好7.1 分类标签（Tags）通过关键字tags我们可以对文档中接口进行归类，tags的本质是一个字符串列表。tags定义在文档的根路径下。 7.1.1 单标签比如说 GET /persons 属于用户（Person） 这个分类的，那么我们可以给它贴个标签： 123456789101112paths: /persons: parameters: - $ref: '#/parameters/userAgent' get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. operationId: searchUsers#START############################################################################ tags: - Persons# END ############################################################################ 7.1.2 多标签一个操作也可以同时贴几个标签，比如： 1234567891011/js-less-consumer-persons: parameters: - $ref: '#/parameters/userAgent' post: summary: Creates a person description: For JS-less partners operationId: createUserJS deprecated: true tags: - JSLess - Persons 贴上标签后，在Swagger Editor和Swagger UI中能够自动归类，我们可以按照标签来筛选接口，试试吧？ 7.2 无处不在的描述文字（Descriptions）description这个属性几乎是无处不在，为了提高文档的可读性，我们应该在必要的地方都加上描述文字。 7.2.1 安全项的描述123456789101112131415161718securityDefinitions: OauthSecurity: description: New Oauth security system. Do not use MediaSecurity or LegacySecurity. type: oauth2 flow: accessCode authorizationUrl: 'https://oauth.simple.api/authorization' tokenUrl: 'https://oauth.simple.api/token' scopes: admin: Admin scope user: User scope MediaSecurity: description: Specific media security for backward compatibility. Use OauthSecurity instead. type: apiKey in: query name: media-api-key LegacySecurity: description: Legacy security system for backward compatibility. Use OauthSecurity instead. type: basic 7.2.2 模式（Schema）的描述每一种模式（Schema），都会有一个标题（title）和一段描述，比如： 1234definitions: Person: title: Human description: A person which can be the user itself or one of his friend 7.2.3 属性的描述比如： 1234properties: firstName: description: first name type: string 7.2.4 参数的描述123456789101112131415161718192021paths: /persons: post: parameters: - name: person in: body required: true description: The person to create. schema: $ref: '#/definitions/Person'parameters: username: name: username in: path required: true description: The person's username type: string pageSize: name: pageSize in: query description: Number of persons returned 7.2.5 操作的概述（summary）、描述和操作ID（operationId）一个操作（Operation）通常都会包含概述和描述信息。而且我们还可以添加一个关键字operationId，这个关键字通常用来指示服务提供者对这个操作的处理函数的函数名。比如： 12345678paths: /persons: parameters: - $ref: '#/parameters/userAgent' get: summary: Gets some persons description: Returns a list containing all persons. The list supports paging. operationId: searchUsers 7.2.6 响应消息的描述12345678910111213paths: /persons: parameters: - $ref: '#/parameters/userAgent' get: …… …… responses: '200': description: A list of Personresponses: Standard500ErrorResponse: description: An unexpected error occured. 7.2.7 响应消息头的描述12345678headers: X-Rate-Limit-Remaining: description: How many calls consumer can do type: integer X-Rate-Limit-Reset: description: When rate limit will be reset type: string format: date-time 7.2.8 标签的描述我们在API文档的根路径下添加了tags的定义，对于其中的每一个标签，我们都可以添加描述信息： 123tags: - name: Persons description: Everything you need to handle users and friends 7.3 在描述中使用Markdown语法在绝大部分的description中，我们可以使用GFM（Github Flavored Markdown）语法。 7.3.1 多行描述使用符号 | 然后在新行中打一个tab（注意：YAML的tab是两个空格 ），就可以编辑多行描述，比如： 123456789'/persons/&#123;username&#125;/collecting-items': parameters: - $ref: '#/parameters/username' - $ref: '#/parameters/userAgent' get: summary: Gets a person's collecting items list description: | Returns a list containing all items this person is looking for. The list supports paging. 7.3.2 简单使用GFM比如我们要强调，可以这样写： 1234externalDocs: description: | **Complete** documentation describing how to use this API url: http://doc.simple.api/ 7.3.3 带信息组的描述12345678910111213141516171819CollectingItem: discriminator: itemType required: - itemType properties: itemType: description: | An item can be of different type: type | definition -----|----------- Vinyl| #/definitions/Vinyl VHS | #/definitions/VHS AudioCassette | #/definitions/AudioCassette type: string enum: - AudioCassette - Vinyl - VHS 7.3.4 带代码的描述1234567891011121314151617181920212223242526272829303132333435363738swagger: '2.0'info: version: 1.1.0 title: Simple API description: | A simple API to learn how to write OpenAPI Specification. This file uses almost every single aspect of the [Open API Specification](https://openapis.org/). This API will use JSON. JSON looks like this: ```JSON &#123; "key": "value", "anotherKey": "anotherValue" &#125;### 7.4 示例数据（Examples）我们已经知道了用Schema来描述参数和属性，有的时候，用示例数据更有表现了。我们可以使用关键字`example`来给原子属性或者对象添加示例数据。#### 7.4.1 原子属性的示例数据```YAML properties: firstName: description: first name type: string#START############################################################################ example: John# END ############################################################################ lastTimeOnline: description: The last time this person was connected to the service as a type: string format: date-time readOnly: true#START############################################################################ example: 2016-06-10T12:36:58.014Z# END ############################################################################ 7.4.2 对象属性的示例数据123456789101112131415161718192021222324252627 Persons: title: Humans description: A list of users or friends required: - items properties: items: description: Array containg the list type: array minItems: 10 maxItems: 100 uniqueItems: true items: $ref: '#/definitions/Person'#START############################################################################ example: - firstname: Robert lastname": Doe username": robdo dateOfBirth: 1970-01-28 lastTimeOnline: 2016-04-10T14:36:58.014Z - firstname: Jane lastname: Doe username: jdoe123 dateOfBirth: 1980-05-12 lastTimeOnline: 2016-05-12T19:23:59.014Z# END ############################################################################ 7.4.3 定义的示例数据跟普通属性一样，定义的对象也能添加示例数据： 1234567891011121314151617 MultilingualErrorMessage: title: MultiLingualMultiDeviceErrorMessage description: An multilingual error message (hashmap) with a long and a short description additionalProperties: $ref: '#/definitions/ErrorMessage' properties: defaultLanguage: $ref: '#/definitions/ErrorMessage'#START############################################################################ example: defaultLanguage: longMessage: We're deeply sorry but an error occured shortMessage: Error fr: longMessage: Nous sommes dÃ©solÃ© mais une erreur est survenu shortMessage: Erreur# END ############################################################################ 7.4.4 响应消息的示例数据我们甚至可以添加响应消息级别的示例数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 '/persons/&#123;username&#125;/collecting-items': …… get: …… responses: '200':#START############################################################################ examples: application/json: &#123; "totalItems": 10, "totalPage": 4, "pageSize": 3, "currentPage": 2, "items": [ &#123; "itemType": "Vinyl", "maxPrice": 20, "imageId": "98096838-04eb-4bac-b32e-cd5b7196de71", "albumName": "Captain Future Original Soundtrack", "artist": "Yuji Ohno" &#125;, &#123; "itemType": "VHS", "maxPrice": 10, "imageId": "b74469bc-e6a1-4a90-858a-88ef94079356", "movieTitle": "Star Crash", "director": "Luigi Cozzi" &#125;, &#123; "itemType": "AudioCassette", "maxPrice": 10, "imageId": "b74469bc-e6a1-4a90-858a-88ef94079356", "albumName": "Star Wars", "artist": "John Williams" &#125; ] &#125;# END ############################################################################ '404': $ref: '#/responses/PersonDoesNotExistResponse' '500': $ref: '#/responses/Standard500ErrorResponse' default: $ref: '#/responses/TotallyUnexpectedResponse' 7.4.5 示例数据的优先级如果我们在各个级别（比如参数、对象、定义、响应消息）都添加了示例数据。支持OpenAPI规范的各解析工具都是以 最高级别 的定义为准。 7.5 标记为弃用我们可以通过关键字deprecated置为 true 来标记接口的弃用状态，比如： 12345678/js-less-consumer-persons: parameters: - $ref: '#/parameters/userAgent' post: summary: Creates a person description: For JS-less partners operationId: createUserJS deprecated: true 7.6 链接到外部文档一般来说，项目中不光只有一篇API文档，还应该有些描述application key，测试用例，操作链以及其他内容的文档，这些文档一般是单独成篇的。如果在描述某个接口时，我们想链接这些文档，可以通过关键字externalDoc来添加，例如： 12345678910111213externalDocs: description: Complete documentation describing how to use this API url: http://doc.simple.api/ /images: parameters: - $ref: '#/parameters/userAgent' post: summary: Uploads an image description: Upload an image, will return an image id. operationId: storeImage externalDocs: description: How to upload media url: http://doc.simple.api/media/upload 八、 分而治之根据前面几张的知识，我们已经可以轻松的构建一个复杂的API文档了。可是作为一个学过 Clean Code 的程序员，我们并不希望所有的接口、定义都在一个大而全的上帝文件里。这一章我们一起来学习拆分文件。 8.1 JSON指针我们在第2章已经知道了怎么定义一个可重用的对象，已经如何使用定义，重温一下： 123456789101112131415161718192021222324252627282930313233 /persons/&#123;username&#125;: get: summary: Gets a person description: Returns a single person for its username. parameters: - name: username in: path required: true description: The person's username type: string responses: 200: description: A Person schema: $ref: "#/definitions/Person" 404: description: The Person does not exists.definitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: string Persons: type: array items: $ref: "#/definitions/Person" 在使用的时候我们这样写：$ref: “#/definitions/Person” ，$ref就是JSON指针（参见定义RFC6901）。例子中的这个指针指向当前文档根路径（#）下的definitions 下的 Person。 JSON指针不仅可以指向当前文件，还可以指向其他文件。 8.2 基础拆分8.2.1 引用本地文件还是上面这个例子，我们在当前文档的同一目录下，新建的一个文件叫 person.yaml，然后把Person 的定义拆出来，放在其中。 12345678910Person: required: - username properties: firstName: type: string lastName: type: string username: type: string 在当前文档中把引用 Person 定义的地方修改为： 1$ref: "person.yaml#/Person" 8.2.2 编辑器报错把上面的代码放到Swagger Editor中编辑，编辑器预览中可能会报错： 提示找不到person.yaml文件。这是因为我们没有指定正确的编辑器的指针解析基础路径（Pointer Resolution Base Path） 。 解决的办法是：进入编辑器的 Preferences -&gt; Preferences 菜单，修改Pointer Resolution Base Path为： 考虑到缓存的原因，如果错误依然存在，请刷新浏览器。 8.2.3 文件夹如果引用子文件夹下的文件，我们可以这样写： 1$ref: "folder/person.yaml#/Person" 如果引用上级文件夹下的文件，我们可以这样写： 1234Persons: type: array items: $ref: "../folder/person.yaml#/Person" 8.2.4 引用远程文件如果我们想引用一个远程文件，应该怎么做呢？可以这样写： 1$ref: https://myserver.com/mypath/myfile.yaml#/example 但是需要注意的是：服务器必须提供跨域（CORS）访问服务。 如果要通过本地服务器上的8080端口引用文件，我们可以这样写： 1$ref: "http://localhost:8080/folder/person.yaml#/Person" 考虑两种比较特别的情况： 8.2.4.1 远端文件引用”本地文件”比如说，我们引用了： 1$ref: "http://localhost:8080/another-folder/persons.yaml#/Persons" 这个远端文件。但是persons.yaml又引用了与它在同一目录下的person.yaml文件，这个时候语法分析器会在localhost:8080上面找person.yaml文件，而不会查找我们本地的person.yaml。 8.2.4.2 远端文件引用 ”更“远端文件如果理解了8.2.4.1这个例子，我们就知道不管怎么引用，都是在相对于被引用的文件下来进行查找的。 8.2.5 整理一个文件中的多个定义比如我们在一个文件中，需要把定义分成几类，可以这样做： 1234567891011121314151617SomeDefinitions: Person: required: - username properties: firstName: type: string lastName: type: string username: type: stringOtherDefinitions: Persons: type: array items: $ref: "#/SomeDefinitions/Person" Person归属于SomeDefinitions这一类，Persons归属于OtherDefinitions这一类。那么我们在引用这两个定义时，分别应该这样书写： 12$ref: "definitions.yaml#/OtherDefinitions/Persons"$ref: "definitions.yaml#/SomeDefinitions/Person" 有点像命名空间的概念。 8.3 实战切分的思路 结构化切分思路： 先把API文档的头部info切下来，放在info.yaml，然后分别切分 paths.yaml，definitions.yaml，responses.yaml，parameters.yaml等文件，最后合并到main.yaml； 分层切分思路：分别按照功能和层次，将文件切分为base.yaml或common.yaml，然后是各个模块的xxx-paths.yaml，然后是各个定义的xxx-definitions.yaml。 其他思路…… 需要注意的是，不管怎么切分，有一个原则必须谨记： **切分出来的子文档，必须遵循OpenAPI规范，能够通过编辑器的校验，不然切分得再漂亮也是徒劳。]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric 示例应用搭建]]></title>
    <url>%2F2019%2F04%2F12%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F3.Fabric%20%E7%A4%BA%E4%BE%8B%E5%BA%94%E7%94%A8%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[生成MSP身份文件配置crypto-config文件crypto-config.yaml文件： 1234567891011121314151617181920212223242526272829OrdererOrgs: - Name: Orderer Domain: example.com Specs: - Hostname: ordererPeerOrgs: - Name: Org1 Domain: org1.example.com CA: Hostname: ca # implicitly ca.org1.example.com Template: Count: 2 SANS: - "localhost" Users: Count: 1 - Name: Org2 Domain: org2.example.com CA: Hostname: ca Template: Count: 2 SANS: - "localhost" Users: Count: 1 生成MSP文件1cryptogen generate --config=./crypto-config.yaml # 生成msp文件 生成创世区块和channel事务创世区块中，包含了组织和联盟的配置信息等 配置configtx.yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758Profiles: TwoOrgsOrdererGenesis: Orderer: &lt;&lt;: *OrdererDefaults Organizations: - *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: &lt;&lt;: *ApplicationDefaults Organizations: - *Org1 - *Org2Organizations: - &amp;OrdererOrg Name: OrdererMSP ID: OrdererMSP MSPDir: crypto-config/ordererOrganizations/example.com/msp - &amp;Org1 Name: Org1MSP ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.example.com/msp AnchorPeers: - Host: peer0.org1.example.com Port: 7051 - &amp;Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.example.com/msp AnchorPeers: - Host: peer0.org2.example.com Port: 7051Orderer: &amp;OrdererDefaults OrdererType: solo Addresses: - orderer.example.com:7050 BatchTimeout: 2s BatchSize: MaxMessageCount: 10 AbsoluteMaxBytes: 98 MB PreferredMaxBytes: 512 KB Kafka: Brokers: - 127.0.0.1:9092 Organizations:Application: &amp;ApplicationDefaults Organizations: 生成创世区块1configtxgen -profile TwoOrgsOrdererGenesis -channelID order-channel -outputBlock genesis.block 生成channel配置事务1export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile TwoOrgsChannel -outputCreateChannelTx channel.tx -channelID $CHANNEL_NAME 生成Anchor peer 配置事务12export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate Org1MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org1MSPexport CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate Org2MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org2MSP 启动网络配置docker-compose文件完成两个ca， 两个组织(每个组织2个节点)，一个order节点和一个cli节点的配置 cli节点可以帮助初始化channel和链码的安装和初始化等工作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162version: '2'services: ca.org1.example.com: image: hyperledger/fabric-ca environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca.org1.example.com - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/0e729224e8b3f31784c8a93c5b8ef6f4c1c91d9e6e577c45c33163609fe40011_sk - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/0e729224e8b3f31784c8a93c5b8ef6f4c1c91d9e6e577c45c33163609fe40011_sk ports: - "7054:7054" command: sh -c 'fabric-ca-server start -b admin:adminpw -d' volumes: - ./channel/crypto-config/peerOrganizations/org1.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca.org1.example.com ca.org2.example.com: image: hyperledger/fabric-ca environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca.org2.example.com - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/a7d47efa46a6ba07730c850fed2c1375df27360d7227f48cdc2f80e505678005_sk - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/a7d47efa46a6ba07730c850fed2c1375df27360d7227f48cdc2f80e505678005_sk ports: - "8054:7054" command: sh -c 'fabric-ca-server start -b admin:adminpw -d' volumes: - ./channel/crypto-config/peerOrganizations/org2.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca.org2.example.com orderer.example.com: container_name: orderer.example.com image: hyperledger/fabric-orderer environment: - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/etc/hyperledger/configtx/genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/etc/hyperledger/crypto/orderer/msp - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/etc/hyperledger/crypto/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/etc/hyperledger/crypto/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/etc/hyperledger/crypto/orderer/tls/ca.crt, /etc/hyperledger/crypto/peerOrg1/tls/ca.crt, /etc/hyperledger/crypto/peerOrg2/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric/orderers command: orderer ports: - 7050:7050 volumes: - ./channel:/etc/hyperledger/configtx - ./channel/crypto-config/ordererOrganizations/example.com/orderers/orderer.example.com/:/etc/hyperledger/crypto/orderer - ./channel/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/:/etc/hyperledger/crypto/peerOrg1 - ./channel/crypto-config/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/:/etc/hyperledger/crypto/peerOrg2 peer0.org1.example.com: container_name: peer0.org1.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer0.org1.example.com - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_ADDRESS=peer0.org1.example.com:7051 ports: - 7051:7051 - 7053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com peer1.org1.example.com: container_name: peer1.org1.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer1.org1.example.com - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_ADDRESS=peer1.org1.example.com:7051 ports: - 8051:7051 - 8053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com peer0.org2.example.com: container_name: peer0.org2.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer0.org2.example.com - CORE_PEER_LOCALMSPID=Org2MSP - CORE_PEER_ADDRESS=peer0.org2.example.com:7051 ports: - 9051:7051 - 9053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com peer1.org2.example.com: container_name: peer1.org2.example.com extends: file: base.yaml service: peer-base environment: - CORE_PEER_ID=peer1.org2.example.com - CORE_PEER_LOCALMSPID=Org2MSP - CORE_PEER_ADDRESS=peer1.org2.example.com:7051 ports: - 10051:7051 - 10053:7053 volumes: - ./channel/crypto-config/peerOrganizations/org2.example.com/peers/peer1.org2.example.com/:/etc/hyperledger/crypto/peer depends_on: - orderer.example.com cli: container_name: cli image: hyperledger/fabric-tools tty: true stdin_open: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock #- CORE_LOGGING_LEVEL=DEBUG - FABRIC_LOGGING_SPEC=INFO - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer0.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/bash -c "./scripts/script.sh" volumes: - /var/run/:/host/var/run/ - ./../chaincode/:/opt/gopath/src/github.com/chaincode - ./channel/crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/ - ./channel:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts depends_on: - orderer.example.com - peer0.org1.example.com - peer1.org1.example.com - peer0.org2.example.com - peer1.org2.example.com 运行docker-compose启动网络1docker-compose -f ./docker-compose.yaml up -d 接下来一个安装并示例好链码的网络环境就已经搭建好了， 接下来查看cli容器中进行了哪些步骤实现了对channel的创建和chaincode的安装和实例化 cli 客户端中执行的内容cli 容器中启动程序代码为， 在脚本中会进行channel的创建，组织节点加入channel，链码的安装，实例化等 启动执行脚本scripts.sh 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283CHANNEL_NAME="$1"DELAY="$2"LANGUAGE="$3"TIMEOUT="$4": $&#123;CHANNEL_NAME:="mychannel"&#125;: $&#123;DELAY:="3"&#125;: $&#123;LANGUAGE:="golang"&#125;: $&#123;TIMEOUT:="10"&#125;LANGUAGE=`echo "$LANGUAGE" | tr [:upper:] [:lower:]`COUNTER=1MAX_RETRY=5ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pemCC_SRC_PATH="github.com/chaincode/chaincode_example02/go/"if [ "$LANGUAGE" = "node" ]; then CC_SRC_PATH="/opt/gopath/src/github.com/chaincode/chaincode_example02/node/"fiecho "Channel name : "$CHANNEL_NAME# import utils. scripts/utils.shcreateChannel() &#123; setGlobals 0 1 if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer channel create -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/channel.tx &gt;&amp;log.txt res=$? set +x else set -x peer channel create -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/channel.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Channel creation failed" echo "===================== Channel \"$CHANNEL_NAME\" is created successfully ===================== " echo&#125;joinChannel () &#123; for org in 1 2; do for peer in 0 1; do joinChannelWithRetry $peer $org echo "===================== peer$&#123;peer&#125;.org$&#123;org&#125; joined on the channel \"$CHANNEL_NAME\" ===================== " sleep $DELAY echo done done&#125;## Create channelecho "Creating channel..."createChannel## Join all the peers to the channelecho "Having all peers join the channel..."joinChannel## Set the anchor peers for each org in the channelecho "Updating anchor peers for org1..."updateAnchorPeers 0 1echo "Updating anchor peers for org2..."updateAnchorPeers 0 2## Install chaincode on peer0.org1 and peer0.org2echo "Installing chaincode on peer0.org1..."installChaincode 0 1echo "Install chaincode on peer0.org2..."installChaincode 0 2## Install chaincode on peer1.org2echo "Installing chaincode on peer1.org1..."installChaincode 1 1echo "Installing chaincode on peer1.org2..."installChaincode 1 2# Instantiate chaincode on peer0.org2echo "Instantiating chaincode on peer0.org2..."instantiateChaincode 0 2 依赖脚本utils.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265verifyResult () &#123; if [ $1 -ne 0 ] ; then echo "!!!!!!!!!!!!!!! "$2" !!!!!!!!!!!!!!!!" echo "========= ERROR !!! FAILED to execute End-2-End Scenario ===========" echo exit 1 fi&#125;# Set OrdererOrg.Admin globalssetOrdererGlobals() &#123; CORE_PEER_LOCALMSPID="OrdererMSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/users/Admin@example.com/msp&#125;setGlobals () &#123; PEER=$1 ORG=$2 if [ $ORG -eq 1 ] ; then CORE_PEER_LOCALMSPID="Org1MSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp if [ $PEER -eq 0 ]; then CORE_PEER_ADDRESS=peer0.org1.example.com:7051 else CORE_PEER_ADDRESS=peer1.org1.example.com:7051 fi elif [ $ORG -eq 2 ] ; then CORE_PEER_LOCALMSPID="Org2MSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp if [ $PEER -eq 0 ]; then CORE_PEER_ADDRESS=peer0.org2.example.com:7051 else CORE_PEER_ADDRESS=peer1.org2.example.com:7051 fi elif [ $ORG -eq 3 ] ; then CORE_PEER_LOCALMSPID="Org3MSP" CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/ca.crt CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp if [ $PEER -eq 0 ]; then CORE_PEER_ADDRESS=peer0.org3.example.com:7051 else CORE_PEER_ADDRESS=peer1.org3.example.com:7051 fi else echo "================== ERROR !!! ORG Unknown ==================" fi env |grep CORE&#125;updateAnchorPeers() &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer channel update -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/$&#123;CORE_PEER_LOCALMSPID&#125;anchors.tx &gt;&amp;log.txt res=$? set +x else set -x peer channel update -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/$&#123;CORE_PEER_LOCALMSPID&#125;anchors.tx --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Anchor peer update failed" echo "===================== Anchor peers for org \"$CORE_PEER_LOCALMSPID\" on \"$CHANNEL_NAME\" is updated successfully ===================== " sleep $DELAY echo&#125;## Sometimes Join takes time hence RETRY at least for 5 timesjoinChannelWithRetry () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG set -x peer channel join -b $CHANNEL_NAME.block &gt;&amp;log.txt res=$? set +x cat log.txt if [ $res -ne 0 -a $COUNTER -lt $MAX_RETRY ]; then COUNTER=` expr $COUNTER + 1` echo "peer$&#123;PEER&#125;.org$&#123;ORG&#125; failed to join the channel, Retry after $DELAY seconds" sleep $DELAY joinChannelWithRetry $PEER $ORG else COUNTER=1 fi verifyResult $res "After $MAX_RETRY attempts, peer$&#123;PEER&#125;.org$&#123;ORG&#125; has failed to Join the Channel"&#125;installChaincode () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG VERSION=$&#123;3:-1.0&#125; set -x peer chaincode install -n mycc -v $&#123;VERSION&#125; -l $&#123;LANGUAGE&#125; -p $&#123;CC_SRC_PATH&#125; &gt;&amp;log.txt res=$? set +x cat log.txt verifyResult $res "Chaincode installation on peer$&#123;PEER&#125;.org$&#123;ORG&#125; has Failed" echo "===================== Chaincode is installed on peer$&#123;PEER&#125;.org$&#123;ORG&#125; ===================== " echo&#125;instantiateChaincode () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG VERSION=$&#123;3:-1.0&#125; # while 'peer chaincode' command can get the orderer endpoint from the peer (if join was successful), # lets supply it directly as we know it using the "-o" option if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer chaincode instantiate -o orderer.example.com:7050 -C $CHANNEL_NAME -n mycc -l $&#123;LANGUAGE&#125; -v $&#123;VERSION&#125; -c '&#123;"Args":["init","a","100","b","200"]&#125;' -P "OR ('Org1MSP.member','Org2MSP.member')" &gt;&amp;log.txt res=$? set +x else set -x peer chaincode instantiate -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -l $&#123;LANGUAGE&#125; -v 1.0 -c '&#123;"Args":["init","a","100","b","200"]&#125;' -P "OR ('Org1MSP.member','Org2MSP.member')" &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Chaincode instantiation on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' failed" echo "===================== Chaincode Instantiation on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' is successful ===================== " echo&#125;upgradeChaincode () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG set -x peer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c '&#123;"Args":["init","a","90","b","210"]&#125;' -P "OR ('Org1MSP.peer','Org2MSP.peer','Org3MSP.peer')" res=$? set +x cat log.txt verifyResult $res "Chaincode upgrade on org$&#123;ORG&#125; peer$&#123;PEER&#125; has Failed" echo "===================== Chaincode is upgraded on org$&#123;ORG&#125; peer$&#123;PEER&#125; ===================== " echo&#125;chaincodeQuery () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG EXPECTED_RESULT=$3 echo "===================== Querying on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME'... ===================== " local rc=1 local starttime=$(date +%s) # continue to poll # we either get a successful response, or reach TIMEOUT while test "$(($(date +%s)-starttime))" -lt "$TIMEOUT" -a $rc -ne 0 do sleep $DELAY echo "Attempting to Query peer$&#123;PEER&#125;.org$&#123;ORG&#125; ...$(($(date +%s)-starttime)) secs" set -x peer chaincode query -C $CHANNEL_NAME -n mycc -c '&#123;"Args":["query","a"]&#125;' &gt;&amp;log.txt res=$? set +x test $res -eq 0 &amp;&amp; VALUE=$(cat log.txt | awk '/Query Result/ &#123;print $NF&#125;') test "$VALUE" = "$EXPECTED_RESULT" &amp;&amp; let rc=0 done echo cat log.txt if test $rc -eq 0 ; then echo "===================== Query on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' is successful ===================== " else echo "!!!!!!!!!!!!!!! Query result on peer$&#123;PEER&#125;.org$&#123;ORG&#125; is INVALID !!!!!!!!!!!!!!!!" echo "================== ERROR !!! FAILED to execute End-2-End Scenario ==================" echo exit 1 fi&#125;# fetchChannelConfig &lt;channel_id&gt; &lt;output_json&gt;# Writes the current channel config for a given channel to a JSON filefetchChannelConfig() &#123; CHANNEL=$1 OUTPUT=$2 setOrdererGlobals echo "Fetching the most recent configuration block for the channel" if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL --cafile $ORDERER_CA set +x else set -x peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL --tls --cafile $ORDERER_CA set +x fi echo "Decoding config block to JSON and isolating config to $&#123;OUTPUT&#125;" set -x configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; "$&#123;OUTPUT&#125;" set +x&#125;# signConfigtxAsPeerOrg &lt;org&gt; &lt;configtx.pb&gt;# Set the peerOrg admin of an org and signing the config updatesignConfigtxAsPeerOrg() &#123; PEERORG=$1 TX=$2 setGlobals 0 $PEERORG set -x peer channel signconfigtx -f "$&#123;TX&#125;" set +x&#125;# createConfigUpdate &lt;channel_id&gt; &lt;original_config.json&gt; &lt;modified_config.json&gt; &lt;output.pb&gt;# Takes an original and modified config, and produces the config update tx which transitions between the twocreateConfigUpdate() &#123; CHANNEL=$1 ORIGINAL=$2 MODIFIED=$3 OUTPUT=$4 set -x configtxlator proto_encode --input "$&#123;ORIGINAL&#125;" --type common.Config &gt; original_config.pb configtxlator proto_encode --input "$&#123;MODIFIED&#125;" --type common.Config &gt; modified_config.pb configtxlator compute_update --channel_id "$&#123;CHANNEL&#125;" --original original_config.pb --updated modified_config.pb &gt; config_update.pb configtxlator proto_decode --input config_update.pb --type common.ConfigUpdate &gt; config_update.json echo '&#123;"payload":&#123;"header":&#123;"channel_header":&#123;"channel_id":"'$CHANNEL'", "type":2&#125;&#125;,"data":&#123;"config_update":'$(cat config_update.json)'&#125;&#125;&#125;' | jq . &gt; config_update_in_envelope.json configtxlator proto_encode --input config_update_in_envelope.json --type common.Envelope &gt; "$&#123;OUTPUT&#125;" set +x&#125;chaincodeInvoke () &#123; PEER=$1 ORG=$2 setGlobals $PEER $ORG # while 'peer chaincode' command can get the orderer endpoint from the peer (if join was successful), # lets supply it directly as we know it using the "-o" option if [ -z "$CORE_PEER_TLS_ENABLED" -o "$CORE_PEER_TLS_ENABLED" = "false" ]; then set -x peer chaincode invoke -o orderer.example.com:7050 -C $CHANNEL_NAME -n mycc -c '&#123;"Args":["invoke","a","b","10"]&#125;' &gt;&amp;log.txt res=$? set +x else set -x peer chaincode invoke -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -c '&#123;"Args":["invoke","a","b","10"]&#125;' &gt;&amp;log.txt res=$? set +x fi cat log.txt verifyResult $res "Invoke execution on peer$&#123;PEER&#125;.org$&#123;ORG&#125; failed " echo "===================== Invoke transaction on peer$&#123;PEER&#125;.org$&#123;ORG&#125; on channel '$CHANNEL_NAME' is successful ===================== " echo&#125; 链代码内容链代码实现的主要方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177package mainimport ( "fmt" "strconv" "github.com/hyperledger/fabric/core/chaincode/shim" pb "github.com/hyperledger/fabric/protos/peer")// SimpleChaincode example simple Chaincode implementationtype SimpleChaincode struct &#123;&#125;func (t *SimpleChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response &#123; fmt.Println("ex02 Init") _, args := stub.GetFunctionAndParameters() var A, B string // Entities var Aval, Bval int // Asset holdings var err error if len(args) != 4 &#123; return shim.Error("Incorrect number of arguments. Expecting 4") &#125; // Initialize the chaincode A = args[0] Aval, err = strconv.Atoi(args[1]) if err != nil &#123; return shim.Error("Expecting integer value for asset holding") &#125; B = args[2] Bval, err = strconv.Atoi(args[3]) if err != nil &#123; return shim.Error("Expecting integer value for asset holding") &#125; fmt.Printf("Aval = %d, Bval = %d\n", Aval, Bval) // Write the state to the ledger err = stub.PutState(A, []byte(strconv.Itoa(Aval))) if err != nil &#123; return shim.Error(err.Error()) &#125; err = stub.PutState(B, []byte(strconv.Itoa(Bval))) if err != nil &#123; return shim.Error(err.Error()) &#125; return shim.Success(nil)&#125;func (t *SimpleChaincode) Invoke(stub shim.ChaincodeStubInterface) pb.Response &#123; fmt.Println("ex02 Invoke") function, args := stub.GetFunctionAndParameters() if function == "invoke" &#123; // Make payment of X units from A to B return t.invoke(stub, args) &#125; else if function == "delete" &#123; // Deletes an entity from its state return t.delete(stub, args) &#125; else if function == "query" &#123; // the old "Query" is now implemtned in invoke return t.query(stub, args) &#125; return shim.Error("Invalid invoke function name. Expecting \"invoke\" \"delete\" \"query\"")&#125;// Transaction makes payment of X units from A to Bfunc (t *SimpleChaincode) invoke(stub shim.ChaincodeStubInterface, args []string) pb.Response &#123; var A, B string // Entities var Aval, Bval int // Asset holdings var X int // Transaction value var err error if len(args) != 3 &#123; return shim.Error("Incorrect number of arguments. Expecting 3") &#125; A = args[0] B = args[1] // Get the state from the ledger // TODO: will be nice to have a GetAllState call to ledger Avalbytes, err := stub.GetState(A) if err != nil &#123; return shim.Error("Failed to get state") &#125; if Avalbytes == nil &#123; return shim.Error("Entity not found") &#125; Aval, _ = strconv.Atoi(string(Avalbytes)) Bvalbytes, err := stub.GetState(B) if err != nil &#123; return shim.Error("Failed to get state") &#125; if Bvalbytes == nil &#123; return shim.Error("Entity not found") &#125; Bval, _ = strconv.Atoi(string(Bvalbytes)) // Perform the execution X, err = strconv.Atoi(args[2]) if err != nil &#123; return shim.Error("Invalid transaction amount, expecting a integer value") &#125; Aval = Aval - X Bval = Bval + X fmt.Printf("Aval = %d, Bval = %d\n", Aval, Bval) // Write the state back to the ledger err = stub.PutState(A, []byte(strconv.Itoa(Aval))) if err != nil &#123; return shim.Error(err.Error()) &#125; err = stub.PutState(B, []byte(strconv.Itoa(Bval))) if err != nil &#123; return shim.Error(err.Error()) &#125; return shim.Success(nil)&#125;// Deletes an entity from statefunc (t *SimpleChaincode) delete(stub shim.ChaincodeStubInterface, args []string) pb.Response &#123; if len(args) != 1 &#123; return shim.Error("Incorrect number of arguments. Expecting 1") &#125; A := args[0] // Delete the key from the state in ledger err := stub.DelState(A) if err != nil &#123; return shim.Error("Failed to delete state") &#125; return shim.Success(nil)&#125;// query callback representing the query of a chaincodefunc (t *SimpleChaincode) query(stub shim.ChaincodeStubInterface, args []string) pb.Response &#123; var A string // Entities var err error if len(args) != 1 &#123; return shim.Error("Incorrect number of arguments. Expecting name of the person to query") &#125; A = args[0] // Get the state from the ledger Avalbytes, err := stub.GetState(A) if err != nil &#123; jsonResp := "&#123;\"Error\":\"Failed to get state for " + A + "\"&#125;" return shim.Error(jsonResp) &#125; if Avalbytes == nil &#123; jsonResp := "&#123;\"Error\":\"Nil amount for " + A + "\"&#125;" return shim.Error(jsonResp) &#125; jsonResp := "&#123;\"Name\":\"" + A + "\",\"Amount\":\"" + string(Avalbytes) + "\"&#125;" fmt.Printf("Query Response:%s\n", jsonResp) return shim.Success(Avalbytes)&#125;func main() &#123; err := shim.Start(new(SimpleChaincode)) if err != nil &#123; fmt.Printf("Error starting Simple chaincode: %s", err) &#125;&#125;]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ca 与 MSP 的介绍]]></title>
    <url>%2F2019%2F04%2F03%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2FCa%20%E4%B8%8E%20MSP%20%E7%9A%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[关于加密的介绍对称加密（共享密钥加密）在对称加密算法中，加密使用的密钥和解密使用的密钥是相同的。也就是说，加密和解密都是使用的同一个密钥。因此对称加密算法要保证安全性的话，密钥要做好保密，只能让使用的人知道，不能对外公开。这个和上面的公钥密码体制有所不同，公钥密码体制中加密是用公钥，解密使用私钥，而对称加密算法中，加密和解密都是使用同一个密钥，不区分公钥和私钥。 非对称加密（公开密钥加密 ）在非对称加密算法中，加密使用的密钥和解密使用的密钥是不相同的。也就是说加密使用的密钥和解密使用的密钥不同，因此它是一个非对称加密算法。 数字签名类似在纸质合同上进行签名以确认合同内容和证明身份，数字签名既可以证实某数字内容的完整性，又可以确认其来源 一个典型的场景是，Alice 通过信道发给 Bob 一个文件（一份信息），Bob 如何获知所收到的文件即为 Alice 发出的原始版本？Alice 可以先对文件内容进行摘要，然后用自己的私钥对摘要进行加密（签名），之后同时将文件和签名都发给 Bob。Bob 收到文件和签名后，用 Alice 的公钥来解密签名，得到数字摘要，与对文件进行摘要后的结果进行比对。如果一致，说明该文件确实是 Alice 发过来的（因为别人无法拥有 Alice 的私钥），并且文件内容没有被修改过（摘要结果一致）。 什么是RSA算法RSA是一种非对称加密算法，现在使用得很广泛。RSA公开密钥密码体制。所谓的公开密钥密码体制就是使用不同的加密密钥与解密密钥，是一种“由已知加密密钥推导出解密密钥在计算上是不可行的”密码体制。 公钥加密的内容可以并且只能由私钥进行解密，并且由私钥加密的内容可以并且只能由公钥进行解密。也就是说，RSA的这一对公钥、私钥都可以用来加密和解密，并且一方加密的内容可以由并且只能由对方进行解密。 X.509证书X.509证书标准 X.509是常见通用的证书格式，定义了数字证书的规范，专门定义了在目录访问中需要身份认证的证书的格式。所有的证书都符合为Public Key Infrastructure (PKI) 制定的 ITU-T X509 国际标准。 X.509 DER 编码(ASCII)的后缀是： .DER .CER .CRTX.509 PAM 编码(Base64)的后缀是： .PEM .CER .CRT X.509 - 这是一种证书标准,主要定义了证书中应该包含哪些内容.其详情可以参考RFC5280,SSL使用的就是这种证书标准. X.509编码格式 同样的X.509证书,可能有不同的编码格式,目前有以下两种编码格式.PEM - Privacy Enhanced Mail,打开看文本格式,以”—–BEGIN…”开头, “—–END…”结尾,内容是BASE64编码.查看PEM格式证书的信息:openssl x509 -in certificate.pem -text -nooutApache和 NIX服务器偏向于使用这种编码格式.DER - Distinguished Encoding Rules,打开看是二进制格式,不可读.查看DER格式证书的信息:openssl x509 -in certificate.der -inform der -text -nooutJava和Windows服务器偏向于使用这种编码格式. X.509相关的文件扩展名 这是比较误导人的地方,虽然我们已经知道有PEM和DER这两种编码格式,但文件扩展名并不一定就叫”PEM”或者”DER”,常见的扩展名除了PEM和DER还有以下这些,它们除了编码格式可能不同之外,内容也有差别,但大多数都能相互转换编码格式.CRT - CRT应该是certificate的三个字母,其实还是证书的意思,常见于NIX系统,有可能是PEM编码,也有可能是DER编码,大多数应该是PEM编码,相信你已经知道怎么辨别.CER - 还是certificate,还是证书,常见于Windows系统,同样的,可能是PEM编码,也可能是DER编码,大多数应该是DER编码.KEY - 通常用来存放一个公钥或者私钥,并非X.509证书,编码同样的,可能是PEM,也可能是DER.查看KEY的办法:openssl rsa -in mykey.key -text -noout如果是DER格式的话,同理应该这样了:openssl rsa -in mykey.key -text -noout -inform derCSR - Certificate Signing Request PFX/P12 - predecessor of PKCS#12,对nix服务器来说,一般CRT和KEY是分开存放在不同文件中的,但Windows的IIS则将它们存在一个PFX文件中,(因此这个文件包含了证书及私钥)这样会不会不安全？应该不会,PFX通常会有一个”提取密码”,你想把里面的东西读取出来的话,它就要求你提供提取密码,PFX使用的时DER编码,如何把PFX转换为PEM编码？openssl pkcs12 -in for-iis.pfx -out for-iis.pem -nodes这个时候会提示你输入提取代码. for-iis.pem就是可读的文本.生成pfx的命令类似这样:openssl pkcs12 -export -out certificate.pfx -inkey privateKey.key -in certificate.crt -certfile CACert.crt其中CACert.crt是CA(权威证书颁发机构)的根证书,有的话也通过-certfile参数一起带进去.这么看来,PFX其实是个证书密钥库.*JKS** - 即Java Key Storage,这是Java的专利,跟OpenSSL关系不大,利用Java的一个叫”keytool”的工具,可以将PFX转为JKS,当然了,keytool也能直接生成JKS,不过在此就不多表了. 证书编码的转换 PEM转为DER openssl x509 -in cert.crt -outform der -out cert.derDER转为PEM openssl x509 -in cert.crt -inform der -outform pem -out cert.pem(提示:要转换KEY文件也类似,只不过把x509换成rsa,要转CSR的话,把x509换成req…) PKCS 标准PKCS是由美国RSA数据安全公司及其合作伙伴制定的一组公钥密码学标准，其中包括证书申请、证书更新、证书作废表发布、扩展证书内容以及数字签名、数字信封的格式等方面的一系列相关协议 可以理解为对数字证书的特定封装形式，不同形式的封装用在不同的使用场合； 常用PKCS标准 PKCS 目前共发布过 15 个标准。 常用的有：PKCS#7 Cryptographic Message Syntax Standard ：定义一种通用的消息语法，包括数字签名和加密等用于增强的加密机制，PKCS#7与PEM兼容，所以不需其他密码操作，就可以将加密的消息转换成PEM消息。PKCS#10 Certification Request Standard：描述证书请求语法。PKCS#12 Personal Information Exchange Syntax Standard：描述个人信息交换语法标准。描述了将用户公钥、私钥、证书和其他相关信息打包的语法。 关于CA的介绍什么是证书“证书”洋文也叫“digital certificate”或“public key certificate”。 它是用来证明某某东西确实是某某东西的东西（是不是像绕口令？）。通俗地说，证书就好比例子里面的公章。通过公章，可以证明该介绍信确实是对应的公司发出的。 理论上，人人都可以找个证书工具，自己做一个证书。那如何防止坏人自己制作证书出来骗人捏？请看后续 CA 的介绍。 什么是CACA是Certificate Authority的缩写，也叫“证书授权中心”。 它是负责管理和签发证书的第三方机构，就好比例子里面的中介——C 公司。一般来说，CA必须是所有行业和所有公众都信任的、认可的。因此它必须具有足够的权威性。就好比A、B两公司都必须信任C公司，才会找 C 公司作为公章的中介。 什么是CA证书CA 证书，顾名思义，就是CA颁发的证书。 前面已经说了，人人都可以找工具制作证书。但是你一个小破孩制作出来的证书是没啥用处的。因为你不是权威的CA机关，你自己搞的证书不具有权威性。 这就好比上述的例子里，某个坏人自己刻了一个公章，盖到介绍信上。但是别人一看，不是受信任的中介公司的公章，就不予理睬。坏蛋的阴谋就不能得逞啦。 什么是根证书普通的证书一般包括三部分：用户信息，用户公钥，以及CA签名 那么我们要验证这张证书就需要验证CA签名的真伪。那么就需要CA公钥。而CA公钥存在于另外一张证书（称这张证书是对普通证书签名的证书）中。因此又需要验证另外一张证书（称这张证书是对另外一张证书签名的证书）的真伪。依次往下回溯，就得到一条证书链。那么这张证书链从哪里结束呢？就是在根证书结束（即验证到根证书结束）。根证书是个很特别的证书，它是CA中心自己给自己签名的证书（即这张证书是用CA公钥对这张证书进行签名）。信任这张证书，就代表信任这张证书下的证书链。 所有用户在使用自己的证书之前必须先下载根证书。 所谓根证书验证就是：用根证书公钥来验证该证书的颁发者签名。所以首先必须要有根证书，并且根证书必须在受信任的证书列表（即信任域）。 证书签发流程 向权威证书颁发机构申请证书， 把本地生成的申请证书(包含公钥)、组织信息、个人信息等 交给权威证书颁发机构,权威证书颁发机构对此进行签名,完成.（保留好csr,当权威证书颁发机构颁发的证书过期的时候,你还可以用同样的csr来申请新的证书,key保持不变.） CA 通过线上、线下等多种手段验证申请者提供信息的真实性，如组织是否存在、企业是否合法，是否拥有域名的所有权等； 如信息审核通过，CA 会向申请者签发认证文件-证书。证书包含以下信息：申请者公钥、申请者的组织信息和个人信息、签发机构 CA 的信息、有效时间、证书序列号等信息的明文，同时包含一个签名； 签名的产生算法：首先，使用散列函数计算公开的明文信息的信息摘要，然后，采用 CA 的私钥对信息摘要进行加密，密文即签名； 客户端证书验证流程 客户端请求服务端 ​ 客户端向服务端发送请求，服务端将自己的证书和用自己私钥加密的原文， 以及原文的摘要一并返回。 数字证书有效性验证​ 客户端 C 读取证书中的相关的明文信息，采用相同的散列函数计算得到信息摘要，然后，利用对应 CA 的公钥解密签名数据，对比证书的信息摘要，如果一致，则可以确认证书的合法性，即公钥合法； ​ 客户端然后验证证书相关的域名信息、有效时间等信息； 根证书验证​ 客户端会内置信任 CA 的证书信息(包含公钥)，如果CA不被信任，则找不到对应 CA 的证书，证书也会被判定非法。 CRL验证​ CRL是经过CA签名的证书作废列表，用于证书冻结和撤销。一般来说证书中有CRL地址，供HTTP或者LDAP方式访问，通过解析可得到CRL地址，然后下载CRL进行验证。​ 并且证书中有CRL生效日期以及下次更新的日期，因此CRL是自动更新的，因此会有延迟性。​ 于是呢，还有另外一种方式OSCP证书状态在线查询，可以即时的查询证书状态。 用https来说明CA认证的流程SSL 介绍SSL SSL - Secure Sockets Layer,现在应该叫”TLS”,但由于习惯问题,我们还是叫”SSL”比较多.http协议默认情况下是不加密内容的,这样就很可能在内容传播的时候被别人监听到,对于安全性要求较高的场合,必须要加密,https就是带加密的http协议,而https的加密是基于SSL的,它执行的是一个比较下层的加密,也就是说,在加密前,你的服务器程序在干嘛,加密后也一样在干嘛,不用动,这个加密对用户和开发者来说都是透明的 SSL是基于非对称加密的原理，在这之上还进行了对称加密的数据传输。当传送数据量过大的时候，客户端和服务器之间互相商定了一个对话密钥（session key），使用这个对话密钥来进行对称加密加快运算速度， 所以说SSL是基于RSA进行的数据传输上的优化，可以加速加密运算速度。 SSL 应用了RSA ， 数字签名，非对称加密等技术，解决了网络通讯被监听，伪装和篡改等问题，一般企业级应用，现在离不开SSL技术； OpenSSL OpenSSL - 简单地说,OpenSSL是SSL的一个实现,SSL只是一种规范.理论上来说,SSL这种规范是安全的,目前的技术水平很难破解,但SSL的实现就可能有些漏洞,如著名的”心脏出血”.OpenSSL还提供了一大堆强大的工具软件,强大到90%我们都用不到. http通信存在的问题 容易被监听 http通信都是明文，数据在客户端与服务器通信过程中，任何一点都可能被劫持。比如，发送了银行卡号和密码，hacker劫取到数据，就能看到卡号和密码，这是很危险的 被伪装 http通信时，无法保证通行双方是合法的，通信方可能是伪装的。比如你请求www.taobao.com,你怎么知道返回的数据就是来自淘宝，中间人可能返回数据伪装成淘宝。 被篡改 hacker中间篡改数据后，接收方并不知道数据已经被更改 https解决的问题https很好的解决了http的三个缺点（被监听、被篡改、被伪装），https不是一种新的协议，它是http+SSL(TLS)的结合体，SSL是一种独立协议，所以其它协议比如smtp等也可以跟ssl结合。https改变了通信方式，它由以前的http—–&gt;tcp，改为http——&gt;SSL—–&gt;tcp；https采用了共享密钥加密+公开密钥加密的方式 防监听 数据是加密的，所以监听得到的数据是密文，hacker看不懂。 防伪装 伪装分为客户端伪装和服务器伪装，通信双方携带证书，证书相当于身份证，有证书就认为合法，没有证书就认为非法，证书由第三方颁布，很难伪造 防篡改 https对数据做了摘要，篡改数据会被感知到。hacker即使从中改了数据也白搭。 https认证的详细流程 客户端发起HTTPS请求 这个没什么好说的，就是用户在浏览器里输入一个https网址，然后连接到server的443端口。 服务端的配置** 采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请。区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl就是个不错的选择，有1年的免费服务)。这套证书其实就是一对公钥和私钥。如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。 传送证书 服务端将证书发送给客户端，这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。 客户端解析证书 这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。如果证书没有问题，那么就生成一个随机值。然后用证书对该随机值进行加密。就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。 传送加密信息 这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。 服务段解密信息 服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密。所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。 传输加密后的信息 这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。 客户端解密信息 客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容。整个过程第三方即使监听到了数据，也束手无策。 总结整个过程： ​ 1）服务器向CA机构获取证书（假设这个证书伪造不了），当浏览器首次请求服务器的时候，服务器返回证书给浏览器。（证书包含：公钥+申请者与颁发者的相关信息+签名） ​ 2）浏览器得到证书后，开始验证证书的相关信息，证书有效（没过期等）。（验证过程，比较复杂，详见上文）。 ​ 3）验证完证书后，如果证书有效，客户端是生成一个随机数，然后用证书中的公钥进行加密，加密后，发送给服务器，服务器用私钥进行解密，得到随机数。之后双方便开始用该随机数作为钥匙，对要传递的数据进行加密、解密。 后续的问题 怎样保证公开密钥的有效性 你也许会想到，怎么保证客户端收到的公开密钥是合法的，不是伪造的，证书很好的完成了这个任务。证书由权威的第三方机构颁发，并且对公开密钥做了签名。 https的缺点 https保证了通信的安全，但带来了加密解密消耗计算机cpu资源的问题 ，不过，有专门的https加解密硬件服务器 各大互联网公司，百度、淘宝、支付宝、知乎都使用https协议，为什么？ 支付宝涉及到金融，所以出于安全考虑采用https这个，可以理解，为什么百度、知乎等也采用这种方式？为了防止运营商劫持！http通信时，运营商在数据中插入各种广告，用户看到后，怒火发到互联网公司，其实这些坏事都是运营商(移动、联通、电信)干的,用了https，运营商就没法插播广告篡改数据了。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric SDK 配置文件]]></title>
    <url>%2F2019%2F04%2F02%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F2.Fabric%20SDK%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[一、Fabric SDK配置Fabric区块链网络应用程序需要大量的参数，用于连接Fabric区块链网络。通常将Fabric区块链网络应用程序所需的参数放到一个配置文件进行管理，配置文件定义Fabric SDK Go的配置和用户自定义参数，指定了连接Fabric区块链网络所需的全部信息，例如Fabric区块链网络组件的主机名和端口等。Fabric SDK GO为应用程序提供的配置文件通常使用yaml文件格式编写，并命名为config.yaml，配置文件会在应用程序代码中被读取。Fabric SDK Go版本提供了config.yaml模板，开发者可以参考fabric-sdk-go/pkg/core/config/testdata/template/config.yaml，也可以根据fabric-sdk-go/test/fixtures/config/config_e2e.yaml实例进行改写。 二、version定义version用于定义config.yaml文件内容的版本，Fabric SDK Go会使用version匹配相应的解析规则。version: 1.0.0 三、channels定义channels部分描述已经存在的通道信息，每个通道包含哪些orderer、peer 。peer部分可以定义peer节点的角色属性，角色如下：endorsingPeer：可选。peer节点节点必须安装链码。peer节点是否会被发送交易提案进行背书。应用程序也可以使用本属性来决定发送链码安装请求到哪个peer节点。默认值：true。chaincodeQuery：可选。peer节点必须安装链码。peer节点是否会被发送交易提案只进行查询。应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true。ledgerQuery：可选。是否可以向peer节点发送不会进行链码查询的提案，如queryBlock（），queryTransaction（）等。默认值：true。eventSource：可选。peer节点是否为SDK事件监听器注册的目标，所有的peer节点都可以生产事件，但通常只需要连接一个对事件进行监听。默认值：true。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# 如果应用程序创建了通道，不需要本部分channels: # 如果没有定义channel配置或是定义的channel没有信息，使用默认channel # 如果channel没有定义peers，使用默认channel的peers # 如果channel没有定义orderes，使用默认channel的orderes # 如果channel没有定义背书策略，使用默认channel的背书策略 # 如果channel定义了背书策略，但某些背书策略缺失，缺失的背书策略会被默认channel填充 _default: # 可选，参与组织的peers列表 peers: peer1.org1.example.com: endorsingPeer: true chaincodeQuery: true ledgerQuery: true eventSource: true # 可选，应用程序使用下列选项执行通道操作，如获取通道配置 policies: #可选，获取通道配置区块 queryChannelConfig: #可选，成功响应节点的最小数量 minResponses: 1 # 可选 maxTargets: 1 # 可选，查询配置区块的重试选项 retryOpts: # 可选，重试次数 attempts: 5 # 可选， 第一次重试的后退间隔 initialBackoff: 500ms # 可选， 重试的最大后退间隔 maxBackoff: 5s backoffFactor: 2.0 # 可选，获取发现信息选项 discovery: maxTargets: 2 # 重试选项 retryOpts: # 可选，重试次数 attempts: 4 initialBackoff: 500ms maxBackoff: 5s backoffFactor: 2.0 # 可选，事件服务选项 eventService: # 可选 resolverStrategy指定连接到peer节点时选择peer节点的决策策略 # 可选值:PreferOrg（默认）, MinBlockHeight, Balanced # PreferOrg: # 基于区块高度滞后阀值决定哪些peer节点是合适的， 虽然会在当前组织中优先选择peer节点 # 如果当前组织中没有合适的peer节点，会从其它组织中选取 # MinBlockHeight: # 根据区块高度滞后阀值选择最好的peer节点， # 所有peer节点的最大区块高度是确定的，区块高度小于最大区块高度但在滞后阀值范围内的peer节点会被加载， # 其它peer节点不会被考虑。 # Balanced: # 使用配置的balancer选择peer节点 resolverStrategy: PreferOrg # 可选 balancer是选择连接到peer节点的负载均衡器 # 可选值: Random (default), RoundRobin balancer: Random # 可选，blockHeightLagThreshold设置区块高度滞后阀值，用于选择连接到的peer节点 # 如果一个peer节点滞后于最新的peer节点给定的区块数，会被排除在选择之外 # 注意：当minBlockHeightResolverMode设置为ResolveByThreshold时，本参数才可用 # 默认: 5 blockHeightLagThreshold: 5 # 可选，reconnectBlockHeightLagThreshold - 如果peer节点的区块高度落后于指定的区块数量， # 事件客户端会从peer节点断开，重新连接到一个性能更好的peer节点 # 如果peerMonitor设置为启用（默认），本参数才可用 # 默认值: 10 # 注意：设置值太低会导致事件客户端频繁断开或重连，影响性能 reconnectBlockHeightLagThreshold: 8 # 可选， peerMonitorPeriod是事件客户端从连接节点断开重新连接到另外一个节点的时间 # 默认: 对于Balanced resolverStrategy禁用，为0; 对于PreferOrg和MinBlockHeight为5s peerMonitorPeriod: 6s #如果_default没有定义，必选；如果_default已经定义，可选。 # 通道名称 assetchannel: # 如果_default peers没有定义，必选；如果_default peers已经定义，可选。 # 参与组织的peer节点列表 peers: peer0.org1.example.com: # 可选。peer节点是否会被发送交易提议只进行查询。peer节点必须安装链码。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true endorsingPeer: true # 可选。peer节点必须安装链码。peer节点是否会被发送交易提案只进行查询。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true chaincodeQuery: true # 可选。是否可以向peer节点发送不会进行链码查询的提案，如queryBlock（）， # queryTransaction（）等。默认值：true。 ledgerQuery: true # 可选。peer节点是否为SDK事件监听器注册的目标，所有的peer节点都可以生产事件， # 但通常只需要连接一个对事件进行监听。默认值：true。 eventSource: true 四、organizations定义organizations描述peer节点所属的组织(org)，证书颁发机构，以及组织的MSP ID。 123456789101112131415161718192021222324252627282930# Fabric区块链网络中参与的组织列表organizations: org1: mspid: Org1MSP # 组织的MSP存储位置，绝对路径或相对cryptoconfig的路径 cryptoPath: peerOrganizations/org1.example.com/users/&#123;username&#125;@org1.example.com/msp peers: - peer0.org1.example.com - peer1.org1.example.com # 可选，证书颁发机构签发×××明，Fabric-CA是一个特殊的证书管理机构，提供REST API支持动态证书管理，如登记、撤销、重新登记 # 下列部分只为Fabric-CA服务器设置 certificateAuthorities: - ca.org1.example.com org2: mspid: Org2MSP # 组织的MSP存储位置，相对于cryptoconfig的相对位置或绝对路径 cryptoPath: peerOrganizations/org2.example.com/users/&#123;username&#125;@org2.example.com/msp peers: - peer0.org2.example.com certificateAuthorities: - ca.org2.example.com # Orderer组织名称 ordererorg: # 组织的MSPID mspID: OrdererMSP # 加载用户需要的密钥和证书，绝对路径或相对路径 cryptoPath: ordererOrganizations/example.com/users/&#123;username&#125;@example.com/msp 五、orderers定义orderers必须指定要连接的Hyperledger Fabric区块链网络中所有orderder节点的主机名和端口。orderers对象可以包含多个orderder节点。 1234567891011121314151617181920# 发送交易请求或通道创建、更新请求到的orderers列表# 如果定义了超过一个orderer，SDK使用哪一个orderer由代码实现时指定orderers: # orderer节点，可以定义多个 orderer.example.com: url: orderer.example.com:7050 # 以下属性由gRPC库定义，会被传递给gRPC客户端构造函数 grpcOptions: ssl-target-name-override: orderer.example.com # 下列参数用于设置服务器上的keepalive策略，不兼容的设置会导致连接关闭 # 当keep-alive-time被设置为0或小于激活客户端的参数，下列参数失效 keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/ordererOrganizations/example.com/tlsca/tlsca.example.com-cert.pem 六、peers定义peers必须指定Hyperledger Fabric区块链网络中所有peer节点的主机名和端口，可能会在其它地方引用，如channels，organizations等部分。 123456789101112131415161718# peers节点列表peers: # peer节点定义，可以定义多个 peer0.org1.example.com: # URL用于发送背书和查询请求 url: peer0.org1.example.com:7051 grpcOptions: ssl-target-name-override: peer0.org1.example.com keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem 七、certificateAuthorities定义certificateAuthorities必须在Hyperledger Fabric区块链网络中指定证书颁发机构（CA）的主机名和端口，以便用于注册现有用户和注册新用户。 12345678910111213141516171819202122# Fabric-CA是Hyperledger Fabric提供了特殊的证书颁发机构，可以通过REST API管理证书。# 应用程序可以选择使用一个标准的证书颁发机构代替Fabric-CA，此时本部分不需要指定certificateAuthorities: # CA机构，可以定义多个 ca.org1.example.com: url: https://ca.org1.example.com:7054 tlsCACerts: # Comma-Separated list of paths path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem # 客户端和Fabric CA进行SSL握手的密钥和证书 client: key: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.key cert: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.crt # Fabric-CA支持通过REST API进行动态用户注册 registrar: enrollId: admin enrollSecret: adminpw # 可选，CA机构名称 caName: ca.org1.example.com 八、clientclient部分必需定义，客户端应用程序代表谁来和Fabric区块链网络来交互，可以定义超时选项。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#客户端定义client: # 客户端所属的组织，必须是organizations定义的组织 organization: org1 #定义日志服务 logging: level: debug #debug级别 # peer、事件服务、orderer超时的全局配置 # 本部分如果忽略，使用下列的值作为默认值 peer: timeout: connection: 10s response: 180s discovery: greylistExpiry: 10s eventService: timeout: registrationResponse: 15s orderer: timeout: connection: 15s response: 15s global: timeout: query: 180s execute: 180s resmgmt: 180s cache: connectionIdle: 30s eventServiceIdle: 2m channelConfig: 30m channelMembership: 30s discovery: 10s selection: 10m # MSP根目录 cryptoconfig: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric/AssetExchange/deploy/crypto-config # 某些SDK支持插件化的KV数据库, 通过指定credentialStore属性实现 credentialStore: # 可选，用于用户证书材料存储，如果所有的证书材料被嵌入到配置文件，则不需要 path: &quot;/tmp/state-store&quot; # 可选，指定Go SDK实现的CryptoSuite实现 cryptoStore: # 指定用于加密密钥存储的底层KV数据库 path: /tmp/msp # 客户端的BCCSP模块配置 BCCSP: security: enabled: true default: provider: &quot;SW&quot; hashAlgorithm: &quot;SHA2&quot; softVerify: true level: 256 tlsCerts: # 可选，当连接到peers，orderes时使用系统证书池，默认为false systemCertPool: true # 可选，客户端和peers与orderes进行TLS握手的密钥和证书 client: key: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.key cert: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.crt 九、config.yaml示例资产交易平台应用的配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170# Copyright SecureKey Technologies Inc. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0### The network connection profile provides client applications the information about the target# blockchain network that are necessary for the applications to interact with it. These are all# knowledge that must be acquired from out-of-band sources. This file provides such a source.name: &quot;assetchannel&quot;## Describe what the target network is/does.#description: &quot;asset exchange network&quot;#指定版本version: 1.0.0#客户端定义client: # 客户端所属的组织，必须是organizations定义的组织 organization: org1 #定义日志服务 logging: level: debug #debug级别 # MSP根目录 cryptoconfig: path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric/AssetExchange/deploy/crypto-config # 某些SDK支持插件化的KV数据库, 通过指定credentialStore属性实现 credentialStore: # 可选，用于用户证书材料存储，如果所有的证书材料被嵌入到配置文件，则不需要 path: &quot;/tmp/state-store&quot; # 可选，指定Go SDK实现的CryptoSuite实现 cryptoStore: # 指定用于加密密钥存储的底层KV数据库 path: /tmp/msp # 客户端的BCCSP模块配置 BCCSP: security: enabled: true default: provider: &quot;SW&quot; hashAlgorithm: &quot;SHA2&quot; softVerify: true level: 256 tlsCerts: # 可选，当连接到peers，orderes时使用系统证书池，默认为false systemCertPool: true # 可选，客户端和peers与orderes进行TLS握手的密钥和证书 client: key: # path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.key cert: #path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/tls.example.com/users/User1@tls.example.com/tls/client.crt# 如果应用程序创建了通道，不需要本部分channels: # 如果没有定义channel配置或是定义的channel没有信息，使用默认channel # 如果channel没有定义peers，使用默认channel的peers # 如果channel没有定义orderes，使用默认channel的orderes # 如果channel没有定义背书策略，使用默认channel的背书策略 # 如果channel定义了背书策略，但某些背书策略缺失，缺失的背书策略会被默认channel填充 #如果_default没有定义，必选；如果_default已经定义，可选。 # 通道名称 assetchannel: # 如果_default peers没有定义，必选；如果_default peers已经定义，可选。 # 参与组织的peer节点列表 peers: peer0.org1.example.com: # 可选。peer节点是否会被发送交易提议只进行查询。peer节点必须安装链码。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true endorsingPeer: true # 可选。peer节点必须安装链码。peer节点是否会被发送交易提案只进行查询。 # 应用程序还可以使用本属性来决定发送链码安装的请求到哪个peer节点。默认值：true chaincodeQuery: true # 可选。是否可以向peer节点发送不会进行链码查询的提案，如queryBlock（）， # queryTransaction（）等。默认值：true。 ledgerQuery: true # 可选。peer节点是否为SDK事件监听器注册的目标，所有的peer节点都可以生产事件， # 但通常只需要连接一个对事件进行监听。默认值：true。 eventSource: true # 可选，应用程序使用下列选项执行通道操作，如获取通道配置 policies: #可选，获取通道配置区块 queryChannelConfig: #可选，成功响应节点的最小数量 minResponses: 1 # 可选 maxTargets: 1 # 可选，查询配置区块的重试选项 retryOpts: # 可选，重试次数 attempts: 5 # 可选， 第一次重试的后退间隔 initialBackoff: 500ms # 可选， 重试的最大后退间隔 maxBackoff: 5s backoffFactor: 2.0# Fabric区块链网络中参与的组织列表organizations: org1: mspid: Org1MSP # 组织的MSP存储位置，绝对路径或相对cryptoconfig的路径 cryptoPath: peerOrganizations/org1.example.com/users/&#123;username&#125;@org1.example.com/msp peers: - peer0.org1.example.com - peer1.org1.example.com # 可选，证书颁发机构签发×××明，Fabric-CA是一个特殊的证书管理机构，提供REST API支持动态证书管理，如登记、撤销、重新登记 # 下列部分只为Fabric-CA服务器设置 certificateAuthorities: #- ca.org1.example.com # Orderer组织名称 ordererorg: # 组织的MSPID mspID: OrdererMSP # 加载用户需要的密钥和证书，绝对路径或相对路径 cryptoPath: ordererOrganizations/example.com/users/&#123;username&#125;@example.com/msp# 发送交易请求或通道创建、更新请求到的orderers列表# 如果定义了超过一个orderer，SDK使用哪一个orderer由代码实现时指定orderers: # orderer节点，可以定义多个 orderer.example.com: url: orderer.example.com:7050 # 以下属性由gRPC库定义，会被传递给gRPC客户端构造函数 grpcOptions: ssl-target-name-override: orderer.example.com # 下列参数用于设置服务器上的keepalive策略，不兼容的设置会导致连接关闭 # 当keep-alive-time被设置为0或小于激活客户端的参数，下列参数失效 keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 # path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/ordererOrganizations/example.com/tlsca/tlsca.example.com-cert.pem# peers节点列表peers: # peer节点定义，可以定义多个 peer0.org1.example.com: # URL用于发送背书和查询请求 url: peer0.org1.example.com:7051 grpcOptions: ssl-target-name-override: peer0.org1.example.com keep-alive-time: 0s keep-alive-timeout: 20s keep-alive-permit: false fail-fast: false allow-insecure: false tlsCACerts: # 证书的绝对路径 #path: $&#123;GOPATH&#125;/src/github.com/hyperledger/fabric-sdk-go/$&#123;CRYPTOCONFIG_FIXTURES_PATH&#125;/peerOrganizations/org1.example.com/tlsca/tlsca.org1.example.com-cert.pem]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric网络搭建]]></title>
    <url>%2F2019%2F03%2F21%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2FHyperledger%20fabric%2F1.Fabric%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[环境搭建Docker 安装123curl -fsSl &lt;https://get.docker.com/&gt; | shsystemctl enable dockerservice docker start Docker-compose 安装1yum -y install epel-release &amp;&amp; yum -y install python-pip &amp;&amp; pip install docker-compose Go安装123456789wget https://dl.google.com/go/go1.11.5.linux-amd64.tar.gztar -C /usr/local -zxvf go1.11.5.linux-amd64.tar.gzecho &quot;export GOROOT=/usr/local/go&quot; &gt;&gt; /etc/profileecho &quot;export GOBIN=/opt/prod/go/bin&quot; &gt;&gt; /etc/profileecho &quot;export GOPATH=/opt/prod/go&quot; &gt;&gt; /etc/profileecho &quot;export PATH=/opt/prod/go/bin:$PATH&quot; &gt;&gt; /etc/profileecho &quot;export PATH=/usr/local/go/bin:$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilemkdir /opt/prod/go 下载Fabric组件bootstrap.sh文件可以通过网络下载 12curl -sSL https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh | bash 1.4.0 -s # 下载Fabric二进制文件 Image 以及示例文件cp fabric-sample/bin/* $/GOPATH/bin Fabric-sdk-go下载github地址: https://github.com/hyperledger/fabric-sdk-go.git sdk 编译， 最好在外网进行编译，用VPN容易超时 12make pupulatemake depend 网络配置创建目录结构创建一个目录用来存储Fabric的所有配置文件 12mkdir -p /opt/prod/Fabric/network # 存储Fabric所有配置文件mkdir -p /opt/prod/Fabric/network/channel-artifacts # 存放创世区块以及通道配置文件等 生成MSP创建MSP配置信息编辑MSP配置文件 crypto-config.yaml 12345678910111213141516171819202122# 排序节点配置OrdererOrgs: - Name: Orderer # 排序节点名称 Domain: myfab.com # 排序节点域名 Specs: - Hostname: orderer # 排序节点HOST PeerOrgs: - Name: Org1 # 节点名称 Domain: org1.myfab.com # 节点域名 EnableNodeOUs: true #如果设置了EnableNodeOUs，就在msp下生成config.yaml文件 Template: Count: 2 # 表示生成几个peer Users: Count: 3 # 表示生成几个user(admin 除外) - Name: Org2 Domain: org2.myfab.com EnableNodeOUs: true Template: Count: 2 Users: Count: 2 Template.Count 指定了所要生成的节点数量 Users.Count 指定了需要生成的初始化用户数量。 生成MSP文件系统1cryptogen generate --config=./crypto-config.yaml 将会生成一个名为crypto-config的目录，里面存储了order 和peer 的MSP文件系统； 使用该cryptogen工具为各种网络实体生成加密材料（x509证书和签名密钥）。这些证书代表身份，它们允许在我们的实体进行通信和交易时进行签名/验证身份验证。 Cryptogen使用文件 - crypto-config.yaml包含网络拓扑，并允许我们为组织和属于这些组织的组件生成一组证书和密钥。每个组织都配置了一个唯一的根证书（ca-cert），它将特定组件（同行和订购者）绑定到该组织。 MSP文件系统的目录结构如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223. ## Orderer 组织配置├── ordererOrganizations│ ││ └── example.com│ ││ ├── ca ## 存放组织根证书 及 私钥 (采用EC算法) 证书为【自签名】，组织内的实体将给予该根证书作为证书根│ ││ │ ├── 56d9c0c46acdda38a174a5ba3ffc44726a2c027e16bb22b460413acbcb9b3a90_sk│ │ ││ │ └── ca.example.com-cert.pem│ ││ ├── msp ## 存放该组织身份信息│ │ ││ │ ├── admincerts ## 组织管理员 身份验证证书，【被根证书签名】│ │ │ ││ │ │ └── Admin@example.com-cert.pem│ │ ││ │ ├── cacerts ## 组织的根证书 【和CA目录 里面一致】│ │ │ ││ │ │ └── ca.example.com-cert.pem│ │ │ │ │ └── tlscacerts ## 用于TLS的CA证书， 【自签名】│ │ │ │ │ └── tlsca.example.com-cert.pem │ ││ ││ ├── orderers ## 存放所有 Orderer 的身份信息 │ │ ││ │ └── orderer.example.com ## 第一个 Orderer 的信息 msp 及 tls│ │ ││ │ ├── msp│ │ │ │ │ │ │ ├── admincerts ## 组织管理员的身份验证证书。Peer将给予这些证书来确认交易签名是否为管理员签名 【和MSP.admincerts 一致】│ │ │ │ ││ │ │ │ └── Admin@example.com-cert.pem│ │ │ ││ │ │ ├── cacerts│ │ │ │ ││ │ │ │ └── ca.example.com-cert.pem ## 存放组织根证书，【和CA目录 里面一致】│ │ │ ││ │ │ ├── keystore ## 本节点的身份私钥，用来签名│ │ │ │ ││ │ │ │ └── 2ec1193fe048848eaa8e20666e26c527b791c4fb127d69cae65095bd31b6c80e_sk│ │ │ ││ │ │ ├── signcerts ## 验证本节点签名的证书，【被根证书签名】│ │ │ │ ││ │ │ │ └── orderer.example.com-cert.pem│ │ │ ││ │ │ └── tlscacerts ## TLS连接用的身份证书， 【和msp.tlscacerts 一致】│ │ │ │ │ │ │ └── tlsca.example.com-cert.pem│ │ │ │ │ └── tls ## tls 的相关信息│ │ ├── ca.crt ## 【组织的根证书】│ │ ├── server.crt ## 验证本节点签名的证书， 【被根证书签名】│ │ └── server.key ## 本节点的身份私钥，用来签名│ │ │ ├── tlsca ## 存放tls相关的证书和私钥│ │ │ │ │ ├── 2d66be83c519da67bb36b0972256a3b24357fa7f5b3a61f11405bc8b1f4d7c53_sk│ │ │ │ │ └── tlsca.example.com-cert.pem│ │ │ └── users ## 存放属于该组织的用户的实体│ │ │ └── Admin@example.com ## 管理员用户的信息，其中包括msp证书和tls证书两类│ │ │ ├── msp│ │ │ │ │ ├── admincerts ## 组织根证书作为管理员身份验证证书，【和MSP.admincerts 一致】│ │ │ │ │ │ │ └── Admin@example.com-cert.pem│ │ │ │ │ ├── cacerts ## 存放组织的根证书，【和CA目录 里面一致】│ │ │ │ │ │ │ └── ca.example.com-cert.pem│ │ │ │ │ ├── keystore ## 本用户的身份私钥，用来签名│ │ │ │ │ │ │ └── a3c1d7e1bc464faf2e3a205cb76ea231bd3ee7010655d3cd31dc6cb78726c4d0_sk│ │ │ │ │ ├── signcerts ## 管理员用户的身份验证证书，被组织根证书签名。要被某个Orderer认可，则必须放到该 Orderer 的msp/admincerts目录下│ │ │ │ │ │ │ └── Admin@example.com-cert.pem│ │ │ │ │ └── tlscacerts ## TLS连接用的身份证书，即组织TLS证书，【和msp.tlscacerts 一致】│ │ │ │ │ └── tlsca.example.com-cert.pem│ │ │ │ │ └── tls ## tls 的相关信息│ ├── ca.crt│ ├── client.crt ## 管理员的身份验证证书，【被 组织根证书签名】│ └── client.key ## 管理员的身份私钥，用来签名│ │ │ │ ## Peer 组织配置└── peerOrganizations │ ├── org1.example.com ## 第一个组织的所有身份证书 │ │ │ ├── ca ## 存放组织根证书及私钥 (采用EC算法) 证书为【自签名】，组织内的实体将给予该根证书作为证书根 │ │ │ │ │ ├── 496d6a41ae5f66bf120df3eab3a9d2dc4d268b2ab9a22af891d33d323bbdb5c8_sk │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ ├── msp ## 存放该组织身份信息 │ │ │ │ │ ├── admincerts ## 组织管理员 身份验证证书，【被根证书签名】 │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ ├── cacerts ## 组织的根证书 【和CA目录 里面一致】 │ │ │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ │ │ ├── config.yaml ## 记录 OrganizationalUnitIdentitifiers 信息，包括 根证书位置 和 ID信息 (主要是 crypto-config.yaml 的peer配置中配了 EnableNodeOUs: true ： 如果设置了EnableNodeOUs，就在msp下生成config.yaml文件) │ │ │ │ │ └── tlscacerts ## 用于TLS的CA证书， 【自签名】 │ │ │ │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ ├── peers ## 存放所有 Peer 的身份信息 │ │ │ │ │ ├── peer0.org1.example.com ## 第一个Peer的信息 msp 及 tls │ │ │ │ │ │ │ ├── msp │ │ │ │ │ │ │ │ │ ├── admincerts ## 组织管理员的身份验证证书。Peer将给予这些证书来确认交易签名是否为管理员签名 【和MSP.admincerts 一致】 │ │ │ │ │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ ├── cacerts ## 存放组织根证书，【和CA目录 里面一致】 │ │ │ │ │ │ │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ ├── config.yaml │ │ │ │ │ │ │ │ │ ├── keystore ## 本节点的身份私钥，用来签名 │ │ │ │ │ │ │ │ │ │ │ └── 0f0c2e1835086161f6a10c4bb38c2d89b2cee4e1128cee0fcda4433feb6eb6f8_sk │ │ │ │ │ │ │ │ │ │ │ │ │ │ ├── signcerts ## 验证本节点签名的证书，【被根证书签名】 │ │ │ │ │ │ │ │ │ │ │ └── peer0.org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ └── tlscacerts ## TLS连接用的身份证书， 【和msp.tlscacerts 一致】 │ │ │ │ │ │ │ │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ │ │ │ │ │ │ └──tls ## tls 的相关信息 │ │ │ ├── ca.crt ## 【组织的根证书】 │ │ │ ├── server.crt ## 验证本节点签名的证书， 【被根证书签名】 │ │ │ └── server.key ## 本节点的身份私钥，用来签名 │ │ │ │ │ │ │ │ └── peer1.org1.example.com │ │ │ │ │ │ │ │ │ ├── tlsca ## 存放tls相关的证书和私钥 │ │ │ │ │ ├── 3d39ea82dd5343c261b0480bc13d645a3cee13b7e7aa8c54fd2b5162f709671f_sk │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ │ │ └── users ## 存放属于该组织的用户的实体 │ │ │ ├── Admin@org1.example.com ## 管理员用户的信息，其中包括msp证书和tls证书两类 │ │ │ │ │ ├── msp │ │ │ │ │ │ │ ├── admincerts ## 组织根证书作为管理员身份验证证书，【和MSP.admincerts 一致】 │ │ │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ │ │ ├── cacerts ## 存放组织的根证书，【和CA目录 里面一致】 │ │ │ │ │ │ │ │ │ └── ca.org1.example.com-cert.pem │ │ │ │ │ │ │ ├── keystore ## 本用户的身份私钥，用来签名 │ │ │ │ │ │ │ │ │ └── 2b933c0740d857284be98ff218bf279261e55eff2b89d973e0a1f435f7c7d28b_sk │ │ │ │ │ │ │ ├── signcerts ## 管理员用户的身份验证证书，被组织根证书签名。要被某个Peer认可，则必须放到该Peer的msp/admincerts目录下 │ │ │ │ │ │ │ │ │ └── Admin@org1.example.com-cert.pem │ │ │ │ │ │ │ └── tlscacerts ## TLS连接用的身份证书，即组织TLS证书，【和msp.tlscacerts 一致】 │ │ │ │ │ │ │ └── tlsca.org1.example.com-cert.pem │ │ │ │ │ └── tls ## tls 的相关信息 │ │ ├── ca.crt │ │ ├── client.crt ## 管理员的身份验证证书，【被 组织根证书签名】 │ │ └── client.key ## 管理员的身份私钥，用来签名 │ │ │ │ │ └── User1@org1.example.com │ ├── msp │ │ ├── admincerts ## 组织根证书作为管理员身份验证证书 │ │ │ └── User1@org1.example.com-cert.pem │ │ ├── cacerts ## 存放组织的根证书，【和CA目录 里面一致】 │ │ │ └── ca.org1.example.com-cert.pem │ │ ├── keystore ## 【参考admin】 │ │ │ └── 11ebc5afac42348f84a8882f329d18beee079efd4fd5d9b30389dc82053fc0c9_sk │ │ ├── signcerts ## 【参考admin】 │ │ │ └── User1@org1.example.com-cert.pem │ │ └── tlscacerts ## 【参考admin】 │ │ └── tlsca.org1.example.com-cert.pem │ └── tls ## 【参考admin】 │ ├── ca.crt │ ├── client.crt │ └── client.key │ └── org2.example.com 生成Order创世区块在 Orderer 节点上维护的有个 system chain, 这个创世区块实际上是这个 system chain 的创世区块。在 Fabric 的上下文中，chain，channel 基本上可以通用，下面有时会称 system chain 为 system channel。 创建system chain的创世区块创建创世区块的配置文件 configtx.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159## 一些列组织的定义，【被其他 部分引用】#### 【注意】：本文件中 &amp;KEY 均为 *KEY 所引用； xx：&amp;KEY 均为 &lt;&lt;: *KEY 所引用##Organizations: ## 定义Orderer组织 【&amp;OrdererOrg 这类语法类似 Go的中的指针及对象地址， 此处是被Profiles 中的 - *OrdererOrg 所引用，以下均为类似做法】 - &amp;OrdererOrg Name: OrdererOrg ## Orderer的组织的名称 ID: OrdererMSP ## Orderer 组织的ID （ID是引用组织的关键） MSPDir: crypto-config/ordererOrganizations/example.com/msp ## Orderer的 MSP 证书目录路径 AdminPrincipal: Role.ADMIN ## 【可选项】 组织管理员所需要的身份，可选项: Role.ADMIN 和 Role.MEMBER ## 定义Peer组织 1 - &amp;Org1 Name: Org1MSP ## 组织名称 ID: Org1MSP ## 组织ID MSPDir: crypto-config/peerOrganizations/org1.example.com/msp ## Peer的MSP 证书目录路径 AnchorPeers: ## 定义组织锚节点 用于跨组织 Gossip 通信 - Host: peer0.org1.example.com ## 锚节点的主机名 Port: 7051 ## 锚节点的端口号 ## 定义Peer组织 2 - &amp;Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.example.com/msp AnchorPeers: - Host: peer0.org2.example.com Port: 7051 ################################################################################## SECTION: Capabilities 【注意】： 该部分是 V1.1.0 版本提出来的， 不可以在更早的版本中使用## - 本节定义了 fabric 网络的功能. 这是v1.1.0中的一个新概念，不应在v1.0.x的peer和orderers中使用. # Capabilities 定义了存在于结构二进制文件中的功能，以便该二进制文件安全地参与结构网络. # 例如，如果添加了新的MSP类型，则较新的二进制文件可能会识别并验证此类型的签名，而没有此支持的旧二进制文件将无法验证这些事务. # 这可能导致具有不同世界状态的不同版本的结构二进制文件。 相反，为通道定义功能会通知那些没有此功能的二进制文件，# 它们必须在升级之前停止处理事务. 对于v1.0.x，如果定义了任何功能（包括关闭所有功能的配置），那么v1.0.x的peer 会主动崩溃.#################################################################################Capabilities: ## 通道功能适用于orderers and the peers，并且必须得到两者的支持。 将功能的值设置为true. Global: &amp;ChannelCapabilities ## V1.1 的 Global是一个行为标记，已被确定为运行v1.0.x的所有orderers和peers的行为，但其修改会导致不兼容。 用户应将此标志设置为true. V1_1: true ## Orderer功能仅适用于orderers，可以安全地操纵，而无需担心升级peers。 将功能的值设置为true Orderer: &amp;OrdererCapabilities ## Orderer 的V1.1是行为的一个标记，已经确定为运行v1.0.x的所有orderers 都需要，但其修改会导致不兼容。 用户应将此标志设置为true V1_1: true ## 应用程序功能仅适用于Peer 网络，可以安全地操作，而无需担心升级或更新orderers。 将功能的值设置为true Application: &amp;ApplicationCapabilities ## V1.2 for Application是一个行为标记，已被确定为运行v1.0.x的所有peers所需的行为，但其修改会导致不兼容。 用户应将此标志设置为true V1_2: true ################################################################################## SECTION: Application## - 应用通道相关配置，主要包括 参与应用网络的可用组织信息#################################################################################Application: &amp;ApplicationDefaults ## 自定义被引用的地址 Organizations: ## 加入通道的组织信息 ################################################################################## SECTION: Orderer## - Orderer 系统通道相关配置，包括 Orderer 服务配置和参与Orderer 服务的可用组织# # Orderer 默认是 solo 的 且不包含任何组织 【主要被 Profiles 部分引用】################################################################################Orderer: &amp;OrdererDefaults ## 自定义被引用的地址 OrdererType: solo ## Orderer 类型，包含 solo 和 kafka 集群 Addresses: ## 服务地址 - orderer.example.com:7050 BatchTimeout: 2s ## 区块打包的最大超时时间 (到了该时间就打包区块) BatchSize: ## 区块打包的最大包含交易数 MaxMessageCount: 10 ## 一个区块里最大的交易数 AbsoluteMaxBytes: 99 MB ## 一个区块的最大字节数， 任何时候都不能超过 PreferredMaxBytes: 512 KB ## 一个区块的建议字节数，如果一个交易消息的大小超过了这个值, 就会被放入另外一个更大的区块中 MaxChannels: 0 ## 【可选项】 表示Orderer 允许的最大通道数， 默认 0 表示没有最大通道数 Kafka: Brokers: ## kafka的 brokens 服务地址 允许有多个 - 127.0.0.1:9092 Organizations: ## 参与维护 Orderer 的组织，默认为空 ################################################################################## Profile ## - 一系列通道配置模板，包括Orderer 系统通道模板 和 应用通道类型模板#################################################################################Profiles: ## Orderer的 系统通道模板 必须包括 Orderer、 Consortiums 两部分 TwoOrgsOrdererGenesis: ## Orderer 系统的通道及创世块配置。通道为默认配置，添加一个OrdererOrg 组织， 联盟为默认的 SampleConsortium 联盟，添加了两个组织 【该名称可以自定义 ？？】 Capabilities: &lt;&lt;: *ChannelCapabilities Orderer: ## 指定Orderer系统通道自身的配置信息 &lt;&lt;: *OrdererDefaults ## 引用 Orderer 部分的配置 &amp;OrdererDefaults Organizations: - *OrdererOrg ## 属于Orderer 的通道组织 该处引用了 【 &amp;OrdererOrg 】位置内容 Capabilities: &lt;&lt;: *OrdererCapabilities Consortiums: ## Orderer 所服务的联盟列表 SampleConsortium: ## 创建更多应用通道时的联盟 引用 TwoOrgsChannel 所示 Organizations: - *Org1 - *Org2 ## 应用通道模板 必须包括 Application、 Consortium 两部分 TwoOrgsChannel: ## 应用通道配置。默认配置的应用通道，添加了两个组织。联盟为SampleConsortium Consortium: SampleConsortium ## 通道所关联的联盟名称 Application: ## 指定属于某应用通道的信息，主要包括 属于通道的组织信息 &lt;&lt;: *ApplicationDefaults Organizations: ## 初始 加入应用通道的组织 - *Org1 - *Org2 Capabilities: &lt;&lt;: *ApplicationCapabilities 在这个文件里有两个 Profile， 一个是 OneOrgOrdererGenesis, 一个是 OneOrgChannel。 一个 Profile 代表了一组配置, 里面包含了通道相关配置，Orderer 节点相关配置，联盟成员相关配置。通道相关配置确定了系统通道的一些权限策略，Orderer 节点配置确定了 Orderer 节点的类型（是 solo 还是 kafaka），Orderer 节点的访问地址，还有出块时间，区块大小，区块内允许包含的交易数量等。联盟配置确定了联盟的名称和联盟所包含的组织，对组织来说这里最为关键的是组织的 MSP ID 和路径，这些信息都会被包含到 system chain 中。 生成创世区块1configtxgen -profile OneOrgOrdererGenesis -channelID order-channel -outputBlock ./channel-artifacts/genesis.block 这个命令需要指定一个 channelID，注意这里的 channelID 为系统链(system chain)的 channelID。 生成通道配置区块现在来创建创世区块交易，通过交易将创世块上传到channel上 生成channel配置事务通过下面的命令，可以生成这样的一个交易。 1export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile OneOrgChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID $CHANNEL_NAME 该命令会读取和生成 Orderer 相同的配置文件，只是使用了 “OneOrgChannel” 这个 Profile，确定了通道的权限策略，创建通道的联盟和组织信息。 只是创建了通道，而不创建锚节点的化，通道区块数据就无法跨组织传播，所以一般还要通过下面的命令创建用来更新锚节点的交易。 为每个组织生成锚节点配置事务1export CHANNEL_NAME=mychannel &amp;&amp; configtxgen -profile OneOrgChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org1MSP 上面的命令生成了为每个组织生成锚节点的交易。 启动网络接下来，就可以通过orderer命令或 docker 容器来启动网络了，orderer 启动的时候会查找一个名为 orderer.yaml 的配置文件。这个配置文件不是必须的，在找不到这个配置文件时，orderer 命令会使用默认配置。我们也可以通过环境变量或命令行参数的方式去对每个配置项进行覆盖。 配置各节点启动参数配置 yaml_config/docker-compose.yml 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: '2'volumes: orderer.example.com: peer0.org1.example.com: peer1.org1.example.com: peer0.org2.example.com: peer1.org2.example.com:networks: byfn:services: ca0: image: hyperledger/fabric-ca:$IMAGE_TAG environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca-org1 - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/CA1_PRIVATE_KEY ports: - "7054:7054" command: sh -c 'fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org1.example.com-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/CA1_PRIVATE_KEY -b admin:adminpw -d' volumes: - ./crypto-config/peerOrganizations/org1.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca_peerOrg1 networks: - byfn ca1: image: hyperledger/fabric-ca:$IMAGE_TAG environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca-org2 - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/CA2_PRIVATE_KEY ports: - "8054:7054" command: sh -c 'fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org2.example.com-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/CA2_PRIVATE_KEY -b admin:adminpw -d' volumes: - ./crypto-config/peerOrganizations/org2.example.com/ca/:/etc/hyperledger/fabric-ca-server-config container_name: ca_peerOrg2 networks: - byfn orderer.example.com: extends: file: base/docker-compose-base.yaml service: orderer.example.com container_name: orderer.example.com networks: - byfn peer0.org1.example.com: container_name: peer0.org1.example.com extends: file: base/docker-compose-base.yaml service: peer0.org1.example.com networks: - byfn peer1.org1.example.com: container_name: peer1.org1.example.com extends: file: base/docker-compose-base.yaml service: peer1.org1.example.com networks: - byfn peer0.org2.example.com: container_name: peer0.org2.example.com extends: file: base/docker-compose-base.yaml service: peer0.org2.example.com networks: - byfn peer1.org2.example.com: container_name: peer1.org2.example.com extends: file: base/docker-compose-base.yaml service: peer1.org2.example.com networks: - byfn 启动网络节点1docker-compose -f docker-compose.yml up -d ca.example.com orderer.example.com peer0.org1.example.com couchdb 注意: 如果启动ca， 之一要修改yaml文件中对应的ca私钥地址 运行Channel(示例为一个组织)这个阶段把网络启动后，还是啥事都做不了，节点之间也没有通信。 因为还没有生成peer节点互相通信的channel，也没有将节点加入到channel中 创建channel创建了一个channel的创世块，逻辑上就等于是创建了一个channel 1docker exec -e "CORE_PEER_LOCALMSPID=Org1MSP" -e "CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp" peer0.org1.example.com peer channel create -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/channel.tx 该命令是，通过Org1向order节点发送事务，order节点验证通过后，会生成mychannel的创世块 将组织加入到channel将组织加入到channel 1docker exec -e "CORE_PEER_LOCALMSPID=Org1MSP" -e "CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp" peer0.org1.example.com peer channel join -b mychannel.block 更新锚节点1docker exec -e "CORE_PEER_LOCALMSPID=Org1MSP" -e "CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp" peer0.org1.example.com peer channel update -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/Org1MSPanchors.tx 目前为止一个基本的网络就已经搭建好了。下面再介绍一些怎么更新我们的网络组织和channel成员 chain code 安装和使用chaincode 安装 将即将安装节点配置到环境变量中 1234CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspCORE_PEER_ADDRESS=peer0.org2.example.com:7051CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt 安装chaincode 1peer chaincode install -n mycc -v 1.0 -p github.com/chaincode/chaincode_example02/go/ 将chaincode实例化 一个channel只需要实例化一次 1peer chaincode instantiate -o orderer.example.com:7050 --tls --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C $CHANNEL_NAME -n mycc -v 1.0 -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;, &quot;100&quot;, &quot;b&quot;,&quot;200&quot;]&#125;&apos; -P &quot;AND (&apos;Org1MSP.peer&apos;,&apos;Org2MSP.peer&apos;)&quot; chaincode使用 查询 1peer chaincode query -C $CHANNEL_NAME -n mycc -c &apos;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]&#125;&apos; 调用 1peer chaincode invoke -o orderer.example.com:7050 --tls true --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C $CHANNEL_NAME -n mycc --peerAddresses peer0.org1.example.com:7051 --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt --peerAddresses peer0.org2.example.com:7051 --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt -c &apos;&#123;&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]&#125;&apos; 创建新组织并添加到channel生成组织的证书和密钥创建一个目录: org3-artifacts, 用来存储生成org3最终配置和事务的配置文件和中间产物等； 该阶段操作都在org3-artifacts目录下进行 加密资料准备 生成组织的MSP文件 创建一个新org的org3-crypto.yaml文件用来生成新组织的MSP资料 org3-crypto.yaml文件内容 1234567891011PeerOrgs: # --------------------------------------------------------------------------- # Org3 # --------------------------------------------------------------------------- - Name: Org3 Domain: org3.example.com EnableNodeOUs: true Template: Count: 2 Users: Count: 1 1cryptogen generate --config=./org3-crypto.yaml 导出MSP数据 configtx.yaml文件内容 1234567891011121314151617Organizations: - &amp;Org3 # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: Org3MSP # ID to load the MSP definition as ID: Org3MSP MSPDir: crypto-config/peerOrganizations/org3.example.com/msp AnchorPeers: # AnchorPeers defines the location of peers which can be used # for cross org gossip communication. Note, this value is only # encoded in the genesis block in the Application section context - Host: peer0.org3.example.com Port: 11051 1export FABRIC_CFG_PATH=$PWD &amp;&amp; configtxgen -printOrg Org3MSP &gt; ../artifacts/org3.json 上面的命令创建一个JSON文件 - org3.json并将其输出到artifacts。此文件包含Org3的策略定义，以及以base 64格式呈现的三个重要证书：管理员用户证书（稍后将充当Org3的管理员），CA根证书和TLS根目录证书 移动order MSP数据到当前目录下 将Orderer Org的MSP材料移植到Org3 crypto-config目录中。特别是，我们关注的是Orderer的TLS根证书，它将允许Org3实体与网络订购节点之间的安全通信。 1cd ../ &amp;&amp; cp -r crypto-config/ordererOrganizations org3-artifacts/crypto-config/ 生成配置文件 进入cli应用 1docker exec -it cli bash 设置环境变量 接下来急需要使用order管理员的身份获取mychannel的区块文件 12export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pemexport CHANNEL_NAME=mychannel 获取channel的配置信息 1peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA 将配置转化为Json 1configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &gt; config.json 这给我们留下了一个精简的JSON对象 - config.json 它将作为我们的配置更新的基线。 获得添加组织后的配置文件 将组织的配置json文件添加到channel的应用的分组字段，并输出到modified_config.json 1jq -s &apos;.[0] * &#123;&quot;channel_group&quot;:&#123;&quot;groups&quot;:&#123;&quot;Application&quot;:&#123;&quot;groups&quot;: &#123;&quot;Org3MSP&quot;:.[1]&#125;&#125;&#125;&#125;&#125;&apos; config.json ./channel-artifacts/org3.json &gt; modified_config.json 现在，在CLI容器中，我们有两个感兴趣的JSON文件 - config.json和modified_config.json。初始文件仅包含Org1和Org2材料，而“modified”文件包含所有三个Orgs。此时，只需重新编码这两个JSON文件并计算增量即可。 将config.json翻译回到config.pb 1configtxlator proto_encode --input config.json --type common.Config --output config.pb 将modified_config.json 编码成modified_config.pb: 1configtxlator proto_encode --input modified_config.json --type common.Config --output modified_config.pb 计算两个pb文件的增量 因为之前的组织MSP材料已经存在于通道的区块中，因此只需要计算和使用两个文件的增量 1configtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_config.pb --output org3_update.pb 将增量内容解码为json格式 1configtxlator proto_decode --input org3_update.pb --type common.ConfigUpdate | jq . &gt; org3_update.json 封装消息 1echo &apos;&#123;&quot;payload&quot;:&#123;&quot;header&quot;:&#123;&quot;channel_header&quot;:&#123;&quot;channel_id&quot;:&quot;mychannel&quot;, &quot;type&quot;:2&#125;&#125;,&quot;data&quot;:&#123;&quot;config_update&quot;:&apos;$(cat org3_update.json)&apos;&#125;&#125;&#125;&apos; | jq . &gt; org3_update_in_envelope.json 将最终的json文件转化为pb格式 1configtxlator proto_encode --input org3_update_in_envelope.json --type common.Envelope --output org3_update_in_envelope.pb 签名并提交配置更新 对更新做签名 在将配置写入分类帐之前，我们需要来自必需管理员用户的签名。 我们的渠道应用程序组的修改策略（mod_policy）设置为默认值“MAJORITY”，这意味着我们需要大多数现有组织管理员对其进行签名。 首先，让我们将此更新原型作为Org1管理员签名。请记住，CLI容器是使用Org1 MSP材质引导的，因此我们只需要发出 命令：peer channel signconfigtx 1peer channel signconfigtx -f org3_update_in_envelope.pb 最后一步是切换CLI容器的标识以反映Org2 Admin用户。我们通过导出特定于Org2 MSP的四个环境变量来实现此目的。 配置Org2环境变量： 123456789# you can issue all of these commands at onceexport CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspexport CORE_PEER_ADDRESS=peer0.org2.example.com:7051 签署 1peer channel signconfigtx -f org3_update_in_envelope.pb 发送更新请求 1peer channel update -f org3_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA 如果您的更新已成功提交，您应该会看到类似于以下内容的消息摘要指示： 12018-02-24 18:56:33.499 UTC [msp/identity] Sign -&gt; DEBU 00f Sign: digest: 3207B24E40DE2FAB87A2E42BC004FEAA1E6FDCA42977CB78C64F05A88E556ABA 您还将看到我们的配置事务的提交： 12018-02-24 18:56:33.499 UTC [channelCmd] update -&gt; INFO 010 Successfully submitted channel update 成功的频道更新呼叫向频道上的所有对等体返回新块 - 块5。如果您还记得，块0-2是初始通道配置，而块3和4是mycc链代码的实例 将新的节点组织加入到channel 配置领导者的选举 新的对等体不能利用gossip，因为它们无法验证其他对等体从其自己的组织转发的块，直到它们获得将该组织添加到该channel的配置事务。 因此，新添加的对等体必须具有以下配置之一，以便它们从订购服务接收块： 要使用静态领导模式，请将对等方配置为组织领导者： 12CORE_PEER_GOSSIP_USELEADERELECTION=falseCORE_PEER_GOSSIP_ORGLEADER=true 注意 对于添加到通道的所有新对等方，此配置必须相同。 要利用动态领导者选举，配置对等方使用领导者选举： 12CORE_PEER_GOSSIP_USELEADERELECTION=trueCORE_PEER_GOSSIP_ORGLEADER=false 注意 由于新添加的组织的对等方将无法形成成员资格视图，因此该选项将类似于静态配置，因为每个对等方将开始宣称自己是领导者。但是，一旦他们更新了将组织添加到渠道的配置事务，组织中将只有一个活跃的领导者。因此，如果您最终希望组织的同行使用领导者选举，建议使用此选项 将Org3 peer加入channel 打开org3 peer 此时，通道配置已更新为包含我们的新组织Org3- 意味着与其关联的对等方现在可以加入mychannel。 首先，让我们为Org3对等体和Org3特定的CLI启动容器。 打开一个新的终端并从first-network启动Org3 docker compose： compose文件内容: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192version: '2'volumes: peer0.org3.example.com: peer1.org3.example.com:networks: byfn:services: peer0.org3.example.com: container_name: peer0.org3.example.com extends: file: base/peer-base.yaml service: peer-base environment: - CORE_PEER_ID=peer0.org3.example.com - CORE_PEER_ADDRESS=peer0.org3.example.com:11051 - CORE_PEER_LISTENADDRESS=0.0.0.0:11051 - CORE_PEER_CHAINCODEADDRESS=peer0.org3.example.com:11052 - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:11052 - CORE_PEER_GOSSIP_BOOTSTRAP=peer1.org3.example.com:12051 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org3.example.com:11051 - CORE_PEER_LOCALMSPID=Org3MSP volumes: - /var/run/:/host/var/run/ - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/msp:/etc/hyperledger/fabric/msp - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls:/etc/hyperledger/fabric/tls - peer0.org3.example.com:/var/hyperledger/production ports: - 11051:11051 networks: - byfn peer1.org3.example.com: container_name: peer1.org3.example.com extends: file: base/peer-base.yaml service: peer-base environment: - CORE_PEER_ID=peer1.org3.example.com - CORE_PEER_ADDRESS=peer1.org3.example.com:12051 - CORE_PEER_LISTENADDRESS=0.0.0.0:12051 - CORE_PEER_CHAINCODEADDRESS=peer1.org3.example.com:12052 - CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:12052 - CORE_PEER_GOSSIP_BOOTSTRAP=peer0.org3.example.com:11051 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org3.example.com:12051 - CORE_PEER_LOCALMSPID=Org3MSP volumes: - /var/run/:/host/var/run/ - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/msp:/etc/hyperledger/fabric/msp - ./org3-artifacts/crypto-config/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls:/etc/hyperledger/fabric/tls - peer1.org3.example.com:/var/hyperledger/production ports: - 12051:12051 networks: - byfn Org3cli: container_name: Org3cli image: hyperledger/fabric-tools:$IMAGE_TAG tty: true stdin_open: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - FABRIC_LOGGING_SPEC=INFO #- FABRIC_LOGGING_SPEC=DEBUG - CORE_PEER_ID=Org3cli - CORE_PEER_ADDRESS=peer0.org3.example.com:11051 - CORE_PEER_LOCALMSPID=Org3MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer0.org3.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/bash volumes: - /var/run/:/host/var/run/ - ./../chaincode/:/opt/gopath/src/github.com/chaincode - ./org3-artifacts/crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./crypto-config/peerOrganizations/org1.example.com:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com - ./crypto-config/peerOrganizations/org2.example.com:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com - ./scripts:/opt/gopath/src/github.com/hyperledger/fabric/peer/scripts/ depends_on: - peer0.org3.example.com - peer1.org3.example.com networks: - byfn 1docker-compose -f docker-compose-org3.yaml up -d 此新组合文件已配置为跨越我们的初始网络，因此两个对等方和CLI容器将能够使用现有对等方和订购节点进行解析。现在运行三个新容器，执行特定于Org3的CLI容器： 1docker exec -it Org3cli bash 就像我们使用初始CLI容器一样，导出两个关键环境变量： 设置环境变量 就像我们使用初始CLI容器一样，导出两个关键环境变量：ORDERER_CA和CHANNEL_NAME： 1export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem &amp;&amp; export CHANNEL_NAME=mychannel 检查以确保已正确设置变量： 1echo $ORDERER_CA &amp;&amp; echo $CHANNEL_NAME 将节点加入到channel 1peer channel join -b mychannel.block 将第二个节点加入到channel 12export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org3.example.com/peers/peer1.org3.example.com/tls/ca.crtexport CORE_PEER_ADDRESS=peer1.org3.example.com:7051peer channel join -b mychannel.block 升级chaincode并更改认可策略更新chaincode的认可策略，添加org3到认可策略中 在org3 cli安装chaincode: 1peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/ 在org1 和 org2 的 cli上安装新版本chaincode peer0.org2上安装 1peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/ 跳到peer0.org1 123456789export CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/mspexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/ 更新认可策略 1peer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;90&quot;,&quot;b&quot;,&quot;210&quot;]&#125;&apos; -P &quot;OR (&apos;Org1MSP.peer&apos;,&apos;Org2MSP.peer&apos;,&apos;Org3MSP.peer&apos;)&quot;]]></content>
      <categories>
        <category>区块链</category>
        <category>Hyperledger fabric</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>Fabric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-静态分析破解Apk]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E9%9D%99%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、前言从这篇文章开始我们开始我们的破解之路，之前的几篇文章中我们是如何讲解怎么加固我们的Apk，防止被别人破解，那么现在我们要开始破解我们的Apk,针对于之前的加密方式采用相对应的破解技术，Android中的破解其实大体上可以分为静态分析和动态分析，对于这两种方式又可以细分为Java层(smail和dex)和native层(so)。所以我们今天主要来讲解如何通过静态分析来破解我们的apk，这篇文章我们会通过破解Java层和native层的例子来讲解。 二、准备工作第一、首先是基本知识： 1、了解Android中的Apk文件的结构。 2、了解Smail语法和dex文件格式 3、apk的签名机制 关于这三个知识点，这里就不做详细介绍了，不理解的同学可以自行网上学习，有很多资料讲解的。 第二、再者就是几个重要的工具 1、apktool：反编译的利器 2、dex2jar：将dex转化成jar 3、jd-gui：很好的查看jar文件的工具 4、IDA：收费的最全破解利器(分析dex和so都可以) 额外：上面四个工具是最基本的，但是现在网上也有一些更好的工具：JEB,GDA等。但是这些工具就是丰富了上面四个工具，所以说我们只要上面的四个工具就足够了。IDA工具我专门给了一个下载地址，其他的工具在我们提供的案例中。 三、技术原理准备工作完了，下面就来看一下今天的破解方式介绍： Android中的破解的静态分析说重要，也不重要，为什么这么说呢？ 因为我们到后面会介绍动态分析，那时候我们在破解一个Apk的时候，发现静态分析的方式几乎毫无用途，因为现在的程序加固的越来越高级，静态分析几乎失效，所以动态分析是必须的，但是要是说静态分析没有用，那么也错了，因为我们在有些场景下，只有静态分析能够开始破解之门，没有静态分析之后的结果，动态分析是无法开展的。这个下面会举例说明。所以说在破解的过程中，静态分析和动态分析一定会结合在一起的，只有这样我们才会勇往直前。下面就来看看我们如何通过静态分析来破解apk. 1. 静态分析的流程1、使用apktool来反编译apk 在这个过程中，我们会发现有些apk很轻易的被反编译了，但是有些apk每次反编译都会报各种错误，这个也是正常的，因为加固了吗。现在网上有很多对apk加密的方式，直接让反编译就通不过，比如Androidmanifest文件，dex文件等，因为apktool他需要解析这些重要的资源，一旦这些文件加密了那么就会终止，所以这里我们暂且都认为apk都能反编译的，因为我们今天是主要介绍怎么通过静态分析来破解，关于这里的反编译失败的问题，我后面会在用一篇文章详细介绍，到时候会列举一些反编译错误的例子。 2、得到程序的smail源码和AndroidManifest.xml文件 我们知道一个Android的程序入口信息都会在AndroidManifest.xml中，比如Application和入口Activity,所以我们肯定会先来分析这个文件，找到我们想要的信息，当然这里还有一个常用的命令需要记住： 1adb shell dumpsys activity top 能够获取到当前程序的Activity信息 之后我们会分析smail代码，进行代码逻辑的修改 3、直接解压apk文件得到classes.dex文件，然后用dex2jar工具得到jar,用jd-gui工具查看 这里我们主要很容易的查看代码，因为我们在第二步中得到了smail源码，就可以分析程序了，但是我们知道虽然smail语法不是很复杂，至少比汇编简单，但是怎么看着都是不方便的，还是看java代码比较方便，所以我们借助jd-gui工具查看代码逻辑，然后在smail代码中进行修改即可，上面说到的JEB工具，就加强了jd-gui工具的功能，它可以直接将smail源码翻译成java代码，这样我们就不需要先用jd-gui工具查看，再去smail源码中修改了，借助JEB即可。 4、分析native层代码 如果程序中有涉及到native层的话，我们可以用IDA打开指定的so文件。我们还是需要先看java代码，找到指定的so文件，在用IDA来静态分析so文件。 2、用到的技术上面介绍了静态分析的流程，下面来看一下静态分析的几个技术，我们在静态分析破解Apk的时候，首先需要找到突破点，找到关键的类和方法，当然这里就需要经验了，不是有方法可循的。但是我们会借助一些技术来加快破解。 1、全局查找关键字符串和日志信息 这个技术完全靠眼，我们在运行程序之后，会看到程序中出现的字符串，比如文本框，按钮上的文本，toast显示的信息等，都可能是重要信息，然后我们可以在jd-gui工具中全局搜索这个字符串，这样就会很快的定位到我们想要找的逻辑地方： 当然我们还有一个重要点就是Android中的Log信息，因为在一个大的项目中，会有多人开发，所以每个模块每个人开发，每个人都会调试信息，所以就会添加一些log信息，但是不是所有的人都会记得在项目发布的时候关闭项目中的所有log信息，这个也是我们在项目开发的过程中不好的习惯。这时候我们就可以通过程序运行起来之后，会打印一些log信息，那么我们可以通过这些信息获取突破点，Android中的log可以根据一个应用来进行过滤的，或者我们可以通过log信息中的字符串在jd-gui中进行全局搜索也是可以的。 2、代码的注入技术 在第一种方式中我们通过全局搜索一些关键的字符串来找突破点，但是这招有时候不好使，所以这时候我们需要加一些代码了来观察信息了，这里有一个通用的方法就是加入我们自己的log代码，来追踪代码的执行逻辑，因为这里讲的是静态分析技术，所以就用代码注入技术来跟踪执行逻辑，后面介绍了动态分析技术之后，那就简单了，我们可以随意的打断点来进行调试。这里的添加代码，就是修改smail代码，添加我们的日志信息即可，在下面我们会用例子来进行讲解，这个也是我们最常用的一种技术。 3、使用系统的Hook技术，注入破解程序进程，获取关键方法的执行逻辑 关于Android中的进程注入和Hook技术，这里就不做详细介绍了，技术介绍： 注入技术：http://blog.csdn.net/jiangwei0910410003/article/details/39292117 Hook技术：http://blog.csdn.net/jiangwei0910410003/article/details/41941393 Xposed使用：https://blog.csdn.net/xingkong_hdc/article/details/82531505 Xposed使用：https://www.jianshu.com/p/2d5f8e98d9f6 这两篇文章介绍了这两项技术，但是我们在实际操作过程中不用这两篇文章中用到的方式，因为这两篇文章只是介绍原理，技术还不是很成熟，关于这两个技术，网上有两个框架很成熟，也很实用，就是人们熟知的：Cydia和Xposed,关于这两个框架的话，网上的资料太多了，而且用起来也很容易，这里就不做太多的详细介绍了。 我们在实际的破解的过程中，这种方式用的有点少，因为这种方式效率有点低，所以只有在特定的场景下会使用。 4、使用IDA来静态分析so文件 这里终于用到了IDA工具了，本人是感觉这个工具太强大了，他可以查看so中的代码逻辑，我们看到的的可能是汇编指令，所以这里就有一个问题了，破解so的时候，我们还必须掌握一项技能，就是能看懂汇编指令，不然用IDA来破解程序，会很费经的，关于汇编指令，大学的时候，我们接触过了，但是我们当时感觉这东西又难，而且用的地方也很少，所以就没太在意，其实不然呀，真正懂汇编的人才是好的程序员： 看到些汇编指令，头立马就大了，不过这个用多了，破解多了，还是可以的。我们可以看到左边栏中有我们的函数，我们可以找到指定函数的定义的地方进行查看即可。其实IDA最强大的地方是在于他动态调试so文件，下一篇文章会介绍怎么动态调试so文件。当然IDA可也是可以直接查看apk文件的： 可以查看apk文件中的所有文件，我们可以选择classes.dex文件： 但是这里我们可能会遇到一个问题，就是如果应用程序太大的话，这个打开的过程中会很慢的，有可能IDA停止工作，所以要慢慢等啦： 打开之后，我们可以看到我们的类和方法名，这里还可以支持搜索类名和方法名Ctrl+F，也可以查看字符串内容(Shirt+F12): 我们发现IDA也是一个分析Java代码的好手，所以说这个工具太强大了啦啦~~ 四、案例分析上面讲解了静态分析的破解技术，那么下面就开始使用一个例子来看看静态分析的技术。 1. 反编译apk，获取smail文件首先我们拿到我们需要破解的Apk,使用apktool.jar工具来反编译： 1java -jar apktool.jar d xxx.apk 这个apk很是容易就被反编译了，看来并没有进行任何的加固。那就好办了，我们这里来改一下他的AndroidManifest.xml中的信息，改成可调式模式，这个是我们后面进行动态调试的前提，一个正式的apk，在AndroidManifest.xml中这个值是false的。 我们看看他的AndroidManifest.xml文件： 我们把这个值改成true.在回编译，这时候我们就可以动态调试这个apk了，所以在这点上我们可以看到，静态分析是动态分析的前提，这个值不修改的话，我们是办法进行后续的动态调试的。 2. 重新打包apk， 并安装修改成功之后，我们进行回编译： 首先进入到apktools工作目录， 进行反编译： 12345java -jar apktool.jar b -d sq -o debug.apk# sq是之前反编译的目录，debug.apk是回编译之后的文件# 这时候，debug.apk是不能安装运行的，因为没有签名，Android中是不允许安装一个没有签名的apk 下面还要继续签名，我们用系统自带的签名文件即可签名： 12java -jar .\sign\signapk.jar .\sign\testkey.x509.pem .\sign\testkey.pk8 debug.apk debug.sig.apk# signapk.jar， testkey.x509.pem, testkey.pk8 需要提前下载 注：其实我们在用IDE工具开发android项目的时候，工具就是用这个签名文件进行签名的，只是这个过程IDE帮我们做了。 后面就是直接安装这个apk,然后运行这个Apk。这个过程中我们只需要知道应用的包名和入口Activity名称即可，这个信息我们在AndroidManifest.xml中也是可以获取到的 当然我们用：adb shell dumpsys activity top 命令也可以得到： 3. 通过jd-gui查看代码，代码注入， Hook使用， 推导代码逻辑回编译之后，我们运行程序，发现有问题，就是点击程序的icon，没反应，运行不起来，我们在查看log中的异常信息，发现也没有抛出任何异常，那么这时候，我们就判断，他内部肯定做了什么校验工作，这个一般回编译之后的程序运行不起来的话，那就是内部做校验了，一般做校验的话，有两种： 1、对dex做校验，防止修改dex的 2、对apk的签名做校验，防止重新打包 那我们就需要从新看看他的代码，来看看是不是做了校验： 我们在分析代码的时候，肯定先看看他有没有自己定义Application,如果有定义的话，就需要看他自己的Application类，这里我们看到他定了自己的Application：com.shuqi.application.ShuqiApplication 我们解压apk,得到dex,然后dex2jar进行转化，得到jar，再用jd-gui查看这个类： 这里我们看到他的代码做混淆了，但是一些系统回调方法肯定不能混淆的，比如onCreate方法，但是这里我们一般找的方法是： 1、首先看这个类有没有静态方法和静态代码块，因为这类的代码会在对象初始化之前运行，可能在这里加载so文件，或者是加密校验等操作 2、再看看这个类的构造方法 3、最后再看生命周期方法 我们这里看到他的核心代码在onCreate中，调用了很多类的方法，猜想这里的某个方法做工作了？ 下面来看看我们怎么添加我们的日志信息，其实很简单，就是添加日志，需要修改smail文件，我们在去查看smail源码： 关于smail语法，本人认为不是很难，所以大家自己网上去搜一些资料学习一下即可，这里我们可以很清晰的看到调用了这些方法，那么我们就在每个方法加上我们的日志信息，这里加日志有两种方式，一种就是直接在这里调用系统的log方法，但是有两个问题： 1、需要导入包，在smail中修改 2、需要定义一个两个参数，一个是tag,msg,才能正常的打印log出来 明显这个方法有点麻烦，这里我们就自己定义一个MyLog类，然后反编译，得到MyLog的smail文件，添加到这个ShuqiApplication.smail的root目录下，然后在代码中直接调用即可，至于为何要放到root目录下，这样在代码中调用就不需要导入包了，比如SuqiApplication.smail中的一些静态方法调用： 编写日志类MyLog，这里就不粘贴代码了，我们新建一个项目之后，反编译得到MyLog.smail文件，放到目录中： 我们得到这个文件的时候，一定要注意，把MyLog.smail的包名信息删除，因为我们放到root目录下的，意味着这个MyLog类是没有任何包名的，这个需要注意，不然最后加的话，也是报错的。 我们在ShuqiApplication的onCreate方法中插入我们的日志方法： invoke-static {}, LMyLog;-&gt;print()V 但是我们在加代码的时候，需要注意的是，要找对地方加，所谓找对地方，就是在上个方法调用完之后添加，比如： invoke-virtual,invoke-static等，而且这些指令后面不能有：move-result-object，因为这个指令是获取方法的返回值，所以我们一般是这么加代码的： 1、在invoke-static/invoke-virtual指令他的返回类型是V之后可以加入 2、在invoke-static/invoke-virtual指令返回类型不是V,之后的move-result-object命令之后可以加入 加好了我们的日志代码之后，下面我们就回编译执行，在这个过程可能会遇到samil语法错误，这个就对应指定的文件修改就可以了，我们得到回编译的apk之后，可以在反编译一下，看看他的java代码： 我们看到了，我们添加的代码，在每个方法之后打印信息。 下面我们运行程序，同时开启我们的log的tag：adb logcat -s JW 看到我们打印的日志了，我们发现打印了三个log,这里需要注意的是，这里虽然打印了三个log,但是都是在不同的进程中，所以说一个进程中的log的话，只打印了一个，所以我们判断，问题出现在vr.h这个方法 我们查看这个方法源码： 果然，这个方法做了签名验证，不正确的话，直接退出程序。那么我们现在要想正常的运行程序的话，很简单了，直接注释这行代码：vr.h(this) 然后回编译，在运行，果然不报错了，这里就不在演示了： 好了，上面就通过注入代码，来跟踪问题，这个方法是很常用，也是很实在的。 4. 静态分析Native代码下面继续来介绍一下，如何使用IDA来静态分析native代码，这里一定要熟悉汇编指令，不然看起来很费劲的。 我们在反编译之后，看到他的onCreate方法中有一个加载so的代码 看看这个代码： 获取密码的方法，是native的，我们就来看看那个getDbPassword方法，用IDA打开libpsProcess.so文件： 我们看看这个函数的实现，我们一般直接看BL/BLX等信息，跳转逻辑，还有就是返回值，我们在函数的最后部分，发现一个重点，就是：BL __android_log_print 这个是在native层调用log的函数，我们在往上看，发现：tag是System.out.c 我们运行程序看起log看看，但是我们此时也可以在java层添加日志的：我们全局搜索这个方法，在yi这个类中调用的 我们修改yi.smail代码： 回编译，在运行程序，开启log： adb logcat -s JW adb logcat -s System.out.c 发现，返回的密码java层和native层是一样的。说明我们静态分析native还是有效的。 五、未解决的问题1、如何搞定apktool工具反编译出错的问题 这个我在开始的时候也说了，这里出错的原因大部分是apk进行加固了，所以后面我会专门介绍一下如何解决这样的问题 2、如何搞定让一个Apk可以调试 我们在上面看到一个apk想要能调试的话，需要修改android:debug的值，但是有时候，我们会遇到修改失败，导致程序不能运行，后面会专门介绍有几种方式来让一个发布后的apk可以调试 六、技术总结这篇文章我们介绍了如何使用静态方式去破解一个apk,我们在破解一个apk的时候，其实就是改点代码，然后能够运行起来，达到我们想要的功能，一般就是： 1、注释特定功能，比如广告展示等 2、得到方法的返回值，比如获取用户的密码 3、添加我们的代码，比如加入我们自己的监测代码和广告等 我们在静态分析代码的时候，需要遵循的大体路线： 1、首先能够反编译，得到AndroidManifest.xml文件，找到程序入口代码 2、找到我们想要的代码逻辑，一般会结合界面分析，比如我们想得让用户登录成功，我们肯定想要得到用户登录界面Activity,这时候我们可以用adb shell dumpsys activity top命令得到Activity名称，然后用Eclipse自带的程序当前视图分析工具：得到控件名称，或者是在代码中获取layout布局文件，一般是setContentView方法的调用地方，然后用布局文件结合代码得到用户登录的逻辑，进行修改 3、在关键的地方通过代码注入技术来跟踪代码执行逻辑 4、注意方法的返回值，条件判断等比较显眼的代码 5、对于有些apk中的源码，可能他有自己的加密算法，这时候我们需要获取到这个加密方法，如果加密方法比较复杂的话，我们就需要大批的测试数据来获取这个加密方法的逻辑，一般是输入和输出作为一个测试用例，比如阿里安全第一届比赛的第一题就可以用静态分析的方式破解，它内部就是一个加密算法，我们需要用测试数据来破解。 6、对于那些System.loadLibrary加载so文件的代码，我们只需要找到这个so文件，然后用IDA打开进行静态分析，因为有些apk中把加密算法放到了so中了，这时候我们也可以通过测试数据来获取加密算法。 7、通过上面的例子，我们可以总结一个方式，就是现在很多apk会做一些校验工作，一般在代码中包含：“signature”字符串信息，所以我们可以全局搜索一下，也许可以获取一些重要信息。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-动态分析破解Apk(Debug)]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E5%8A%A8%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk(Debug)%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、知识概要分析首先需要解释一下，这里为什么说是调试smali源码，不是Java源码，因为我们弄过反编译的人知道，使用apktool反编译apk之后，会有一个smali文件夹，这里就存放了apk对应的smali源码，关于smali源码这里不解释了，网上有介绍。 二、案例分析因为这一篇是一个教程篇，所以不能光说，那样会很枯燥的，所以这里用一个例子来介绍一下： 我们就用阿里2014年安全挑战赛的第一题：AliCrack_one.apk 看到这张图了，阿里还挺会制造氛围的，那么其实很简单，我们输入密码就可以破解了，下面我们就来看看如何获取这个密码。 第一步：使用apktool来破解apk1java -jar apktool_2.0.0rc4.jar d -d AliCraceme_1.apk -o out 这里的命令不做解释了。 但是有一个参数必须带上，那就是：-d 因为这个参数代表我们反编译得到的smali是java文件，这里说的文件是后缀名是java，如果不带这个参数的话，后缀名是smali的，但是Eclipse中是不会识别smali的，而是识别java文件的，所以这里一定要记得加上这个参数。 反编译成功之后，我们得到了一个out目录，如下： 源码都放在smali文件夹中，我们进入查看一下文件： 看到了，这里全是Java文件的，其实只是后缀名为java了，内容还是smali的： 第二步、修改AndroidManifest.xml中的debug属性和在入口代码中添加waitDebug上面我们反编译成功了，下面我们为了后续的调试工作，所以还是需要做两件事： 1》修改AndroidManifest.xml中的android:debuggable=”true” 关于这个属性，我们前面介绍run-as命令的时候，也提到了，他标识这个应用是否是debug版本，这个将会影响到这个应用是否可以被调试，所以这里必须设置成true。 2》在入口处添加waitForDebugger代码进行调试等待。 这里说的入口处，就是程序启动的地方，就是我们一般的入口Activity，查找这个Activity的话，方法太多了，比如我们这里直接从上面得到的AndroidManifest.xml中找到，因为入口Activity的action和category是固定的。 当然还有其他方式，比如aapt查看apk的内容方式，或者是安装apk之后用 adb dumpsys activity top 命令查看都是可以的。 找到入口Activity之后，我们直接在他的onCreate方法的第一行加上waitForDebugger代码即可，找到对应的MainActivity的smali源码： 然后添加一行代码： 1invoke-static &#123;&#125;, Landroid/os/Debug;-&gt;waitForDebugger()V 这个是smali语法的，其实对应的Java代码就是：android.os.Debug.waitForDebugger(); 这里把Java语言翻译成smali语法的，不难，网上有smali的语法解析，这里不想再解释了。 第三步：回编译apk并且进行签名安装1java -jar apktool_2.0.0rc4.jar b -d out -o debug.apk 还是使用apktool进行回编译 编译完成之后，将得到debug.apk文件，但是这个apk是没有签名的，所以是不能安装的，那么下面我们需要在进行签名，这里我们使用Android中的测试程序的签名文件和sign.jar工具进行签名： 1java -jar .\sign\signapk.jar .\sign\testkey.x509.pem .\sign\testkey.pk8 debug.apk debug.sig.apk 签名之后，我们就可以进行安装了。 第四步：在Eclipse中新建一个Java工程，导入smali源码这里我们新建一个Java工程，记住不是Android工程，因为我们最后调试其实是借助于Java的调试器，然后勾选掉Use default location选项，选择我们的smali源码目录，也就是我们上面反编译之后的out目录，点击完成 我们导入源码之后的项目工程结构： 主要看MainActivity类： 第五步：找到关键点，然后打断点这一步我们看到，其实说的比较广义了，这个要具体问题具体分析了，比如这个例子中，我们知道当我们输入密码之后，肯定要点击按钮，然后触发密码的校验过程，那么这里我们知道找到这个button的定义的地方，然后进入他的点击事件中就可以了。这里分为三步走： 1》使用Eclipse自带的View分析工具找到Button的ResId 点击之后，需要等待一会，分析View之后的结果： 看到了，这里我们能够看到整个当前的页面的全部布局，已经每个控件的属性值，我们需要找到button的resource-id 这里我们看到定义是@+id/button这个值。 2》我们得到这个resId之后，能否在smali工程中全局搜索这个值，就可以定位到这个button的定义的地方呢？ 然后我们看看搜到的结果： 这时候我们其实是在资源文件中搜到了这个id的定义，这个id值对应的是0x7F05003E。 当然除了这种方式，我们还有一种方式能快速找到这个id对应的整型值，那就是在反编译之后的values/public.xml文件中： 这个文件很有用的，他是真个apk中所有资源文件定义的映射内容，比如drawable/string/anim/attr/id 等这些资源文件定义的值，名字和整型值对应的地方： 这个文件很重要，是我们在寻找突破口的重要关键，比如我们有时候需要通过字符串内容来定位到关键点，这里就可以通过string的定义来找到对应的整型值即可。 当我们找到了button对应的id值了之后，我们就可以用这个id值在一次全局搜索一下，因为我们知道，Android中编译之后的apk，在代码中用到的resId都是用一个整型值代替的，这个整型值就是在R文件中做了定义，将资源的id和一个值对应起来，然后代码里面一般使用R.id.button这样的值，在编译出apk的时候，这个值就会被替换成对应的整型值，所以在全局搜索0x7F05003E 搜索的结果如下： 看到了，这里就定位到了代码中用到的这个button，我们进入代码看看： 在这里，看到了，使用了findViewById的方式定义Button,我们在往下面简单分析一下smali语法，下面是给button添加一个按钮事件，这里用的是内部类MainActivity$1，我们到这个类看看，他肯定实现了OnClickListener接口，那么直接搜onClick方法： 在这里我们就可以下个断点了，这里就是触发密码校验过程。 第六步：运行程序，设置远程调试工程在第五步中，我们找到了关键点，然后打上断点，下面我们就来运行程序，然后在Eclipse中设置远程调试的工程 首先我们运行程序，因为我们加入了waitForDebug的代码，所以启动的时候会出现一个Wait debug的对话框。不过，我测试的时候，我的手机没有出现这个对话框，而是一个白屏，不过这个不影响，程序运行起来之后，我们看看如何在Eclipse中设置远程调试工程，首先我们找到需要调试的程序对应远程调试服务端对应的端口： 这里我们看到有几个点： 1》在程序等待远程调试服务器的时候，前面会出现一个红色的小蜘蛛 2》在调试服务端这里我们会看到两个端口号：8600/8700，这里需要解释一下，为什么会有两个端口号呢？ 首先在这里的端口号，代表的是，远程调试服务器端的端口，下面在简单来看一下，Java中的调试系统： 这里我们看到，这里有三个角色： 111》JDB Client端(被调试的客户端)，这里我们可以认为我们需要破解的程序就是客户端，如果一个程序可以被调试，当启动的时候，会有一个jdwp线程用来和远程调试服务端进行通信 这里我们看到，我们需要破解的程序启动了JDWP线程，注意这个线程也只有当程序是debug模式下才有的，也就是AndroidManifest.xml中的debug属性值必须是true的时候，也就是一开始为什么我们要修改这个值的原因。 222》JDWP协议(用于传输调试信息的，比如调试的行号，当前的局部变量的信息等)，这个就可以说明，为什么我们在一开始的时候，反编译成java文件，因为为了Eclipse导入能够识别的Java文件，然后为什么能够调试呢？因为smali文件中有代码的行号和局部变量等信息，所以可以进行调试的。 333》JDB Server端(远程调试的服务端，一般是有JVM端)，就是开启一个JVM程序来监听调试端，这里就可以认为是本地的PC机，当然这里必须有端口用来监听，那么上面的8600端口就是这个作用，而且这里端口是从8600开始，后续的程序端口后都是依次加1的，比如其他调试程序： 那么有了8600端口，为什么还有一个8700端口呢？他是干什么的？ 其实他的作用就是远程调试端备用的基本端口，也就是说比如这里的破解程序，我们用8600端口可以连接调试，8700也是可以的，但是其他程序，比如demo.systemapi他的8607端口可以连接调试，8700也是可以的： 所以呀，可以把8700端口想象成大家都可以用于连接调试的一个端口，不过，在实际过程中，还是建议使用程序独有的端口号8600，我们可以查看8600和8700端口在远程调试端(本地pc机)的占用情况： 看到了，这里的8600端口和8700端口号都是对应的javaw程序，其实javaw程序就是启动一个JVM来进行监听的。 好了，到这里我们就弄清楚了，Java中的调试系统以及远程调试的端口号。 注意： 其实我们可以使用adb jdwp命令查看，当前设备中可以被调试的程序的进程号信息： 下面继续，我们知道了远程调试服务端的端口：8600，以及ip地址，这里就是本地ip：localhost/127.0.0.1 我们可以在Eclipse中新建一个远程调试项目，将我们的smali源码工程和设备中需要调试的程序关联起来： 右击被调试的项目=》选择Debug Configurations： 然后开始设置调试项目 选择Romote Java Application，在Project中选择被调试的smali项目，在Connection Type中选择SocketAttach方式，其实还有一种方式是Listener的，关于这两种方式其实很好理解： #Listner方式：是调试客户端启动就准备好一个端口，当调试服务端准备好了，就连接这个端口进行调试 #Attach方式：是调试服务端开始就启动一个端口，等待调试端来连接这个端口 我们一般都是选择Attach方式来进行操作的。 好了，我们设置完远程调试的工程之后，开始运行，擦发现，设备上的程序还是白屏，这是为什么呢？看看DDMS中调试程序的状态： 擦，关联到了这个进程，原因也很简单，我们是上面使用的是8700端口号，这时候我们选中了这个进程，所以就把smali调试工程关联到了这个进程，所以破解的进程没反应了，我们立马改一下，用8600端口： 好了，这下成功了，我们看到红色的小蜘蛛变成绿色的了，说明调试端已经连接上远程调试服务端了。 注意： 我们在设置远程调试项目的时候，一定要注意端口号的设置，不然没有将调试项目源码和调试程序关联起来，是没有任何效果的 第七步：开始运行调试程序，进入调试下面我们就开始操作了，在程序的文本框中输入：gggg内容，点击开始： 好了，到这里我们看到期待已久的调试界面出来了，到了我们开始的时候加的断点处，这时候我们就可以开始调试了，使用F6单步调试，F5单步跳入，F7单步跳出进行操作： 看到了，这里使用v3变量保存了我们输入的内容 这里有一个关键的地方，就是调用MainActivity的getTableFromPic方法，获取一个String字符串，从变量的值来看，貌似不是规则的字符串内容，这里先不用管了，继续往下走： 这里又遇到一个重要的方法：getPwdFromPic，从字面意义上看，应该是获取正确的密码，用于后面的密码字符串比对。 查看一下密码的内容，貌似也是一个不规则的字符串，但是我们可以看到和上面获取的table字符串内容格式很像，接着往下走： 这里还有一个信息就是，调用了系统的Log打印，log的tag就是v6保存的值：lil 这时候，我们看到v3是保存的我们输入的密码内容，这里使用utf-8获取他的字节数组，然后传递给access$0方法，我们使用F5进入这个方法： 在这个方法中，还有一个bytesToAliSmsCode方法，使用F5进入： 那么这个方法其实看上去还是很简单的，就是把传递进来的字节数组，循环遍历，取出字节值，然后转化成int类型，然后在调用上面获取到的table字符串的chatAt来获取指定的字符，使用StringBuilder进行拼接，然后返回即可。 按F7跳出，查看，我们返回来加密的内容是：日日日日，也就是说gggg=&gt;日日日日 最后再往下走，可以看到是进行代码比对的工作了。 那么上面我们就分析完了所有的代码逻辑，还不算复杂，我们来梳理一下流程： A&gt;调用MainActivity中的getTableFromPic方法，获取一个table字符串 我们可以进入看看这个方法的实现： 这里可以大体了解了，他是读取asset目录下的一个logo.png图片，然后获取图片的字节码，在进行操作，得到一个字符串，那么我们从上面的分析可以知道，其实这里的table字符串类似于一个密钥库。 B&gt;通过MainActivity中的getPwdFromPic方法，获取正确的密码内容 C&gt;获取我们输入内容的utf-8的字节码，然后调用access$0方法，获取加密之后的内容 D&gt;access$0方法中在调用bytesToAliSmsCode方法，获取加密之后的内容 这个方法是最核心的，我们通过分析知道，他的逻辑是，通过传递进来的字节数组，循环遍历数组，拿到字节转化成int类型，然后在调用密钥库字符串table的charAt得到字符，使用StringBuilder进行拼接。 通过上面的分析之后，我们知道获取加密之后的输入内容和正确的密码内容做比较，那么我们现在有的资源是：密钥库字符串和正确的加密之后的密码，以及加密的逻辑 那么我们的破解思路其实很简单了，相当于，我们知道了密钥库字符串，也知道了，加密之后的字符组成的字符串，那么可以通过遍历加密之后的字符串，循环遍历，获取字符，然后再去密钥库找到指定的index，然后在转成byte,保存到字节数组，然后用utf-8获取一个字符串，那么这个字符串就是我们要的密码。 下面我们就用代码来实现这个功能： 代码逻辑，很简单吧，其实这个函数相当于上面加密函数的bytesToAliSmsCode的反向实现，运行结果： OK，得到了正确的密码，下面来验证一下： 哈哈，不要太激动，成功啦啦~~。破解成功。 补充： 刚刚我们在断点调试的时候，看到了代码中用了Log来打印日志，tag是lil，那么我们可以打印这个log看看结果： 看到了，这里table是密钥库，pw是正确的加密之后的密码，enPassword是我们输入之后加密的密码。 所以从这里可以看到，这个例子，其实我们在破解apk的时候，有时候日志也是一个非常重要的信息。 三、思路整理1、我们通过apktool工具进行apk的反编译，得到smali源码和AndroidManifest.xml，然后修改AndroidManifest.xml中的debug属性为true，同时在入口处加上waitForDebug代码，进行debug等待，一般入口都是先找到入口Activity，然后在onCreate方法中的第一行这里需要注意的是：apktool工具一定要加上-d参数，这样反编译得到的文件是java文件，这样才能够被Eclipse识别，进行调试。 2、修改完成AndroidManifest.xml和添加waitForDebug之后，我们需要在使用apktool进行回编译，回编译之后得到的是一个没有签名的apk，我们还需要使用signapk.jar来进行签名，签名文件直接使用测试程序的签名文件就可以，最后在进行安装。 3、然后我们将反编译之后的smali源码导入到Eclipse工程中，找到关键点，进行下断点，这里的关键点，一般是我们先大致了解程序运行的结构，然后找到我们需要破解的地方，使用View分析工具，或者是使用jd-gui工具直接查看apk源码(使用dex2jar将dex文件转化成jar文件，然后用jd-gui进行查看)，找到代码的大体位置。然后下断点，这里我们可以借助Eclipse的DDMS自带的View分析工具找到对应控件的resid，然后在全局搜索这个控件的resid，或者直接在values/public.xml中查找，最终定位到这个控件位置，在查看他的点击事件即可。 4、设置远程调试工程，首先运行需要调试程序，然后在DDMS中找到对应的调试服务端的端口号，然后在Debug Configurations中设置远程调试项目，设置对应的调试端口和ip地址(一般都是本机pc,那就是localhost)，然后红色小蜘蛛变成绿色的，表示我们的远程调试项目连接关联上了调试程序，这里需要注意的是，一定需要关联正确，不然是没有任何效果的，关联成功之后，就可以进行操作。 5、操作的过程中，会进入到关键的断点处，通过F6单步，F5单步进入，F7单步跳出，来进行调试，找到关键方法，然后通过分析smali语法，了解逻辑，如果逻辑复杂的，可以通过查看具体的环境变量的值来观察，这里也是最重要的，也是最复杂的，同时这里也是没有规章可寻的，这个和每个人的逻辑思维以及破解能力有关系，分析关键的加密方法是需要功底的，当然这里还需要注意一个信息，就是Log日志，有时候也是很重要的一个信息。 6、最后一般当我们知道了核心方法的逻辑，要想得到正确的密码，还是需要自己用语言去实现逻辑的，比如本文中的加密方法，我们需要手动的code一下加密的逆向方法，才能得到正确的密码。 五、遗留问题1、使用apktool工具进行反编译有时候并不是那么顺利，比如像这样的报错： 这个一般都是apktool中解析出现了错误，其实这个都是现在apk为了抵抗apktool，做的apk加固策略，这个后面会写一篇文章如何应对这些加固策略，如何进行apk修复，其实原理就是分析apktool源码，找到指定的报错位置，进行apktool代码修复即可。 2、本文中说到了Java的调试系统，但是为了篇幅限制，没有详细的讲解了整个内容，后面会写一篇文章具体介绍Java中的调试系统以及Android的调试系统。 3、有时候我们还会遇到回编译成功了，然后遇到运行不起来的错误，这个就需要使用静态方式先去分析程序启动的逻辑，看看是不是程序做了什么运行限制，比如我们在静态分析那篇文章中，提到了应用为了防止反编译在回编译运行，在程序的入口处作了签名校验，如果校验失败，直接kill掉自己的进程，退出程序了，所以这时候我们还是需要使用静态方式去分析apk。 4、如何做到不修改AndroidManifest.xml中的debug属性就可以进行调试： 1》 修改boot.img,从而打开系统调试，这样就可以省去给app添加android:debuggable=”true”，再重打包的步骤了。2》直接修改系统属性，使用setpropex工具在已经root的设备上修改只读的系统属性。使用此工具来修改ro.secure和ro.debuggable的值。 这个也会在后面详细介绍这两种方法 四、总结这篇文章我们就介绍了如何使用Eclipse去动态调试反编译之后的smali源码，这种方式比静态方式高效很多的，比如本文中的这个例子，其实我们也可以使用静态方式进行破解的，但是肯定效率没有动态方式高效，所以以后我们又学会了一个技能，就是动态的调试smali源码来跟踪程序的核心点，但是现在市场上的大部分应用没有这么简单就破解了，比如核心的加密算法放到了native层去做，那么这时候就需要我们去动态调试so文件跟踪，这个是我们下一篇文章的内容，也有的时候，apk进行加固了，直接在apktool进行反编译就失败了，这时候我们就需要先进行apk修复，然后才能后续的操作，这个是我们下下篇的文章，如何应对apk的加固策略。通过这篇文章我们可以看到动态方式破解比静态方式高效的多，但是有时候我们还需要使用静态方式先做一些准备工作，所以在破解apk的时候，动静结合，才能做到完美的破解。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-动态分析破解Apk(破解加固)]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E5%8A%A8%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk(%E7%A0%B4%E8%A7%A3%E5%8A%A0%E5%9B%BA)%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、前言介绍一下如何应对现在市场中一些加固的apk的破解之道，现在市场中加固apk的方式一般就是两种：一种是对源apk整体做一个加固，放到指定位置，运行的时候在解密动态加载，还有一种是对so进行加固，在so加载内存的时候进行解密释放。我们今天主要看第一种加固方式，就是对apk整体进行加固。 二、案例分析按照国际惯例，咋们还是得用一个案例来分析讲解，这次依然采用的是阿里的CTF比赛的第三题： 题目是：要求输入一个网页的url，然后会跳转到这个页面，但是必须要求弹出指定内容的Toast提示，这个内容是：祥龙！ 了解到题目，我们就来简单分析一下，这里大致的逻辑应该是，输入的url会传递给一个WebView控件，进行展示网页，如果按照题目的逻辑的话，应该是网页中的Js会调用本地的一个Java方法，然后弹出相应的提示，那么这里我们就来开始操作了。 按照我们之前的破解步骤： 第一步：肯定是先用解压软件搞出来他的classes.dex文件，然后使用dex2jar+jd-gui进行查看java代码 擦，这里我们看到这里只有一个Application类，从这里我们可以看到，这个apk可能被加固了，为什么这么说呢？因为我们知道一个apk加固，外面肯定得套一个壳，这个壳必须是自定义的Application类，因为他需要做一些初始化操作，那么一般现在加固的apk的壳的Application类都喜欢叫StubApplication。而且，这里我们可以看到，除了一个Application类，没有其他任何类了，包括我们的如可Activity类都没有了，那么这时候会发现，很蛋疼，无处下手了。 第二步：我们会使用apktool工具进行apk的反编译，得到apk的AndroidManifest.xml和资源内容 反编译之后，看到程序会有一个入口的Activity就是MainActivity类，我们记住一点就是，不管最后的apk如何加固，即使我们看不到代码中的四大组件的定义，但是肯定会在AndroidManifest.xml中声明的，因为如果不声明的话，运行是会报错的。那么这里我们也分析完了该分析的内容，还是没发现我们的入口Activity类，而且我们知道他肯定是放在本地的一个地方，因为需要解密动态加载，所以不可能是放在网上的，肯定是本地，所以这里就有一些技巧了： 当我们发现apk中主要的类都没有了，肯定是apk被加固了，加固的源程序肯定是在本地，一般会有这么几个地方需要注意的： 1、应用程序的asset目录，我们知道这个目录是不参与apk的资源编译过程的，所以很多加固的应用喜欢把加密之后的源apk放到这里 2、把源apk加密放到壳的dex文件的尾部，这个肯定不是我们这里的案例，但是也有这样的加固方式，这种加固方式会发现使用dex2jar工具解析dex是失败的，我们这时候就知道了，肯定对dex做了手脚 3、把源apk加密放到so文件中，这个就比较难了，一般都是把源apk进行拆分，存到so文件中，分析难度会加大的。 一般都是这三个地方，其实我们知道记住一点：就是不管源apk被拆分，被加密了，被放到哪了，只要是在本地，我们都有办法得到他的。 好了，按照这上面的三个思路我们来分析一下，这个apk中加固的源apk放在哪了？通过刚刚的dex文件分析，发现第二种方式肯定不可能了，那么会放在asset目录中吗？我们查看asset目录： 看到asset目录中的确有两个jar文件，而且我们第一反应是使用jd-gui来查看jar，可惜的是打开失败，所以猜想这个jar是经过处理了，应该是加密，所以这里很有可能是存放源apk的地方。但是我们上面也说了还有第三种方式，我们去看看libs目录中的so文件： ** 擦，这里有三个so文件，而我们上面的Application中加载的只有一个so文件：libmobisec.so，那么其他的两个so文件很有可能是拆分的apk文件的藏身之处。 通过上面的分析之后，我们大致知道了两个地方很有可能是源apk的藏身地方，一个是asset目录，一个是libs目录，那么分析完了之后，我们发现现在面临两个问题： 第一个问题：asset目录中的jar文件被处理了，打不开，也不知道处理逻辑 第二个问题：libs目录中的三个so文件，唯一加载了libmobisec.so文件了 那么这里现在的唯一入口就是这个libmobisec.so文件了，因为上层的代码没有，没法分析，下面来看一下so文件： 擦，发现蛋疼的是，这里没有特殊的方法，比如Java_开头的什么，所以猜测这里应该是自己注册了native方法，混淆了native方法名称，那么到这里，我们会发现我们遇到的问题用现阶段的技术是没法解决了。 三、获取正确的dex内容分析完上面的破解流程之后，发现现在首要的任务是先得到源apk程序，通过分析知道，处理的源apk程序很难找到和分析，所以这里就要引出今天说的内容了，使用动态调试，给libdvm.so中的函数：dvmDexFileOpenPartial 下断点，然后得到dex文件在内存中的起始地址和大小，然后dump处dex数据即可。 那么这里就有几个问题了： 第一个问题：为何要给dvmDexFileOpenPartial 这个函数下断点？ 因为我们知道，不管之前的源程序如何加固，放到哪了，最终都是需要被加载到内存中，然后运行的，而且是没有加密的内容，那么我们只要找到这的dex的内存位置，把这部分数据搞出来就可以了，管他之前是如何加固的，我们并不关心。那么问题就变成了，如何获取加载到内存中的dex的地址和大小，这个就要用到这个函数了：dvmDexFileOpenPartial 因为这个函数是最终分析dex文件，加载到内存中的函数： int dvmDexFileOpenPartial(const void* addr, int len, DvmDex** ppDvmDex); 第一个参数就是dex内存起始地址，第二个参数就是dex大小。 第二个问题：如何使用IDA给这个函数下断点 我们在之前的一篇文章中说到了，在动态调试so，下断点的时候，必须知道一个函数在内存中的绝对地址，而函数的绝对地址是：这个函数在so文件中的相对地址+so文件映射到内存中的基地址，这里我们知道这个函数肯定是存在libdvm.so文件中的，因为一般涉及到dvm有关的函数功能都是存在这个so文件中的，那么我们可以从这个so文件中找到这个函数的相对地址，运行程序之后，在找到libdvm.so的基地址，相加即可，那么我们如何获取到这个libdvm.so文件呢？这个文件是存放在设备的/system/lib目录下的： 那么我们只需要使用adb pull 把这个so文件搞出来就可以了。 好了，解决了这两个问题，下面就开始操作了： 第一步：运行设备中的android_server命令，使用adb forward进行端口转发 这里的android_server工具可以去ida安装目录中dbgsrv文件夹中找到 第二步：使用命令以debug模式启动apk adb shell am start -D -n com.ali.tg.testapp/.MainActivity 因为我们需要给libdvm.so下断点，这个库是系统库，所以加载时间很早，所以我们需要像之前给JNI_OnLoad函数下断点一样，采用debugger模式运行程序，这里我们通过上面的AndroidManifest.xml中，得到应用的包名和入口Activity： 而且这里的android:debuggable=true，可以进行debug调试的。 第三步：双开IDA，一个用于静态分析libdvm.so，一个用于动态调试libdvm.so 通过IDA的Debugger菜单，进行进程附加操作： 第四步：使用jdb命令启动连接attach调试器 1jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 但是这里可能会出现这样的错误： 这个是因为，我们的8700端口没有指定，这时候我们可以通过Eclipse的DDMS进行端口的查看： 看到了，这里是8600端口，但是基本端口8700不在，所以这里我们有两种处理方式，一种是把上面的命令的端口改成8600，还有一种是选中这个应用，使其具有8700端口： 点击这个条目即可，这时候我们在运行上面的jdb命令： 处于等待状态。 第四步：给dvmDexFileOpenPartial函数下断点 使用一个IDA静态分析得到这个函数的相对地址：43308 在动态调试的IDA解密，使用Ctrl+S键找到libdvm.so的在内存中的基地址：41579000 然后将两者相加得到绝对地址：43308+41579000=415BC308，使用G键，跳转： 跳转到dvmDexFileOpenPartial函数处，下断点： 第五步：点击运行按钮或者F9运行程序 之前的jdb命令就连接上了： IDA出现如下界面，不要理会，一路点击取消按钮即可 运行到了dvmDexFileOpenPartial函数处： 使用F8进行单步调试，但是这里需要注意的是，只要运行过了PUSH命令就可以了，记得不要越过下面的BL命令，因为我们没必要走到那里，当执行了PUSH命令之后，我们就是使用脚本来dump处内存中的dex数据了，这里有一个知识点，就是R0~R4寄存器一般是用来存放一个函数的参数值的，那么我们知道dvmDexFileOpenPartial函数的第一个参数就是dex内存起始地址，第二个参数就是dex大小： 那么这里就可以使用这样的脚本进行dump即可： static main(void){ auto fp, dex_addr, end_addr; fp = fopen(“F:\dump.dex”, “wb”); end_addr = r0 + r1; for ( dex_addr = r0; dex_addr &lt; end_addr; dex_addr ++ ) fputc(Byte(dex_addr), fp);} 脚本不解释了，非常简单，而且这个是固定的格式，以后dump内存中的dex都是这段代码，我们将dump出来的dex保存到F盘中。 然后这时候，我们使用：Shirt+F2 调出IDA的脚本运行界面： 点击运行，这里可能需要等一会，运行成功之后，我们去F盘得到dump.dex文件，其实这里我们的IDA使命就完成了，因为我们得到了内存的dex文件了，下面开始就简单了，只要分析dex文件即可 四、分析正确的dex文件内容我们拿到dump.dex之后，使用dex2jar工具进行反编译： 可惜的是，报错了，反编译失败，主要是有一个类导致的，开始我以为是dump出来的dex文件有问题，最后我用baksmali工具得到smali文件是可以的，所以不是dump出来的问题，我最后用baksmali工具将dex转化成smali源码： j 1ava -jar baksmali-2.0.3.jar -o C:\classout/ dump.dex 得到的smali源码目录classout在C盘中： 我们得到了指定的smali源码了。 那么下面我们就可以使用静态方式分析smali即可了： 首先找到入口的MainActivity源码： 这里不解释了，肯定是找按钮的点击事件代码处，这里是一个btn_listener变量，看这个变量的定义： 是MainActivity$1内部类定义，查看这个类的smali源码，直接查看他的onClick方法： 这里可以看到，把EditText中的内容，用Intent传递给WebViewActivity中，但是这里的intent数据的key是加密的。 下面继续看WebViewActivity这个类： 我们直接查找onCreate方法即可，看到这里是初始化WebView，然后进行一些设置，这里我们看到一个@JavascriptInterface 这个注解，我们在使用WebView的时候都知道，他是用于Js中能够访问的设置了这个注解的方法，没有这个注解的方法Js是访问不了的 注意： 我们知道这个注解是在SDK17加上的，也就是Android4.2版本中，那么在之前的版本中没有这个注解，任何public的方法都可以在JS代码中访问，而Java对象继承关系会导致很多public的方法都可以在JS中访问，其中一个重要的方法就是 getClass()。然后JS可以通过反射来访问其他一些内容。那么这里就有这个问题了：比如下面的一段JS代码： function findobj(){ for (var obj in window) { if ("getClass" in window[obj]) { return window[obj] } } } 看到了，这段js代码很危险的，使用getClass方法，得到这个对象(java中的每个对象都有这个方法的)，用这个方法可以得到一个java对象，然后我们就可以调用这个对象中的方法了。这个也算是WebView的一个漏洞了。 所以通过引入 @JavascriptInterface注解，则在JS中只能访问 @JavascriptInterface注解的函数。这样就可以增强安全性。 回归到正题，我们上面分析了smali源码，看到了WebView的一些设置信息，我们可以继续往下面看： 这里的我们看到了一些重要的方法，一个是addJavascriptInterface，一个是loadUrl方法。 我们知道addjavaascriptInterface方法一般的用法： mWebView.addJavascriptInterface(new JavaScriptObject(this), “jiangwei”); 第一个参数是本地的Java对象，第二个参数是给Js中使用的对象的名称。然后js得到这个对象的名称就可以调用本地的Java对象中的方法了。 看了这里的addjavaascriptInterface方法代码，可以看到，这里用 ListViewAutoScrollHelpern;-&gt;decrypt_native(Ljava/lang/String;I)Ljava/lang/String; 将js中的名称进行混淆加密了，这个也是为了防止恶意的网站来拦截url，然后调用我们本地的Java中的方法。 注意： 这里又存在一个关于WebView的安全问题，就是这里的js访问的对象的名称问题，比如现在我的程序中有一个Js交互的类，类中有一个获取设备重要信息的方法，比如这里获取设备的imei方法，如果我们的程序没有做这样名称的混淆的话，破解者得到这个js名称和方法名，然后就伪造一个恶意url，来调用我们程序中的这个方法，比如这样一个例子： 然后在设置js名称： 我们就可以伪造一个恶意的url页面来访问这个方法，比如这个恶意的页面代码如下： 运行程序： 看到了，这里恶意的页面就成功的调用了程序中的一个重要方法。 所以，我们可以看到，对Js交互中的对象名称做混淆是必要的，特别是本地一些重要的方法。 回归到正题，我们分析完了WebView的一些初始化和设置代码，而且我们知道如果要被Js访问的方法，那么必须要有@JavascriptInterface注解 因为在Java中注解也是一个类，所以我们去注解类的源码看看那个被Js调用的方法： 这里看到了有一个showToast方法，展示的内容：\u7965\u9f99\uff01 ，我们在线转化一下： 擦，这里就是题目要求展示的内容。 好了，到这里我们就分析完了apk的逻辑了，下面我们来整理一下： 1、在MainActivity中输入一个页面的url，跳转到WebViewActivity进行展示 2、WebViewActivity有Js交互，需要调用本地Java对象中的showToast方法展示消息 问题： 因为这里的js对象名称进行了加密，所以这里我们自己编写一个网页，但是不知道这个js对象名称，无法完成showToast方法的调用 五、破解的方法下面我们就来分析一下如何解决上面的问题，其实解决这个问题，我们现有的方法太多了 第一种方法：修改smali源码，把上面的那个js对象名称改成我们自己想要的，比如：jiangwei，然后在自己编写的页面中直接调用：jiangwei.showToast方法即可，不过这里需要修改smali源码，在使用smali工具回编译成dex文件，在弄到apk中，在运行。方法是可行的，但是感觉太复杂，这里不采用 第二种方法：利用Android4.2中的WebView的漏洞，直接使用如下Js代码即可 这里根本不需要任何js对象的名称，只需要方法名就可以完成调用，所以这里可以看到这个漏洞还是很危险的。 第三种方法：我们看到了那个加密方法，我们自己写一个程序，来调用这个方法，尽然得到正确的js对象名称，这里我们就采用这种方式，因为这个方式有一个新的技能，所以这里我就讲解一下了。 那么如果用第三种方法的话，就需要再去分析那个加密方法逻辑了： android.support.v4.widget.ListViewAutoScrollHelpern在这个类中，我们再去查找这个smali源码： 这个类加载了libtranslate.so库，而且加密方法是native层的，那么我们用IDA查看libtranslate.so库： 我们搜一下Java开头的函数，发现并没有和decrypt_native方法对应的native函数，说明这里做了native方法的注册混淆，我们直接看JNI_OnLoad函数： 这里果然是自己注册了native函数，但是分析到这里，我就不往下分析了，为什么呢？因为我们其实没必要搞清楚native层的函数功能，我们知道了Java层的native方法定义，那么我们可以自己定义一个这么个native方法来调用libtranslate.so中的加密函数功能： 我们新建一个Demo工程，仿造一个ListViewAutoScrollHelpern类，内部在定义一个native方法： 然后我们在MainActivity中加载libtranslate.so： 然后调用那个native方法，打印结果： 这里的方法的参数可以查看smali源码中的那个方法参数： 点击运行，发现有崩溃的，我们查看log信息： 是libtranslate.so中有一个PagerTitleStripIcsn类找不到，这个类应该也有一个native方法，我们在构造这个类： 再次运行，还是报错，原因差不多，还需要在构造一个类：TaskStackBuilderJellybeann 好了，再次点击运行： OK了，成功了，从这个log信息可以看出来了，解密之后的js对象名称是：SmokeyBear，那么下面就简单了，我们在构造一个url页面，直接调用：SmokeyBear.showToast即可。 注意： 这里我们看到，如果知道了Java层的native方法的定义，那么我们就可以调用这个native方法来获取native层的函数功能了，这个还是很不安全的，但是我们如何防止自己的so被别人调用呢？可以在so中的native函数做一个应用的签名校验，只有属于自己的签名应用才能调用，否则直接退出。 六，开始测试上面已经知道了js的对象名称，下面我们就来构造这个页面了： 那么这里又有一个问题了，这个页面构造好了？放哪呢？有的同学说我有服务器，放到服务器上，然后输入url地址就可以了，的确这个方法是可以的，但是有的同学没有服务器怎么办呢？这个也是有方法的，我们知道WebView的loadUrl方法是可以加载本地的页面的，所以我们可以把这个页面保存到本地，但是需要注意的是，这里不能存到SD卡中，因为这个应用没有读取SD的权限，我们可以查看他的AndroidManifest.xml文件： 我们在不重新打包的情况下，是没办法做到的，那么放哪呢？其实很简单了，放在这个应用的/data/data/com.ali.tg.testapp/目录下即可，因为除了SD卡位置，这个位置是最好的了，那么我们知道WebView的loadUrl方法在加载本地的页面的格式是： file:///data/data/com.ali.tg.testapp/crack.html 那么我们直接输入即可 注意： 这里在说一个小技巧：就是我们在一个文本框中输入这么多内容，是不是有点蛋疼，我们其实可以借助于命令来实现输入的，就是使用：adb shell input text ”我们需要输入的内容“。 具体用法很简单，打开我们需要输入内容的EditText，点击调出系统的输入法界面，然后执行上面的命令即可： 不过这里有一个小问题，就是他不识别分号： 不过我们直接修改成分号点击进入： 运行成功，看到了toast的展示。 七、内容整理到这里我们就破解成功了，下面来看看整理一下我们的破解步骤： 1、破解的常规套路 我们按照破解惯例，首先解压出classses.dex文件，使用dex2jar工具查看java代码，但是发现只有一个Application类，所以猜测apk被加壳了，然后用apktool来反编译apk，得到他的资源文件和AndroidManifest.xml内容，找到了包名和入口的Activity类。 2、加固apk的源程序一般存放的位置 知道是加固apk了，那么我们就分析，这个加固的apk肯定是存放在本地的一个地方，一般是三个地方： 1》应用的asset目录中 2》应用的libs中的so文件中 3》应用的dex文件的末尾 我们分析了一下之后，发现asset目录中的确有两个jar文件，但是打不开，猜测是被经过处理了，所以我们得分析处理逻辑，但是这时候我们也没有代码，怎么分析呢？所以这时候就需要借助于dump内存dex技术了： 不管最后的源apk放在哪里，最后都是需要经历解密动态加载到内存中的，所以分析底层加载dex源码，知道有一个函数：dvmDexFileOpenPartial 这个函数有两个重要参数，一个是dex的其实地址，一个是dex的大小，而且知道这个函数是在libdvm.so中的。所以我们可以使用IDA进行动态调试获取信息 3、双开IDA开始获取内存中的dex内容 双开IDA，走之前的动态破解so方式来给dvmDexFileOpenPartial函数下断点，获取两个参数的值，然后使用一段脚本，将内存中的dex数据保存到本地磁盘中。 4、分析获取到的dex内容 得到了内存中的dex之后，我们在使用dex2jar工具去查看源码，但是发现保存，以为是dump出来的dex格式有问题，但是最后使用baksmali工具进行处理，得到smali源码是可以的，然后我们就开始分析smali源码。 5、分析源码了解破解思路 通过分析源码得知在WebViewActivity页面中会加载一个页面，然后那个页面中的js会调用本地的Java对象中的一个方法来展示toast信息，但是这里我们遇到了个问题：Js的Java对象名称被混淆加密了，所以这时候我们需要去分析那个加密函数，但是这个加密函数是native的，然后我们就是用IDA去静态分析了这个native函数，但是没有分析完成，因为我们不需要，其实很简单，我们只需要结果，不需要过程，现在解密的内容我们知道了，native方法的定义也知道了，那么我们就去写一个简单的demo去调用这个so的native方法即可，结果成功了，我们得到了正确的Js对象名称。 6、了解WebView的安全性 WebView的早期版本的一个漏洞信息，在Android4.2之前的版本WebView有一个漏洞，就是可以执行Java对象中所有的public方法，那么在js中就可以这么处理了，先获取geClass方法获取这个对象，然后在调用这个对象中的一些特定方法即可，因为Java中所有的对象都有一个getClass方法，而这个方法是public的，同时能够返回当前对象。所以在Android4.2之后有了一个注解： @JavascriptInterface ，只有这个注解标识的方法才能被Js中调用。 7、获取输入的新技能 验证结果的过程中我们发现了一个技巧，就是我们在输入很长的文本的时候，比较繁琐，可以借助adb shell input text命令来实现。 八、技术点概要1、通过dump出内存中的dex数据，可以佛挡杀佛了，不管apk如何加固，最终都是需要加载到内存中的。 2、了解到了WebView的安全性的相关知识，比如我们在WebView中js对象名称做一次混淆还是有必要的，防止被恶意网站调用我们的本地隐私方法。 3、可以尝试调用so中的native方法，在知道了这个方法的定义之后 4、adb shell input text 命令来辅助我们的输入 九、总结这里就介绍了Android中如何dump出那些加固的apk程序，其实核心就一个：不管上层怎么加固，最终加载到内存的dex肯定不是加固的，所以这个dex就是我们想要的，这里使用了IDA来动态调试libdvm.so中的dvmDexFileOpenPartial函数来获取内存中的dex内容，同时还可以使用gdb+gdbserver来获取，这个感兴趣的同学自行搜索吧。结合了之前的两篇文章，就算善始善终，介绍了Android中大体的破解方式，当然这三种方式不是万能的，因为加固和破解是相生相克的，没有哪个有绝对的优势，只是两者相互进步罢了，当然还有很多其他的破解方式，后面如果遇到的话，会在详细说明。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android逆向</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android逆向-动态分析破解Apk(调试so)]]></title>
    <url>%2F2019%2F03%2F01%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E9%80%86%E5%90%91-%E5%8A%A8%E6%80%81%E5%88%86%E6%9E%90%E7%A0%B4%E8%A7%A3APk(%E8%B0%83%E8%AF%95so)%2F</url>
    <content type="text"><![CDATA[转载: http://www.520monkey.com/ 一、知识准备**我们在介绍如何调试so文件的时候，先来看一下准备知识： 1. native方法的定义和调用什么是NDKAndroid 平台从一开就已经支持了C/C++了。我们知道Android的SDK主要是基于Java的，所以导致了在用Android SDK进行开发的工程师们都必须使用Java语言。不过，Google从一开始就说明Android也支持JNI编程方式，也就是第三方应用完成可以通过JNI调用自己的C动态度。于是NDK就应运而生了。 Android NDK 是一套允许您使用原生代码语言(例如C和C++) 实现部分应用的工具集。在开发某些类型应用时，这有助于您重复使用以这些语言编写的代码库。 Android NDK 就是一套工具集合，允许你使用C/C++语言来实现应用程序的部分功能。 NDK 是Native Develop Kit的含义，从含义很容易理解，本地开发。大家都知道，Android 开发语言是Java，不过我们也知道，Android是基于Linux的，其核心库很多都是C/C++的，比如Webkit等。那么NDK的作用，就是Google为了提供给开发者一个在Java中调用C/C++代码的一个工作。NDK本身其实就是一个交叉工作链，包含了Android上的一些库文件，然后，NDK为了方便使用，提供了一些脚本，使得更容易的编译C/C++代码。总之，在Android的SDK之外，有一个工具就是NDK，用于进行C/C++的开发。一般情况，是用NDK工具把C/C++编译为.co文件，然后在Java中调用。 从NDK到.so 从上图这个Android系统框架来看，我们上层通过JNI来调用NDK层的，使用这个工具可以很方便的编写和调试JNI的代码。因为C语言的不跨平台，在Mac系统的下使用NDK编译在Linux下能执行的函数库——so文件。其本质就是一堆C、C++的头文件和实现文件打包成一个库。目前Android系统支持以下七种不用的CPU架构，每一种对应着各自的应用程序二进制接口ABI：(Application Binary Interface)定义了二进制文件(尤其是.so文件)如何运行在相应的系统平台上，从使用的指令集，内存对齐到可用的系统函数库。 什么是JNIJava调用C/C++在Java语言里面本来就有的，并非Android自创的，即JNI。JNI就是Java调用C++的规范。当然，一般的Java程序使用的JNI标准可能和android不一样，Android的JNI更简单。 JNI，全称为Java Native Interface，即Java本地接口，JNI是Java调用Native 语言的一种特性。通过JNI可以使得Java与C/C++机型交互。即可以在Java代码中调用C/C++等语言的代码或者在C/C++代码中调用Java代码。由于JNI是JVM规范的一部分，因此可以将我们写的JNI的程序在任何实现了JNI规范的Java虚拟机中运行。同时，这个特性使我们可以复用以前用C/C++写的大量代码JNI是一种在Java虚拟机机制下的执行代码的标准机制。代码被编写成汇编程序或者C/C++程序，并组装为动态库。也就允许非静态绑定用法。这提供了一个在Java平台上调用C/C++的一种途径，反之亦然。 实现JNI的过程第1步：在Java中先声明一个native方法 第2步：编译Java源文件javac得到.class文件 第3步：通过javah -jni命令导出JNI的.h头文件 第4步：使用Java需要交互的本地代码，实现在Java中声明的Native方法（如果Java需要与C++交互，那么就用C++实现Java的Native方法。） 第5步：将本地代码编译成动态库(Windows系统下是.dll文件，如果是Linux系统下是.so文件，如果是Mac系统下是.jnilib) 第6步：通过Java命令执行Java程序，最终实现Java调用本地代码。 在Java开发中的静态类定义方法12345678public class NDKTools &#123; static &#123; System.loadLibrary("ndkdemotest-jni"); &#125; public static native String getStringFromNDK();&#125; 2. 在python中怎么调用.so文件在linux系统中可以使用python的ctypes库来导入并调用.sh静态库 12345from ctypes import cdll cur = cdll.LoadLibrary('./libmax.so') a = cur.``max(1, 2) 3. IDA工具的使用 这里有多个窗口，也有多个视图，用到最多的就是： 1、Function Window对应的so函数区域：这里我们可以使用ctrl+f进行函数的搜索 2、IDA View对应的so中代码指令视图：这里我们可以查看具体函数对应的arm指令代码 3、Hex View对应的so的十六进制数据视图：我们可以查看arm指令对应的数据等 当然在IDA中我们还需要知道一些常用的快捷键： 1、强大的F5快捷键可以将arm指令转化成可读的C语言，帮助分析 首先选中需要翻译成C语言的函数，然后按下F5： 看到了，立马感觉清爽多了，这些代码看起来应该会好点了。 下面我们还需要做一步，就是还原JNI函数方法名一般JNI函数方法名首先是一个指针加上一个数字，比如v3+676。然后将这个地址作为一个方法指针进行方法调用，并且第一个参数就是指针自己，比如(v3+676)(v3…)。这实际上就是我们在JNI里经常用到的JNIEnv方法。因为Ida并不会自动的对这些方法进行识别，所以当我们对so文件进行调试的时候经常会见到却搞不清楚这个函数究竟在干什么，因为这个函数实在是太抽象了。解决方法非常简单，只需要对JNIEnv指针做一个类型转换即可。比如说上面提到a1和v4指针： 我们可以选中a1变量，然后按一下y键： 然后将类型声明为：JNIEnv*。 确定之后再来看： 修改之后，是不是瞬间清晰了很多？另外有人（ 貌似是看雪论坛上的）还总结了所有JNIEnv方法对应的数字，地址以及方法声明： 2、Shirt+F12快捷键，速度打开so中所有的字符串内容窗口 有时候，字符串是一个非常重要的信息，特别是对于破解的时候，可能就是密码，或者是密码库信息。 3、Ctrl+S快捷键，有两个用途，在正常打开so文件的IDA View视图的时候，可以查看so对应的Segement信息 可以快速得到，一个段的开始位置和结束位置，不过这个位置是相对位置，不是so映射到内存之后的位置，关于so中的段信息，不了解的同学可以参看这篇文章：Android中so文件格式详解 这篇文章介绍的很很清楚了，这里就不在作介绍了。 当在调试页面的候，ctrl+s可以快速定位到我们想要调试的so文件映射到内存的地址： 因为一般一个程序，肯定会包含多个so文件的，比如系统的so就有好多的，一般都是在/system/lib下面，当然也有我们自己的so，这里我们看到这里的开始位置和结束位置就是这个so文件映射到内存中： 这里我们可以使用cat命令查看一个进程的内存映射信息：cat /proc/[pid]/maps 我们看到映射信息中有多so文件，其实这个不是多个so文件，而是so文件中对应的不同Segement信息被映射到内存中的，一般是代码段，数据段等，因为我们需要调试代码，所以我们只关心代码段，代码段有一个特点就是具有执行权限x，所以我们只需要找到权限中有x的那段数据即可。 4、G快捷键：在IDA调试页面的时候，我们可以使用S键快速跳转到指定的内存位置 这里的跳转地址，是可以算出来的，比如我现在想跳转到A函数，然后下断点，那么我们可以使用上面说到的ctrl+s查找到so文件的内存开始的基地址，然后再用IDA View中查看A函数对应的相对地址，相加就是绝对地址，然后跳转到即可，比如这里的： Java_cn_wjdiankong_encryptdemo_MainActivity_isEquals 函数的IDA View中的相对地址(也就是so文件的地址)：E9C 上面看到so文件映射到内存的基地址：74FE4000 那么跳转地址就是：74FE4000+E9C=74FE4E9C 注意： 一般这里的基地址只要程序没有退出，在运行中，那么他的值就不会变，因为程序的数据已经加载到内存中了，基地址不会变的，除非程序退出，又重新运行把数据加载内存中了，同时相对地址是永远会变的，只有在修改so文件的时候，文件的大小改变了，可能相对地址会改变，其他情况下不会改变，相对地址就是数据在整个so文件中的位置。 这里我们可以看到函数映射到内存中的绝对地址了。 注意： 有时候我们发现跳转到指定位置之后，看到的全是DCB数据，这时候我们选择函数地址，点击P键就可以看到arm指令源码了： 5、调试快捷键：F8单步调试，F7单步进入调试 上面找到函数地址之后，我们可以下断点了，下断点很简单，点击签名的绿色圈点，变成红色条目即可，然后我们可以点击F9快捷键，或者是点击运行按钮，即可运行程序： 其中还有暂停和结束按钮。我们运行之后，然后在点击so的native函数，触发断点逻辑： 这时候，我们看到进入调试界面，点击F8可以单步调试，看到有一个PC指示器，其实在arm中PC是一个特殊的寄存器，用来存储当前指令的地址，这个下面会介绍到。 好了到这里，我们就大致说了一下关于IDA在调试so文件的时候，需要用到的快捷键： 1&gt;、Shift+F12快速查看so文件中包含的字符串信息 2&gt;、F5快捷键可以将arm指令转化成可读的C代码，这里同时可以使用Y键，修改JNIEnv的函数方法名 3&gt;、Ctrl+S有两个用途，在IDA View页面中可以查看so文件的所有段信息，在调试页面可以查看程序所有so文件映射到内存的基地址 4&gt;、G键可以在调试界面，快速跳转到指定的绝对地址，进行下断点调试，这里如果跳转到目的地址之后，发现是DCB数据的话，可以在使用P键，进行转化即可，关于DCB数据，下面会介绍的。 5&gt;、F7键可以单步进入调试，F8键可以单步调试 4.常用的ARM指令集知识我们在上面看到IDA打开so之后，看到的是纯种的汇编指令代码，所以这就要求我们必须会看懂汇编代码，就类似于我们在调试Java层代码的时候一样，必须会smali语法，庆幸的是，这两种语法都不是很复杂，所以我们知道一些大体的语法和指令就可以了，下面我们来看看arm指令中的寻址方式，寄存器，常用指令，看完这三个知识点，我们就会对arm指令有一个大体的了解，对于看arm指令代码也是有一个大体的认知了。 1、arm指令中的寻址方式 1&gt;. 立即数寻址也叫立即寻址，是一种特殊的寻址方式，操作数本身包含在指令中，只要取出指令也就取到了操作数。这个操作数叫做立即数，对应的寻址方式叫做立即寻址。例如：MOV R0,#64 ；R0 ← 642&gt;. 寄存器寻址寄存器寻址就是利用寄存器中的数值作为操作数，也称为寄存器直接寻址。例如： ADD R0，R1， R2 ；R0 ← R1 + R23&gt;. 寄存器间接寻址寄存器间接寻址就是把寄存器中的值作为地址，再通过这个地址去取得操作数，操作数本身存放在存储器中。例如：LDR R0，[R1] ；R0 ←[R1]4&gt;. 寄存器偏移寻址这是ARM指令集特有的寻址方式，它是在寄存器寻址得到操作数后再进行移位操作，得到最终的操作数。例如：MOV R0，R2，LSL #3 ；R0 ← R2 * 8 ，R2的值左移3位，结果赋给R0。5&gt;. 寄存器基址变址寻址寄存器基址变址寻址又称为基址变址寻址，它是在寄存器间接寻址的基础上扩展来的。它将寄存器（该寄存器一般称作基址寄存器）中的值与指令中给出的地址偏移量相加，从而得到一个地址，通过这个地址取得操作数。例如：LDR R0，[R1，#4] ；R0 ←[R1 + 4]，将R1的内容加上4形成操作数的地址，取得的操作数存入寄存器R0中。6&gt;. 多寄存器寻址这种寻址方式可以一次完成多个寄存器值的传送。例如：LDMIA R0，{R1，R2，R3，R4} ；R1←[R0]，R2←[R0+4]，R3←[R0+8]，R4←[R0+12]7&gt;. 堆栈寻址堆栈是一种数据结构，按先进后出（First In Last Out，FILO）的方式工作，使用堆栈指针（Stack Pointer, SP）指示当前的操作位置，堆栈指针总是指向栈顶。堆栈寻址举例如下：STMFD SP！，｛R1－R7, LR｝ ；将R1－R7, LR压入堆栈。满递减堆栈。LDMED SP！，｛R1－R7, LR｝ ；将堆栈中的数据取回到R1－R7, LR寄存器。空递减堆栈。 2、ARM中的寄存器 R0-R3:用于函数参数及返回值的传递R4-R6, R8, R10-R11:没有特殊规定，就是普通的通用寄存器R7:栈帧指针(Frame Pointer).指向前一个保存的栈帧(stack frame)和链接寄存器(link register， lr)在栈上的地址。R9:操作系统保留R12:又叫IP(intra-procedure scratch )R13:又叫SP(stack pointer)，是栈顶指针R14:又叫LR(link register)，存放函数的返回地址。R15:又叫PC(program counter)，指向当前指令地址。 3、ARM中的常用指令含义 ADD 加指令SUB 减指令STR 把寄存器内容存到栈上去LDR 把栈上内容载入一寄存器中.W 是一个可选的指令宽度说明符。它不会影响为此指令的行为，它只是确保生成 32 位指令。Infocenter.arm.com的详细信息BL 执行函数调用，并把使lr指向调用者(caller)的下一条指令，即函数的返回地址BLX 同上，但是在ARM和thumb指令集间切换。CMP 指令进行比较两个操作数的大小 4、ARM指令简单代码段分析 C代码： #include &lt;stdio.h&gt;int func(int a, int b, int c, int d, int e, int f){ ​ int g = a + b + c + d + e + f;​ return g;} 对应的ARM指令： add r0, r1 将参数a和参数b相加再把结果赋值给r0ldr.w r12, [sp] 把最的一个参数f从栈上装载到r12寄存器add r0, r2 把参数c累加到r0上ldr.w r9, [sp, #4] 把参数e从栈上装载到r9寄存器add r0, r3 累加d累加到r0add r0, r12 累加参数f到r0add r0, r9 累加参数e到r0 二、构造so案例好了，关于ARM指令的相关知识，就介绍这么多了，不过我们在调试分析的时候，肯定不能做到全部的了解，因为本身ARM指令语法就比较复杂，不过幸好大学学习了汇编语言，所以稍微能看懂点，如果不懂汇编的同学那就可能需要补习一下了，因为我们在使用IDA分析so文件的时候，不会汇编的话，那是肯定行不通的，所以我们必须要看懂汇编代码的，如果遇到特殊指令不了解的同学，可以网上搜一下即可。 上面我们的准备知识做完了，一个是IDA工具的时候，一个是ARM指令的了解，下面我们就来开始操刀了，为了方便开始，我们先自己写一个简单的Android native层代码，然后进行IDA进行分析即可。 这里可以使用AndroidStudio中进行新建一个简单工程，然后创建JNI即可： 这里顺便简单说一下AndroidStudio中如何进行NDK的开发吧： 第一步：在工程中新建jni目录 第二步：使用javah生成native的头文件 注意： javah执行的目录，必须是类包名路径的最上层，然后执行： javah 类全名 注意没有后缀名java哦 第三步：配置项目的NDK目录 选择模块的设置选线：Open Module Settings： 设置NDK目录即可 第四步：copy头文件到jni目录下，然后配置gradle中的ndk选项 这里只需要设置编译之后的模块名，就是so文件的名称，需要产生那几个平台下的so文件，还有就是需要用到的lib库，这里我们看到我们用到了Android中打印log的库文件。 第五步：编译运行，在build目录下生成指定的so文件，copy到工程的libs目录下即可 好了，到这里我们就快速的在AndroidStudio中新建了一个Native项目，这里关于native项目的代码不想解释太多，就是Java层 传递了用户输入的密码，然后native做了校验过程，把校验结果返回到Java层即可： 具体的校验过程这里不再解释了。我们运行项目之后，得到apk文件，那么下面我们就开始我们的破解旅程了 三、开始破解so文件开始破解我们编译之后的apk文件 第一、首先我们可以使用最简单的压缩软件，打开apk文件，然后解压出他的so文件 我们得到libencrypt.so文件之后，使用IDA打开它： 我们知道一般so中的函数方法名都是：Java_类名_方法名 那么这里我们直接搜：Java关键字即可，或者使用jd-gui工具找到指定的native方法 双击，即可在右边的IDA View页面中看到Java_cn_wjdiankong_encryptdemo_MainActivity_isEquals 函数的指令代码： 我们可以简单的分析一下这段指令代码： 1&gt;、PUSH {r3-r7,lr} 是保存r3,r4,r5,r6,r7,lr 的值到内存的栈中，那么最后当执行完某操作后，你想返回到lr指向的地方执行，当然要给pc了，因为pc保留下一条CPU即将执行的指令，只有给了pc，下一条指令才会执行到lr指向的地方 pc：程序寄存器，保留下一条CPU即将执行的指令lr: 连接返回寄存器，保留函数返回后，下一条应执行的指令 这个和函数最后面的POP {r3-r7,pc}是相对应的。 2&gt;、然后是调用了strlen,malloc,strcpy等系统函数，在每次使用BLX和BL指令调用这些函数的时候，我们都发现了一个规律：就是在调用他们之前一般都是由MOV指令，用来传递参数值的，比如这里的R5里面存储的就是strlen函数的参数，R0就是is_number函数的参数，所以我们这样分析之后，在后面的动态调试的过程中可以得到函数的入口参数值，这样就能得到一些重要信息 3&gt;、在每次调用有返回值的函数之后的命令，一般都是比较指令，比如CMP，CBZ，或者是strcmp等，这里是我们破解的突破点，因为一般加密再怎么牛逼，最后比较的参数肯定是正确的密码(或者是正确的加密之后的密码)和我们输入的密码(或者是加密之后的输入密码)，我们在这里就可以得到正确密码，或者是加密之后的密码： 到这里，我们就分析完了native层的密码比较函数：Java_cn_wjdiankong_encryptdemo_MainActivity_isEquals 如果觉得上面的ARM指令看的吃力，可以使用F5键，查看他的C语言代码： 我们这里看到其实有两个函数是核心点： 1&gt;is_number函数，这个函数我们看名字应该猜到是判断是不是数字，我们可以使用F5键，查看他对应的C语言代码： 这里简单一看，主要是看return语句和if判断语句，看到这里有一个循环，然后获取_BYTE这里地址的值，并且自增加一，然后存到v2中，如果v3为’\0’的话，就结束循环，然后做一次判断，就是v2-48是否大于9，那么这里我们知道48对应的是ASCII中的数字0，所以这里可以确定的是就是：用一个循环遍历_BYTE这里存的字符串是否为数字串。 2&gt;get_encrypt_str函数，这个函数我们看到名字可以猜测，他是获取我们输入的密码加密之后的值，再次使用F5快捷键查看： 这里我们看到，首先是一个if语句，用来判断传递的参数是否为NULL，如果是的话，直接返回，不是的话，使用strlen函数获取字符串的长度保存到v2中，然后使用malloc申请一块堆内存，首指针保存到result，大小是v2+1也就是传递进来的字符串长度+1，然后就开始进入循环，首指针result，赋值给i指针，开始循环，v3是通过v1-1获取到的，就是函数传递进来字符串的地址，那么v6就是获取传递进来字符串的字符值，然后减去48，赋值给v7，这里我们可以猜到了，这里想做字符转化，把char转化成int类型，继续往下看，如果v6==48的话，v7=1，也就是说这里如果遇到字符’0’，就赋值1，在往下看，看到我们上面得到的v7值，被用来取key_src数组中的值，那么这里我们双击key_src变量，就跳转到了他的值地方，果不其然，这里保存了一个字符数组，看到他的长度正好是18，那么这里我们应该明白了，这里通过传递进来的字符串，循环遍历字符串，获取字符，然后转化成数字，在倒序获取key_src中的字符，保存到result中。然后返回。 好了，到这里我们就分析完了这两个重要的函数的功能，一个是判断输入的内容是否为数字字符串，一个是通过输入的内容获取密码内容，然后和正确的加密密码：ssBCqpBssP 作比较。 第二、开始使用IDA进行调试设置那么下面我们就用动态调试来跟踪传入的字符串值，和加密之后的值，这里我们看到没有打印log的函数，所以很难知道具体的参数和寄存器的值，所以这里需要开始调试，得知每个函数执行之后的寄存器的值，我们在用IDA进行调试so的时候，需要以下准备步骤： 1、在IDA安装目录下获取android_server命令文件 在IDA安装目录\dbgsrv\android_server 其实是使用gdb和gdbserver来做到的，gdb和gdbserver在调试的时候，必须注入到被调试的程序进程中，但是非root设备的话，注入别的进程中只能借助于run-as这个命令了，所以我们知道，如果要调试一个应用进程的话，必须要注入他内部，那么IDA调试so也是这个原理，他需要注入(Attach附加)进程，才能进行调试，但是IDA没有自己弄了一个类似于gdbserver这样的工具，那就是android_server了，所以他需要运行在设备中，保证和PC端的IDA进行通信，比如获取设备的进程信息，具体进程的so内存地址，调试信息等。 所以我们把android_server保存到设备的/data目录下，修改一下他的运行权限，然后必须在root环境下运行，因为他要做注入进程操作，必须要root。 注意： 这里把他放在了/data目录下，然后运行./android_server，这里提示了IDA Android 32-bit，所以后面我们在打开IDA的时候一定要是32位的IDA，不是64位的，不然保存，IDA在安装之后都是有两个可执行的程序，一个是32位，一个是64位的，如果没打开正确会报这样的错误： 同样还有一类问题： error: only position independent executables (PIE) are supported 这个主要是Android5.0以上的编译选项默认开启了pie，在5.0以下编译的原生应用不能运行，有两种解决办法，一种是用Android5.0以下的手机进行操作，还有一种就是用IDA6.6+版本即可。 然后我们再看，这里开始监听了设备的23946端口，那么如果要想让IDA和这个android_server进行通信，那么必须让PC端的IDA也连上这个端口，那么这时候就需要借助于adb的一个命令了： adb forward tcp:远端设备端口号(进行调试程序端) tcp:本地设备端口(被调试程序端) 那么这里，我们就可以把android_server端口转发出去： 然后这时候，我们只要在PC端使用IDA连接上23946这个端口就可以了，这里面有人好奇了，为什么远程端的端口号也是23946，因为后面我们在使用IDA进行连接的时候，发现IDA他把这个端口设置死了，就是23946，所以我们没办法自定义这个端口了。 我们可以使用netstat命令查看端口23946的使用情况，看到是ida在使用这个端口 2、上面就准备好了android_server，运行成功，下面就来用IDA进行尝试连接，获取信息，进行进程附加注入 我们这时候需要在打开一个IDA，之前打开一个IDA是用来分析so文件的，一般用于静态分析，我们要调试so的话，需要在打开一个IDA来进行，所以这里一般都是需要打开两个IDA，也叫作双开IDA操作。动静结合策略。 这里记得选择go这个选项，就是不需要打开so文件了，进入是一个空白页： 我们选择Debugger选项，选择Attach，看到有很多debugger，所以说IDA工具真的很强大，做到很多debugger的兼容，可以调试很多平台下的程序。这里我们选择Android debugger： 这里看到，端口是写死的：23946，不能进行修改，所以上面的adb forward进行端口转发的时候必须是23946。这里PC本地机就是调试端，所以host就是本机的ip地址：127.0.0.1，点击确定： 这里可以看到设备中所有的进程信息就列举出来的，其实都是android_server干的事，获取设备进程信息传递给IDA进行展示。 注意： 如果我们当初没有用root身份去运行android_server: 这里就会IDA是不会列举出设备的进程信息： 还有一个注意的地方，就是IDA和android_server一定要保持一致。 我们这里可以ctrl+F搜索我们需要调试的进程，当然这里我们必须运行起来我们需要调试的进程，不然也是找不到这个进程的 双击进程，即可进入调试页面： 这里为什么会断在libc.so中呢？ android系统中libc是c层中最基本的函数库，libc中封装了io、文件、socket等基本系统调用。所有上层的调用都需要经过libc封装层。所以libc.so是最基本的，所以会断在这里，而且我们还需要知道一些常用的系统so,比如linker： 我们知道，这个linker是用于加载so文件的模块，所以后面我们在分析如何在.init_array处下断点 还有一个就是libdvm.so文件，他包含了DVM中所有的底层加载dex的一些方法： 我们在后面动态调试需要dump出加密之后的dex文件，就需要调试这个so文件了。 3、找到函数地址，下断点，开始调试 我们使用Ctrl+S找到需要调试so的基地址：74FE4000 然后通过另外一个IDA打开so文件，查看函数的相对地址：E9C 那么得到了函数的绝对地址就是：74FE4E9C，使用G键快速跳转到这个绝对地址： 跳转到指定地址之后，开始下断点，点击最左边的绿色圆点即可下断点： 然后点击左上角的绿色按钮，运行，也可以使用F9键运行程序： 我们点击程序中的按钮： 触发native函数的运行： 看到了，进入调试阶段了，这时候，我们可以使用F8进行单步调试，F7进行单步进入调试： 我们点击F8进行单步调试，达到is_number函数调用出，看到R0是出入的参数值，我们可以查看R0寄存器的内容，然后看到是123456，这个就是Java层传入的密码字符串，接着往下走： 这里把is_number函数返回值保存到R0寄存中，然后调用CBZ指令，判断是否为0，如果为0就跳转到locret_74FE4EEC处，查看R0寄存器的值不是0，继续往下走： 看到了get_encrypt_str函数的调用，函数的返回值保存在R1寄存器中，查看内容：zytyrTRAB了，那么看到，上层传递的：123456=》zytyrTRAB了，前面我们静态分析了get_encrypt_str函数的逻辑，继续往下看： 看到了，这里把上面得到的字符串和ssBCqpBssP作比较，那么这里ssBCqpBssP就是正确的加密密码了，那么我们现在的资源是： 正确的加密密码：ssBCqpBssP，加密密钥库：zytyrTRA*BniqCPpVs，加密逻辑get_encrypt_str 那么我们可以写一个逆向的加密方法，去解析正确的加密密码得到值即可，这里为了给大家一个破解的机会，这里就不公布正确答案了，这个apk我随后会上传，手痒的同学可以尝试破解一下。 第三、总结IDA调试的流程到这里，我们就分析了如何破解apk的流程，下面来总结一下： 1、我们通过解压apk文件，得到对应的so文件，然后使用IDA工具打开so,找到指定的native层函数 2、通过IDA中的一些快捷键：F5,Ctrl+S,Y等键来静态分析函数的arm指令，大致了解函数的执行流程 3、再次打开一个IDA来进行调试so 1&gt;将IDA目录中的android_server拷贝到设备的指定目录下，修改android_server的运行权限，用Root身份运行android_server 2&gt;使用adb forward进行端口转发，让远程调试端IDA可以连接到被调试端 3&gt;使用IDA连接上转发的端口，查看设备的所有进程，找到我们需要调试的进程。 4&gt;通过打开so文件，找到需要调试的函数的相对地址，然后在调试页面使用Ctrl+S找到so文件的基地址，相加之后得到绝对地址，使用G键，跳转到函数的地址处，下好断点。点击运行或者F9键。 5&gt;触发native层的函数，使用F8和F7进行单步调试，查看关键的寄存器中的值，比如函数的参数，和函数的返回值等信息 总结就是：在调试so的时候，需要双开IDA，动静结合分析。 四、使用IDA来解决反调试问题那么到这里我们就结束了我们这期的破解旅程了？答案是否定的，因为我们看到上面的例子其实是我自己先写了一个apk,目的就是为了给大家演示，如何使用IDA来进行动态调试so，那么下面我们还有一个操刀动手的案例，就是2014年，阿里安全挑战赛的第二题：AliCrackme_2： 阿里真会制造氛围，还记得我们破解的第一题吗，这次看到了第二题，好吧，下面来看看破解流程吧： 首先使用aapt命令查看他的AndroidManifest.xml文件，得到入口的Activity类： 然后使用dex2jar和jd-gui查看他的源码类：com.yaotong.crackme.MainActivity： 看到，他的判断，是securityCheck方法，是一个native层的，所以这时候我们去解压apk文件，获取他的so文件，使用IDA打开查看native函数的相对地址：11A8 这里的ARM指令代码不在分析了，大家自行查看即可，我们直接进入调试即可： 在打开一个IDA进行关联调试： 选择对应的调试进程，然后确定： 使用Ctrl+S键找到对应so文件的基地址：74EA9000 和上面得到的相对地址相加得到绝对地址：74EA9000+11A8=74EAA1A8 使用G键直接跳到这个地址： 下个断点，然后点击F9运行程序： 擦，IDA退出调试页面了，我们再次进入调试页面，运行，还是退出调试页面了，好了，这下蛋疼了，没法调试了。 这里其实是阿里做了反调试侦查，如果发现自己的程序被调试了，就直接退出程序，那么这里有问题了，为什么知道是反调试呢？这个主要还是看后续自己的破解经验了，没技术可言，还有一个就是阿里如何做到的反调试策略的，这里限于篇幅，只是简单介绍一下原理： 前面说到，IDA是使用android_server在root环境下注入到被调试的进程中，那么这里用到一个技术就是Linux中的ptrace，关于这个这里也不解释了，大家可以自行的去搜一下ptrace的相关知识，那么Android中如果一个进程被另外一个进程ptrace了之后，在他的status文件中有一个字段：TracerPid 可以标识是被哪个进程trace了，我们可以使用命令查看我们的被调试的进行信息：**status文件在：/proc/[pid]/status** 看到了，这里的进程被9187进程trace了，我们在用ps命令看看9187是哪个进程： 果不其然，是我们的android_server进程，好了，我们知道原理了，也大致猜到了阿里在底层做了一个循环检测这个字段如果不为0，那么代表自己进程在被人trace，那么就直接停止退出程序，这个反检测技术用在很多安全防护的地方，也算是一个重要的知识点了。 那么下面就来看看如何应对这个反调试？ 我们刚刚看到，只要一运行程序，就退出了调试界面，说明，这个循环检测程序执行的时机非常早，那么我们现在知道的最早的两个时机是：一个是.init_array，一个是JNI_OnLoad .init_array是一个so最先加载的一个段信息，时机最早，现在一般so解密操作都是在这里做的 JNI_OnLoad是so被System.loadLibrary调用的时候执行，他的时机要早于哪些native方法执行，但是没有.init_array时机早 那么知道了这两个时机，下面我们先来看看是不是在JNI_OnLoad函数中做的策略，所以我们需要先动态调试JNI_OnLoad函数 我们既然知道了JNI_OnLoad函数的时机，如果阿里把检测函数放在这里的话，我们不能用之前的方式去调试了，因为之前的那种方式时机太晚了，只要运行就已经执行了JNI_OnLoad函数，所以就会退出调试页面 幸好这里IDA提供了在so文件load的时机，我们只需要在Debug Option中设置一下就可以了： 在调试页面的Debugger 选择 Debugger Option选项： 然后勾选Suspend on library load/unload即可 这样设置之后，还是不行，因为我们程序已经开始运行，就在static代码块中加载so文件了，static的时机非常早，所以这时候，我们需要让程序停在加载so文件之前即可。 那么我想到的就是添加代码waitForDebugger代码了，这个方法就是等待debug，我们还记得在之前的调试smali代码的时候，就是用这种方式让程序停在了启动出，然后等待我们去用jdb进行attach操作。 那么这一次我们可以在System.loadLibrary方法之前加入waitForDebugger代码即可，但是这里我们不这么干了，还有一种更简单的方式就是用am命令，本身am命令可以启动一个程序，当然可以用debug方式启动： adb shell am start -D -n com.yaotong.crackme/.MainActivity 这里一个重要参数就是-D,用debug方式启动 运行完之后，设备是出于一个等待Debugger的状态： 这时候，我们再次使用IDA进行进程的附加，然后进入调试页面，同时设置一下Debugger Option选项，然后定位到JNI_OnLoad函数的绝对地址。 但是我们发现，这里没有RX权限的so文件，说明so文件没有加载到内存中，想一想还是对的，以为我们现在的程序是wait Debugger，也就是还没有走System.loadLibrary方法，so文件当然没有加载到内存中，所以我们需要让我们程序跑起来，这时候我们可以使用jdb命令去attach等待的程序，命令如下： jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 其实这条命令的功能类似于，我们前一篇说到用Eclipse调试smali源码的时候，在Eclipse中设置远程调试工程一样，选择Attach方式，调试机的ip地址和端口，还记得8700端口是默认的端口，但是我们运行这个命令之后，出现了一个错误： 擦，无法连接到目标的VM，那么这种问题大部分都出现在被调试程序不可调试，我们可以查看apk的android:debuggable属性： 果不其然，这里没有debug属性，所以这个apk是不可以调试的，所以我们需要添加这个属性，然后在回编译即可： 回编译：java -jar apktool.jar b -d out -o debug.apk 签名apk：java -jar .\sign\signapk.jar .\sign\testkey.x509.pem .\sign\testkey.pk8 debug.apk debug.sig.apk 然后在次安装，使用am 命令启动： 第一步：运行：adb shell am start -D -n com.yaotong.crackme/.MainActivity 出现Debugger的等待状态 第二步：启动IDA 进行目标进程的Attach操作 第三步：运行：jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 第三步：设置Debugger Option选项 第四步：点击IDA运行按钮，或者F9快捷键，运行 看到了，这次jdb成功的attach住了，debug消失，正常运行了， 但是同时弹出了一个选择提示： 这时候，不用管它，全部选择取消按钮，然后就运行到了linker模块了： 这时候，说明so已经加载进来了，我们再去获取JNI_OnLoad函数的绝对地址 Ctrl+S查找到了基地址：7515A000 用静态方式IDA打开so查看相对地址：1B9C 相加得到绝对地址：7515A000+1B9C=7515BB9C，然后点击S键，跳转： 跳转到指定的函数位置： 这时候再次点击运行，进入了JNI_OnLoad处的断点： 下面咋们就开始单步调试了，但是当我们每次到达BLX R7这条指令执行完之后，就JNI_OnLoad就退出了： 经过好几次尝试都是一样的结果，所以我们发现这个地方有问题，可能就是反调试的地方了 我们再次进入调试，看见BLX跳转的地方R7寄存器中是pthread_create函数，这个是Linux中新建一个线程的方法 所以阿里的反调试就在这里开启一个线程进行轮训操作，去读取/proc/[pid]/status文件中的TrackerPid字段值，如果发现不为0，就表示有人在调试本应用，在JNI_OnLoad中直接退出。其实这里可以再详细进入查看具体代码实现的，但是这里限于篇幅问题，不详细解释了，后续在写一篇文章我们自己可以实现这种反调试机制的。本文的重点是能够动态调试即可。 那么问题找到了，我们现在怎么操作呢？ 其实很简单，我们只要把BLX R7这段指令干掉即可，如果是smali代码的话，我们可以直接删除这行代码即可，但是so文件不一样，他是汇编指令，如果直接删除这条指令的话，文件会发生错乱，因为本身so文件就有固定的格式，比如很多Segement的内容，每个Segement的偏移值也是有保存的，如果这样去删除会影响这些偏移值，会破坏so文件格式，导致so加载出错的，所以这里我们不能手动的去删除这条指令，我们还有另外一种方法，就是把这条指令变成空指令，在汇编语言中，nop指令就是一个空指令，他什么都不干，所以这里我们直接改一下指令即可，arm中对应的nop指令是：00 00 00 00 那么我们看到BLX R7对应的指令位置为：1C58 查看他的Hex内容是：37 FF 2F E1 我们可以使用一些二进制文件软件进行内容的修改，这里使用010Editor工具进行修改： 这里直接修改成00 00 00 00： 这时候，保存修改之后的so文件，我们再次使用IDA进行打开查看： 哈哈，指令被修改成了：ANDEQ R0，R0，R0了 那么修改了之后，我们在替换原来的so文件，再次重新回编译，签名安装，再次按照之前的逻辑给主要的加密函数下断点，这里不需要在给JNI_OnLoad函数下断点了，因为我们已经修改了反调试功能了，所以这里我们只需要按照这么简单几步即可： 第一步：启动程序 第二步：使用IDA进行进程的attach 第三步：找到Java_com_yaotong_crackme_MainActivity_securityCheck函数的绝对地址 第四步：打上断点，点击运行，进行单步调试 看到了吧，这里我们可以单步调试进来了啦啦，说明我们修改反调试指令成功了。 下面就继续F8单步调试： 调试到这里，发现一个问题，就是CMP指令之后，BNE 指令就开始跳转到loc_74FAF2D0处了，那么我们就可以猜到了，CMP指令比较的应该就是我们输入的密码和正确的密码，我们再次从新调试，看看R3和R1寄存器的值 看到了这里的R3寄存器的值就是用寄存器寻址方式，赋值字符串的，这里R2寄存器就是存放字符串的地址，我们看到的内容是aiyou…但是这里肯定不是全部字符串，因为我们没看到字符串的结束符：’\0’，我们点击R2寄存器，进入查看完整内容： 这里是全部内容：aiyou,bucuoo 我们继续查看R1寄存器的内容： 这里也是同样用寄存器寻址，R0寄存器存储的是R1中字符串的地址，我们看到这里的字符串内容是：jiangwei 这个就是我输入的内容，那么这里就可以豁然开朗了，密码是上面的：aiyou,bucuoo 我们再次输入这个密码： 哈哈哈，破解成功啦啦~~ 五、技术总结到这里我们算是讲解完了如何使用IDA来调试so代码，从而破解apk的知识了，因为这里IDA工具比较复杂，所以这篇文章篇幅有点长，所以同学们可以多看几遍，就差不多了。下面我们来整理一下这篇文章中涉及到的知识点吧： 第一、IDA中的常用快捷键使用 1、Shift+F12可以快速查看so中的常量字符串内容，有时候，字符串内容是一个很大的突破点 2、使用强大的F5键，可以查看arm汇编指令对应的C语言代码，同时可以使用Y键，进行JNIEnv*方法的还原 3、使用Ctrl+S键，可以在IDA View页面中查看so的所有段信息，在调试页面可以查找对应so文件映射到内存的基地址，这里我们还可以使用G键，进行地址的跳转 4、使用F8进行单步调试，F7进行单步跳入调试，同时可以使用F9运行程序 第二、ARM汇编指令相关知识 1、了解了几种寻址方式，有利于我们简单的读懂arm汇编指令代码 2、了解了arm中的几种寄存器的作用，特别是PC寄存器 3、了解了arm中常用的指令，比如：MOV，ADD，SUB，LDR，STR，CMP，CBZ，BL，BLX 第三、使用IDA进行调试so的步骤，这里分两种情况 1、IDA调试无反调试的so代码步骤： 1》把IDA安装目录中的android_server拷贝到设备的指定目录中，修改android_server的权限，并且用root方式运行起来，监听23946端口 2》使用adb forward命令进行端口的转发，将设备被调试端的端口转发到远程调试端中 3》双开IDA工具，一个是用来打开so文件，进行文件分析，比如简单分析arm指令代码，知道大体逻辑，还有就是找到具体函数的相对位置等信息，还有一个IDA是用来调试so文件的，我们在Debugger选项中设置Debugger Option，然后附加需要调试的进程 4》进入调试页面之后，通过Ctrl+S和G快捷键，定位到需要调试的关键函数，进行下断点 5》点击运行或者快捷键F9，触发程序的关键函数，然后进入断点，使用F8单步调试，F7单步跳入调试，在调试的过程中主要观察BL，BLX指令，以及CMP和CBZ等比较指令，然后在查看具体的寄存器的值。 2、IDA调试有反调试的so代码步骤： 1》查看apk是否为可调式状态，可以使用aapt命令查看他的AndroidManifest.xml文件中的android:debuggeable属性是否为true，如果不是debug状态，那么就需要手动的添加这个属性，然后回编译，在签名打包从新安装 2》使用adb shell am start -D -n com.yaotong.crackme/.MainActivity 命令启动程序，出于wait Debug状态 3》打开IDA，进行进程附加，进入到调试页面 4》使用 jdb -connect com.sun.jdi.SocketAttach:hostname=127.0.0.1,port=8700 命令attach之前的debug状态，让程序正常运行 5》设置Debug Option选项，设置Suspend on library start/exit/Suspend on library load/unload/Suspend on process entry point选项 6》点击运行按钮或者F9键，程序运行停止在linker模块中，这时候表示so文件加载进来了，我们通过Ctrl+S和G键跳转到JNI_OnLoad函数出，进行下断点 7》然后继续运行，进入JNI_OnLoad断点处，使用F8进行单步调试，F7进行单步跳入调试，找到反调试代码处 8》然后使用二进制软件修改反调试代码为nop指令，即00值 9》修改之后，在替换原来的so文件，进行回编译，从新签名打包安装即可 10》按照上面的无反调试的so代码步骤即可 第四、学习了如何做到反调试检测 现在很多应用防止别的进程调试或者注入，通常会用自我检测装置，原理就是： 循环检测/proc/[mypid]/status文件，查看他的TracerPid字段是否为0，如果不为0，表示被其他进程trace了 那么这时候就直接退出程序。因为现在的IDA调试时需要进程的注入，进程注入现在都是使用Linux中的ptrace机制，那么这里的TracePid就可以记录trace的pid，我们可以发现我们的程序被那个进程注入了，或者是被他在调试。进而采取一些措施。 第五、IDA调试的整体原理 我们知道了上面的IDA调试步骤，其实我们可以仔细想一想，他的调试原理大致是这样的： 首先他得在被调试端安放一个程序，用于IDA端和调试设备通信，这个程序就是android_server，因为要附加进程，所以这个程序必须要用root身份运行，这个程序起来之后，就会开启一个端口23946，我们在使用adb forward进行端口转发到远程调试端，这时候IDA就可以和调试端的android_server进行通信了。后面获取设备的进程列表，附加进程，传递调试信息，都可以使用这个通信机制完成即可。IDA可以获取被调试的进程的内存数据，一般是在 /proc/[pid]maps 文件中，所以我们在使用Ctrl+S可以查看所有的so文件的基地址，可以遍历maps文件即可做到。 破解法则：**时刻需要注意关键的BL/BLX等跳转指令，在他们执行完之后，肯定会有一些CMP/CBZ等比较指令，这时候就可以查看重要的寄存器内容来获取重要信息。**]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android反编译尝试(裁判文书app示例)]]></title>
    <url>%2F2019%2F02%2F27%2F02.python%E7%88%AC%E8%99%AB%2F%E7%88%AC%E8%99%AB%E6%89%A9%E5%B1%95%E5%8A%9F%E8%83%BD%2FAndroid%E5%8F%8D%E7%BC%96%E8%AF%91%E7%88%AC%E8%99%AB((%E8%A3%81%E5%88%A4%E6%96%87%E4%B9%A6app%E7%A4%BA%E4%BE%8B))%2F</url>
    <content type="text"><![CDATA[一、工具准备 fiddler 安卓模拟器 android studio enjarify jd-gui python3 jdk apktool adb SignApk.jar apk签名工具 IDA 二、 抓包查看apk的请求逻辑1）设置fiddler下载fiddler后，打开菜单栏 Tools=》Options=》HTTPS， 勾选【Decrypt HTTPS traffic】选项，对于【Ignore server certificate errors (unsafe)】选项可以不必勾选，然后点击【Actions】点击【Export Root Certificate to Desktop】这时候就会将Fiddler根证书FiddlerRoot.cer保存到桌面上，这个根证书在如果开启了Fiddler的HTTPS解密的时候火狐浏览器访问HTTPS地址时候出现【您的连接并不安全】的错误页面时候使用。 然后点击HTTPS标签栏旁边的Connections标签， 这里我们要记得【Fiddler listen on port】中显示的端口号（关于这个端口号，如果当前默认的8888端口号已经被占用了，那么需要重新设置另外的端口号），然后将【Allow remote computers to connect】前面的勾打上。点击确定，然后重新启动Fiddler。重新启动后，打开Fiddler后，在Fiddler界面的右上角的三角形上点击就会显示一个【Online】图标，把鼠标放到【Online】图标上，会显示当前机器的IP地址， 该地址直接用来和后面安卓设备的链接 2） 设置安卓设备不同的安卓设备会有一些区别，但是设置的思路一样 点击桌面上的【系统应用】=》【设置】=》【WLAN】， 鼠标放到当前已经连接的网络上长按， 在弹出的消息窗口中点击【修改网络】，输入上面我们得到的IP地址和端口号，点击保存。 然后在模拟器中打开浏览器，输入：http://ipv4.fiddler:8888 ，出现下面的页面说明我们刚刚设置的http代理正确，然后点击红线框的【FiddlerRoot certificate】，下载Fiddler的根证书： 然后我们来到桌面【系统应用】=》【设置】=》【安全】=》【从SD卡安装】中找到我们刚刚下载的证书： 点击证书，然后输入证书名称点击【确定】 安卓设备设置完成，打开浏览器可以检验一下fiddler中是否可以检测到网络请求，如果检测到说明配置成功 3）操作apk并进行请求和抓包抓包目的 下载apk，并进行相应的请求，查找所需数据的网络请求包和必要的请求参数、以及数据书否被加密等 找到数据包 以裁判文书Apk为例， 找到按关键字搜索文书的数据结果 Apk主页 在搜索框中输入关键字 “阿里”, 点击搜索按钮，在fiddler中找到包含数据的请求 第一个主要的请求主要是从服务端获取reqtoken，该参数作为后面请求的表单参数 第二个主要的请求，便是从服务端获取搜索的结果 点击列表页的item，获取详情内容，抓取数据包； 可以确定获取详情内容的数据包为红色框内请求，需携带详情的fieldId; 确定破解目标 综上分析，从抓包上来看，在请求中不确定的参数主要有，headers中的devid，signature，nonce， timespan参数，并且第二次对列表页的请求的响应数据并不是明文，也就是加密后的结果，所以只有经过解密后才能获得真正的结果 接下来将要对其apk代码进行分析和debug调试，需要完成以下两个任务： 1.获取请求参数的生成方式，devid，signature，nonce, timespan 2.详情页的fieIdId生成方式； 3.列表页和详情页响应内容的解密方式 三、反编译获取Java代码关于apk获取静态java代码以及分析请参考《Android逆向-静态分析破解Apk》 1）配置环境，apk下载使用enjarify进行反编译， 需要提前配置好python3运行环境 配置好python3开发环境后，下载将要进行反编译的apk 2）运行enjarify进行反编译将下载好的apk导入到同enjarify同一级目录下 输入命令 1python3 -O -m enjarify.main yourapp.apk 反编译效果， 反编译生成的文件就是 wenshuapp-enjarify.jar 3）JD-GUI查看java源码这一步不用过多介绍，将反编译生成的jar文件导入进来就可以查看其源码部分 如图就是通过反编译工具获取的java class 源代码， 开始寻找我们需要分析的代码 可以看到大部分的逻辑源码都在红色区域内 下面开始动态调试，通过和源码内容的结合来分析整个apk请求和解析数据的逻辑 enjarify工具下载地址：https://github.com/google/enjarify jd-gui工具下载地址: http://jd.benow.ca/ 四、制作debug模式Apk关于debug模式的apk制作和smali的基础语法，和调试方法请参考《Android逆向-动态分析破解Apk(Debug)》 1）使用apktool来破解apk1java -jar apktool/apktool.jar d wenshu/wenshuapp.apk -o wenshu_out 这里的命令不做解释了。 反编译成功之后，我们得到了一个out目录，如下： 源码都放在smail文件夹中，我们进入查看一下文件： 2）修改AndroidManifest.xml中的debug属性和在入口代码中添加waitDebug上面我们反编译成功了，下面我们为了后续的调试工作，所以还是需要做两件事： 1》修改AndroidManifest.xml中的android:debuggable=”true” 关于这个属性，我们前面介绍run-as命令的时候，也提到了，他标识这个应用是否是debug版本，这个将会影响到这个应用是否可以被调试，所以这里必须设置成true。 当然还有其他方式，比如aapt查看apk的内容方式，或者是安装apk之后用 adb dumpsys activity top 命令查看都是可以的。 2》在入口处添加waitForDebugger代码进行调试等待。 这里说的入口处，就是程序启动的地方，就是我们一般的入口Activity，查找这个Activity的话，方法太多了，比如我们这里直接从上面得到的AndroidManifest.xml中找到，因为入口Activity的action和category是固定的, 如上图第二个红框。 找到入口Activity之后，我们直接在他的onCreate方法的第一行加上waitForDebugger代码即可，找到对应的MainActivity的smali源码，然后添加一行代码： 1invoke-static&#123;&#125;, Landroid/os/Debug;-&gt;waitForDebugger()V 这个是smali语法的，其实对应的Java代码就是：android.os.Debug.waitForDebugger(); 这里把Java语言翻译成smali语法的，网上有smali的语法解析。 3）回编译apk并且进行签名安装1java -jar apktool.jar b out -o wenshu_debug.apk 还是使用apktool进行回编译 编译完成之后，将得到debug.apk文件，但是这个apk是没有签名的，所以是不能安装的，那么下面我们需要在进行签名，这里我们使用Android中的测试程序的签名文件和sign.jar工具进行签名： 1java -jar signApk/signapk.jar ./signApk/testkey.x509.pem ./signApk/testkey.pk8 wenshu_debug.apk wenshu_debug.sig.apk 签名之后，我们就可以进行安装了。 五、进行调试1) android studio配置下载 smalidea 下载地址: https://bitbucket.org/JesusFreke/smali/downloads/ 安装smalidea 打开AndroidStudio，点击Preferences… | Plugins, 选择Install plugin from disk 2）将反编译的文件夹导入Android Studio选择Import Project 选择Create preject from existing sources 一直选择“Next”，直至导入工程完成 然后在AndroidStudio工程中右键点击smali文件夹，设定Mark Directory as -&gt; Sources Root AndroidStudio的File -&gt; Project Structure, 配置JDK。 3）配置项目远程调试在AndroidStudio里面配置远程调试的选项，选择Run -&gt; Edit Configurations， 并增加一个Remote调试的调试选项，端口选择:8700 4） 将进程和android studio之间建立连接启动apk 启动模拟器上重新打包并安装好的apk， apk将会卡在启动过程中等待接入debug 查看进程端口信息 找到apk进程的端口号 1adb shell ps | grep com.lawyee.wenshuapp 建立进程和android studio之间的链接 1adb forward tcp:5005 jdwp:29685 5) 开始调试启动程序调试 点击程序调试按钮, 控制台如果显示connect 成功，表示成功启动 六、参数破解代码都是混淆过后的，所以没什么办法，耐心的调试吧，找到想要破解的各个参数的生成位置， 找到生成参数的位置 获取请求参数生成方法因为参数最终都是要经过网络请求发包，所以可以从网络请求发起处入手，然后反向追溯参数的生成过程 比如文书app的网络请求所在位置为com.lawyee.wenshuapp.util.a.a 可以通过调试找到参数生成的位置，再去生成的方法里面去看详细的生成逻辑就好，每个参数的生成 各参数对应的生成方法 timespan 就是当前的datetime去掉所有符号格式 nonce生成方法 devid生成方法 signature生成方法 获取列表\详情的解密方法获取解密方法 找到解密响应数据的位置: 方法: com.lawyee.wenshuapp.util.aa.a， 用jd-gui查看一下该方法的源码 位置: 粗略一看可以看出是通过AES进行的解密，但是方法接收了3个参数现在还不确定其生成方式，下面寻找三个参数的生成方法 获取方法入参 paramString1：通过ide，通过调试窗口可获得其为请求的响应值，即加密后待解密的内容 paramString2：为以下函数的返回值 com.lawuee.wenshuapp.vo.DevInfoVO.getIvParameter， 作为aes解密参数iv param； paramString3：为以下方法的返回值 com.lawyee.wenshuapp.config.ApplicationSet.a， 作为aes解密参数key， 接收发送请求的timespan参数为入参 this实例属性 更新this.f 属性的方法i，通过方法i 向this.f绑定20个生成key的方法； this.f是一个解密方法的集合，在初始化时更新this.f，其内容为20个获取key的方法，在解密的时候，会选择其中一个用来生成key，20个方法就不贴上了 在这20个生成key的方法中，其中用到了三个native方法，如下。 这三个静态方法来自静态库 lienutil-lib, 下面需要从apktool反编译出来的文件中找到该静态文件，将其反编译成汇编，破解这三个方法 分析so文件 关于IDA的使用方法参考《Android逆向-动态分析破解Apk(调试so)》 用IDA打开 libenutil.so文件， 分别选中三个需要分析的方法 按F5键，将方法翻译成c代码， 然后再翻译成目标打码即可 StrToLong方法 StrToLong2方法 StrToLong3方法 目前所有的请求参数和解密过程中涉及的方法都找到了，下面可以实现破解了 七、破解代码生成直接将上一步找到的参数生成方法，和解密方法等翻译成目标代码，python为例 请求参数生成方法 123456789101112131415161718192021222324252627282930import hashlibimport datetimeimport randomimport hashlib# 生成signaturedef get_signature(timespan, nonce, devid): string_buffer = [timespan, nonce, devid] string_buffer.sort() str_join = "".join(string_buffer) m = hashlib.md5() m.update(str_join.encode()) return m.hexdigest()# 生成noncedef get_nonce(): str_array = "abcdefghijklmnopqrstuvwxyz0123456789" nonce = "" for _ in range(4): index_choose = random.randint(0, len(str_array) - 1) nonce = nonce + str_array[index_choose] return nonce# 生成deviddef get_uuid(): return uuid.uuid4().replace("-", "")# 生成timespandef get_timespan(): return datetime.datetime.now().strftime("%Y%m%d%H%M%S") 解密数据方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding=utf-8from appcrack.f_methods import e, f, g, h, k, l, m, n, o, p, q, r, s, t, u, v, w, x, j # 获取key参数的20的方法，暂不列举from appcrack.f_methods import i as iifrom Crypto.Cipher import AESimport base64import hashlib# 3个native方法的python实现def str2long(param_str, param_int): v10 = param_int v4 = 0 v5 = param_str if not param_str.strip(): return v4 v6 = len(v5) v4 = v10 if v6 &gt;= 2: v7 = 1 while v7 &lt; v6: v4 += v10 + v7 + ord(v5[v7] % 256 &lt;&lt; (v7 &amp; 7)) - ord(v5[v7]) % 256 v7 += 1 return v4def str2long2(param_str, param_int): v10 = param_int v4 = 0 v5 = param_str if not param_str.strip(): return v4 v6 = len(v5) v4 = v10 if v6 &gt;= 2: v7 = 1 while v7 &lt; v6: v4 += v10 + v7 + ord(v5[v7] % 256 &lt;&lt; (v7 &amp; 15)) - ord(v5[v7]) % 256 v7 += 1 return v4def str2long3(param_str, param_int): v10 = param_int v4 = 0 v5 = param_str if not param_str.strip(): return v4 v6 = len(v5) v4 = v10 if v6 &gt;= 2: v7 = 1 while v7 &lt; v6: v4 += v10 + v7 + ord(v5[v7] % 256 &lt;&lt; (v7 &amp; 15)) + v7 % 4 - ord(v5[v7]) % 256 v7 += 1 return v4# 数据解密iv param(paramString2)参数生成方法def get_iv_param(devid): if len(devid.strip()) &lt; 16: return None return devid[len(devid) - 16]# key(paramString3)的生成方法def get_key(timespan, token): key_producer_list = [e, f, q, r, s, t, u, v, w, x, g, h, ii, j, k, l, m, n, o, p] i = str2long(token, 1) % 20 return key_producer_list[i](token + timespan)# aes解密方法实现def aes_decrypt(param_str1, param_str2, param_str3): block_size = AES.block_size pkc5Padding = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size) \ if (block_size - len(s) % block_size) else s + block_size * chr(16) unpkc5Padding = lambda s: s[:-ord(s[len(s) - 1:])] mode = AES.MODE_CBC localCipher = AES.new(key=param_str2.encode('ascii'), mode=mode, iv=param_str3.encode('utf8')) cryptedStr = base64.b64decode(param_str1) recovery = unpkc5Padding(localCipher.decrypt(cryptedStr)) return recovery.decode('utf-8')# 数据解密方法接口def decrypt(devid, token, timespan, context): iv_param = get_iv_param(devid) key = get_key(timespan, token) return aes_decrypt(context, iv_param, key) 好了，目前app请求过程中的所有参数的生成方法，和响应数据的解密方法都已经构建好了，接下来就可以随意的编写爬虫进行数据爬取了。]]></content>
      <categories>
        <category>python爬虫</category>
        <category>爬虫拓展功能</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Android反编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链密码学]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F2.%E5%8C%BA%E5%9D%97%E9%93%BE%E5%AF%86%E7%A0%81%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[密码学算法Hash算法定义 Hash（哈希或散列）算法，又常被称为指纹（fingerprint）或摘要（digest）算法，是非常基础也非常重要的一类算法。可以将任意长度的二进制明文串映射为较短的（通常是固定长度的）二进制串（Hash 值），并且不同的明文很难映射为相同的 Hash 值，且算法不可逆，例如: MD5、SHA-1、SHA-2、SHA-256； 比如判断两个字符串是否相等，只需要判断两字符串的hash值是否相等就可以基本断定两字符串是否相等； Hash 算法并不是一种加密算法，不能用于对信息的保护。但 Hash 算法可被应用到对登录口令的保存上。例如网站登录时需要验证用户名和密码，如果网站后台直接保存用户的口令原文，一旦发生数据库泄露后果不堪设想（事实上，网站数据库泄露事件在国内外都不少见）。利用 Hash 的防碰撞特性，后台数据库可以仅保存用户口令的 Hash 值，这样每次通过 Hash 值比对，即可判断输入口令是否正确。即便数据库泄露了，攻击者也无法轻易从 Hash 值还原回口令。 性质 正向快速：给定原文和 Hash 算法，在有限时间和有限资源内能计算得到 Hash 值； 逆向困难：给定（若干）Hash 值，在有限时间内无法（基本不可能）逆推出原文； 输入敏感：原始输入信息发生任何改变，新产生的 Hash 值都应该发生很大变化； 碰撞避免：很难找到两段内容不同的明文，使得它们的 Hash 值一致（即发生碰撞）。 常见hash算法 目前常见的 Hash 算法包括国际上的 Message Digest（MD）系列和 Secure Hash Algorithm（SHA）系列算法，以及国内的 SM3 算法。 MD 算法主要包括 MD4 和 MD5 两个算法。MD4（RFC 1320）是 MIT 的 Ronald L. Rivest 在 1990 年设计的，其输出为 128 位。MD4 已证明不够安全。MD5（RFC 1321）是 Rivest 于 1991 年对 MD4 的改进版本。它对输入仍以 512 位进行分组，其输出是 128 位。MD5 比 MD4 更加安全，但过程更加复杂，计算速度要慢一点。MD5 已于 2004 年被成功碰撞，其安全性已不足应用于商业场景。。 SHA 算法由美国国家标准与技术院（National Institute of Standards and Technology，NIST）征集制定。首个实现 SHA-0 算法于 1993 年问世，1998 年即遭破解。随后的修订版本 SHA-1 算法在 1995 年面世，它的输出为长度 160 位的 Hash 值，安全性更好。SHA-1 设计采用了 MD4 算法类似原理。SHA-1 已于 2005 年被成功碰撞，意味着无法满足商用需求。 为了提高安全性，NIST 后来制定出更安全的 SHA-224、SHA-256、SHA-384，和 SHA-512 算法（统称为 SHA-2 算法）。新一代的 SHA-3 相关算法也正在研究中。 目前， MD5和SHA1已经被破解，一半推荐使用SHA2-256或更安全的算法 对称加密算法定义 对称加密算法，顾名思义，加密和解密过程的密钥是相同的。该类算法的优点是加解密效率，和加密强度强度都很高。 缺点是参与方需要提前持有密钥，一旦有人泄露则系统安全性被破坏；另外如何在不安全通道中提前分发密钥也是个问题，需要借助额外的 Diffie–Hellman 协商协议或非对称加密算法来实现。 对称密码从实现原理上可以分为两种：分组密码和序列密码。前者将明文切分为定长数据块作为基本加密单位，应用最为广泛。后者则每次只对一个字节或字符进行加密处理，且密码不断变化，只用在一些特定领域（如数字媒介的加密）。 分组对称加密算法 DES（Data Encryption Standard）：经典的分组加密算法，最早是 1977 年美国联邦信息处理标准（FIPS）采用 FIPS-46-3，将 64 位明文加密为 64 位的密文。其密钥长度为 64 位（包括 8 位校验码），现在已经很容易被暴力破解； 3DES：三重 DES 操作：加密 –&gt; 解密 –&gt; 加密，处理过程和加密强度优于 DES，但现在也被认为不够安全； AES（Advanced Encryption Standard）：由美国国家标准研究所（NIST）采用，取代 DES 成为对称加密实现的标准，1997~2000 年 NIST 从 15 个候选算法中评选 Rijndael 算法（由比利时密码学家 Joan Daemon 和 Vincent Rijmen 发明）作为 AES，标准为 FIPS-197。AES 也是分组算法，分组长度为 128、192、256 位三种。AES 的优势在于处理速度快，整个过程可以数学化描述，目前尚未有有效的破解手段； IDEA（International Data Encryption Algorithm）：1991 年由密码学家 James Massey 与来学嘉共同提出。设计类似于 3DES，密钥长度增加到 128 位，具有更好的加密强度。 序列加密算法 RC4算法: 可以通过“一次性密码本”的对称加密处理。即通信双方每次使用跟明文等长的随机密钥串对明文进行加密处理。序列密码采用了类似的思想，每次通过伪随机数生成器来生成伪随机密钥串。 加解密原理 用相同的密钥对原文进行加密和解密 加密过程: 密钥 + 原文 =&gt; 密文 解密过程: 密文 + 密钥 =&gt; 原文 缺点 无法确保密钥被安全的传递 无法保证数据不会被篡改 非对称加密定义 非对称加密是现代密码学的伟大发明，它有效解决了对称加密需要安全分发密钥的问题。 顾名思义，非对称加密中，加密密钥和解密密钥是不同的，分别被称为公钥（Public Key）和私钥（Private Key）。私钥一般通过随机数算法生成，公钥可以根据私钥生成。 其中，公钥一般是公开的，他人可获取的；私钥则是个人持有并且要严密保护，不能被他人获取。 非对称加密算法优点是公私钥分开，无需安全通道来分发密钥。缺点是处理速度（特别是生成密钥和解密过程）往往比较慢，一般比对称加解密算法慢 2~3 个数量级；同时加密强度也往往不如对称加密。 非对称加密算法的安全性往往基于数学问题，包括大数质因子分解、离散对数、椭圆曲线等经典数学难题。 代表算法包括：RSA、ElGamal、椭圆曲线（Elliptic Curve Crytosystems，ECC）、SM2 等系列算法。 常用非对称加密算法 RSA：经典的公钥算法，1978 年由 Ron Rivest、Adi Shamir、Leonard Adleman 共同提出，三人于 2002 年因此获得图灵奖。算法利用了对大数进行质因子分解困难的特性，但目前还没有数学证明两者难度等价，或许存在未知算法可以绕过大数分解而进行解密。 ElGamal：由 Taher ElGamal 设计，利用了模运算下求离散对数困难的特性，比 RSA 产生密钥更快。被应用在 PGP 等安全工具中。 椭圆曲线算法（Elliptic Curve Cryptography，ECC）：应用最广也是强度最早的系列算法，基于对椭圆曲线上特定点进行特殊乘法逆运算（求离散对数）难以计算的特性。最早在 1985 年由 Neal Koblitz 和 Victor Miller 分别独立提出。ECC 系列算法具有多种国际标准（包括 ANSI X9.63、NIST FIPS 186-2、IEEE 1363-2000、ISO/IEC 14888-3 等），一般被认为具备较高的安全性，但加解密过程比较费时。其中，密码学家 Daniel J.Bernstein 于 2006 年提出的 Curve25519/Ed25519/X25519 等算法（分别解决加密、签名和密钥交换），由于其设计完全公开、性能突出等特点，近些年引起了广泛关注和应用。 SM2（ShangMi 2）：中国国家商用密码系列算法标准，由中国密码管理局于 2010 年 12 月 17 日发布，同样基于椭圆曲线算法，一般认为其安全强度优于 RSA 系列算法。 非对称加密算法适用于签名场景或密钥协商过程，但不适于大量数据的加解密。除了 SM2 之外，大部分算法的签名速度要比验签速度慢（1~2个数量级）。 RSA 类算法被认为已经很难抵御现代计算设备的破解，一般推荐商用场景下密钥至少为 2048 位。如果采用安全强度更高的椭圆曲线算法，256 位密钥即可满足绝大部分安全需求。 性质 公钥用于加密, 私钥用于解密 公钥由私钥生成，私钥可以推导出公钥 从公钥无法推导出私钥 优点 解决了密钥传输中的安全性问题 解决了数据源确定性的问题 缺点 没有解决数据可以中途被篡改的问题 消息认证码与数字签名消息认证码和数字签名技术通过对消息的摘要进行加密，可以防止消息被篡改和认证身份。 消息认证码定义 消息认证码（Hash-based Message Authentication Code，HMAC），利用对称加密，对消息完整性（Integrity）进行保护。 基本过程为对某个消息，利用提前共享的对称密钥和 Hash 算法进行处理，得到 HMAC 值。该 HMAC 值持有方可以向对方证明自己拥有某个对称密钥，并且确保所传输消息内容未被篡改。 典型的 HMAC 生成算法包括 K，H，M 三个参数。K 为提前共享的对称密钥，H 为提前商定的 Hash 算法（如 SHA-256），M 为要传输的消息内容。三个参数缺失了任何一个，都无法得到正确的 HMAC 值。 消息认证码可以用于简单证明身份的场景。如 Alice、Bob 提前共享了 K 和 H。Alice 需要知晓对方是否为 Bob，可发送一段消息 M 给 Bob。Bob 收到 M 后计算其 HMAC 值并返回给 Alice，Alice 检验收到 HMAC 值的正确性可以验证对方是否真是 Bob。当然，消息认证码起作用的前提是网络中没有中间人攻击的情况，假设网络是安全的，因此在公网中，一半不会使用消息认证码来进行身份验证； 消息认证码的主要问题是需要提前共享密钥，并且当密钥可能被多方同时拥有（甚至泄露）的场景下，无法追踪消息的真实来源。如果采用非对称加密算法，则能有效的解决这个问题，即数字签名。 缺点 不能解决密钥安全传递的问题 数字签名定义 类似在纸质合同上进行签名以确认合同内容和证明身份，数字签名既可以证实某数字内容的完整性，又可以确认其来源（即不可抵赖，Non-Repudiation）。 一个典型的场景是，Alice 通过信道发给 Bob 一个文件（一份信息），Bob 如何获知所收到的文件即为 Alice 发出的原始版本？Alice 可以先对文件内容进行摘要，然后用自己的私钥对摘要进行加密（签名），之后同时将文件和签名都发给 Bob。Bob 收到文件和签名后，用 Alice 的公钥来解密签名，得到数字摘要，与对文件进行摘要后的结果进行比对。如果一致，说明该文件确实是 Alice 发过来的（因为别人无法拥有 Alice 的私钥），并且文件内容没有被修改过（摘要结果一致）。 理论上所有的非对称加密算法都可以用来实现数字签名，实践中常用算法包括 1991 年 8 月 NIST 提出的 DSA（Digital Signature Algorithm，基于 ElGamal 算法）和安全强度更高的 ECSDA（Elliptic Curve Digital Signature Algorithm，基于椭圆曲线算法）等。 除普通的数字签名应用场景外，针对一些特定的安全需求，产生了一些特殊数字签名技术，包括盲签名、多重签名、群签名、环签名等。 数字签名的类型 盲签名 盲签名（Blind Signature），1982 年由 David Chaum 在论文《Blind Signatures for Untraceable Payment》中提出。签名者需要在无法看到原始内容的前提下对信息进行签名。 盲签名可以实现对所签名内容的保护，防止签名者看到原始内容；另一方面，盲签名还可以实现防止追踪（Unlinkability），签名者无法将签名内容和签名结果进行对应。典型的实现包括 RSA 盲签名算法等。 多重签名 多重签名（Multiple Signature），即 n 个签名者中，收集到至少 m 个（n &gt;= m &gt;= 1）的签名，即认为合法。 其中，n 是提供的公钥个数，m 是需要匹配公钥的最少的签名个数。 多重签名可以有效地被应用在多人投票共同决策的场景中。例如双方进行协商，第三方作为审核方。三方中任何两方达成一致即可完成协商。 比特币交易中就支持多重签名，可以实现多个人共同管理某个账户的比特币交易。 群签名 群签名（Group Signature），即某个群组内一个成员可以代表群组进行匿名签名。签名可以验证来自于该群组，却无法准确追踪到签名的是哪个成员。 群签名需要存在一个群管理员来添加新的群成员，因此存在群管理员可能追踪到签名成员身份的风险。 群签名最早在 1991 年由 David Chaum 和 Eugene van Heyst 提出。 环签名 环签名（Ring Signature），由 Rivest，Shamir 和 Tauman 三位密码学家在 2001 年首次提出。环签名属于一种简化的群签名。 签名者首先选定一个临时的签名者集合，集合中包括签名者自身。然后签名者利用自己的私钥和签名集合中其他人的公钥就可以独立的产生签名，而无需他人的帮助。签名者集合中的其他成员可能并不知道自己被包含在最终的签名中。 环签名在保护匿名性方面也具有很多用途。 优点 解决了密钥传输中的安全性问题 解决了数据中途可能被篡改的问题 解决了数据源验证的问题 证书授权中心-CA定义 对于非对称加密算法和数字签名来说，很重要的步骤就是公钥的分发。理论上任何人都可以获取到公开的公钥。然而这个公钥文件有没有可能是伪造的呢？传输过程中有没有可能被篡改呢？一旦公钥自身出了问题，则整个建立在其上的的安全性将不复成立。 数字证书机制正是为了解决这个问题，它就像日常生活中的证书一样，可以确保所记录信息的合法性。比如证明某个公钥是某个实体（个人或组织）拥有，并且确保任何篡改都能被检测出来，从而实现对用户公钥的安全分发。 根据所保护公钥的用途，数字证书可以分为加密数字证书（Encryption Certificate）和签名验证数字证书（Signature Certificate）。前者往往用于保护用于加密用途的公钥；后者则保护用于签名用途的公钥。两种类型的公钥也可以同时放在同一证书中。 一般情况下，证书需要由证书认证机构（Certification Authority，CA）来进行签发和背书。权威的商业证书认证机构包括 DigiCert、GlobalSign、VeriSign 等。用户也可以自行搭建本地 CA 系统，在私有网络中进行使用。 CA解决的问题 CA解决了电子商务中公钥的可信度的问题 负责证明 “我确实是我” CA是受信任的第三方，公钥的合法性校验 CA证书的内容 证书的持有人的公钥 证书授权中心的名称 证书的有效期 证书授权中心的数字签名 区块链的密码学应用区块链使用的密码学函数区块链技术的运行中使用了多项密码学函数，其中最主要的函数包括以下算法 哈希算法 数字签名 零知识证明 哈希算法在区块链系统中的应用哈希函数的特性 确定性：无论在同一个哈希函数中解析多少次，输入同一个A总是能得到相同的输出h(A)。 高效运算：计算哈希值的过程是高效的。 抗原像攻击（隐匿性）：对一个给定的输出结果h(A)，想要逆推出输入A，在计算上是不可行的。 抗碰撞性（抗弱碰撞性）：对任何给定的A和B，找到满足B≠A且h(A)=h(B)的B，在计算上是不可行的。 细微变化影响：任何输入端的细微变化都会对哈希函数的输出结果产生剧烈影响。 谜题友好性：对任意给定的Hash码Y和输入值x而言，找到一个满足h(k|x)=Y的k值在计算上是不可行的。 区块链数据结构区块链账本的数据结构和链表结构比较类似， 只不过区块指向的是老区块头部的hash值； 区块链的构成如下图： 区块链本质上是一个链表，其中的每个新区块都包含一个哈希指针。指针指向前一区块及其含有的所有数据的哈希值。假设修改某一个区块内容，即使是很微小的变化，那也会对其hash值产生很大的影响，导致区块不能和下一个区块组成链； 因此，每一个区块之间都有了不可篡改的特性。 梅克耳树默克尔树（又叫哈希树）是一种典型的二叉树结构，由一个根节点、一组中间节点和一组叶节点组成。默克尔树最早由 Merkle Ralf 在 1980 年提出，曾广泛用于文件系统和 P2P 系统中。 其主要特点为： 最下面的叶节点包含存储数据或其哈希值。 非叶子节点（包括中间节点和根节点）都是它的两个孩子节点内容的哈希值。 进一步地，默克尔树可以推广到多叉树的情形，此时非叶子节点的内容为它所有的孩子节点的内容的哈希值。 默克尔树逐层记录哈希值的特点，让它具有了一些独特的性质。例如，底层数据的任何变动，都会传递到其父节点，一层层沿着路径一直到树根。这意味树根的值实际上代表了对底层所有数据的“数字摘要”。 默克尔树主要功能有: 1. 快速比较大量数据； 2. 快速定位修改； 3. 零知识证明； 在区块链中每个区块都有自己的梅克尔根（Merkle Root）。现在，正如你已知道的，每个区块里都包含多笔交易。如果将这些交易按线性存储，那么在所有交易中寻找一笔特定交易的过程会变得无比冗长。 这就是我们使用梅克尔树的原因， 让交易在区块中变得容易寻找，由于hash的微细变化影响的特性，也让交易不能被修改。 在梅克尔树中，所有的交易通过哈希算法都能向上追溯至同一根。这就使得搜索变得非常容易。因此，如果想要在区块里面找到某一个指定的数据，可以直接通过梅克尔树里的哈希值来进行搜索，而不需要遍历所有的交易进行搜索。 比如，先要验证红色圆圈的交易数据是否在区块内，只需要提供蓝色圆圈的交易，就可以根据最终生成的hash值和梅克耳根对比，如果结果相同，就说明交易确实属于该区块，并且是正确的。 挖矿加密谜题被用来挖掘新的区块，因此哈希算法仍然至关重要。其工作原理是调整难度值的设定。随后，一个被命名为“nonce”的随机字符串被添加到新区块的哈希值上，然后被再次哈希。接着，再来检验其是否低于已设定的难度值水平。如果低于，那么产生的新区块会被添加至链上，而负责挖矿的矿工就会获得奖励。如果没有低于，则矿工继续修改随即字符串“nouce”，直至低于难度值水平的值出现。 正如你所见，哈希算法是区块链和加密经济学中一个至关重要的部分。 工作量证明概念 当矿工们通过“挖矿”来产生新区块并添加至区块链上时，其中验证及添加区块涉及到的共识系统被称为“工作量证明”。矿工们使用庞大的计算机算力来解决这道密码学谜题，而难度值决定了这道题的所需要的计算量。这是区块链技术中最具开拓意义的机制之一。早期的去中心化点对点数字货币系统之所以会失败，是由于“拜占庭将军问题”导致的，而工作量证明的共识系统为该问题提供了一种解决方案。 工作量证明实际上就是对hash算法消耗的算力的应用，让攻击者很难去在规定时间集中如此大的算力篡改数据内容； 拜占庭将军问题 假设有一群拜占庭将军想要攻打一座城市，他们将面临两个不同的问题： 每个将军及其军队在地理上相距甚远，因此不可能通过中央来统一指挥，这使得协同作战变得异常困难。 被攻打的城市拥有一只庞大的军队，他们能获得胜利的唯一方式是所有人在同一时刻一同发起进攻。 为了让合作成功，位于城堡左边的军队派遣一位信使，向城堡右边的军队发送了一则内容为“周三攻击”的信息， 如果所有军队都准备好了， 那就可以确定周三攻击。但是，如果邮编的军队没有做好攻击准备，并让信使携带一则内容为“不，周五攻击”的信息返回。而信使需要通过穿越被攻打的城市返回到左边的军队； 问题就来了。这位信使身上可能会发生很多事情。比如，他有可能被抓获、泄露信息、或被攻打的城市杀害后将其替换了。这将导致军队获得被篡改过的信息，从而使作战计划无法达成一致而失败。 上述例子对区块链有明显借鉴意义。区块链是一个巨型网络，你要如何信任他们呢？如果你想从钱包里发送4个以太币给某人，你如何确认网络中的某人不会篡改信息，将4个以太币改成40个， 或者更改了其接受者？中本聪发明了工作量证明机制来绕过拜占庭将军问题。其运行原理是：假设左边的军队想要发送内容为“周三进攻”的信息给右边的军队，他们需要执行如下步骤： 首先，他们会给初始文本添加一个“nonce”，这个nonce可以是任何一个随机十六进制值。 其次，他们将添加了“nonce”的文本进行哈希，得到一个结果。假设说他们决定仅当哈希结果前5位是零的时候，才进行信息共享。 如果哈希结果满足条件，他们就会让信使带着有哈希结果的信息出发。否则，他们会持续随机改变nonce的值，直到得到想要的结果。这一过程不仅冗长耗时，且占用大量的算力。 如果敌人抓到了信使，并企图篡改信息，那么根据哈希函数的特性，哈希结果将会剧烈变化。如果城市右边的将军看到信息没有以规定数量的0作为开头，那么他们就会叫停攻击。 工作量证明的本质 寻找一个符合哈希目标的nonce值，是一个非常困难且耗时的过程， 想要篡改需要浪费大量的算例。 然而，验证结果中是否有作恶行为却是非常简单的。 数字签名在区块链系统中的应用概念假设Alan想把信息“m”发送出去，Alan有一把私钥Ka-和一把公钥Ka+。那么，当他把信息发送给Tyrone时，他会用私钥将该条信息加密，于是信息变成了Ka-(m)。当Tyrone收到这条信息时，他可以使用Alan的公钥来取回信息，Ka+(Ka-(m))，于是便得到了原始信息“m”; 签名的作用就是用来保证信息在传输的过程中没有被修改过， 也可以用来验证信息确实是从发送者处发送出来。 数字签名使用的过程 Alan有一笔交易“m”，并且Tyrone知道他正在接收该笔交易。 Alan对m进行哈希运算，得到h(m)。 Alan用自己的私钥对哈希结果进行加密，得到Ka-(h(m))。 Alan将加密数据发送给Tyrone。 Tyrone使用Alan的公钥来解密，Ka+(Ka-(h(m)))，并得到原来的哈希结果h(m)。 Tyrone用已知的“m”进行哈希运算，可以得到h(m)。 哈希函数的确定性特征决定了如果h(m)=h(m)，就意味着这笔交易是真实有效的。 数字签名的特性 可验证性：如果加密信息能够用Alan的公钥进行解密，那就可以100%确定是Alan发送了该条信息。 不可伪造性：如果说有其他人，例如Bob，拦截了该条信息，并用自己的私钥发送了一条自己的信息，那么Alan的公钥将无法对其解密。Alan的公钥只能用来解密Alan用自己的私钥加密过的信息。 不可抵赖性：同样的，如果Alan宣称，“我没有发送信息，是Bob发的”，但Tyrone却能够用Alan的公钥来解密信息，那就证明Alan在撒谎。如此，Alan就无法收回他之前发出的信息，并将其归咎于他人。 零知识证明概念(ZKP)零知识证明是这样一个过程， 证明着不向验证着提供任何额外的信息的前提下， 使验证着相信某个论断是正确的。 ZKP意味着A可以向B证明，他知道特定的信息，而不必告诉对方自己具体知道些什么，这尤为有用，因为这将为证明者提供一层额外的隐私保护。 在区块链系统中，为了做到完全的隐私，对数据的验证可以通过零知识证明，对交易的详情等可以做到完全的隐藏，避免泄露信息； 零知识证明具备的性质 完整性：如果陈述属实，那么诚实的验证者能被诚实的证明者说服。 可靠性：如果证明者不诚实，他们无法通过说谎来说服验证者相信陈述是可靠的。零知识：如果陈述属实，那么验证者无法得知陈述的内容是什么。 举个栗子，证明者（P）对验证者（V）说，他知道洞穴后面暗门的密码，并提出在不向验证者透露密码的情况下证明此事。那么，其验证过程如下图所示： 证明者可以走路径A或者路径B，假设他们一开始决定通过路径A到达暗门。同时，验证者V来到入口，他对证明者选择哪条路径并不知情，并宣称他们希望见到证明者在路径B出现。 如图所示，证明者确实出现在路径B上，但万一这仅是巧合呢？也有可能是证明者凭运气在出发时选择了路径B，却因不知道密码被困在了门口。 所以，我们需要通过多次试验来确定测试的有效性。如果证明者每次都能出现在正确的路径上，那么证明者的确可以在不向验证者透露密码的情况下，证明自己知道密码。 零知识证明使用实例许多基于区块链的技术都在使用Zk-Snarks。事实上，以太坊在大都会阶段就计划引入Zk-Snarks，并且将其加入以太坊的功能库。Zk-Snarks是“零知识简洁无交互知识认证”的简称，是一种在无需泄露数据本身情况下证明某些数据运算的一种零知识证明。 以上内容可用来生成一个证明，通过对每笔交易创建一个简单的快照来验证其有效性。这足以向信息接收方证明交易的有效性，而无需泄露交易的实质内容。 通过零知识证明，解决了区块链的一些因为问题: 1.实现了交易的完整性和隐私性; 2.实现了系统的抽象性。由于无需展示整个交易内部的工作方式，因此系统非常易用。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊技术栈]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F4.%E4%BB%A5%E5%A4%AA%E5%9D%8A%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[以太坊的介绍以太坊简介以太坊（Ethereum）项目的最初目标，是打造一个运行智能合约的平台（Platform for Smart Contract）。该平台支持图灵完备的应用，按照智能合约的约定逻辑自动执行，理想情况下将不存在故障停机、审查、欺诈，以及第三方干预等问题。 以太坊平台目前支持 Golang、C++、Python 等多种语言实现的客户端。由于核心实现上基于比特币网络的核心思想进行了拓展，因此在很多设计特性上都与比特币网络十分类似。 基于以太坊项目，以太坊团队目前运营了一条公开的区块链平台——以太坊网络。智能合约开发者使用官方提供的工具和以太坊专用应用开发语言 Solidity，可以很容易开发出运行在以太坊网络上的“去中心化”应用（Decentralized Application，DApp）。这些应用将运行在以太坊的虚拟机（Ethereum Virtual Machine，EVM）里。用户通过以太币（Ether）来购买燃料（Gas），维持所部署应用的运行。 以太坊和比特币的区别以太坊的目的是创建一种去中心化应用的协议，提供一套对大量去中心化应用程序非常有用的新方案，特别强调快速开发，对小的和少数人使用的应用也非常安全（小而使用人少的应用容易被51%攻破），以及不同应用程序之间能够有效的互动。以太坊用过建立在本质上是抽象的基础层来完成这一工作； 一个区块链其内置了图灵完备的编程语言，允许任何人编写智能合约和去中心化的应用程序，在这些应用程序中，他们可以创建任意的属于他们自己的规则、交易格式和状态转换函数。名字币的一个简单版本在以太坊可以用两行代码来编写完成，而其他协议如货币和信用系统则可以用不到 20 行的代码来构建。智能合约-包含价值而且只有满足某些条件才能打开的加密箱子-也能在我们的平台上构建，并且因为图灵完备性、价值知晓（value-awareness）、区块链知晓（blockchain-awareness）和多状态所增加的力量，远比特币脚本所能提供的功能强大得多； 以太坊主要特点以太坊区块链底层也是一个类似比特币网络的 P2P 网络平台，智能合约运行在网络中的以太坊虚拟机里。网络自身是公开可接入的，任何人都可以接入并参与网络中数据的维护，提供运行以太坊虚拟机的资源。 跟比特币项目相比，以太坊区块链的技术特点主要包括： 支持图灵完备的智能合约，设计了编程语言 Solidity 和虚拟机 EVM； 选用了内存需求较高的哈希函数，避免出现强算力矿机、矿池攻击； 叔块（Uncle Block）激励机制，降低矿池的优势，并减少区块产生间隔（10 分钟降低到 15 秒左右）； 采用账户系统和世界状态，而不是 UTXO，容易支持更复杂的逻辑； 通过 Gas 限制代码执行指令数，避免循环执行攻击； 支持 PoW 共识算法，并计划支持效率更高的 PoS 算法。 此外，开发团队还计划通过分片（Sharding）方式来解决网络可扩展性问题。 这些技术特点，解决了比特币网络在运行中被人诟病的一些问题，让以太坊网络具备了更大的应用潜力。 以太坊的核心概念智能合约什么是智能合约 智能合约（Smart Contract）是以太坊中最为重要的一个概念，即以计算机程序的方式来缔结和运行各种合约。最早在上世纪 90 年代，Nick Szabo 等人就提出过类似的概念，但一直依赖因为缺乏可靠执行智能合约的环境，而被作为一种理论设计。区块链技术的出现，恰好补充了这一缺陷。 以太坊支持通过图灵完备的高级语言（包括 Solidity、Serpent、Viper）等来开发智能合约。智能合约作为运行在以太坊虚拟机（Ethereum Virual Machine，EVM）中的应用，可以接受来自外部的交易请求和事件，通过触发运行提前编写好的代码逻辑，进一步生成新的交易和事件，可以进一步调用其它智能合约。 智能合约的执行结果可能对以太坊网络上的账本状态进行更新。这些修改由于经过了以太坊网络中的共识，一旦确认后无法被伪造和篡改。 只能合约作用 智能合约是在以太坊虚拟机上运行的应用程序。这是一个分布的“世界计算机”，计算能力由所有以太坊节点提供。提供计算能力的任何节点都将以Ether数字货币作为资源支付。 他们被命名为智能合约，因为您可以编写满足要求时自动执行的“合同”。 例如，想象一下在以太坊之上建立一个类似Kickstarter的众筹服务。有人可以建立一个以太坊智能合约，将资金汇集到别人身上。这个智能合约可以写成这样的话：当将100,000美元的货币添加到池中时，它将全部发送给收件人。或者，如果一个月内没有达到100,000美元的门槛，所有的货币都将被发回给货币的原始持有人。当然，这将使用以太币代替美元。 这一切都将根据智能合同代码进行，智能合同代码可自动执行交易，而无需可信任的第三方持有货币并签署交易。例如，Kickstarter在5％的付款处理费之上收取5％的费用，这意味着在$100,000的众筹项目中将收取8000到10000美元的费用。智能合约不需要向像Kickstarter这样的第三方支付费用。 智能合约可以用于许多不同的事情。开发人员可以创建智能合约，为其他智能合约提供功能，类似于软件库的工作方式。或者，智能合约可以简单地用作应用程序来存储以太坊区块链上的信息。 为了真正执行智能合同代码，有人必须发送足够的以太网代币作为交易费 - 多少取决于所需的计算资源。这为以太坊节点参与并提供计算能力付出了代价。 账户ACCOUNT账户介绍 在之前章节中，笔者介绍过比特币在设计中并没有账户（Account）的概念，而是采用了 UTXO 模型记录整个系统的状态。任何人都可以通过交易历史来推算出用户的余额信息。而以太坊则采用了不同的做法，直接用账户来记录系统状态。每个账户存储余额信息、智能合约代码和内部数据存储等。以太坊支持在不同的账户之间转移数据，以实现更为复杂的逻辑。 具体来看，以太坊账户分为两种类型：合约账户（Contracts Accounts）和外部账户（Externally Owned Accounts，或 EOA）。 合约账户：存储执行的智能合约代码，只能被外部账户来调用激活； 外部账户：以太币拥有者账户，对应到某公钥。账户包括 nonce、balance、storageRoot、codeHash 等字段，由个人来控制。 在以太坊中，全网的状态是由被“账户”的对象组成的，账户之间可以直接的进行价值和信息的转移，一个以太坊的账户包含下面 4 个字段: 随机数, 一个计数器，用以确保每个交易都只会被处理一次 账户当前的以太币额度 账户的合约代码, 如果有的话 这个账户的 存储 (默认空) 合约账户外部账户之间关系 当合约账户被调用时，存储其中的智能合约会在矿工处的虚拟机中自动执行，并消耗一定的燃料。燃料通过外部账户中的以太币进行购买。 对于大多数用户来说，最基本的区别在于，用户掌握着 EOA 账号，因为用户掌握着控制 EOA 账号的私钥。而合约账号由内部程序代码来控制的，当然掌控私钥的 EOA 账户可以通过编写特定的程序代码来掌控合约账户。流行的术语“智能合约”就是合约账号中的代码，当一个交易被发送到该账户时，合约中的代码就会被执行。用户可以通过把代码部署到区块链中来创建一个新合约，也即创建了一个新的合约账户。 合约账户只有在 EOA 账户发出一个指令的时候才会去执行一个操作。所以一个合约账户是不可能自己去执行一个操作的，如生产一个随机数或执行一个 API 调用等，它只有在 EOA 账户作出确认的情况下才会去做这些事情。这是因为以太坊要求节点能够对计算的结果无论对错都达成一致，这就对操作有了一个必定会执行的要求。 从外部拥有账户到合约账户的消息会激活合约账户的代码，允许它执行各种动作。（比如转移代币，写入内部存储，挖出一个新代币，执行一些运算，创建一个新的合约等等）。 以太币以太币（Ether）是以太坊网络中的货币。 以太币主要用于购买燃料，支付给矿工，以维护以太坊网络运行智能合约的费用。以太币最小单位是 wei，一个以太币等于 10^18 个 wei。 以太币同样可以通过挖矿来生成，成功生成新区块的以太坊矿工可以获得 3 个以太币的奖励，以及包含在区块内交易的燃料费用。用户也可以通过交易市场来直接购买以太币。 目前每年大约可以通过挖矿生成超过一千万个以太币，单个以太币的市场价格目前超过 300 美金。 燃料Gas燃料（Gas），控制某次交易执行指令的上限。每执行一条合约指令会消耗固定的燃料。当某个交易还未执行结束，而燃料消耗完时，合约执行终止并回滚状态。 Gas 可以跟以太币进行兑换。需要注意的是，以太币的价格是波动的，但运行某段智能合约的燃料费用可以是固定的，通过设定 Gas 价格等进行调节。 以太坊虚拟机以太坊是一个可编程的区块链，不仅仅是给用户一些预定义操作（如比特币只交易），以太坊允许用户创建属于他们自己的复杂的操作。以太坊作为一个平台为不同的区块链应用提供服务。 狭义来说，以太坊是一系列协议，其核心就是一个以太坊虚拟机，能执行遵守协议的任何复杂的代码。以太坊虚拟机是图灵完备的，开发者可以在虚拟机上使用像 javascript，python 这样的友好的编程语言来创建应用。 和任何的区块链一样，以太坊包含了一个点对点的网络协议。这以太坊区块链是被链接着这个网络的各个节点维护和更新的。网络中的各个节点的虚拟机都执行相同的指令来共同维护区块数据库，因为这个原因，以太坊有时候被人称为“世界计算机”。 以太坊全网的大规模并行计算不是只为了提计算效率，而是为了保证全网的数据一致性。实际上，这使得在以太网上的 运算要比传统的电脑慢的多，成本也昂贵得多。全网中的每一台虚拟机的运行都是为确保全网数据库的一致性。去中心化的一致性给全网极端的容错能力;抗审查能力和永不宕机能力等。 以太坊账户以太坊的基本单元是账号。每一个账户都有一个 20 个字节长度的地址 。以太坊区块链跟踪每一个账号的状态，区块链上所有状态的转移都是账户之间的令牌（令牌即以太币）和信息的转移。以太坊有 2 种账户类型： 外部账号，简称 EOA,是由私钥来控制的。 有一个以太币的余额 可以发送交易(以太币转账或者激活合约代码) 通过私钥控制 没有相关联的代码 合约帐户,由合约代码来控制,且只能由一个 EOA 账号来操作 有一个以太币余额 相关联的代码 代码执行是通过交易后者其他的合约发送的call来激活 当被执行时 – 运行在随机复杂度(图灵完备性) – 只能操作其拥有的特定存储， 例如可以拥有其永久的state – 可以call其他合约 运行环境和语言运行环境以太坊采用以太坊虚拟机作为智能合约的运行环境。以太坊虚拟机是一个隔离的轻量级虚拟机环境，运行在其中的智能合约代码无法访问本地网络、文件系统或其它进程。 对同一个智能合约来说，往往需要在多个以太坊虚拟机中同时运行多份，以确保整个区块链数据的一致性和高度的容错性。另一方面，这也限制了整个网络的容量。 开发语言以太坊为编写智能合约设计了图灵完备的高级编程语言，降低了智能合约开发的难度。 目前 Solidity 是最常用的以太坊合约编写语言之一。 智能合约编写完毕后，用编译器编译为以太坊虚拟机专用的二进制格式（EVM bytecode），由客户端上传到区块链当中，之后在矿工的以太坊虚拟机中执行。 消息和交易以太坊交易交易（Transaction），在以太坊中是指从一个账户到另一个账户的消息数据。消息数据可以是以太币或者合约执行参数。 名词“交易”在以太坊中是指签名的数据包，这个数据包中存储了从外部账户发送的消息，以太坊是用交易作为操作的最小单位，交易包含以下内容: 消息的接受者 一个可以识别发送者的签名 发送方给接收方的以太币数量 一个可选的数据字段 一个 STARTGAS 值, 表示执行这个交易允许消耗的最大计算步骤 一个 GASPRICE 值, 表示发送方的每个计算步骤的费用 前面三个是每一个加密货币都有的标准字段。默认情况下第四个数据字段没有任何功能，但是合约可以访问这里的数据；举个例子，如果一个合约是在一个区块链上提供域名注册服务的，那么它就会想把这数据字段中的数据解析成 2 个字段，第一个字段是域名，第二个字段是域名对应的 IP 地址。这个合约会从数据字段中读取这些值，然后适当把它们保存下来。 这个 STARTGAS 和 GASPRICE 字段 是以太坊的预防拒绝式攻击用的，非常重要。为了防止在代码中出现意外或敌对的无限循环或其他计算浪费，每个交易都需要设置一个限制，以限制它的计算总步骤是一个明确的值。这计算的基本单位是“汽油（gas）”； 通常，一个计算成本是一个 1 滴汽油，但是一些操作需要消耗更多的汽油，因为它们的计算成本更高。在交易数据中每一个字节需要消耗 5 滴汽油。这样做的目的是为了让攻击者为他们所消耗的每一种资源，包括计算，带宽和存储支付费用；所以消耗网络资源越多，则交易成本就越大。 交易模型UTXO模型UTXO 模型中，交易只是代表了 UTXO 集合的变更。而账户和余额的概念是在 UTXO 集合上更高的抽象，账号和余额的概念只存在于钱包中。 优点： 计算是在链外的，交易本身既是结果也是证明。节点只做验证即可，不需要对交易进行额外的计算，也没有额外的状态存储。交易本身的输出 UTXO 的计算是在钱包完成的，这样交易的计算负担完全由钱包来承担，一定程度上减少了链的负担。 除 Coinbase 交易外，交易的 Input 始终是链接在某个 UTXO 后面。交易无法被重放，并且交易的先后顺序和依赖关系容易被验证，交易是否被消费也容易被举证。 UTXO 模型是无状态的，更容易并发处理。 对于 P2SH 类型的交易，具有更好的隐私性。交易中的 Input 是互不相关联的，可以使用 CoinJoin 这样的技术，来增加一定的隐私性。 缺点： 无法实现一些比较复杂的逻辑，可编程性差。对于复杂逻辑，或者需要状态保存的合约，实现难度大，且状态空间利用率比较低。 当 Input 较多时，见证脚本也会增多。而签名本身是比较消耗 CPU 和存储空间的。 ACCOUNT模型出于智能合约的便利考虑，以太坊采用了账户的模型，状态可以实时的保存到账户里，而无需像比特币的 UXTO 模型那样去回溯整个历史。 对于 Account 模型，Account 模型保存了世界状态，链的状态一般在区块中以 StateRoot 和 ReceiptRoot 等形式进行共识。交易只是事件本身，不包含结果，交易的共识和状态的共识本质上可以隔离的。 优点： 合约以代码形式保存在 Account 中，并且 Account 拥有自身状态。这种模型具有更好的可编程性，容易开发人员理解，场景更广泛。 批量交易的成本较低。设想矿池向矿工支付手续费，UTXO 中因为每个 Input 和 Out 都需要单独 Witness script 或者 Locking script，交易本身会非常大，签名验证和交易存储都需要消耗链上宝贵的资源。而 Account 模型可以通过合约的方式极大的降低成本。 缺点： Account 模型交易之间没有依赖性，需要解决重放问题。 对于实现闪电网络/雷电网络，Plasma 等，用户举证需要更复杂的 Proof 证明机制，子链向主链进行状态迁移需要更复杂的协议。 UTXO和ACCOUNT区别 计算问题 UTXO 交易本身对于区块链并没有复杂的计算，这样简单的讲其实并不完全准确，原因分有两个，一是 Bitcoin 本身的交易多为 P2SH，且 Witness script 是非图灵完备的，不存在循环语句。而对于 Account 模型，例如 Ethereum，由于计算多在链上，且为图灵完备，一般计算较为复杂，同时合约安全性就容易成为一个比较大的问题。当然是否图灵完备对于是否是账户模型并没有直接关联。但是账户模型引入之后，合约可以作为一个不受任何人控制的独立实体存在，这一点意义重大。 UTXO更易并发 在 UTXO 模型中，世界状态即为 UTXO 的集合，节点为了更快的验证交易，需要在内存中存储所有的 UTXO 的索引，因此 UTXO 是非常昂贵的。对于长期不消费的 UTXO，会一直占用节点的内存。所以对于此种模型，理论上应该鼓励用户减少生产 UTXO，多消耗 UTXO。但是如果要使用 UTXO 进行并行交易则需要更多的 UTXO 作为输入，同时要产生更多的 UTXO 来保证并发性，这本质上是对网络进行了粉尘攻击。并且由于交易是在钱包内构造，所以需要钱包更复杂的设计。反观 Account 模型，每个账户可以看成是单独的互不影响的状态机，账户之间通过消息进行通信。所以理论上用户发起多笔交易时，当这些交易之间不会互相调用同一 Account 时，交易是完全可以并发执行的。 Account模型的交易重放 Ethereum 使用了在 Account 中增加 nonce 的方式，每笔交易对应一个 nonce，nonce 每次递增。这种方式虽然意在解决重放的问题，但是同时引入了顺序性问题，同时使得交易无法并行。例如在 Ethereum中，用户发送多笔交易，如果第一笔交易打包失败，将引起后续多笔交易都打包不成功。在 CITA 中我们使用了随机 nonce 的方案，这样用户的交易之间没有顺序性依赖，不会引起串联性失败，同时使得交易有并行处理的可能。 存储问题 因为 UTXO 模型中，只能在交易中保存状态。而 Account 模型的状态是在节点保存，在 Ethereum 中使用 MPT 的方式存储，Block 中只需要共识 StateRoot 等即可。这样对于链上数据，Account 模型实际更小，网络传输的量更小，同时状态在节点本地使用 MPT 方式保存，在空间使用上也更有效率。例如 A 向 B 转账，如果在 UTXO 中假设存在 2 个 Input 和2个 Output，则需要 2 个 Witness script 和 2 个 Locking script；在 Account 模型中则只需要一个签名，交易内容只包含金额即可。在最新的隔离见证实现后，Bitcoin 的交易数据量也大大减少，但是实际上对于验证节点和全节点仍然需要针对 Witness script 进行传输和验证。 轻节点获取地址状态难易 例如钱包中，需要向全节点请求所有关于某个地址的所有 UTXO，全节点可以发送部分 UTXO，钱包要验证该笔 UTXO 是否已经被消费，有一定的难度，而且钱包很难去证明 UTXO 是全集而不是部分集合。而对于 Account 模型则简单很多，根据地址找到 State 中对应状态，当前状态的 State Proof 则可以证明合约数据的真伪。当然对于 UTXO 也可以在每个区块中对 UTXO 的 root 进行验证，这一点与当前 Bitcoin 的实现有关，并非 UTXO 的特点。 综上 ​ 综上来看，Account 模型在可编程性，灵活性等方面更有优势；在简单业务和跨链上，UTXO 有其非常独到和开创性的优点。对于选择何种模型，要从具体的业务场景进行出发。 UTXO和ACCOUNT的对比 以太坊消息合约具有发送”消息”到其他合约的能力。消息是一个永不串行且只在以太坊执行环境中存在的虚拟对象。他们可以被理解为函数调用（function calls）。 一个消息包括： 明确的消息发送者 消息的接收者 一个可选的数据域，这是合约实际上的输入数据 一个GASLIMIT值，用来限制这个消息出发的代码执行可用的最大gas数量 总的来说，一个消息就像是一个交易，除了它不是由外部账户生成，而是合约账户生成。当合约正在执行的代码中运行了CALL 或者DELEGATECALL这两个命令时，就会生成一个消息。消息有的时候也被称为”内部交易”。与一个交易类似，一个消息会引导接收的账户运行它的代码。因此，合约账户可以与其他合约账户发生关系，这点和外部账户一样。有许多人会误用交易这个词指代消息，所以可能消息这个词已经由于社区的共识而慢慢退出大家的视野，不再被使用。 以太坊状态转移函数 以太坊的状态转移函数 APPLY(S,TX) -&gt; S’ 可以被定义成下面的: 检查这个交易是不是合法的 ,签名是不是合法的, 这随机数是不是匹配这个发送者的账户，如果答案是否定的，那返回错误。 用 STARTGAS * GASPRICE 计算交易的费用，并且从签名中确定这个发送者的地址。 从发送者的余额中减去费用，并且增加发送者的随机值。如果余额不够，则返回错误。 初始化 GAS = STARTGAS, 并根据这交易中的字节数拿走一定量的汽油。 把交易的值从发送的账户转移到接收者的账户。如果接收者的账户还不存在，就创建一个。如果这个接收者的账户是一个合约，那么就运行合约的代码直到完成，或者报汽油消耗光的异常。 如果值转移失败了，因为发送者没有足够多的余额，或代码执行消耗光了汽油，恢复除了支付的费用外的所有的状态，并且把这个费用添加到矿工的账户上。 另外,把所有剩下的汽油退还给发送者，然后把用于支付费用的汽油发送给矿工。举例，假设合约的代码是这样的:if !self.storage[calldataload(0)]: self.storage[calldataload(0)] = calldataload(32)注意，真实的合约代码是用底层的 EVM 代码编写的；这个列子是用一个叫 Serpent 的高级语言写的。假设这个合约的存储开始是空的，并且发送了一个交易，其中包含 10 个以太币，2000 个汽油，汽油价格是 0.001 比特币，和 64 字节的数据，其中 0-31 字节代表数字 2,32-63 字节代表字符串 CHARLIE。在这个案例中，这状态转移函数的处理如下： 检查者交易是否有效并且格式完好。 检查者交易的发送者是否至少有 2000 * 0.001 = 2 以太币。如果有，则从发送者的账户中减去 2 以太币。 初始化 汽油（gas）= 2000;假设这个交易是 170 个字节长度并且每个字节的费用是 5，那么减去 850，汽油还剩 1150。 从发送者的账户减去 10 个以太币，并且添加到合约的账户中。 运行合约的代码. 在这里例子中:检查合约的存储的第 2 个索引是否已经被使用，注意到它没 有，然后就把这数据存储的第二个索引的值设置为 CHARLIE. 假设这个操作消耗了 187 个汽 油，那么剩下的汽油总量是 1150 – 187 = 963 把 963 * 0.001 = 0.963 以太币加到发送者的账户，然后反正结果状态。 如果交易的接收端没有合约，那么这总的交易费用就简单的等于汽油的价格乘以这个交易的字节长 度，与交易一起发送的数据字段的数据将无关重要。 注意，在恢复这个方面，消息和交易的处理方式是相同的： 如果一个消息执行消耗光了汽油，那么 这消息的执行和其他被触发的执行都会被恢复，但是父类的执行不会恢复。 Gas的详细介绍什么是gas以太坊在区块链上实现了一个运行环境，被称为以太坊虚拟机（EVM）。每个参与到网络的节点都会运行都会运行EVM作为区块验证协议的一部分。他们会验证区块中涵盖的每个交易并在EVM中运行交易所触发的代码。每个网络中的全节点都会进行相同的计算并储存相同的值。合约执行会在所有节点中被多次重复，这个事实得使得合约执行的消耗变得昂贵，所以这也促使大家将能在链下进行的运算都不放到区块链上进行。对于每个被执行的命令都会有一个特定的消耗，用单位gas计数。每个合约可以利用的命令都会有一个相应的gas值。这里列了一些命令的gas消耗。 交易消耗的gas每笔交易都被要求包括一个gas limit（或startGas）和一个交易愿为单位gas支付的费用。矿工可以有选择的打包这些交易并收取这些费用。在现实中，今天所有的交易最终都是由矿工选择的，但是用户所选择支付的交易费用多少会影响到该交易被打包所需等待的时长。如果该交易由于计算，包括原始消息和一些触发的其他消息，需要使用的gas数量小于或等于所设置的gas limit，那么这个交易会被处理。如果gas总消耗超过gas limit，那么所有的操作都会被复原，但交易是成立的并且交易费任会被矿工收取。区块链会显示这笔交易完成尝试，但因为没有提供足够的gas导致所有的合约命令都被复原。所以交易里没有被使用的超量gas都会以以太币的形式打回给交易发起者。因为gas消耗一般只是一个大致估算，所以许多用户会超额支付gas来保证他们的交易会被接受。这没什么问题，因为多余的gas会被退回给你。 区块的gas limit是由在网络上的矿工决定的。与可调整的区块gas limit协议不同的是一个默认的挖矿策略，即大多数客户端默认最小区块gas limit为4,712,388。 以太坊上的矿工需要用一个挖矿软件，例如ethminer。它会连接到一个geth或者Parity以太坊客户端。Geth和Pairty都有让矿工可以更改配置的选项。这里是geth挖矿命令行选项以及Parity的选项。 估算交易消耗一个交易的交易费由两个因素组成： gasUsed：该交易消耗的总gas数量 gasPrice：该交易中单位gas的价格（用以太币计算） 交易费 = gasUsed * gasPrice gasUsed 每个EVM中的命令都被设置了相应的gas消耗值。gasUsed是所有被执行的命令的gas消耗值总和。 gasPrice 一个用户可以构建和签名一笔交易，但每个用户都可以各自设置自己希望使用的gasPrice，甚至可以是0。然而，以太坊客户端的Frontier版本有一个默认的gasPrice，即0.05e12 wei。矿工为了最大化他们的收益，如果大量的交易都是使用默认gasPrice即0.05e12 wei，那么基本上就很难又矿工去接受一个低gasPrice交易，更别说0 gasPrice交易了。 以太坊挖矿以太坊挖矿过程 这以太坊的区块链和比特币的区块链有很多相似的地方，也有很多不同的地方。这个以太坊和比特币在区块链体系中最重要的不同点是 ：以太坊的区块同时包含了交易列表和最近区块的状态。除此之外，2 个其他的值，区块的编号和难度值也存在在区块中。以太坊中最基本的区块验证算法如下： 检查上一个区块是否存在和其有效性。 检测这区块的时间戳，是不是比上一个区块的大，并且小于 15 分钟 检查这区块编号，难度值，交易根（transaction root） , 叔根（uncle root）和汽油限制是否有效 检查这区块的工作证明是否有效 把 S[0] 设置成上一个区块的末端的状态 让 TX 成为这区块的交易列表，如果有 n 个交易。则做 for 循环 For i in 0…n-1, 设置 S[i+1] = APPLY(S[i],TX[i]). 如果任何一个应用发生错误，或这区块中汽油的总的消耗达到了 GASLIMIT, 则返回一个错误. 让 S_FINAL 等于 S[n], 但是把支付给矿工的奖励添加到这区块里。 检查这个状态 S_FINAL 的默克尔树树根是不是和区块头信息中所提供的状态根是一样的。如果是，则区块有效，不然则无效。 乍看上去，这种方法似乎效率很低，因为它需要将整个状态存储在每个块中，但在现实中，效率应该与比特币相当。原因在于，状态存储在树结构中，并且每个块后，只需要修改树的一小部分。此外，由于所有的状态信息都是最后一个区块的一部分，所以不需要存储整个区块链的历史——这一策略，如果它可以应用于比特币，那么它的磁盘空间将节省 5-20 倍。 以太坊网络中交易会被验证这网络的节点收集起来。这些“矿工”在以太坊网络中收集、传播、验证和执行交易，然后整理归档这些交易，打包成一个区块，与别的矿工竞争将区块添加到区块链中，添加成功的矿工将收到奖励。通过这样的措施，鼓励人们为区块链全网提供更多的硬件和电力支持。 共识机制以太坊目前采用了基于成熟的 PoW 共识的变种算法 Ethash 协议作为共识机制。 为了防止 ASIC 矿机矿池的算力攻击，跟原始 PoW 的计算密集型 Hash 运算不同，Ethash 在执行时候需要消耗大量内存，反而跟计算效率关系不大。这意味着很难制造出专门针对 Ethash 的芯片，反而是通用机器可能更加有效。 虽然，Ethash 相对原始的 PoW 进行了改进，但仍然需要进行大量无效的运算，这也为人们所诟病。 社区已经有计划在未来采用更高效的 Proof-of-Stake（PoS）作为共识机制。相对 PoW 机制来讲，PoS 机制无需消耗大量无用的 Hash 计算，但其共识过程的复杂度要更高一些，还有待进一步的检验。]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币技术栈]]></title>
    <url>%2F2019%2F01%2F28%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F3.%E6%AF%94%E7%89%B9%E5%B8%81%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[比特币设计原理比特币网络是一个分布式的点对点网络，网络中的矿工通过“挖矿”来完成对交易记录的记账过程，维护网络的正常运行。 区块链网络提供一个公共可见的记账本，通过共识机制所有节点共同维护同一份账本，该记账本并非记录每个账户的余额，而是用来记录发生过的交易的历史信息。该设计可以避免重放攻击，即某个合法交易被多次重新发送造成攻击。 其交易是通过销毁“未被花费的交易输出”(即UTXO)， 和创建新的UTXO来实现资金的流转； 比特币可以理解为: BTC = UTXO + 共识机制 + 区块链账本 比特币交易比特币交易系统—状态转移系统比特币中没有账户的概念。因此，每次发生交易，用户需要将交易记录写到比特币网络账本中，等网络确认后即可认为交易完成。 除了挖矿获得奖励的 coinbase 交易只有输出，正常情况下每个交易需要包括若干输入和输出，未经使用（引用）的交易的输出（Unspent Transaction Outputs，UTXO）可以被新的交易引用作为其合法的输入。被使用过的交易的输出（Spent Transaction Outputs，STXO），则无法被引用作为合法输入。 因此，比特币网络中一笔合法的交易，必须是引用某些已存在交易的 UTXO（必须是属于付款方才能合法引用）作为新交易的输入，并生成新的 UTXO（将属于收款方）。 那么，在交易过程中，付款方如何证明自己所引用的 UTXO 合法？比特币中通过“签名脚本”来实现，并且指定“输出脚本”来限制将来能使用新 UTXO 者只能为指定收款方。对每笔交易，付款方需要进行签名确认。并且，对每一笔交易来说，总输入不能小于总输出。总输入相比总输出多余的部分称为交易费用（Transaction Fee），为生成包含该交易区块的矿工所获得。目前规定每笔交易的交易费用不能小于 0.0001 BTC，交易费用越高，越多矿工愿意包含该交易，也就越早被放到网络中。交易费用在奖励矿工的同时，也避免了网络受到大量攻击。 交易中金额的最小单位是“聪”，即一亿分之一（10^-8）比特币。 下图展示了一些简单的示例交易。更一般情况下，交易的输入、输出可以为多方。 交易 目的 输入 输出 签名 差额 T0 A 转给 B 他人向 A 交易的输出 B 账户可以使用该交易 A 签名确认 输入减输出，为交易服务费 T1 B 转给 C T0 的输出 C 账户可以使用该交易 B 签名确认 输入减输出，为交易服务费 … X 转给 Y 他人向 X 交易的输出 Y 账户可以使用该交易 X 签名确认 输入减输出，为交易服务费 需要注意，刚放进网络中的交易（深度为 0）并非是实时得到确认的。进入网络中的交易存在被推翻的可能性，一般要再生成几个新的区块后（深度大于 0）才认为该交易被确认。 下面分别介绍比特币网络中的重要概念和主要设计思路。 账户(地址)比特币采用了非对称的加密算法，用户自己保留私钥，对自己发出的交易进行签名确认，并公开公钥。 比特币的账户地址其实就是用户公钥经过一系列 Hash（HASH160，或先进行 SHA256，然后进行 RIPEMD160）及编码运算后生成的 160 位（20 字节）的字符串。 一般地，也常常对账户地址串进行 Base58Check 编码，并添加前导字节（表明支持哪种脚本）和 4 字节校验字节，以提高可读性和准确性。 注：账户并非直接是公钥内容，而是 Hash 后的值，避免公钥过早公开后导致被破解出私钥。 UTXO基本概念在比特币中，一比交易’’在黑盒子里” 实际运作的方式是: 花费一种东西的集合，这种东西被称为“未被花费的交易输出”(即UTXO) ， 这些输出由一个或多个之前的交易所创造，并在其后制造出一比或多笔新的UTXO，可以在未来的交易中花费。每一笔UTXO他有面额、所有者。 而且，一笔交易若要有效，必须满足的两个规则是： 1）该交易必须包含一个有效的签名，来自它所花费的 UTXO 的拥有者； 2）被花费的 UTXO 的总面额必须等于或者大于该交易产生的 UTXO 的总面额。一个用户的余额因此并不是作为一个数字储存起来的；而是用他占有的 UTXO 的总和计算出来的。 如果一个用户想要发送一笔交易，发送 X 个币到一个特定的地址，有时候，他们拥有的 UTXO 的一些子集组合起来面值恰好是 X，在这种情况下，他们可以创造一个交易：花费他们的 UTXO 并创造出一笔新的、价值 X 的 UTXO ，由目标地址占有。当这种完美的配对不可能的时候，用户就必须打包其和值 大于 X 的 UTXO 输入集合，并添加一笔拥有第二个目标地址的 UTXO ，称为“零钱输出”，分配剩下的币到一个由他们自己控制的地址。 UTXO具有如下性质 UTXO， 用比特币拥有者的公钥(锁定)(加密)的一个数字 UTXO == 比特币 比特币系统里没有比特币， 只有UTXO 比特币系统中没有账户，只有UTXO(公钥锁定) 比特币系统里没有账户余额， 只有UTXO( 账户余额只是比特币钱包的概念 ) UTXO存在前节点的数据库里 转账将消耗掉属于你自己的UTXO， 同时生成新的UTXO， 并用接受者的公钥锁定。 交易的原理一笔交易包含的信息 交易的输入的交易 ID（UTXO） 交易的金额: 多少钱，和输入的差额为交易的服务费 时间戳: 交易合适能生效 锁定脚本（用接受者的公钥哈希）- 将比特币地址锁定到接收者 解锁脚本（发送者用私钥对交易的数字签名, 发送者的公钥）- 用来证明比特币确实属于发送者，并保证内容不被篡改 交易的生成和验证过程 1）钱包软件生成交易，并想临近节点传播 2）节点对收到的交易进行验证，并丢弃不合法的交易 交易是否已经处理过 交易的size要小于区块size的上限 （比如比特币之前的大小限制是1M ） 交易的输入UTXO是存在的 交易输入UTXO没有被其他交易引用-防止双花（Double Spending） 输入总金额 &gt; 输出的总金额 解锁脚本的验证 将合格的交易加入到本地的Transaction数据库中，并将合法的交易转给临近节点 3）旷工将合格的交易打包进区块 交易脚本脚本系统 即便没有任何扩展，比特币的协议实际上却是促进了一个弱化版的”智能合约”的概念。。UTXO 在比特币中不是只被一个公钥持有, 而是还被在一个基于堆栈的程序语言组成的复杂的脚本所持有着。 在这个范例中，一个交易消耗的 UTXO 必须提供满足脚本的数据。实际上，这最基本的公钥所有权机制也是通过一个脚本来实现的：这个脚本使用一个椭圆曲线签名作为一个输入，验证拥有这个 UTXO 的交易和地址，如果验证成功，则返回 1，不然则返回 0。其他，更加复杂的脚本存在于各种复杂的用例中。 例如，你可以构造一个脚本，它要求从给定的三个私钥中，至少要选其中的 2 个来做签名验证（“多重签名”），这个对公司账本，储蓄账户等来说非常有用。脚本也能用来对解决计算问题的用户支付报酬。人们甚至可以创建这样的脚本“如果你能够提供你已经发送一定数额的的狗币给我的简化确认支付证明，这一比特币就是你的了”，本质上，比特币系统允许不同的密码学货币进行去中心化的兑换。然而，在比特币中的脚本语言有几个重要的限制： 缺乏图灵-完备 那就是说，虽然比特币脚本语言支持的计算方式很多，但是它不是所有的都支持。在主要类别中缺失循环。 它这样做的目的是为了防止对交易的验证出现死循环；理论上，它的脚本是可以克服这个障碍的，因为任何的循环都可以通过 if 语句重复多次底层代码来模拟，但是这样的脚本运行效率非常低下。 值的盲区 一个 UTXO 脚本没有办法提供资金的颗粒度可控的出金操作。比如, 一个预言合约（oracle contract ）的其中一个强大的用例就是一个套期保值的合约，A 和 B 都把价值1000$的 BTC 放到合约中，30 天后，这个合约把价值 1000$的 BTC 发给了 A，剩下的发给了B。 这就需要合约要确定 1BTC 以美元计值多少钱。然而，因为 UTXO 是不可分割的，为实现此合约，唯一的方法是非常低效地采用许多有不同面值的 UTXO（例如有 2^k 的 UTXO，其中 K可以最大到 30)并使预言合约挑出正确的 UTXO 发送给 A 和 B。 状态缺失 UTXO 要么被使用了，要么没有被使用；这会使得多阶段的合约和脚本没有机会保持任何其他的内部状态。这使得制作多阶段的期权合约、去中心化的交换协议或两阶段加密承诺协议变得困难(对于安全计算奖金来说是必要的)。这也意味着，UTXO 只能用于构建简单的、一次性的合约，而不是更复杂的“有状态”的合约，比如去中心化的组织，并且使得元协议难以实现。 区块链盲区 UTXO 对某些区块链数据视而不见，比如随机数和之前的区块的哈希。这严重限制了博彩和其他一些类别的应用，因为它剥夺了一种潜在的有价值的脚本语言：随机数！也就是在比特币的脚本中是没有随机数的。 因此，我们看到了在加密货币之上构建高级应用程序的三种方法：一，构建一个新的区块链，二，在比特币之上使用脚本，三，在比特币之上构建一个元协议。 构建一个新的区块链可以无限制扩展功能集，但是这样做非常消耗时间。使用脚本很容易实现和标准化，但在其功能上非常有限，而元协议虽然容易，但在可伸缩性方面却存在缺陷。在以太坊中，我们打算建立一个替代性的框架，它提供了更大的开发和更强大的轻客户属性，同时允许应用程序共享一个经济环境和区块链安全。 比特币的锁定脚本和解锁脚本脚本（Script）是保障交易完成（主要用于检验交易是否合法）的核心机制，当所依附的交易发生时被触发。通过脚本机制而非写死交易过程，比特币网络实现了一定的可扩展性。比特币脚本语言是一种非图灵完备的语言。 一般每个交易都会包括两个脚本：负责输入的解锁脚本（scriptSig）和负责输出的锁定脚本（scriptPubKey）。 输出脚本一般由付款方对交易设置锁定，用来对能动用这笔交易的输出（例如，要花费该交易的输出）的对象（收款方）进行权限控制，例如限制必须是某个公钥的拥有者才能花费这笔交易。 认领脚本则用来证明自己可以满足交易输出脚本的锁定条件，即对某个交易的输出（比特币）的拥有权。 输出脚本目前支持两种类型： P2PKH：Pay-To-Public-Key-Hash，允许用户将比特币发送到一个或多个典型的比特币地址上（证明拥有该公钥），前导字节一般为 0x00； P2SH：Pay-To-Script-Hash，支付者创建一个输出脚本，里边包含另一个脚本（认领脚本）的哈希，一般用于需要多人签名的场景，前导字节一般为 0x05； 以 P2PKH 为例，输出脚本的格式为 1scriptPubKey: OP_DUP OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG 其中，OP_DUP 是复制栈顶元素；OP_HASH160 是计算 hash 值；OP_EQUALVERIFY 判断栈顶两元素是否相等；OP_CHECKSIG 判断签名是否合法。这条指令实际上保证了只有 pubKey 的拥有者才能合法引用这个输出。 另外一个交易如果要花费这个输出，在引用这个输出的时候，需要提供认领(输入)脚本格式为 1scriptSig: &lt;sig&gt; &lt;pubKey&gt; 其中， 是拿 pubKey 对应的私钥对交易（全部交易的输出、输入和脚本）Hash 值进行签名，pubKey 的 Hash 值需要等于 pubKeyHash。 进行交易验证时，会按照先 scriptSig 后 scriptPubKey 的顺序进行依次入栈处理，即完整指令为： 1&lt;sig&gt; &lt;pubKey&gt; OP_DUP OP_HASH160 &lt;pubKeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG 读者可以按照栈的过程来进行推算，理解整个脚本的验证过程。 引入脚本机制带来了灵活性，但也引入了更多的安全风险。比特币脚本支持的指令集十分简单，基于栈的处理方式，并且非图灵完备，此外还添加了额外的一些限制（大小限制等）。 比特币脚本语言-基于栈的脚本语言 栈（stack）- 操作数据的一种结构 只能从一端操作数据，后进先出LIFO 如同子弹匣， 先压如的子弹最后打出 压栈（PUSH）， 出栈（POP） 基于栈的脚本语言 对栈的操作： OP_DUP 逻辑运算符： OP_EQUALIVERIFY 加密运算符： OP_HASH160, OP_CHECKSIG 算数运算符: OP_ADD, OP_SUB, OP_MUL, OP_DIV 交易验证-锁定脚本和解锁脚本 锁定脚本 OP_DUP OP_HASH160 &lt;发送者的公钥哈希&gt; OP_EQUALVERIFY OP_CHECKSIG 解锁脚本 &lt;发送者的签名&gt; &lt;发送者的公钥&gt; 交易验证 运行解锁脚本 + 锁定脚本 =&gt; True 比特币区块区块的基本信息比特币一个区块大小不能超过1MB， 主要包括以下内容: 区块大小：4 字节； 区块头：80 字节： 交易个数计数器：1~9 字节； 所有交易的具体内容，可变长，匹配 Merkle 树叶子节点顺序。 区块头包含信息 版本号：4 字节； 上一个区块头的 Hash 值：链接到上一个合法的块上，对其区块头进行两次 SHA256 操作，32 字节； 本区块所包含的所有交易的 Merkle 树根的哈希值：两次 SHA256 操作，32 字节； 时间戳：4 字节； 难度指标：4 字节； Nonce：4 字节，PoW 问题的答案。 可见，要对区块链的完整性进行检查，只需要检验各个区块头部信息即可，无需获取到具体的交易内容，这也是简单交易验证（Simple Payment Verification，SPV）的基本原理。另外，通过头部的链接，提供时序关系的同时加大了对区块中数据进行篡改的难度。 区块的生成 旷工在挖矿前要组件区块 将coinbase交易打包进区块 将交易池中高优先级的交易打包进去快 优先级 = 交易的额度 * UTXO的深度 / 交易的size 创建去开的头部 挖矿成功后，将计算出来的随机数nonce填入区块头部， 并向临近节点传播 区块的验证p2p节点接受到新区块后，立即做以下的检查 验证POW的nonce值是否符合难度值 检查时间戳是否小于当前时间2小时 检查Merkle树根是否正确 检查区块size要小于区块的size上限 第一笔交易必须是coinbase交易 验证每个交易 默克尔树 在默克尔树中只要提供少数的介个节点就可以给出 一个分支的有效性证明，比如验证12c5是否有效，只需要提供标记为蓝色的几点就可以证明12c5这个交易是否有效 试图改变默克尔树的任意一部分都会导致链条上在某处放生不一致的情况 比特币的一个重要特性，即区块是存在一个多级数据结构中的 。一个区块的“哈希值”实际上只是这个区块的头信息的哈希值，一个大约 200 个字节的数据，其中包含了时间戳，随机数，上一个区块的哈希和一个存储了这个区块中所有交易的称之为默克尔树的数据结构的根哈希。 默克尔树是一种二叉树，包含了一组节点，它们的含有基础信息的树根有大量的叶子节点，一组中间节点，每一个节点都是它的 2 个子节点的哈希，然后，最终的一个根节点，也是由它的 2 个子节点的哈希形成，代表着这树的“顶端”。 这个默克尔树的目的是允许在一个区块中的数据能够被零散的传递: 一个节点只能从一个源来下载一个区块的头信息，树的一小部分关联着另一个源 ，并且任然可以保证所有的数据都是正确的。之所以这样做行得通，是因为哈希值都是向上传导的: 如果一个恶意的用户试图在默克尔树的底部替换一个假的交易, 这个更改将导致上面的节点发生变化，然后上面的节点的变化又会导致上上面的节点发生变化，最终改变这个数根节点，因此也改变了这区块的哈希，导致这个协议把它注册成一个完全不同的区块 (几乎可以肯定是一个无效的工作证明).这默克尔树协议对比特币的长期可持续发展是必不可少的。比特币网络中的一个“完整节点” , 截止到 2014 年，占用了大约 15G 的磁盘空间，并且每月正在以 10 亿字节的速度递增。目前，这对于电脑来说是没有问题的，但是在手机上却是不现实的。在以后的将来，只有商业的和业余爱好者才能参与玩比特币。一个称之为 “简化支付验证（simplified payment verification）” (SPV)的协议 允许另一种类型的节点存在，这种节点称之为 “轻节点（light nodes）”, 其下载区块的头信息,在这区块头信息上验证工作证明，然后只下载与之交易相关的“分支” 。 这使得轻节点只要下载整个区块链的一小部分，就可以安全地确定任何一笔比特币交易的状态和账户的当前余额。 挖矿挖矿的基本原理了解比特币，最应该知道的一个概念就是“挖矿”。挖矿是参与维护比特币网络的节点，通过协助生成新区块来获取一定量新增的比特币的过程。 当用户向比特币网络中发布交易后，需要有人将交易进行确认，形成新的区块，串联到区块链中。在一个互相不信任的分布式系统中，该由谁来完成这件事情呢？比特币网络采用了“挖矿”的方式来解决这个问题。 目前，每 10 分钟左右生成一个不超过 1 MB 大小的区块（记录了这 10 分钟内发生的验证过的交易内容），串联到最长的链尾部，每个区块的成功提交者可以得到系统 12.5 个比特币的奖励（该奖励作为区块内的第一个交易，一定区块数后才能使用），以及用户附加到交易上的支付服务费用。即便没有任何用户交易，矿工也可以自行产生合法的区块并获得奖励。 每个区块的奖励最初是 50 个比特币，每隔 21 万个区块自动减半，即 4 年时间，最终比特币总量稳定在 2100 万个。因此，比特币是一种通缩的货币。 挖矿的过程 如果我们有一个可信任的中央服务器, 那么实现这个系统是一件很简单的事情; 就按照需求所描述的去编写代码即可，把状态记录在中央服务器的硬盘上。 然而，与比特币一样，我们试图去建立一个去中心化的货币系统, 所以，我们需要把状态转移系统和一致性系统结合起来，以确保每个人都同意这交易的顺序。 比特币的去中心化的一致性处理进程要求网络中的节点连续不断的去尝试对交易进行打包，这些被打成的包就称为“区块”。 这个网络会故意的每隔 10 分钟左右就创建一个区块, 每一个区块里都包含一个时间戳，一个随机数，一个对上一个区块的引用 ，和从上一个区块开始的所有交易的列表。随着时间的推移，这会创建一个持久的，不断增长的区块链，这个区块链不断的被更新，使其始终代表着最新的比特币总账的状态。 验证一个区块是否有效的算法如下: 检查其引用的上一个区块是否存在并且有效. 检查这个区块的时间戳是否大于上一个区块的时间戳 并且小于 2 小时之内 检查这区块上的工作证明是否有效. 让 S[0] 成为上一个区块的最末端的状态. 假设 TX 是这个区块的交易列表，且有 n 个交易。 做 for 循环，把 i 从 0 加到到 n-1， 设置 S[i+1] = APPLY(S[i],TX[i]) 如果任何一个应用(APPLY)返回错误，则退出并且返回。 返回 true,并且把 S[n] 设置成这个区块最末端的状态。 从本质上说，区块中的每一个交易都必须提供一个有效的状态，从交易执行前的标准状态到执行后的一个新的状态。 注意，状态并没有以任何方式编码进区块中;它纯粹是一个被验证节点所记住的抽象，并且它只能用来被从创世区块起的每一个区块进行安全的计算，然后按照顺序的应用在每一个区块中的每一次交易中。此外，请注意矿工把交易打包进区块的顺序是很重要的; 如果一个区块中有 2 个交易 A 和 B,B 花了一个由 A 创建的 UTXO, 那么如果 A 比 B 更早的进入区块，那么这个区块将是有效的，不然就是无效的。 在上述列出的验证条件中，“工作证明” 这一明确的条件就是每一个区块的 2 次 SHA256 哈希值, 它作为一个 256 位的数字,必须小于一个动态调整的目标值, 截止到本文写作的时间，该动态调整的值的大小大约是 2 的 187 次方。这样做的目的是为了让创建区块的算法变难, 从而，阻止幽灵攻击者从对它们有利的角度出来，来对区块链进行整个的改造。因为 SHA256 被设计成一个完全不可预测的伪随机函数, 这创建一个有效区块的唯一的方法只有是不断的尝试和出错， 不断对随机数进行递增，然后查看新的哈希值是否匹配。 按照当前的目标值 2 的 187 次方，这个网络在找到一个有效的区块前，必须进行 2 的 69 次方次的尝试; 一般来说,每隔 2016 个区块，这个目标值就会被网络调整一次 ，因此网络中平均每隔 10 分钟就会有一些节点产生出一个新的区块。为了补偿这些矿工的计算工作, 每一个区块的矿工有权要求包含一笔发给他们自己的 12.5BTC（不知道从哪来的）的交易。另外,如果任何交易，它的总的输入的面值比总的输出要高，这一差额会作为“交易费用”转给矿工。顺便提一下，对矿工的奖励是比特币发行的唯一途径，创世状态中并没有比特币。 负反馈调解比特币网络在设计上，很好的体现了负反馈的控制论基本原理。 比特币网络中矿工越多，系统就越稳定，比特币价值就越高，但挖到矿的概率会降低。 反之，网络中矿工减少，会让系统更容易导致被攻击，比特币价值越低，但挖到矿的概率会提高。 因此，比特币的价格理论上应该稳定在一个合适的值（网络稳定性也会稳定在相应的值），这个价格乘以挖到矿的概率，恰好达到矿工的收益预期。 从长远角度看，硬件成本是下降的，但每个区块的比特币奖励每隔 4 年减半，最终将在 2140 年达到 2100 万枚，之后将完全依靠交易的服务费来鼓励矿工对网络的维护。 注：比特币最小单位是“聪”，即 10^(-8) 比特币，总“聪”数为 2.1E15。对于 64 位处理器来说，高精度浮点计数的限制导致单个数值不能超过 2^53 约等于 9E15。 共识机制共识机制介绍比特币网络是完全公开的，任何人都可以匿名接入，因此共识协议的稳定性和防攻击性十分关键。 比特币区块链采用了 Proof of Work（PoW）的机制来实现共识，该机制最早于 1998 年在 B-money 设计中提出。 目前，Proof of X 系列中比较出名的一致性协议包括 PoW、PoS 和 DPoS 等，都是通过经济惩罚来限制恶意参与。 工作量证明工作量证明，通过计算来猜测一个数值（nonce），使得拼凑上交易数据后内容的 Hash 值满足规定的上限（来源于 hashcash）。由于 Hash 难题在目前计算模型下需要大量的计算，这就保证在一段时间内，系统中只能出现少数合法提案。反过来，能够提出合法提案，也证明提案者确实已经付出了一定的工作量。 同时，这些少量的合法提案会在网络中进行广播，收到的用户进行验证后，会基于用户认为的最长链基础上继续难题的计算。因此，系统中可能出现链的分叉（Fork），但最终会有一条链成为最长的链。 Hash 问题具有不可逆的特点，因此，目前除了暴力计算外，还没有有效的算法进行解决。反之，如果获得符合要求的 nonce，则说明在概率上是付出了对应的算力。谁的算力多，谁最先解决问题的概率就越大。当掌握超过全网一半算力时，从概率上就能控制网络中链的走向。这也是所谓 51% 攻击的由来。 参与 PoW 计算比赛的人，将付出不小的经济成本（硬件、电力、维护等）。当没有最终成为首个算出合法 nonce 值的“幸运儿”时，这些成本都将被沉没掉。这也保障了，如果有人尝试恶意破坏，需要付出大量的经济成本。也有设计试图将后算出结果者的算力按照一定比例折合进下一轮比赛考虑。 权益证明权益证明（Proof of Stake，PoS），最早在 2013 年被提出，类似现实生活中的股东机制，拥有股份越多的人越容易获取记账权（同时越倾向于维护网络的正常工作）。 典型的过程是通过保证金（代币、资产、名声等具备价值属性的物品即可）来对赌一个合法的块成为新的区块，收益为抵押资本的利息和交易服务费。提供证明的保证金（例如通过转账货币记录）越多，则获得记账权的概率就越大。合法记账者可以获得收益。 PoS 试图解决在 PoW 中大量资源被浪费的缺点，受到了广泛关注。恶意参与者将存在保证金被罚没的风险，即损失经济利益。 一般的，对于 PoS 来说，需要掌握超过全网 1/3 的资源，才有可能左右最终的结果。这个也很容易理解，三个人投票，前两人分别支持一方，这时候，第三方的投票将决定最终结果。 PoS 也有一些改进的算法，包括授权股权证明机制（DPoS），即股东们投票选出一个董事会，董事会中成员才有权进行代理记账。这些算法在实践中得到了不错的验证，但是并没有理论上的证明。 2017 年 8 月，来自爱丁堡大学和康涅狄格大学的 Aggelos Kiayias 等学者在论文《Ouroboros: A Provably Secure Proof-of-Stake Blockchain Protocol》中提出了 Ouroboros 区块链共识协议，该协议可以达到诚实行为的近似纳什均衡，认为是首个可证实安全的 PoS 协议。 隔离见证什么是隔离见证隔离见证，即 Segregated Witness（简称SegWit），由Pieter Wuille（比特币核心开发人员、Blockstream联合创始人）在2015年12月首次提出。 见证（Witness） 见证，在比特币里指的是对交易合法性的验证。举个例子，Alice发起一笔交易，给Bob支付1个BTC，该笔交易信息由三部分组成： a.元数据：交易信息格式的版本号；交易锁定时间等 b.付款人：Alice用于付款的BTC来源，一般来源于某历史区块上某笔交易的输出（详 见UTXO）；证明Alice拥有该笔交易的输出，即见证（Witness）数据 c.收款人：Bob的收款地址和金额 可见，见证数据包含在交易信息里头。 隔离（Segregated） 指的就是把见证数据从交易信息里抽离出来，单独存放。 隔离见证的来源为什么要把见证数据隔离出来呢，或者说这样做有什么好处呢？这就涉及到比特币里的另一个概念–扩容。 扩容，指的是增加比特币每秒的交易量。比特币每10分钟左右挖出一个大小小于1MB的区块，每笔交易平均250字节，即每个区块最多放进4000笔交易，这样算下来，比特币每秒处理的交易数不超过7个。对比其它交易平台，PayPal每秒数百笔、Visa每秒数千笔、支付宝能达到每秒数万笔，可见比特币是一个非常低效的交易系统。如果使用人数增多，则会造成比特币的拥堵。 如何解决拥堵呢？ 有两种方式，一是简单的增加每个区块的大小，比如将区块大小增加到8M；另一种就是隔离见证+闪电网络啦。 扩容方案一: 增加区块大小如果将区块大小增至8M，简单思考一下，比特币每秒处理的交易数似乎也增加到原来的8倍，即56笔每秒。如果每个区块1个GB，比特币每秒将处理7000笔交易，拥堵问题不就解决了吗？ 中本聪可没那么傻，之所以将区块大小设定为1M，是有重要原因的。比特币白皮书的标题为：一种点对点的电子现金系统，相比于传统货币系统，比特币的核心价值在于实现了一种去中心而且安全的货币。如果区块的大小过大，则会危害到比特币的安全模型，作为一种货币应用，这显然是不能令人接受的。 为什么这么说呢？ POW机制的安全基础，是假设一个人的算力无法超过全网算力的50%。如果增大区块，可能一个人的算力超过全网的1/3，就危害到了比特币的安全。举个例子，为了达到每秒7000笔的交易速度，我们把区块的大小增加到1GB： a.假设1GB的区块从产生到广播到全网节点需要10分钟； b.有一个叫Byzantium的节点，拥有的算力超过全网1/3； c.当Byzantium节点挖出一个新区块时，假设该时间点为0秒，那么Byzantium节点 获取新区块的时间点为0秒；根据假设a，全网最后一个获取新区块的节点的获取时间 为600秒，如果获取速度是线性的，全网其它节点获取新区块的平均时间是300秒。 d.因为在新区块上挖坑的算力才是有效算力；根据c，全网其它节点的有效算力只剩下 一半，也就是说，全网其它节点的有效算力小于1/3 e.根据b和d，这种情况下，Byzantium节点算力超过全网其它节点算力，如果Byzantium 节点在自己挖出的区块上继续挖矿且不公布广播，则Byzantium节点上没公布的区块 长度，会大于全网区块长度；一旦Byzantium节点公布这些区块，则全网其它节点挖 出的区块全部作废。 可见，区块设计过大，会威胁到比特币的安全。换句话说，比特币区块的大小是有上限的，《On Scaling Decentralized Blockchain》这篇论文指出，在目前的互联网环境下，如果十分钟产生一个区块，区块的大小最好不能超过4MB。这样看来，增加区块大小这种扩容方案，效果就十分有限了。 扩容方案二: 隔离见证 + 闪电网络隔离见证为什么能扩容呢？先来看看比特币区块的数据结构： 每笔交易平均250字节，见证部分的数据约为150字节，其余部分100字节。如果将见证数据隔离出来，原来1MB空间的区块可以放下10000笔交易（原来为4000笔），交易速度约提升2.5倍。隔离出来的见证数据放到了区块末尾，大小为1.5到2MB，所以隔离见证的整个区块大小为2.5到3MB左右。 隔离见证的意义: 解决了交易延展性问题； 为闪电网络铺路 其他优化 交易延展性 中本聪在设计比特币的时候直接把这两个信息直接放在了区块内，所以一个区块就承载不了更多的交易信息，如果隔离了“见证信息”，那么区块链只记录交易信息，那么一个区块可承载的交易更多交易。中本聪设计比特币时，并没有把两部分资料分开处理，因此导致交易ID的计算混合了交易和见证。因为见证本身包括签名，而签名不可能对其自身进行签名，因此见证可以由任何人在未得到交易双方同意的情况下进行改变，造成所谓的交易可塑性（malleability）。在交易发出后、确认前，交易ID可以被任意更改，因此基于未确认交易的交易是绝对不安全的。在2014年就曾有人利用这个漏洞大规模攻击比特币网络。 指的是一笔交易发起后，交易数据中的见证部分可以被篡改，而且篡改后的交易仍然有效。具体的说，见证的实现依靠一种签名算法，比如椭圆曲线数字签名算法（ECDSA），这种算法下签名（r，s）和签名（r，-s（mod n））都是有效的，所以可以把一种有效见证数据篡改成另一种有效见证数据，该笔交易仍然是有效的。每笔交易有个交易ID，交易ID是对整个交易数据的Hash值，为该笔交易的唯一标识。通过对见证数据的篡改，可以改变Hash值，从而改变该笔交易的唯一标识。隔离见证通过把见证数据隔离移出，生成交易ID时Hash的数据不包括见证数据，因此也就无法改变交易ID值。 从此以后，只有发出交易的人才可以改变交易ID，没有任何第三方可以做到。如果是多重签名交易，就只有多名签署人同意才能改变交易ID。这可以保证一连串的未确认交易的有效性，是双向支付通道或闪电网络所必须的功能。有了双向支付通道或闪电网络，二人或多人之间就可以实际上进行无限次交易，而无需把大量零碎交易放在区块链，大为减低区块空间压力。 闪电网络 通过增加区块大小无法从根本上解决比特币的扩容问题。闪电网络通过在比特币基础上，构建第二层网络，将交易转移到链下的方式，来减轻公链负担，以实现扩容的效果。目前看来，在公链基础上构建协议层网络，是解决公链拥堵问题最合适也是最有前景的方案 隔离见证所带来的改变，为闪电网络的实现提供了一些便利，主要有3点： a.交易延展性的解决，让交易无法被干扰，闪电网络白皮书中提到的“人质状态” （hostage situation），得以避免； b.在通道的生命周期上，隔离见证让闪电网络的通道永久开启更方便实现； c.虽然从理论上系统是安全的，但用户还是要查看区块链中的交易是否广播撤回，防止交易方的欺诈行为，隔离见证使得这项活动可以外包出去，只要给服务器传送少量信息， 就能代替你完成这一过程。 d.此外，隔离见证给比特币带来了一些细节优化，比如增加了脚本版本（Script Version），使得脚本语言可以以一种向后兼容的方式来发展；签名算法复杂度有了较大优化等等。 比特币的缺陷比特币的缺陷 交易确认时间长，吞吐量低 POW挖矿浪费计算资源 ASIC矿机出现是全民参与性降低，算力集中 不完全匿名 无法存储太多的数字资产 不支持复杂的脚本语言 缩短交易时间的方法 缩短平均产生区块的时间 中心化服务 信任地址多重签名 开放交易和联合服务器 POS和DPOS Segwit与闪电网络]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[供应链金融的痛点]]></title>
    <url>%2F2019%2F01%2F19%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%E7%9A%84%E7%97%9B%E7%82%B9%2F</url>
    <content type="text"><![CDATA[供应链的介绍什么是供应链金融？ 供应链金融是指将供应链上的核心企业以及与其相关的上下游企业看做一个整体，以核心企业为依托，以真实的贸易为前提，运用自偿性贸易融资的方式，对供应链上下游企业提供的综合性金融产品服务。 根据融资担保品的不同，金融机构将供应链金融分为应收账款类、预付类和存货类融资，其中应收账款类的规模尤为巨大。 供应链金融痛点1. 供应链上的中小企业融资难供应链上的中小企业融资难，成本高。由于银行依赖的是核心企业的控货能力和调节销售能力，出于风控的考虑，银行仅愿意对核心企业有 直接，应付账款义务的上游供应商(仅限于一级供应商)提供保理服务，或者对其下游经销商（一级经销商），提供预付款或者存货融资。 这就导致有巨大融资需求的二级、三级等供应商/经销商需求得不到满足，供应链金融的业务量受到限制，而中小企业得不到及时的融资易导致产品质量问题，会伤害到整个供应链体系。 区块链解决方法 我们在区块链上发行，运行一种数字票据，可以在公开透明、多方见证的情况下进行随意的拆分和转移。 这种模式相当于把整个商业体系中的信用将变得可传导、可追溯，为大量原本无法融资的中小企业提供了融资机会，极大地提高票据的流转效率和灵活性，降低中小企业的资金成本。 据统计，过去传统的供应链金融公司大约仅能为15%的供应链上的供应商们（中小企业）提供融资服务，而采用区块链技术以后，85%的供应商们都能享受到融资便利。 2. 转让难度较大作为供应链金融的主要融资工具，现阶段的商业汇票、银行汇票使用场景受限，转让难度较大。商业汇票的使用受限制与企业的信誉，银行汇票贴现的到账时间难以把控。同时，如果要把这些汇票进行转让，难度也不小。 因为在实际金融操作中，银行非常关注应收账款债券”转让通知”的法律效应，如果核心企业无法签回，银行不会愿意授信。据了解，银行对于签署这个债券“转让通知”的法律效应很谨慎，甚至要求核心企业的法人代表去银行当面签署，显然这种方式操作难度是极大的。 区块链解决方案: 银行与核心企业之间可以打造一个联盟链，提供给供应链上的所有成员企业使用，利用区块链多方签名、不可篡改的特点，使得债权转让得到多方共识，降低操作难度。 当然，系统设计要能达到债券转让的法律通知效果。同时，银行还可以追溯每个节点的交易，勾画出可视性的交易流程图。 3. 系统难以自证清白供应链金融平台/核心企业系统难以自证清白，导致资金端风控成本居高不下。 目前的供应链金融业务中，银行或其他资金端除了担心企业的还款能力和还款意愿以外，也很担心交易信息本身的真实性，而交易信息是由核心企业的ERP系统所记录的。 虽然ERP篡改难度大，但是也并非绝对可信，银行依然担心核心企业和供应商/经销商勾结篡改信息，因而需要投入人力物力去验证交易的真伪，这就增加了额外的风控成本。 区块链解决方案: 区块链作为“信任的机器”，具有可溯源、共识和去中心化的特性，且区块链上的数据都带有时间戳，即使某个节点的数据被修改，也无法只手遮天，因而区块链能够提供绝对可信的环境，减少资金端的风控成本，解决银行对于被信息篡改的疑虑。 区块链 + 供应链金融 (应收账款)供应链金融需求需要供应链的行业在供应链金融方面，典型的客户不只是金融企业，比如说银行；也包括产业的龙头，比如物流行业的顺丰。核心企业只要有上下游，供应商和经销商都会有复杂交易的问题，这些公司其实是最需要区块链供应链金融的。 区块链适合的企业 需要多方参与的复杂交易，并且需要方便快速的交易 需要方便的进行账本的溯源 不适合区块链的企业以银行账户为例 本身不需要快速，是通过各种机构间的对账来确保账户的正确性； 也不需要很大的溯源，除非账户有错，通过复式记账法也可以溯源，只是没有那么方便而已。 区块链可以解决供应链金融的问题 票据的拆分转让 现阶段的融资工资普遍是商业贴现票据，和应收账款不同的是，应收账款是有核心企业来签发的，商业贴现票据是有银行来签发的，并且商业贴现票据是不能拆分的。应收账款的情况更适合1+N的供应链金融模式，就是一个核心企业带来N个小企业。 现在，通过区块链技术，记录拆分的动作，并通过签名技术，能够实现票据的拆分，并且可以让交易更加迅速。 应收账款的溯源 通过区块链账本的记账功能和不可篡改的特性，可以让应收账款可以实现快速的溯源功能。 通过区块链技术能够让供应链金融运行的更加快速票据可拆分和可溯源，贷款的需求就可以旺盛起来 通过区块链的不可篡改的特性，可以让供应链金融的票据实现安全快速的拆分转让，并且可溯源，可以解决部分供应链的小企业融资和转让难度大的问题，但是还是解决不了数据源头去伪的问题，即还是解决不了系统数据源头的清白，这部分痛点可以依靠物料网来解决。 如何应用区块链切入供应链金融在市场选择上，区块链初创公司应选择天花板足够高的细分领域，比如家电、汽车零售、服装、药品行业等。 切入区块链金融的两种模式1. 直接与核心企业/平台合作直接与核心企业/平台合作，为其提供区块链底层的解决方案，在积累足够多数据之后，通过搭建联盟链，对接资金方提供金融服务。(联盟链模式) 由于区块链本身不能解决风控问题，现阶段企业级的风控还是需要围绕着强势的核心企业，同时，获得核心企业的支持还可以有效解决获客问题，因为一家大型核心企业一般都会有上千家的各类供应商。 2. 第二种方式是从供应链管理服务入手比如溯源、追踪、可视化等，将信息流、物流和资金流整合到一起，在此基础之上从事金融服务。(私有链模式) 这种模式相当于同去看连搭建起了一个 应用场景。 打造供应链金融交易所步骤 前提， 我们需要先打造一个区块链+供应链金融的联盟，联盟的参与者包括供应链金融平台，核心企业、专业的金融中介机构、资金方、保理机构等。 每个参与者都需要承担相应的义务，比如平台负责提供供应链信息，客户信息这些类似水电的基础服务，而核心企业了解行业状况，对供应链上的企业具有掌控力，负责风险控制。 专业的金融中介机构可以对平台信息进行整合分析，提供定制化的供应链金融产品，比如个性化的区块链电子票据。资金方包括银行、互联网金融机构等负责对接相应风险偏好的客户。 1. 数据上链将供应链联盟里的数据放到链上，利用区块链的特性使其不可篡改，并提供数据的确权，溯源等服务。 2. 资产数字化将资产数字化，把仓单、合同、以及课代表融资需求的区块链票据都变为数字资产，且具有唯一、不可篡改、不可复制等特点。 3. 数字资产的交易数字资产的交易，供应链金融平台将转变成一个金融资产交易所，将非标的企业贷款需求转变为标准化的金融产品，进行代币化，对接投融资需求，进行价值交易。 最终，区块链技术将能有效地增强供应链金融资产的流动性，调动新型的融资工具和风控体系帮助覆盖中小企业融资的长尾市场，催生供应链金融服务。]]></content>
      <categories>
        <category>区块链</category>
        <category>供应链金融</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>供应链金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[供应链金融三种模式]]></title>
    <url>%2F2019%2F01%2F12%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%2F%E4%BE%9B%E5%BA%94%E9%93%BE%E9%87%91%E8%9E%8D%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[什么是供应链金融供应链金融其本质是对供应链结构特点、交易细节的把握，将核心企业和上下游企业联系在一起提供灵活运用的金融产品和服务的一种融资模式。也就是把资金作为供应链的一个溶剂，增加其流动性。 供应链金融促进供应链的发展体现在资金注入和信用注入两方面：一方面解决相对弱势的上、下游配套中小企业融资难和供应链地位失衡的问题；另一方面，将银行信用融入上、下游配套企业，实现其商业信用增级，促进配套企业与核心企业建立起长期战略协同关系，从而提升整个供应链的竞争能力。 传统融资模式中银行与供应链成员的关系银行围绕某一家核心企业，从原材料采购，到最终产品，最后由销售网络把产品送到消费者手中这一条供应链，将供应商、制造商、分销商、零售商直到最终用户连成一个整体，全方位的为链条上的N个企业提供融资服务，通过相关企业的职能分工合作，实现整个供应链的不断增值 供应链金融使银行从新的视角评估中小企业的信用风险，从专注于对中小企业本身信用风险的评估，转变为对整个供应链及其交易的评估，这样既真正评估了业务的真实风险，同时也使更多的中小企业能够进入银行的服务范围。一方面，将资金有效注入处于相对弱势的上下游配套中小企业，解决供应链失衡；另一方面，将银行信用融入上下游企业的购销行为，增强其商业信用，改善其谈判地位，使供应链成员更加平等的协商和逐步建立长期战略协同关系，提升供应链的竞争能力，促进了整个供应链的持续稳定发展； 供应链金融的三种模式三种分类介绍供应链金融的实质是帮助企业盘活流动资产，即应收、预付和存货。因此通常将产品分为三类：应收类、预付类和存货类。 应收类 应收类产品帮助上游企业将应收账款转换成现金或应付票据; 预付类 预付类产品则帮助下游企业扩大了单次采购额，提高了采购能力，将本应即期支出的现金资产转换为短期借款或应付票据; 存货类 现货质押更为直接，以企业的存货作为担保方式，换取流动性更强的现金资产。虽然现货融资通常没有核心企业参与，但因该业务涉及对货权的控制和物流监管企业的管理，从管理上与供应链金融流行的预付款渠道融资相近。 应收账款融资以未到期的应收账款向金融机构办理融资的行为，称为应收账款融资，这种模式使得企业可以及时获得机构的短期融资，不但有利于解决融资企业短期资金的需求，加快中小企业健康稳定的发展和成长，而且有利于整个供应链的持续高效运作。 应收类产品主要应用于核心企业的上游融资，如果销售已经完成，但尚未收妥货款，则适用产品为保理或应收账款质押融资;如融资目的是为了完成订单生产，则为订单融资，担保方式为未来应收账款质押，实质是信用融资。 应收账款质押融资是是指企业与银行等金融机构签订合同。以应收账款作为质押品。在合同规定的期限和信贷限额条件下，采取随用随支的方式，向银行等金融机构取得短期借款的融资方式。其中放款需要发货来实现物权转移，促使合同生效。同时也需要告知核心企业，得到企业的确权。 风险要点： 应在《应收账款质押登记公示系统》内登记，避免重复质押 供应商和加工企业之间应有长期稳定贸易关系 货物应有明确验收标准，交割标准，准确付款依据 供应商发货后，银行方应定期、不定期与生产企业进行对账 核心企业付款应转入银行方监管的专用账户 保理和应收账款质押区别 保理业务是以债权人转让其应收账款为前提，集应收账款催收、管理、坏账担保及融资于一体的综合性金融服务。与应收账款质押融资的差别是，保理是一种债权的转让行为，适用于《合同法》，而应收账款质押是一种物权转让行为，适用于《物权法》。银行拥有对货物物权的处置权。 票据池 票据池业务是银行一种常见的供应链金融服务。票据是供应链金融使用最多的支付工具，银行向客户提供的票据托管、委托收款、票据池授信等一揽子结算、融资服务。票据池授信是指客户将收到的所有或部分票据做成质押或转让背书后，纳入银行授信的资产支持池，银行以票据池余额为限向客户授信。用于票据流转量大、对财务成本控制严格的生产和流通型企业，同样适用于对财务费用、经营绩效评价敏感并追求报表优化的大型企业、国有企业和上市公司。 对客户而言. 票据池业务将票据保管和票据托收等工作全部外包给银行，减少了客户自己保管和到期托收票据的工作量。而且，票据池融资可以实现票据拆分、票据合并、短票变长票等效果，解决了客户票据收付过程中期限和金额的不匹配问题。对银行而言，通过票据的代保管服务，可以吸引票据到期后衍生的存款沉淀。 未来提货权融资未来货权融资（又称为保兑仓融资）是下游购货商向平台申请贷款，用于支付上游核心供应商在未来一段时期内交付货物的款项，同时供应商承诺对未被提取的货物进行回购，并将提货权交由金融机构控制的一种融资模式。 融资企业通过保兑仓业务获得的是分批支付货款并分批提取货物的权利，因而不必一次性支付全额货款，有效缓解了企业短期的资金压力，实现了融资企业的杠杆采购和供应商的批量销售。 预付类产品则主要用于核心企业的下游融资，即主要为核心企业的销售渠道融资，包含两种主要业务模式： 一是，银行给渠道商融资，预付采购款项给核心企业，核心企业发货给银行指定的仓储监管企业，然后仓储监管企业按照银行指令逐步放货给借款的渠道商，此即为所谓的未来货权融资或者先款后货融资; 二是，核心企业不再发货给银行指定的物流监管企业，而是本身承担了监管职能，按照银行指令逐步放货给借款的渠道商，此即所谓的保兑仓业务模式。 先货后款(或先票后货)是指买方从银行取得授信，在交纳一定比例保证金的前提下，向卖方支付全额货款;卖方按照购销合同以及合作协议书的约定发运货物，货物到达后设定抵质押，作为银行技信的担保。一些热销产品的库存往往较少，因此企业的资金需求集中在预付款领域。同时，该产品因为涉及到卖家及时发货、发货不足的退款、到货通知及在途风险控制等环节，因此客户对卖家的谈判地位也是操作该产品的条件之一。 保兑仓(又称为担保提货授信)是在客户交纳一定保证金的前提下，银行贷出金额货款供客户(买方)向核心企业(卖方)采购，卖方出具金额提单作为授信的抵质押物。随后，客户分次向银行提交提货保证金，银行再分次通知卖方向客户发货。卖方就发货不足部分的价值承担向银行的退款责任。该产品又被称为卖方担保买方信贷模式。 保兑仓是基于特殊贸易背景，如：1、客户为了取得大批量采购的析扣，采取一次性付款方式，而厂家因为排产问题无怯一次性发货;2、客户在淡季向上游打款，支持上游生产所需的流动资金，并锁定优惠的价格。然后在旺季分次提货用于销售;3、客户和上游都在异地，银行对在途物流和到货后的监控缺乏有效手段。保兑仓是一项可以让买方、核心企业和银行均收益的业务。 融通仓融资所谓融通仓即存货融资，是企业以存货作为质押向金融机构办理融资业务的行为。所以融通仓服务不仅可以为企业提供高水平的物流服务，又可以为中小型企业解决融资问题，解决企业运营中现金流的资金缺口，以提高供应链的整体绩效。 存货类融资主要分为现货融资和仓单融资两大类，现货质押又分为静态质押和动态质押，仓单融资里又包含普通仓单和标准仓单。 静态抵质押授信是指客户以自有或第三人合法拥有的动产为抵质押，银行委托第三方物流公司对客户提供的抵质押的商品实行监管，抵质押物不允许以货易货，客户必须打款赎货。此项业务适用于除了存货以外没有其他合适的抵质押物的客户，而且客户的购销模式为批量进货、分次销售。利用该产品，客户得以将原本积压在存货上的资金盘活，扩大经营规模。 动态抵质押授信是静态抵质押授信的延伸产品——它是指客户以自有或第三人合法拥有的动产为抵质押，银行对于客户抵质押的商品价值设定最低限额，允许在限额以上的商品出库，客户可以以货易货。该产品适用于库存隐定、货物品类较为一致、抵质押物的价值核定较为容易的客户。同时，对于一些客户的存货进出颇繁，难以采用静态抵质押授信的情况，也可运用这类产品。该产品多用于生产型客户。对于客户而言，由于可以以货易货，因此抵质押设定对于生产经营活动的影响相对较小。特别对于库存稳定的客户而言，在合理设定抵质押价值底线的前提下，授信期间内几乎无须启动追加保证金赎货的梳程，因此对盘活存货的作用非常明显。 仓单质押是以仓单为标的物而成立的一种质权。仓单质押作为一种新型的服务项目，为仓储企业拓展服务项目。仓单是保管人收到仓储物后给存货人开付的提取仓储物的凭证。仓单除作为已收取仓储物的凭证和提取仓储物的凭证外，还可以通过背书，转让仓单项下货物的所有权，或者用于出质。存货人在仓单上背书并经保管人签字或者盖章，转让仓单始生效力。存货人以仓单出质应当与质权人签订质押合同，在仓单上背书并经保管人签字或者盖章，将仓单交付质权人后，质押权始生效力。 标准仓单质押授信是指客户以自有或第三人合法拥有的标准仓单为质押的授信业务。标准仓单是指符合交易所统一要求的、由指定交割仓库在完成入库商品验收、确认合格后签发给货主用干提取商品的、并经交易所注册生效的标准化提货凭证。该产品适用于通过期货交易市场进行采购或销售的客户，以及通过期货交易市场套期保值、规避经营风险的客户。对于客户而言，相比动产抵质押，标准仓单质押手续简便、成本较低。对银行而言，成本和风险都较低。此外，由于标准仓单的流动性很强，也利于银行在客户违约情况下对货押物的处置。 普通仓单质押授信是指客户提供由仓库或其他第三方物流公司提供的非期货交割用仓单作为质押物，并对仓单作出质背书，银行提供融资的一种银行产品。在涉及货押的融资模式里，目前最大的问题是监管企业的职责边界、风险认定和收益权衡问题。监管企业承担的责任法律界定模糊，当监管企业获取的收益较低时，承担过大的风险与其并不匹配。]]></content>
      <categories>
        <category>区块链</category>
        <category>供应链金融</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>供应链金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链简介]]></title>
    <url>%2F2019%2F01%2F08%2F12.%E5%8C%BA%E5%9D%97%E9%93%BE%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%2F1.%E5%8C%BA%E5%9D%97%E9%93%BE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[区块链介绍区块链的历史上世纪 80 年代和 90 年代的匿名电子现金协议主要使用了一种被称为“乔姆盲签（Chaumian Blinding）”的加密技术，这种技术为这些新货币提供了很高的隐私保护，但是由于他们的基础协议在很大程序上需要依赖于一个中央中介，因此未能获得支持。1998 年，戴伟（Wei Dai）的 b-money 首次引入了通过解决计算难题和去中心化的共识来创造货币的思想，但是该建议并未给出实现去中心化的共识的具体方法。2005 年，芬尼（Hal Finney）引入了“可重复的工作证明”（reusable proofs of work）概念，它同时使用 b-money 的思想和 Adam Back 提出的计算困难的哈希现金（Hashcash）难题来创造密码学货币。但是，这种概念再次迷失于理想化，因为它需要可信任的计算作为后端。在2009 年，一种去中心化的货币由中本聪在实践中首次得到实现，通过使用公共密钥密码来管理所有权，并过一种一致性算法来跟踪货币的持有者，这种算法被称为“工作证明”。这“工作证明” 的算法是一种突破，因为它同时决了 2 个问题。首先，它提供了一种简单的、有节制的有效的共识算法，允许网络中的节点一起同意比特币总帐状一组更新。其次，它提供了一种机制，允许任何节点自由进入共识的处理进程，从而解决了谁来影响共识的政治难题，同时阻止女巫攻击。 这参与共识投票的节点的投票权比重是直接和节点它们自己的算力挂钩的。 这就是比特币，其核心思想就是建立一个共识机制，不会由某一个服务中心作为计算后端。 区块链的基本概念区块链是一种分布式账本，由不可更改的数字记录数据包构成。这些数据包被称作为区块。然后使用加密签名呢将每个区块 “链接” 到下一个区块。这样就使区块链像分类账本一样被使用，可以有具有适当权限的任何人共享和访问。 在区块中，我们会写入如下信息 谁在转账给谁 交易金额 签名等其他信息 区块的链接方式 ​ 假设我们有 3 个区块，包含如下信息： 区块 1 包含的信息为 I1，I1 的哈希值为 H1。 区块 2 包含的信息为 I2 ，I2 的哈希值为 H2。 区块 3 包含的信息为 I3 ，I3 的哈希值为 H3。 H2 是由 H1 和 I2 结合起来算出的。同样地，H3 是由 H2 和 I3 结合起来算出的，依此类推。 分布式去中心化分类账区块链是一个分布式的去中心化分类账，用来存储交易信息等数据，这些数据为整个区块链网络中的节点所共享。 分类账分类账是承载区块列表的主要记录载体。 存储数据区块能存储数据（信息）。此处对数据的定义很广泛，可以是我们能想到的任何数据。我们就拿交易信息这一数据来举个例子。 分布式的去中心化分类账数据处理通常由一个中心机器负责。但是区块链里有很多机器（因此它不是中心化的），且所有机器都是点对点相互连接。另外，这些机器维护的是同一本分类账。因此，区块链被称为分布式的去中心化分类账。 换句话说，因为同一区块链网络中的所有人都共享同一本分类账，所以说区块链是分布式的。每个人都有整个分类账的副本，一旦有什么东西添加进去，副本马上就会更新。 为整个区块链网络中的节点所共享在区块链网络中，所有机器全都相互连接。每个节点（机器）都持有相同的分类账副本。这就意味着整个区块链网络中的节点都共享一本分类账。 区块链是如何确保安全性的区块链利用密码学来生成数字签名，并通过数字签名的方式防止数据完整性。 当创建一个交易的时候，利用自己的私钥对信息进行加密创建一个数字签名 然后把交易(内含信息、公钥以及数字签名)提交到其他的临近节点进行审批。 在这一过程中，网络会利用公钥来解密数字签名，并从签名中提取信息。 如果原信息与上图所示的签名中提取出来的信息相匹配，就可以通过审批，否则就无法通过。 如果量信息不匹配， 就可能是一下原因: 原信息在中途被操控了。 生成数字签名时所用的私钥与所提供的公钥不匹配。 这就是区块链网络如何能够防止篡改的方法，因此区块链相对上是安全的。 区块链技术的应用 区块链货币兑换 通过电子加密货币可以低成本的实现货币兑换和汇款 数据存储 区块链的实质是分布式账本，并且具有不可更改的特性，因此非常适合金融科技行业的数据存储。从另一个角度看，区块链也是分布式数据库，可以满足个人用户数据存储的需要。 区块链物联网 通过分部署账本记录某个设备与其他设备、web服务、或者人类用户之间的数据交换，就可以跟踪他的历史。 投票系统 区块链很好的解决了无需依赖第三方而达成信任的问题，因此特别适合实现公众投票系统。 预测平台 通常情况下，群体的智慧大于少数个体。对未来发生的时间，群体的预测结果通常会更加准确。利用区块链全名参与、只能合约等特性，可以创造新型的预测平台 支付 、借贷 通过电子加密货币可以实现低成本的跨境支付与个人借贷。 区块链技术栈 共识机制 用来筛选和竞争出让哪个节点来进行记账，并广播给所有节点进行同步的机制； 密码算法 用来计算区块的哈希值来关联各个区块， 和计算每个区块交易事务的哈希值(梅克尔根) 网络路由 用来发现各个节点，实现节点之间的相互通讯，完成各节点数据的同步； 脚本系统(可以类比以太坊智能合约和比特币的交易过程) 一般用来驱动数据的收发，即一套数据的收发和处理规则；在不同的系统中也可以通过改编脚本系统程序。拓展区块链系统功能，比如以太坊就可以根据自定义功能的脚本系统，进而实现智能合约的功能； 区块链账本 用来记录数据，将数据以区块的方式记录下来, 每一个区块一般包含[1. 区块头；2.前一区块头的哈希值 ； 3. 梅克耳根(交易事务的哈希值) ]， 根据下图, 一般六个区块就能确定 一比交易成功； 中心服务器和区块链的区别第三方介入的缺点 交易成本高 对小额交易不友好 调解纠纷的成本被平摊到交易手续费 跨地域交易时需要额外费用 对卖家不公平 当发生纠纷时通常保护买家的利益 某些产品可以轻易复制，如文档，包含源代码的程序等。当产生退货时对卖家不利； 交易时间长 第三方核实交易需要时间较长 隐私暴露 商家会向客户索要完全不必要的个人信息 如何去除第三方 电子加密货币的核心思想 全民参与 让全网尽可能多的节点参与核实，记录交易 给参与见证的节点讲理，维持整个系统持续 运行 任何人都可以参与挖矿，拥有代币，发起交易 匿名&amp;公开 交易的双方无需透露个人信息 交易信息是公开，非加密的。根据钱包地址可查所有的交易记录 去中心化 采用p2p网络，所有节点都来自于互联网，解决了第三方单独进行数据处理的问题 区块链分类根据网络范围分类 公有链 完全对外开放，任何人都可以任意使用； 没有权限的设定，有没有身份的认证； 所有的数据参与其中的任何人都可以任意查看，完全的公开透明； 节点数量不固定，节点是否在线也无法控制，通过网络中大多数节点承认的链就是主链； 公有链中共识机制一般是工作量证明（POW）和权益证明（POS）; 最具代表的就是比特币； 私有链 不对外开发，仅仅在组织内部使用； 通常需要注册，或者需要身份认证； 数据只对私链中的节点可见； 节点的数量和节点的状态通常的可控的; 因为私有链的节点都有很高的信任度，一般不需要通过竞争的方式来筛选数据打包者，可以采用更加节能的方式进行筛选； 比较代表的应用就是企业的票据管理，账务审计，供应链管理等系统； 联盟链 联盟链的网络范围介于公有链和私有链之间， 仅限联盟成员使用； 可以给不同的联盟成员设定不同的权限，所以记账规则和数据权限都可以私人订制； 一般也是具有身份认证和权限设置的； 节点的数量和节点的状态通常的可控的； 联盟链几乎不采用工作量证明共识机制而是采用权益证明或PBTF等共识算法; 代表的应用有银行之间的支付结算、企业之间的物流、政府机关的对外数据公开系统等； 根据部署环境分类 主链 部署在正式生产环境的区块链系统； 测试链 部署在测试环境，用来开发和测试Bug的区块链系统； 根据对接类型分类 单链 能够单独运行的区块链系统都可以称之为 “单链”，例如比特币主链，测试链， 这些区块链系统拥有完备的组件模块，自称一个体系； 侧链 区块链系统与侧链系统本身就是一个独立的链系统，两者之间可以按照一定的协议进行数据互动，通过这种方式，侧链能够起到一个对主链进行功能拓展的作用，很多在主链中不方便实现的功能可以实现在侧链中，而侧链再通过与主链的数据交互增强自己的可靠性； 互联链(多链) 就是区块链系统之间的互联，通过各自的优势，彼此互补，可以大大增强了系统的可靠性以及性能； 区块链技术特点 数据不可篡改性 区块链系统不是一个中心化软件设施，因此数据没有被某一家机构控制，数据肯定不可能被第三方篡改； 分布式存储 在区块链系统中，每个运行的节点都拥有一份完整的数据副本，这样的设计不仅避免了存储的单节点故障问题，还可以让每个节点能够独立的验证和检索数据，大大增加了整个系统的可靠性，节点之间的数据副本还可以互相保持同步，并使用类似梅克尔树这样的技术结构保证数据的完整性和一致性； 匿名性 使用传统的服务软件时，通常都是需要注册一个用户名，绑定手机号等，进行一些认证等；但是在区块链系统中，目前几乎所有的区块链产品都是使用所谓的地址来表示用户的，不需要提供其他的任何能表示出用户身份的信息，地址通常也是通过公开密钥算法生成的公钥转换而来的，这通常就是一串如乱码一般的字符串，因为即使这些公链系统是完全公开透明的，我们却不知道背后的操作者是谁； 价值传递 以比特币为例，比特币是一种数字资产，他是由比特币软件组成的网络所维护的，在这个网络中，不需要其他的第三方，自己可以根据规则发型比特币，并且能够确保发行的比特币是具有价值的(工作量证明)，而这种价值的认定是通过网络中所有的节点来自动进行验证的，节点之间打成公式就算认可了，整个过程都是自成一个体系来运行的，人们在交易比特币的时候就产生了价值传递； 区块链系统是可以自己创造信任机制的，无需第三方信任的环境中，大大简化了各种交易个过程，降低了交易的成本； 自动网络共识 生活中一半得事情都需要双方或者多方达成共识，比如签订一份合同，都是在多方达成共识，并且需要做各种确认；比特币从发型到转账交易，都是由网络中的节点自动及逆行身份认证和一系列的检查的，检查通过后就打成了网络共识，因为每个节点都遵守一份共同的约定和规则，只要意向交易符合所有的约定规则就能被确认； 可编程合约 比如比特币，在比特币系统中，并不是想银行账户一样，将金额存储在账户下就代表用户拥有的，而是通过脚本解锁和锁定一比资产，简单地说，就是让资产具备更强的编程可控能力，比如配置程序，让一比资产需要多个人共同签名才能被转移或者需要达到某个条件的时候才能被使用，这就是编程合约的思想。区块链系统具有数据不可篡改，价值传递等能力，加上编程合约，就能完成商业上各种的需求； 软分叉和硬分叉软分叉软分叉：是指区块链网络系统版本或协议升级后，旧的节点并不会意识到比特币代码发生改变，并继续接受由新节点创造的区块，新老节点始终还是在同一条链上工作。 硬分叉是指比特币区块格式或交易格式（共识机制）发生改变时，未升级的节点拒绝验证已经升级的节点产生的区块，然后大家各自延续自己认为正确的链，所以分成两条链。 产生两个币种，如BCC]]></content>
      <categories>
        <category>区块链</category>
        <category>区块链概念</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine原理以及channel使用]]></title>
    <url>%2F2019%2F01%2F05%2F11.Golang%2FGolang%E7%89%B9%E6%80%A7%2Fgoroutine%E5%8E%9F%E7%90%86%E5%92%8Cchannel%2F</url>
    <content type="text"><![CDATA[一、goroutine作用goroutine的本质是协程，是实现并行计算的核心，和python的async编程类似，但是又不需要我们手动的去创建携程并通过实践循环来启动。goroutine异步执行函数，只需使用go关键字+函数名即可启动一个协程，函数则直接处于异步执行的状态。 1go func()//通过go关键字启动一个协程来运行函数 二、goroutine的原理携程概念介绍协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此，协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。线程和进程的操作是由程序触发系统接口，最后的执行者是系统；协程的操作执行者则是用户自身程序，goroutine也是协程。 调度模型简介groutine是通过GPM调度模型实现，下面就来解释下goroutine的调度模型。 Go的调度器内部有四个重要的结构：M，P，S，Sched，如上图所示（Sched未给出）M:M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。P:P全称是Processor，处理器，它的主要用途就是用来执行goroutine的，所以它也维护了一个goroutine队列，里面存储了所有需要它来执行的goroutineSched：代表调度器，它维护有存储M和G的队列以及调度器的一些状态信息等。 调度实现 从上图中看，有2个物理线程M，每一个M都拥有一个处理器P，每一个也都有一个正在运行的goroutine。P的数量可以通过GOMAXPROCS()来设置，它其实也就代表了真正的并发度，即有多少个goroutine可以同时运行。图中灰色的那些goroutine并没有运行，而是出于ready的就绪态，正在等待被调度。P维护着这个队列（称之为runqueue），Go语言里，启动一个goroutine很容易：go function 就行，所以每有一个go语句被执行，runqueue队列就在其末尾加入一个goroutine，在下一个调度点，就从runqueue中取出 当一个OS线程M0陷入阻塞时(例如爬虫或者其他的io操作， 这里就和python的多线程类似)（如下图)，P转而在运行M1，图中的M1可能是正被创建，或者从线程缓存中取出。 当MO返回时，它必须尝试取得一个P来运行goroutine，一般情况下，它会从其他的OS线程那里拿一个P过来，如果没有拿到的话，它就把goroutine放在一个global runqueue里，然后自己睡眠（放入线程缓存里）。所有的P也会周期性的检查global runqueue并运行其中的goroutine，否则global runqueue上的goroutine永远无法执行。 另一种情况是P所分配的任务G很快就执行完了（分配不均），这就导致了这个处理器P很忙，但是其他的P还有任务，此时如果global runqueue没有任务G了，那么P不得不从其他的P里拿一些G来执行。一般来说，如果P从其他的P那里要拿任务的话，一般就拿runqueue的一半，这就确保了每个OS线程都能充分的使用，如下图： 三、goroutine使用基本使用设置goroutine运行的CPU数量，最新版本的go已经默认已经设置了。 12num := runtime.NumCPU() //获取主机的逻辑CPU个数runtime.GOMAXPROCS(num) //设置可同时执行的最大CPU数，即设置多少个P 使用示例 123456789101112131415161718192021222324252627282930package mainimport ( "fmt" "time")func cal(a int , b int ) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c)&#125;func main() &#123; for i :=0 ; i&lt;10 ;i++&#123; go cal(i,i+1) //启动10个goroutine 来计算 &#125; time.Sleep(time.Second * 2) // sleep作用是为了等待所有任务完成&#125; //结果//8 + 9 = 17//9 + 10 = 19//4 + 5 = 9//5 + 6 = 11//0 + 1 = 1//1 + 2 = 3//2 + 3 = 5//3 + 4 = 7//7 + 8 = 15//6 + 7 = 13 goroutine异常捕捉当启动多个goroutine时，如果其中一个goroutine异常了，并且我们并没有对进行异常处理，那么整个程序都会终止，所以我们在编写程序时候最好每个goroutine所运行的函数都做异常处理，异常处理采用recover 123456789101112131415161718192021222324252627282930313233343536package mainimport ( "fmt" "time")func addele(a []int ,i int) &#123; defer func() &#123; //匿名函数捕获错误 err := recover() if err != nil &#123; fmt.Println("add ele fail") &#125; &#125;() a[i]=i fmt.Println(a)&#125;func main() &#123; Arry := make([]int,4) for i :=0 ; i&lt;10 ;i++&#123; go addele(Arry,i) &#125; time.Sleep(time.Second * 2)&#125;//结果add ele fail[0 0 0 0][0 1 0 0][0 1 2 0][0 1 2 3]add ele failadd ele failadd ele failadd ele failadd ele fail 同步的goroutine由于goroutine是异步执行的，那很有可能出现主程序退出时还有goroutine没有执行完，此时goroutine也会跟着退出。此时如果想等到所有goroutine任务执行完毕才退出，go提供了sync包和channel来解决同步问题，当然如果你能预测每个goroutine执行的时间，你还可以通过time.Sleep方式等待所有的groutine执行完成以后在退出程序(如上面的列子)。 方法一：使用sync包同步goroutine 这种方法和python的Thread.join() 很类似WaitGroup 等待一组goroutinue执行完毕. 主程序调用 Add 添加等待的goroutinue数量. 每个goroutinue在执行结束时调用 Done ，此时等待队列数量减1.，主程序通过Wait阻塞，直到等待队列为0. 123456789101112131415161718192021222324252627282930313233package mainimport ( "fmt" "sync")func cal(a int , b int ,n *sync.WaitGroup) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c) defer n.Done() //goroutinue完成后, WaitGroup的计数-1&#125;func main() &#123; var go_sync sync.WaitGroup //声明一个WaitGroup变量 for i :=0 ; i&lt;10 ;i++&#123; go_sync.Add(1) // WaitGroup的计数加1 go cal(i,i+1,&amp;go_sync) &#125; go_sync.Wait() //等待所有goroutine执行完毕&#125;//结果9 + 10 = 192 + 3 = 53 + 4 = 74 + 5 = 95 + 6 = 111 + 2 = 36 + 7 = 137 + 8 = 150 + 1 = 18 + 9 = 17 示例二：通过channel实现goroutine之间的同步。 这种方法类似于python的Thread.Queue()通讯，来实现异步程序和主程序之间的数据交换实现方式：通过channel能在多个groutine之间通讯，当一个goroutine完成时候向channel发送退出信号,等所有goroutine退出时候，利用for循环channe去channel中的信号，若取不到数据会阻塞原理，等待所有goroutine执行完毕，使用该方法有个前提是你已经知道了你启动了多少个goroutine。 12345678910111213141516171819202122232425package mainimport ( "fmt" "time")func cal(a int , b int ,Exitchan chan bool) &#123; c := a+b fmt.Printf("%d + %d = %d\n",a,b,c) time.Sleep(time.Second*2) Exitchan &lt;- true&#125;func main() &#123; Exitchan := make(chan bool,10) //声明并分配管道内存 for i :=0 ; i&lt;10 ;i++&#123; go cal(i,i+1,Exitchan) &#125; for j :=0; j&lt;10; j++&#123; &lt;- Exitchan //取信号数据，如果取不到则会阻塞 &#125; close(Exitchan) // 关闭管道&#125; goroutine之间的通讯goroutine本质上是协程，可以理解为不受内核调度，而受go调度器管理的线程。goroutine之间可以通过channel进行通信或者说是数据共享，当然你也可以使用全局变量来进行数据共享。 示例：使用channel模拟消费者和生产者模式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport ( "fmt" "sync")func Productor(mychan chan int,data int,wait *sync.WaitGroup) &#123; mychan &lt;- data fmt.Println("product data：",data) wait.Done()&#125;func Consumer(mychan chan int,wait *sync.WaitGroup) &#123; a := &lt;- mychan fmt.Println("consumer data：",a) wait.Done()&#125;func main() &#123; datachan := make(chan int, 100) //通讯数据管道 var wg sync.WaitGroup for i := 0; i &lt; 10; i++ &#123; go Productor(datachan, i,&amp;wg) //生产数据 wg.Add(1) &#125; for j := 0; j &lt; 10; j++ &#123; go Consumer(datachan,&amp;wg) //消费数据 wg.Add(1) &#125; wg.Wait()&#125;//结果consumer data： 4product data： 5product data： 6product data： 7product data： 8product data： 9consumer data： 1consumer data： 5consumer data： 6consumer data： 7consumer data： 8consumer data： 9product data： 2consumer data： 2product data： 3consumer data： 3product data： 4consumer data： 0product data： 0product data： 1 四、Golang - channel简介channel俗称管道，用于数据传递或数据共享，其本质是一个先进先出的队列，使用goroutine+channel进行数据通讯简单高效，同时也线程安全，多个goroutine可同时修改一个channel，不需要加锁。 channel可分为三种类型：只读channel：只能读channel里面数据，不可写入只写channel：只能写数据，不可读一般channel：可读可写 channel使用定义和声明12345678910// 定义var readOnlyChan &lt;-chan int // 只读chanvar writeOnlyChan chan&lt;- int // 只写chanvar mychan chan int //读写channel//声明read_only := make (&lt;-chan int,10)//定义只读的channel， 有缓存write_only := make (chan&lt;- int,10)//定义只写的channel， 有缓存read_write := make (chan int,10)//可同时读写， 有缓存read_write := make (chan int)//可同时读写， 无缓存 读写数据需要注意 如果没有使用goroutine，如果未关闭channel，读取数据会产生deadlock异常 管道如果关闭后再继续写入数据会pannic 如果是有缓存channel，当管道中没有数据时候再行读取会读取到默认值，如int类型默认值是0 1234ch &lt;- "wd" //写数据a := &lt;- ch //读取数据a, ok := &lt;-ch //优雅的读取数据for v := range ch &#123;&#125; // 遍历已经关闭的channel 循环管道需要注意 使用range循环管道，如果管道未关闭会引发deadlock错误。 如果采用for range 循环已经关闭的管道，只能遍历有默认值的管道， 当管道没有数据时候，读取的数据会是管道的默认值，并且循环不会退出。 12345678910111213141516171819package mainimport ( "fmt" "time")func main() &#123; mychannel := make(chan int,10) for i := 0;i &lt; 10;i++&#123; mychannel &lt;- i &#125; close(mychannel) //关闭管道 fmt.Println("data lenght: ",len(mychannel)) for v := range mychannel &#123; //循环管道 fmt.Println(v) &#125; fmt.Printf("data lenght: %d",len(mychannel))&#125; 带缓冲区channe和不带缓冲区channel带缓冲区channel：定义声明时候制定了缓冲区大小(长度)，可以保存多个数据。不带缓冲区channel：只能存一个数据，并且只有当该数据被取出才能存下个数据 12ch := make(chan int) //不带缓冲区ch := make(chan int ,10) //带缓冲区 不带缓冲区示例： 12345678910111213141516171819202122232425262728293031323334353637383940package mainimport "fmt"func test(c chan int) &#123; for i := 0; i &lt; 10; i++ &#123; fmt.Println("send ", i) c &lt;- i &#125;&#125;func main() &#123; ch := make(chan int) go test(ch) for j := 0; j &lt; 10; j++ &#123; fmt.Println("get ", &lt;-ch) &#125;&#125;//结果：send 0send 1get 0get 1send 2send 3get 2get 3send 4send 5get 4get 5send 6send 7get 6get 7send 8send 9get 8get 9 Channel的deadlock一个死锁的例子: 1234func main() &#123; ch := make(chan int) &lt;- ch // 阻塞main goroutine, 信道ch被锁&#125; 执行这个程序你会看到Go报这样的错误: 1fatal error: all goroutines are asleep - deadlock! *何谓死锁? *操作系统有讲过的，所有的线程或进程都在等待资源的释放。如上的程序中, 只有一个goroutine, 所以当你向里面加数据或者存数据的话，都会锁死信道， 并且阻塞当前 goroutine, 也就是所有的goroutine(其实就main线一个)都在等待信道的开放(没人拿走数据信道是不会开放的)，也就是死锁咯。 其实，总结来看，为什么会死锁？非缓冲信道上如果发生了流入无流出，或者流出无流入，也就导致了死锁。或者这样理解 Go启动的所有goroutine里的非缓冲信道一定要一个线里存数据，一个线里取数据，要成对才行 那么死锁的解决办法呢？最简单的，把没取走的数据取走，没放入的数据放入， 因为无缓冲信道不能承载数据，那么就赶紧拿走！ Go的deadlock 信道数据你也许发现，上面的代码一个一个地去读取信道简直太费事了，Go语言允许我们使用range来读取信道: 12345678910func main() &#123; ch := make(chan int, 3) ch &lt;- 1 ch &lt;- 2 ch &lt;- 3 for v := range ch &#123; fmt.Println(v) &#125;&#125; 如果你执行了上面的代码，会报死锁错误的，原因是range不等到信道关闭是不会结束读取的。也就是如果 缓冲信道干涸了，那么range就会阻塞当前goroutine, 所以死锁咯。 那么，我们试着避免这种情况，比较容易想到的是读到信道为空的时候就结束读取: 12345678910ch := make(chan int, 3)ch &lt;- 1ch &lt;- 2ch &lt;- 3for v := range ch &#123; fmt.Println(v) if len(ch) &lt;= 0 &#123; // 如果现有数据量为0，跳出循环 break &#125;&#125; 以上的方法是可以正常输出的，但是注意检查信道大小的方法不能在信道存取都在发生的时候用于取出所有数据，这个例子 是因为我们只在ch中存了数据，现在一个一个往外取，信道大小是递减的。 另一个方式是显式地关闭信道: 1234567891011ch := make(chan int, 3)ch &lt;- 1ch &lt;- 2ch &lt;- 3// 显式地关闭信道close(ch)for v := range ch &#123; fmt.Println(v)&#125; 被关闭的信道会禁止数据流入, 是只读的。我们仍然可以从关闭的信道中取出数据，但是不能再写入数据了。 channel实现作业池我们创建三个channel，一个channel用于接受任务，一个channel用于保持结果，还有个channel用于决定程序退出的时候。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( "fmt")func Task(taskch, resch chan int, exitch chan bool) &#123; defer func() &#123; //异常处理 err := recover() if err != nil &#123; fmt.Println("do task error：", err) return &#125; &#125;() for t := range taskch &#123; // 处理任务 fmt.Println("do task :", t) resch &lt;- t // &#125; exitch &lt;- true //处理完发送退出信号&#125;func main() &#123; taskch := make(chan int, 20) //任务管道 resch := make(chan int, 20) //结果管道 exitch := make(chan bool, 5) //退出管道 go func() &#123; for i := 0; i &lt; 10; i++ &#123; taskch &lt;- i &#125; close(taskch) &#125;() for i := 0; i &lt; 5; i++ &#123; //启动5个goroutine做任务 go Task(taskch, resch, exitch) &#125; go func() &#123; //等5个goroutine结束 for i := 0; i &lt; 5; i++ &#123; &lt;-exitch &#125; close(resch) //任务处理完成关闭结果管道，不然range报错 close(exitch) //关闭退出管道 &#125;() for res := range resch&#123; //打印结果 fmt.Println("task res：",res) &#125;&#125; 只读channel和只写channel一般定义只读和只写的管道意义不大，更多时候我们可以在参数传递时候指明管道可读还是可写，即使当前管道是可读写的。 1234567891011121314151617181920212223242526package mainimport ( "fmt" "time")//只能向chan里写数据func send(c chan&lt;- int) &#123; for i := 0; i &lt; 10; i++ &#123; c &lt;- i &#125;&#125;//只能取channel中的数据func get(c &lt;-chan int) &#123; for i := range c &#123; fmt.Println(i) &#125;&#125;func main() &#123; c := make(chan int) go send(c) go get(c) time.Sleep(time.Second*1)&#125;//结果 select-case实现非阻塞channel原理通过select+case加入一组管道，当满足（这里说的满足意思是有数据可读或者可写)select中的某个case时候，那么该case返回，若都不满足case，则走default分支。 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt")func send(c chan int) &#123; for i :=1 ; i&lt;10 ;i++ &#123; c &lt;-i fmt.Println("send data : ",i) &#125;&#125;func main() &#123; resch := make(chan int,20) strch := make(chan string,10) go send(resch) strch &lt;- "wd" select &#123; case a := &lt;-resch: fmt.Println("get data : ", a) case b := &lt;-strch: fmt.Println("get data : ", b) default: fmt.Println("no channel actvie") &#125;&#125;//结果：get data : wd channel频率控制在对channel进行读写的时，go还提供了非常人性化的操作，那就是对读写的频率控制，通过time.Ticke实现 示例： 123456789101112131415161718192021222324package mainimport ( "time" "fmt")func main()&#123; requests:= make(chan int ,5) for i:=1;i&lt;5;i++&#123; requests&lt;-i &#125; close(requests) limiter := time.Tick(time.Second*1) for req:=range requests&#123; &lt;-limiter fmt.Println("requets",req,time.Now()) //执行到这里，需要隔1秒才继续往下执行，time.Tick(timer)上面已定义 &#125;&#125;//结果：requets 1 2018-07-06 10:17:35.98056403 +0800 CST m=+1.004248763requets 2 2018-07-06 10:17:36.978123472 +0800 CST m=+2.001798205requets 3 2018-07-06 10:17:37.980869517 +0800 CST m=+3.004544250requets 4 2018-07-06 10:17:38.976868836 +0800 CST m=+4.000533569]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang特性</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang 常识]]></title>
    <url>%2F2019%2F01%2F05%2F11.Golang%2FGolang%E7%89%B9%E6%80%A7%2Fgolang%20%E5%B8%B8%E8%AF%86%2F</url>
    <content type="text"><![CDATA[golang 基础常识recover能处理所有的异常吗recover能处理程序主动触发的panic以及空指针访问、异常地址访问等错误，因此可以认为是能处理所有异常了。 golang中常量是怎么实现的从汇编中看是对字符串常量加了一个标号，同时设置为SRODATA，也就是只读，对数字常量直接在代码中作为立即数使用了 golang的make和new的区别是什么 new有点像c++里面的new,用来初始化各种type，然后返回其指针。 只不过由于没有构造函数的存在，所以全部用零值来填充，比较特殊的是slice,map,channel， 它们的零值都是nil。另外由于golang直接可以用&amp;struct{} 形式来初始化，所以平时用到new的机会也比较少。make是用来初始化map,slice,以及channel的，并且可以指定初始长度， 它返回的不是指针，而是对象本身。另外，make出来的map,slice,channel都是可以直接使用的。 golang 的channel是怎么实现的 golang的channel是个结构体，里面大概包含了三大部分： a. 指向内容的环形缓存区，及其相关游标 b. 读取和写入的排队goroutine链表 c. 锁 任何操作前都需要获得锁， 当写满或者读空的时候，就将当前goroutine加入到recvq或者sendq中， 并出让cpu(gopark)。 golang 的GC算法golang 采用的是三色标记法 标记-清除算法：对象只有黑白两色 stop the world,即停止所有goroutine 从根对象（全局指针和栈上的对象）出发，把所有能直接或间接访问到的对象标记为黑色，其它所有对象标志为白色 清除所有白色对象 start the world 三色标记法：对象有黑白灰三色 stop the world 将根对象全部标记为灰色 start the world 在goroutine中进行对灰色对象进行遍历， 将灰色对象引用的每个对象标记为灰色，然后将该灰色对象标记为黑色。 重复执行4， 直接将所有灰色对象都变成黑色对象。 stop the world，清除所有白色对象 这里4，5是与用户程序是并发执行的，所以stw的时间被大大缩短了。 不过这样做可能会导致新创建的对象被误清除，因此使用了写屏障技术来解决该问题，大体逻辑是当创建新对象时将新对象置为灰色。 Golang中除了加Mutex锁以外还有哪些方式安全读写共享变量？Golang中Goroutine 可以通过 Channel 进行安全读写共享变量。 go语言的并发机制以及它所使用的CSP并发模型．Golang的CSP并发模型，是通过Goroutine和Channel来实现的。 Goroutine 是Go语言中并发的执行单位。有点抽象，其实就是和传统概念上的”线程“类似，可以理解为”线程“。 Channel是Go语言中各个并发结构体(Goroutine)之前的通信机制。通常Channel，是各个Goroutine之间通信的”管道“，有点类似于Linux中的管道。 通信机制channel也很方便，传数据用channel &lt;- data，取数据用&lt;-channel。 在通信过程中，传数据channel &lt;- data和取数据&lt;-channel必然会成对出现，因为这边传，那边取，两个goroutine之间才会实现通信。 而且不管传还是取，必阻塞，直到另外的goroutine传或者取为止。 Golang 中常用的并发模型？ 通过channel通知实现并发控制 通过sync包中的WaitGroup实现并发控制 在Go 1.7 以后引进的强大的Context上下文，实现并发控制 协程，线程，进程的区别。 进程 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。 线程 线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。 协程 协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 10. 什么是channel，为什么它可以做到线程安全？Channel是Go中的一个核心类型，可以把它看成一个管道，通过它并发核心单元就可以发送或者接收数据进行通讯(communication),Channel也可以理解是一个先进先出的队列，通过管道进行通信。 Golang的Channel,发送一个数据到Channel 和 从Channel接收一个数据 都是 原子性的。而且Go的设计思想就是:不要通过共享内存来通信，而是通过通信来共享内存，前者就是传统的加锁，后者就是Channel。也就是说，设计Channel的主要目的就是在多任务间传递数据的，这当然是安全的。 11. Epoll原理.开发高性能网络程序时，windows开发者们言必称Iocp，linux开发者们则言必称Epoll。大家都明白Epoll是一种IO多路复用技术，可以非常高效的处理数以百万计的Socket句柄，比起以前的Select和Poll效率提高了很多。 先简单了解下如何使用C库封装的3个epoll系统调用。 123int epoll_create(int size); int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout); 使用起来很清晰，首先要调用epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。 epoll_ctl可以操作上面建立的epoll，例如，将刚建立的socket加入到epoll中让其监控，或者把 epoll正在监控的某个socket句柄移出epoll，不再监控它等等。 epoll_wait在调用时，在给定的timeout时间内，当在监控的所有句柄中有事件发生时，就返回用户态的进程。 从调用方式就可以看到epoll相比select/poll的优越之处是,因为后者每次调用时都要传递你所要监控的所有socket给select/poll系统调用，这意味着需要将用户态的socket列表copy到内核态，如果以万计的句柄会导致每次都要copy几十几百KB的内存到内核态，非常低效。而我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。 所以，实际上在你调用epoll_create后，内核就已经在内核态开始准备帮你存储要监控的句柄了，每次调用epoll_ctl只是在往内核的数据结构里塞入新的socket句柄。 在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。 epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，通常来讲，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。 1234567891011121314static int __init eventpoll_init(void) &#123; ... ... /* Allocates slab cache used to allocate &quot;struct epitem&quot; items */ epi_cache = kmem_cache_create(&quot;eventpoll_epi&quot;, sizeof(struct epitem), 0, SLAB_HWCACHE_ALIGN|EPI_SLAB_DEBUG|SLAB_PANIC, NULL, NULL); /* Allocates slab cache used to allocate &quot;struct eppoll_entry&quot; */ pwq_cache = kmem_cache_create(&quot;eventpoll_pwq&quot;, sizeof(struct eppoll_entry), 0, EPI_SLAB_DEBUG|SLAB_PANIC, NULL, NULL); ... ... &#125; epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。 而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已，因此就会非常的高效！ 然而,这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。 如此，一个红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。 最后看看epoll独有的两种模式LT和ET。无论是LT和ET模式，都适用于以上所说的流程。区别是，LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时每次返回这个句柄，而ET模式仅在第一次返回。 当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait需要做的事情，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会每次从epoll_wait返回的。 因此epoll比select的提高实际上是一个用空间换时间思想的具体应用.对比阻塞IO的处理模型, 可以看到采用了多路复用IO之后, 程序可以自由的进行自己除了IO操作之外的工作, 只有到IO状态发生变化的时候由多路复用IO进行通知, 然后再采取相应的操作, 而不用一直阻塞等待IO状态发生变化,提高效率.]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang特性</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beego swagger]]></title>
    <url>%2F2019%2F01%2F02%2F11.Golang%2FGolang%E5%B7%A5%E5%85%B7%E5%8C%85%2Fbeego%20%20swagger%2F</url>
    <content type="text"><![CDATA[beego swager 自动化文档配置配置区域 全局配置 全局配置是对整个项目api的介绍和版本和名称相关的配置，配置位置在/routers/router.go，配置在router文件的最顶部 示例: 12345678// @APIVersion 1.0.0// @Title 项目的标题// @Description 项目的描述.// @Contact xxx@xxx.com// @TermsOfServiceUrl termsOfServiceUrl// @License licence// @LicenseUrl licenceUrlpackage routers 全局的注释如上所示，是显示给全局应用的设置信息，有如下这些设置 @APIVersion： 项目版本 @Title: 项目标题 @Description: 项目描述 @Contact: 联系方式 @License：支持证书 @LicenseUrl: 证书地址 路由配置 目前自动化文档只支持如下的写法的解析，其他写法函数不会自动解析，即 namespace+Include 的写法，而且只支持二级解析，一级版本号，二级分别表示应用模块 注意: 还需要添加 beego.SetStaticPath(“/swagger”, “swagger”) 这条路由规则，这样才可以路由到swagger文档 示例: 123456789101112func init() &#123; ns := beego.NewNamespace("/v1", beego.NSNamespace("/demo", beego.NSInclude( &amp;controllers.DemoController&#123;&#125;, ), ), ) beego.AddNamespace(ns) beego.SetStaticPath("/swagger", "swagger")&#125; 应用配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package controllersimport ( "beego-swagger/models" "fmt" "github.com/astaxie/beego" "strconv" "time")// Controller 的标题type DemoController struct &#123; beego.Controller&#125;func (c *DemoController) URLMapping() &#123; c.Mapping("GetDemo", c.GetDemo) c.Mapping("PostDemo", c.PostDemo)&#125;// @Title getDemo api的标题// @Description getDemo api的介绍信息// Param api 参数信息, 包含五个参数: 1.参数名 2.参数类型(query、path、header) 3.参数数据类型 4.是否必须 5.注释// @Param key path string true 路径参数key,1-100之间的数字// @Param param query string false url参数param// Success 成功相应消息, 接受三个参数: 1.status code 2.参数类型，必须用&#123;&#125;括起来 3.对象的字符串信息，如果是 &#123;object&#125; 类型，那么 bee 工具在生成 docs 的时候会扫描对应的对象，这里填写的是想对你项目的目录名和对象// @Success 200 &#123;object&#125; models.SuccessGetResponse// Failure 失败相应消息, 接受两个参数: 1.status code 2.错误信息// @Failure 400 Invalid email supplied// @Failure 500 get getDemo response error// router 路由信息, 接受两个参数: 1.请求的路由地址 2.支持的请求方法, 放在[]中，可以写多个用,隔开// @router /getdemo/:key [get]func (c *DemoController) GetDemo() &#123; rsp := models.SuccessGetResponse&#123; Time: time.Now().Format("2006-01-02 15:04:05"), &#125; keyParam := c.Ctx.Input.Param(":key") if keyParam == "" &#123; c.Ctx.Output.Status = 400 &#125; else &#123; keyInt, err := strconv.Atoi(keyParam) if err != nil || (1&gt;keyInt || keyInt&gt;100) &#123; c.Ctx.Output.Status = 400 &#125; else &#123; rsp.Message = fmt.Sprintf("receire key: %s, param: %s", keyParam, c.GetString("param")) &#125; &#125; c.Data["json"] = rsp c.ServeJSON()&#125;// @Title postDemo api的标题// @Description postDemo api的介绍信息// Param api 参数信息, 包含五个参数: 1.参数名 2.参数类型(formData、query、path、body、header) 3.参数数据类型 4.是否必须 5.注释// @Param form1 formData int true formData参数1,必须是偶数// @Param form2 formData string false formData参数2// @Param X-header header string true header参数// Success 成功相应消息, 接受三个参数: 1.status code 2.参数类型，必须用&#123;&#125;括起来 3.对象的字符串信息，如果是 &#123;object&#125; 类型，那么 bee 工具在生成 docs 的时候会扫描对应的对象，这里填写的是想对你项目的目录名和对象// @Success 200 &#123;object&#125; models.SuccessPostResponse// Failure 失败相应消息, 接受两个参数: 1.status code 2.错误信息// @Failure 400 no enough input// @Failure 500 get postDemo response error// router 路由信息, 接受两个参数: 1.请求的路由地址 2.支持的请求方法, 放在[]中，可以写多个用,隔开// @router /postdemo [post]func (c *DemoController) PostDemo() &#123; rsp := models.SuccessPostResponse&#123; Time: time.Now().Format("2006-01-02 15:04:05"), &#125; form1Param, err := c.GetInt("form1") form2Param := c.GetString("form2") header := c.Ctx.Input.Header("X-header") if form1Param%2 != 0 || err != nil &#123; c.Ctx.Output.Status = 400 &#125; else &#123; rsp.Message = fmt.Sprintf("receire form1: %d, form2: %s, X-header: %s", form1Param, form2Param, header) &#125; c.Data["json"] = rsp c.ServeJSON()&#125; 注释解释: DemoController上面的注释， 这个是该controller模块的标题; @Title 这个 API 所表达的含义，是一个文本，空格之后的内容全部解析为 title @Description 这个 API 详细的描述，是一个文本，空格之后的内容全部解析为 Description @Param 参数，表示需要传递到服务器端的参数，有五列参数，使用空格或者 tab 分割，五个分别表示的含义如下 参数名 参数类型，可以有的值是 formData、query、path、body、header，formData 表示是 post 请求的数据，query 表示带在 url 之后的参数，path 表示请求路径上得参数，例如上面例子里面的 key，body 表示是一个 raw 数据请求，header 表示带在 header 信息中得参数。 参数类型 是否必须 注释 @Success 成功返回给客户端的信息，三个参数，第一个是 status code。第二个参数是返回的类型，必须使用 {} 包含，第三个是返回的对象或者字符串信息，如果是 {object} 类型，那么 bee 工具在生成 docs 的时候会扫描对应的对象，这里填写的是想对你项目的目录名和对象，例如 models.ZDTProduct.ProductList 就表示 /models/ZDTProduct 目录下的 ProductList 对象。 @Failure ​ 失败返回的信息，包含两个参数，使用空格分隔，第一个表示 status code，第二个表示错误信息 @router 路由信息，包含两个参数，使用空格分隔，第一个是请求的路由地址，支持正则和自定义路由，和之前的路由规则一样，第二个参数是支持的请求方法,放在 [] 之中，如果有多个方法，那么使用 , 分隔。 注意: 每一行注释内的每隔参数之间用空格隔开，并且最好只用一个空格，不然可能会产生显示问题 文档自动生成文档生成和启动步骤 项目不管是不是用go-module作为包管理工具，项目必须在 gopath/src 目录 开启文档开关 在配置文件中设置：EnableDocs = true 启动beego项目, 并启动swagger文档 12# -gendoc=true 表示每次自动化的 build 文档，-downdoc=true 就会自动的下载 swagger 文档查看器bee run -gendoc=true -downdoc=true 查看文档 地址: localhost:8080/swagger/ 文档示例]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang工具包</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang reflect interface 理解]]></title>
    <url>%2F2018%2F12%2F22%2F11.Golang%2FGolang%E7%89%B9%E6%80%A7%2F%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%8F%8D%E5%B0%84%E7%9A%84%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[一、Golang的interface interface是方法的集合， 其方法不需要实现 interface的更重要的作用在于多态实现 interface定义方法12345type 接口名称 interface &#123;method1 (参数列表) 返回值列表method2 (参数列表) 返回值列表...&#125; interface使用 接口的使用不仅仅针对结构体，自定义类型、变量等等都可以实现接口。 如果一个接口没有任何方法，我们称为空接口，由于空接口没有方法，所以任何类型都实现了空接口。 要实现一个接口，必须实现该接口里面的所有方法。 1234567891011121314151617181920212223242526272829303132package mainimport "fmt"//定义接口type Skills interface &#123; Running() Getname() string&#125;type Student struct &#123; Name string Age int&#125;// 实现接口func (p Student) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Student) Running() &#123; // 实现 Running方法 fmt.Printf("%s running",p.Name)&#125;func main() &#123; var skill Skills var stu1 Student stu1.Name = "wd" stu1.Age = 22 skill = stu1 skill.Running() //调用接口&#125; interface嵌套go语言中的接口可以嵌套，可以理解为继承，子接口拥有父接口的所有方法，并且想要使用该子接口的话，必须将父接口和子接口的所有方法都实现。 12345678910type Skills interface &#123; Running() Getname() string&#125;type Test interface &#123; sleeping() Skills //继承Skills&#125; 多态的概念上面提到了，go语言中interface是实现多态的一种形式，所多态，就是一种事物的多种形态，就像python调用方法的时候，我们不需要关注传入参数的类型，我们只需要让传入的参数可以被方法使用就可以。 同一个interface，不同的类型实现，都可以进行调用，它们都按照统一接口进行操作。 在上面的示例中，我们增加一个Teacher结构体，同样实现接口进行说明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport "fmt"type Skills interface &#123; Running() Getname() string&#125;type Student struct &#123; Name string Age int&#125;type Teacher struct &#123; Name string Salary int&#125;func (p Student) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Student) Running() &#123; // 实现 Running方法 fmt.Printf("%s running",p.Name)&#125;func (p Teacher) Getname() string&#123; //实现Getname方法 fmt.Println(p.Name ) return p.Name&#125;func (p Teacher) Running() &#123; // 实现 Running方法 fmt.Printf("\n%s running",p.Name)&#125;func main() &#123; var skill Skills var stu1 Student var t1 Teacher t1.Name = "wang" stu1.Name = "wd" stu1.Age = 22 skill = stu1 skill.Running() skill = t1 t1.Running()&#125;//wd running//wang running 通过interface 判断类型&amp;类型转换由于接口是一般类型，当我们使用接口时候可能不知道它是那个类型实现的，基本数据类型我们有对应的方法进行类型转换，当然接口类型也有类型转换。 当然我们也可以用这个方式来进行类型的判断。 类型转换示例： 12345var s intvar x interfacex = sy , ok := x.(int) //将interface 转为int,ok可省略 但是省略以后转换失败会报错，true转换成功，false转换失败, 并采用默认值 类型判断示例： 12345678910111213141516package mainimport "fmt"func main() &#123; var x interface&#123;&#125; s := "WD" x = s y,ok := x.(int) z,ok1 := x.(string) fmt.Println(y,ok) fmt.Println(z,ok1)&#125;//0 false//WD true 类型判断示例： 1234567891011121314151617181920212223242526272829303132333435package mainimport "fmt"type Student struct &#123; Name string&#125;func TestType(items ...interface&#123;&#125;) &#123; for k, v := range items &#123; switch v.(type) &#123; case string: fmt.Printf("type is string, %d[%v]\n", k, v) case bool: fmt.Printf("type is bool, %d[%v]\n", k, v) case int: fmt.Printf("type is int, %d[%v]\n", k, v) case float32, float64: fmt.Printf("type is float, %d[%v]\n", k, v) case Student: fmt.Printf("type is Student, %d[%v]\n", k, v) case *Student: fmt.Printf("type is Student, %d[%p]\n", k, v) &#125;&#125;&#125;func main() &#123; var stu StudentTestType("WD", 100, stu,3.3)&#125;//type is string, 0[WD]//type is int, 1[100]//type is Student, 2[&#123;&#125;]//type is float, 3[3.3] interface 和 reflect反射Golang的变量包含 type 和value两部分 type 包括 static type和concrete type. static type就类似python的不可变类型(如int、string)，concrete type是runtime系统看见的类型 类型断言能否成功，取决于变量的concrete type，而不是static type. Golang的静态类型和interface类型 Golang的指定类型的变量的type是静态的（也就是指定int、string这些的变量，它的type是static type），在创建变量的时候就已经确定 Golang的interface的type是concrete type。 在Golang的实现中，每个interface变量都有一个对应pair，pair中记录了实际变量的值和类型(value, type): Golang的反射 interface及其pair的存在，是Golang中实现反射的前提，理解了pair，就更容易理解反射。反射就是用来检测存储在接口变量内部(值value；类型concrete type) pair对的一种机制。 value是实际变量值，type是实际变量的类型。一个interface{}类型的变量包含了2个指针，一个指针指向值的类型【对应concrete type】，另外一个指针指向实际的值【对应value】。 二、反射reflect反射是程序执行时检查其所拥有的结构。尤其是类型的一种能力。这是元编程的一种形式。它同一时候也是造成混淆的重要来源。 每一个语言的反射模型都不同， python比较类似反射的例子就是类的 hasattr 方法， 用来检测类是否包含某个方法或者属性 go语言中的反射通过refect包实现，reflect包实现了运行时反射，允许程序操作任意类型的对象。 reflect.TypeType：Type类型用来表示一个go类型。 不是所有go类型的Type值都能使用所有方法。请参见每个方法的文档获取使用限制。在调用有分类限定的方法时，应先使用Kind方法获知类型的分类。调用该分类不支持的方法会导致运行时的panic。 获取Type对象的方法： 1func TypeOf(i interface&#123;&#125;) Type 示例： 123456789101112package mainimport ( "reflect" "fmt")func main() &#123; str := "wd" res_type := reflect.TypeOf(str) fmt.Println(res_type) //string&#125; reflect.Type的方法通用方法： 12345678910111213141516171819202122232425262728293031323334// 通用方法func (t *rtype) String() string // 获取 t 类型的字符串描述，不要通过 String 来判断两种类型是否一致。func (t *rtype) Name() string // 获取 t 类型在其包中定义的名称，未命名类型则返回空字符串。func (t *rtype) PkgPath() string // 获取 t 类型所在包的名称，未命名类型则返回空字符串。func (t *rtype) Kind() reflect.Kind // 获取 t 类型的类别。func (t *rtype) Size() uintptr // 获取 t 类型的值在分配内存时的大小，功能和 unsafe.SizeOf 一样。func (t *rtype) Align() int // 获取 t 类型的值在分配内存时的字节对齐值。func (t *rtype) FieldAlign() int // 获取 t 类型的值作为结构体字段时的字节对齐值。func (t *rtype) NumMethod() int // 获取 t 类型的方法数量。func (t *rtype) Method() reflect.Method // 根据索引获取 t 类型的方法，如果方法不存在，则 panic。// 如果 t 是一个实际的类型，则返回值的 Type 和 Func 字段会列出接收者。// 如果 t 只是一个接口，则返回值的 Type 不列出接收者，Func 为空值。func (t *rtype) MethodByName(string) (reflect.Method, bool) // 根据名称获取 t 类型的方法。func (t *rtype) Implements(u reflect.Type) bool // 判断 t 类型是否实现了 u 接口。func (t *rtype) ConvertibleTo(u reflect.Type) bool // 判断 t 类型的值可否转换为 u 类型。func (t *rtype) AssignableTo(u reflect.Type) bool // 判断 t 类型的值可否赋值给 u 类型。func (t *rtype) Comparable() bool // 判断 t 类型的值可否进行比较操作####注意对于：数组、切片、映射、通道、指针、接口 func (t *rtype) Elem() reflect.Type // 获取元素类型、获取指针所指对象类型，获取接口的动态类型 示例： 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; inf := new(Skills) stu_type := reflect.TypeOf(stu1) inf_type := reflect.TypeOf(inf).Elem() // 特别说明，引用类型需要用Elem()获取指针所指的对象类型 fmt.Println(stu_type.String()) //main.Student fmt.Println(stu_type.Name()) //Student fmt.Println(stu_type.PkgPath()) //main fmt.Println(stu_type.Kind()) //struct fmt.Println(stu_type.Size()) //24 fmt.Println(inf_type.NumMethod()) //2 fmt.Println(inf_type.Method(0),inf_type.Method(0).Name) // &#123;reading main func() &lt;invalid Value&gt; 0&#125; reading fmt.Println(inf_type.MethodByName("reading")) //&#123;reading main func() &lt;invalid Value&gt; 0&#125; true&#125; 其他方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 数值func (t *rtype) Bits() int // 获取数值类型的位宽，t 必须是整型、浮点型、复数型------------------------------// 数组func (t *rtype) Len() int // 获取数组的元素个数------------------------------// 映射func (t *rtype) Key() reflect.Type // 获取映射的键类型------------------------------// 通道func (t *rtype) ChanDir() reflect.ChanDir // 获取通道的方向------------------------------// 结构体func (t *rtype) NumField() int // 获取字段数量func (t *rtype) Field(int) reflect.StructField // 根据索引获取字段func (t *rtype) FieldByName(string) (reflect.StructField, bool) // 根据名称获取字段func (t *rtype) FieldByNameFunc(match func(string) bool) (reflect.StructField, bool) // 根据指定的匹配函数 math 获取字段func (t *rtype) FieldByIndex(index []int) reflect.StructField // 根据索引链获取嵌套字段------------------------------// 函数func (t *rtype) NumIn() int // 获取函数的参数数量func (t *rtype) In(int) reflect.Type // 根据索引获取函数的参数信息func (t *rtype) NumOut() int // 获取函数的返回值数量func (t *rtype) Out(int) reflect.Type // 根据索引获取函数的返回值信息func (t *rtype) IsVariadic() bool // 判断函数是否具有可变参数。// 如果有可变参数，则 t.In(t.NumIn()-1) 将返回一个切片。 示例： 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; stu_type := reflect.TypeOf(stu1) fmt.Println(stu_type.NumField()) //2 fmt.Println(stu_type.Field(0)) //&#123;Name string 0 [0] false&#125; fmt.Println(stu_type.FieldByName("Age")) //&#123;&#123;Age int 16 [1] false&#125; true&#125; reflect.Value不是所有go类型值的Value表示都能使用所有方法。请参见每个方法的文档获取使用限制。在调用有分类限定的方法时，应先使用Kind方法获知该值的分类。调用该分类不支持的方法会导致运行时的panic。 Value为go值提供了反射接口，获取Value对象方法： 1func ValueOf(i interface&#123;&#125;) Value 示例： 123str := "wd"val := reflect.ValueOf(str)//wd reflect.Value的方法reflect.Value.Kind()：获取变量类别，返回常量 123456789101112package mainimport ("reflect" "fmt")func main() &#123; str := "wd" val := reflect.ValueOf(str).Kind() fmt.Println(val)//string&#125; 用于获取值方法： 1234567891011121314151617181920212223242526272829303132333435363738func (v Value) Int() int64 // 获取int类型值，如果 v 值不是有符号整型，则 panic。func (v Value) Uint() uint64 // 获取unit类型的值，如果 v 值不是无符号整型（包括 uintptr），则 panic。func (v Value) Float() float64 // 获取float类型的值，如果 v 值不是浮点型，则 panic。func (v Value) Complex() complex128 // 获取复数类型的值，如果 v 值不是复数型，则 panic。func (v Value) Bool() bool // 获取布尔类型的值，如果 v 值不是布尔型，则 panic。func (v Value) Len() int // 获取 v 值的长度，v 值必须是字符串、数组、切片、映射、通道。func (v Value) Cap() int // 获取 v 值的容量，v 值必须是数值、切片、通道。func (v Value) Index(i int) reflect.Value // 获取 v 值的第 i 个元素，v 值必须是字符串、数组、切片，i 不能超出范围。func (v Value) Bytes() []byte // 获取字节类型的值，如果 v 值不是字节切片，则 panic。func (v Value) Slice(i, j int) reflect.Value // 获取 v 值的切片，切片长度 = j - i，切片容量 = v.Cap() - i。// v 必须是字符串、数值、切片，如果是数组则必须可寻址。i 不能超出范围。func (v Value) Slice3(i, j, k int) reflect.Value // 获取 v 值的切片，切片长度 = j - i，切片容量 = k - i。// i、j、k 不能超出 v 的容量。i &lt;= j &lt;= k。// v 必须是字符串、数值、切片，如果是数组则必须可寻址。i 不能超出范围。func (v Value) MapIndex(key Value) reflect.Value // 根据 key 键获取 v 值的内容，v 值必须是映射。// 如果指定的元素不存在，或 v 值是未初始化的映射，则返回零值（reflect.ValueOf(nil)）func (v Value) MapKeys() []reflect.Value // 获取 v 值的所有键的无序列表，v 值必须是映射。// 如果 v 值是未初始化的映射，则返回空列表。func (v Value) OverflowInt(x int64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是有符号整型。func (v Value) OverflowUint(x uint64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是无符号整型。func (v Value) OverflowFloat(x float64) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是浮点型。func (v Value) OverflowComplex(x complex128) bool // 判断 x 是否超出 v 值的取值范围，v 值必须是复数型。 设置值方法： 12345678910111213141516171819func (v Value) SetInt(x int64) //设置int类型的值func (v Value) SetUint(x uint64) // 设置无符号整型的值func (v Value) SetFloat(x float64) // 设置浮点类型的值func (v Value) SetComplex(x complex128) //设置复数类型的值func (v Value) SetBool(x bool) //设置布尔类型的值func (v Value) SetString(x string) //设置字符串类型的值func (v Value) SetLen(n int) // 设置切片的长度，n 不能超出范围，不能为负数。func (v Value) SetCap(n int) //设置切片的容量func (v Value) SetBytes(x []byte) //设置字节类型的值func (v Value) SetMapIndex(key, val reflect.Value) //设置map的key和value，前提必须是初始化以后，存在覆盖、不存在添加 其他方法： 123456789101112131415161718192021222324252627282930##########结构体相关：func (v Value) NumField() int // 获取结构体字段（成员）数量func (v Value) Field(i int) reflect.Value //根据索引获取结构体字段func (v Value) FieldByIndex(index []int) reflect.Value // 根据索引链获取结构体嵌套字段func (v Value) FieldByName(string) reflect.Value // 根据名称获取结构体的字段，不存在返回reflect.ValueOf(nil)func (v Value) FieldByNameFunc(match func(string) bool) Value // 根据匹配函数 match 获取字段,如果没有匹配的字段，则返回零值（reflect.ValueOf(nil)）########通道相关：func (v Value) Send(x reflect.Value)// 发送数据（会阻塞），v 值必须是可写通道。func (v Value) Recv() (x reflect.Value, ok bool) // 接收数据（会阻塞），v 值必须是可读通道。func (v Value) TrySend(x reflect.Value) bool // 尝试发送数据（不会阻塞），v 值必须是可写通道。func (v Value) TryRecv() (x reflect.Value, ok bool) // 尝试接收数据（不会阻塞），v 值必须是可读通道。func (v Value) Close() // 关闭通道########函数相关func (v Value) Call(in []Value) (r []Value) // 通过参数列表 in 调用 v 值所代表的函数（或方法）。函数的返回值存入 r 中返回。// 要传入多少参数就在 in 中存入多少元素。// Call 即可以调用定参函数（参数数量固定），也可以调用变参函数（参数数量可变）。func (v Value) CallSlice(in []Value) []Value // 调用变参函数 示例一：获取和设置普通类型的值 12345678910111213141516package mainimport ( "reflect" "fmt")func main() &#123; str := "wd" age := 11 fmt.Println(reflect.ValueOf(str).String()) //获取str的值，结果wd fmt.Println(reflect.ValueOf(age).Int()) //获取age的值，结果age str2 := reflect.ValueOf(&amp;str) //获取Value类型 str2.Elem().SetString("jack") //设置值 fmt.Println(str2.Elem(),age) //jack 11&#125; 示例二：简单结构体操作 1234567891011121314151617181920212223242526272829303132333435package mainimport ( "fmt" "reflect")type Skills interface &#123; reading() running()&#125;type Student struct &#123; Name string Age int&#125;func (self Student) runing()&#123; fmt.Printf("%s is running\n",self.Name)&#125;func (self Student) reading()&#123; fmt.Printf("%s is reading\n" ,self.Name)&#125;func main() &#123; stu1 := Student&#123;Name:"wd",Age:22&#125; stu_val := reflect.ValueOf(stu1) //获取Value类型 fmt.Println(stu_val.NumField()) //2 fmt.Println(stu_val.Field(0),stu_val.Field(1)) //wd 22 fmt.Println(stu_val.FieldByName("Age")) //22 stu_val2 := reflect.ValueOf(&amp;stu1).Elem() stu_val2.FieldByName("Age").SetInt(33) //设置字段值 ，结果33 fmt.Println(stu1.Age) &#125; 示例三：通过反射调用结构体中的方法，通过reflect.Value.Method(i int).Call()或者reflect.Value.MethodByName(name string).Call()实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( "fmt" "reflect")type Student struct &#123; Name string Age int&#125;func (this *Student) SetName(name string) &#123; this.Name = name fmt.Printf("set name %s\n",this.Name )&#125;func (this *Student) SetAge(age int) &#123; this.Age = age fmt.Printf("set age %d\n",age )&#125;func (this *Student) String() string &#123; fmt.Printf("this is %s\n",this.Name) return this.Name&#125;func main() &#123; stu1 := &amp;Student&#123;Name:"wd",Age:22&#125; val := reflect.ValueOf(stu1) //获取Value类型，也可以使用reflect.ValueOf(&amp;stu1).Elem() val.MethodByName("String").Call(nil) //调用String方法 params := make([]reflect.Value, 1) params[0] = reflect.ValueOf(18) val.MethodByName("SetAge").Call(params) //通过名称调用方法 params[0] = reflect.ValueOf("jack") val.Method(1).Call(params) //通过方法索引调用 fmt.Println(stu1.Name,stu1.Age)&#125;//this is wd//set age 18//set name jack//jack 18]]></content>
      <categories>
        <category>Golang</category>
        <category>Golang特性</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
</search>
