<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="刘小恺" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="刘小恺(Kyle) 的个人博客">
<meta property="og:url" content="http://blog.kyleliu.cn/index.html">
<meta property="og:site_name" content="刘小恺(Kyle) 的个人博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="刘小恺(Kyle) 的个人博客">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="刘小恺(Kyle) 的个人博客" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>刘小恺(Kyle) 的个人博客</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: 
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">刘小恺</a></h1>
        </hgroup>

        
        <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
        

        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/octave/">octave</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/博客/">博客</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/梯度下降/">梯度下降</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">刘小恺</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">刘小恺</a></h1>
            </hgroup>
            
            <p class="header-subtitle">Python 、  Machine learning 、 Docker、 爬虫</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-10.机器学习/基础内容/octave 基础操作" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/26/10.机器学习/基础内容/octave 基础操作/" class="article-date">
      <time datetime="2018-10-25T16:01:00.000Z" itemprop="datePublished">2018-10-26</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/26/10.机器学习/基础内容/octave 基础操作/">Octave基础操作</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h2 id="基础功能命令"><a href="#基础功能命令" class="headerlink" title="基础功能命令"></a>基础功能命令</h2><h4 id="修改命令行的提示"><a href="#修改命令行的提示" class="headerlink" title="修改命令行的提示"></a>修改命令行的提示</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PS1('&gt;&gt; ')   % &gt;&gt; 就是修改后的提示符号</span><br></pre></td></tr></table></figure>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a><a class="article-category-link" href="/categories/机器学习/基础内容/">基础内容</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/octave/">octave</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2018/10/26/10.机器学习/基础内容/octave 基础操作/#more">阅读全文 >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-10.机器学习/基础内容/线型回归" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/20/10.机器学习/基础内容/线型回归/" class="article-date">
      <time datetime="2018-10-19T16:01:00.000Z" itemprop="datePublished">2018-10-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/20/10.机器学习/基础内容/线型回归/">线型回归 &amp; 梯度下降</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h2 id="1-模型表示"><a href="#1-模型表示" class="headerlink" title="1.模型表示"></a>1.模型表示</h2><h3 id="问题的概述"><a href="#问题的概述" class="headerlink" title="问题的概述"></a><strong>问题的概述</strong></h3><blockquote>
<p>在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。</p>
</blockquote>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a><a class="article-category-link" href="/categories/机器学习/基础内容/">基础内容</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/梯度下降/">梯度下降</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2018/10/20/10.机器学习/基础内容/线型回归/#more">阅读全文 >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-10.机器学习/基础内容/机器学习介绍" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/19/10.机器学习/基础内容/机器学习介绍/" class="article-date">
      <time datetime="2018-10-19T15:03:00.000Z" itemprop="datePublished">2018-10-19</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/19/10.机器学习/基础内容/机器学习介绍/">机器学习介绍</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h3 id="机器学习的发展"><a href="#机器学习的发展" class="headerlink" title="机器学习的发展"></a>机器学习的发展</h3><blockquote>
<p>机器学习被应用到越来越多的领域这里有一些机器学习的案例。比如说，数据库挖掘。机器学习被用于数据挖掘的原因之一是网络和自动化技术的增长，这意味着，我们有史上最大的数据集比如说，大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。这在硅谷有巨大的市场。</p>
<p>再比如，医疗记录。随着自动化的出现，我们现在有了电子医疗记录。如果我们可以把医疗记录变成医学知识，我们就可以更好地理解疾病。再如，计算生物学。还是因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组，大家都知道这对人类意味着什么。再比如，工程方面，在工程的所有领域，我们有越来越大、越来越大的数据集，我们试图使用学习算法，来理解这些数据。另外，在机械应用中，有些人不能直接操作。例如，我已经在无人直升机领域工作了许多年。我们不知道如何写一段程序让直升机自己飞。我们唯一能做的就是让计算机自己学习如何驾驶直升机。</p>
<p>手写识别：现在我们能够非常便宜地把信寄到这个美国甚至全世界的原因之一就是当你写一个像这样的信封，一种<br>学习算法已经学会如何读你信封，它可以自动选择路径，所以我们只需要花几个美分把这封信寄到数千英里外。<br>事实上，如果你看过自然语言处理或计算机视觉，这些语言理解或图像理解都是属于AI领域。大部分的自然语言处<br>理和大部分的计算机视觉，都应用了机器学习。学习算法还广泛用于自定制程序。每次你去亚马逊或Net</p>
</blockquote>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a><a class="article-category-link" href="/categories/机器学习/基础内容/">基础内容</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2018/10/19/10.机器学习/基础内容/机器学习介绍/#more">阅读全文 >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/数据格式化输出" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/数据格式化输出/" class="article-date">
      <time datetime="2018-10-18T12:22:23.512Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>数据格式化输出<br>自动格式化输出文件<br>scrapy支持的格式化输出文件格式<br>json<br>jsonlines<br>csv<br>xml<br>通过配置setting中属性，来定义存储方式<br>FEED_URI   存储路径，必须配置，如果不配置，需要在启动时候指定路径<br>示例：file://tmp/export.csv<br>备注：<br>路径的名称可以格式化，例如：</p>
<p>FEED_FORMAT   用于序列化输出的文件格式，就是上面的几种<br>FEED_EXPORT_ENCODING   文件编码格式，一般默认utf-8，json默认是安全编码格式<br>FEED_EXPORT_FIELDS    定义输出文件中包含的字段，为列表格式<br>FEED_EXPORT_INDENT    定义输出内容的缩进，默认为0,<br>0和负数表示内容会放在一行上<br>None会选择最紧凑的方式，将数据进行摆放<br>其他，将会按照指定的缩进，对对象的成员进行格式化显示<br>FEED_STORE_EMPTY     如果没输出是否会导出文件，默认为False<br>FEED_STORAGES_BASE    字典类型，包含所有支持存储方式模板引擎， 一般不需要手动配置<br>默认为</p>
<p>FEED_STORAGES     一个字典类型，可以手动添加存储方式的模板引擎<br>默认为 { }<br>如果想要禁用某一个模板文件类型的引擎，可以将上面对应字段值设置为None<br>FEED_EXPORTERS_BASE    字典类型，包含了所有支持的格式文件处理引擎， 一般不需要手动配置<br>默认为</p>
<p>FEED_EXPORTERS      一个字典类型，可以手动添加文件处理引擎<br>默认为{ }<br>如果想要禁用某一个模板文件类型的引擎，可以将上面对应字段值设置为None<br>scrapy手动输出指定csv格式文件<br>目标：<br>获取如下格式的csv文件<br>配置的步骤</p>
<ol>
<li><p>在scrapy的spiders同层目录，新建my_project_csv_item_exporter.py文件内容如下（文件名可改，目录定死）<br>   ​    from scrapy.conf import settings<br>   ​    from scrapy.contrib.exporter import CsvItemExporter</p>
<pre><code>class MyProjectCsvItemExporter(CsvItemExporter):

def __init__(self, *args, **kwargs):
    delimiter = settings.get(&apos;CSV_DELIMITER&apos;, &apos;,&apos;)
    kwargs[&apos;delimiter&apos;] = delimiter

    fields_to_export = settings.get(&apos;FIELDS_TO_EXPORT&apos;, [])
    if fields_to_export :
        kwargs[&apos;fields_to_export&apos;] = fields_to_export

    super(MyProjectCsvItemExporter, self).__init__(*args, **kwargs)
</code></pre></li>
<li><p>在同层目录，settings.py文件新增如下内容（指定item,field顺序）<br>   ​    FEED_EXPORTERS = {<br>   ​        ‘csv’: ‘my_project.my_project_csv_item_exporter.MyProjectCsvItemExporter’,<br>   ​    } #这里假设你的project名字为my_project</p>
<pre><code>FIELDS_TO_EXPORT = [
    &apos;id&apos;,
    &apos;name&apos;,
    &apos;email&apos;,
    &apos;address&apos;
</code></pre></li>
<li><p>在同层目录，settings.py文件指定分隔符<br>CSV_DELIMITER = “\t”<br>4.启动项目<br>全部设定完后，执行scrapy crawl spider -o spider.csv的时候，字段就按顺序来了。</p>
</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/Scrapy框架介绍" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/Scrapy框架介绍/" class="article-date">
      <time datetime="2018-10-18T12:22:23.495Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>scrapy框架介绍<br>scrapy框架的介绍<br>什么是scrapy框架<br>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，我们只需要实现少量的代码，就能够快速的抓取<br>Scrapy 使用了 Twisted[‘twɪstɪd]异步网络框架，可以加快我们的下载速度和数据在模块中间的转载速度。<br><a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html</a><br>异步和非阻塞的概念<br>非阻塞<br>关注的是程序在等待调用结果（消息，返回值）时的状态，指在不能立刻得到结果之前，该调用不会阻塞当前线程。<br>异步<br>如果整个程序没有中介的等待过程，我们就说整个过程是一个异步的过程<br>多线程爬取数据的流程</p>
<p>scrapy的爬虫流程</p>
<p>scrapy框架各模块功能</p>
<p>创建爬虫项目（命令）<br>创建项目<br>命令<br>scrapy startproject +&lt;项目名字&gt;<br>创建后项目目录结构</p>
<p>创建爬虫<br>命令<br>scrapy genspider  +&lt;爬虫名字&gt; + &lt;允许爬取的域名&gt;          生成普通的spider爬虫<br>scrapy genspider -t crawl &lt;爬虫名字&gt;  &lt;允许爬取的域名&gt;        生成crawl_Spider爬虫<br>示例：<br>scrapy genspider itcast “itcast.cn”<br>爬虫应用创建的位置</p>
<p>启动爬虫应用<br>命令<br>scrapy crawl 爬虫名   执行单个爬虫<br>-o: 将爬虫的item输出存储到文件中.<br>quotes.jl : 将每一个item输出为一行的jsonline<br>quotes.csv: 将每一个item输出为每一行的csv格式文件<br>quotes.pickle: 存储为二进制文件<br>Scrapy–scrapy shell<br>什么是scrapy shell<br>Scrapy shell是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath表达式<br>查看实例属性/方法<br>在scrapy shell中查看<br>进行请求<br>scrapy shell ‘url’   对url地址进行请求获取响应<br>查看请求的属性<br>可使用任意scrapy实例 + TAB查看实例有哪些属性或者方法<br>response.url：当前响应的url地址<br>response.request.url：当前响应对应的请求的url地址<br>response.headers：响应头<br>response.body：响应体，也就是html代码，默认是byte类型<br>response.requests.headers：当前响应的请求头<br>response.meta: 下载延迟，请求深度等信息<br>response.xpath(***).extract()    验证xpath的正确性<br>在程序中<br>print（dir（实例对象））  查看实例对象的所有属性和方法</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/scrapy对接splash" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/scrapy对接splash/" class="article-date">
      <time datetime="2018-10-18T12:22:23.477Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>scrapy对接splash<br>Scrapy 对接 Splash<br>环境准备<br>首先在这之前请确保已经正确安装好了Splash并正常运行，同时安装好了ScrapySplash库<br>Scrapy-Splash文档<br><a href="https://github.com/scrapy-plugins/scrapy-splash" target="_blank" rel="noopener">https://github.com/scrapy-plugins/scrapy-splash</a><br>Scrapy-splash的配置<br>新建项目和spider<br>scrapy startproject scrapysplashtest     新建项目<br>scrapy genspider taobao <a href="http://www.taobao.com" target="_blank" rel="noopener">www.taobao.com</a>     新建spider<br>修改setting.py文件, 添加splash配置<br>SPLASH_URL = ‘<a href="http://localhost:8050&#39;" target="_blank" rel="noopener">http://localhost:8050&#39;</a>         添加splash服务的地址<br>DUPEFILTER_CLASS = ‘scrapy_splash.SplashAwareDupeFilter’       配置去重类<br>HTTPCACHE_STORAGE = ‘scrapy_splash.SplashAwareFSCacheStorage’     还需要配置一个Cache存储HTTPCACHE_STORAGE<br>添加splash中间件<br>DOWNLOADER_MIDDLEWARES = {<br>‘scrapy_splash.SplashCookiesMiddleware’: 723,<br>‘scrapy_splash.SplashMiddleware’: 725,<br>‘scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware’: 810,<br>}<br>SPIDER_MIDDLEWARES = {<br>‘scrapy_splash.SplashDeduplicateArgsMiddleware’: 100,<br>}<br>SplashRequest请求的使用<br>使用splash请求的说明<br>配置完成之后我们就可以利用Splash来抓取页面了，例如我们可以直接生成一个SplashRequest对象并传递相应的参数，Scrapy会将此请求转发给Splash<br>Splash对页面进行渲染加载，然后再将渲染结果传递回来，此时Response的内容就是渲染完成的页面结果了，最后交给Spider解析即可。<br>使用请求的方法<br>第一种方法<br>通过SplashRequest发送请求</p>
<p>第二种方法<br>scrapy.Request对象发送请求给splash服务器，只需将配置属性给meta参数即可</p>
<p>通过lua源码控制splash服务的示例<br>我们把Lua脚本定义成长字符串，通过SplashRequest的args来传递参数，同时接口修改为execute，另外args参数里还有一个lua_source字段用于指定Lua脚本内容，这样我们就成功构造了一个SplashRequest，对接Splash的工作就完成了。<br>​                                from scrapy import Spider<br>​                                from urllib.parse import quote<br>​                                from scrapysplashtest.items import ProductItem<br>​                                from scrapy_splash import SplashRequest</p>
<pre><code>script = &quot;&quot;&quot;
function main(splash, args)
splash.images_enabled = false
assert(splash:go(args.url))
assert(splash:wait(args.wait))
js = string.format(&quot;document.querySelector(&apos;#mainsrp-pager div.form &gt; input&apos;).value=%d;document.querySelector(&apos;#mainsrp-pager div.form &gt; span.btn.J_Submit&apos;).click()&quot;, args.page)
splash:evaljs(js)
assert(splash:wait(args.wait))
return splash:html()
end
&quot;&quot;&quot;

class TaobaoSpider(Spider):
name = &apos;taobao&apos;
allowed_domains = [&apos;www.taobao.com&apos;]
base_url = &apos;https://s.taobao.com/search?q=&apos;

def start_requests(self):
for keyword in self.settings.get(&apos;KEYWORDS&apos;):
for page in range(1, self.settings.get(&apos;MAX_PAGE&apos;) + 1):
url = self.base_url + quote(keyword)
yield SplashRequest(
        url,
        callback=self.parse,
        endpoint=&apos;execute&apos;,
        args={&apos;lua_source&apos;: script, &apos;page&apos;: page, &apos;wait&apos;: 7})
</code></pre><p>使用scrapy-splash比使用selenium的优点<br>由于Splash和Scrapy都支持异步处理，我们可以看到同时会有多个抓取成功的结果，而Selenium的对接过程中每个页面渲染下载过程是在Downloader Middleware里面完成的，所以整个过程是堵塞式的，Scrapy会等待这个过程完成后再继续处理和调度其他请求，影响了爬取效率。<br>使用Splash，是在中间件中将请求和渲染等工作交给了splash服务器, 各请求之间是异步的，因此使用Splash爬取效率上比Selenium高出很多。<br>因此，在Scrapy中要处理JavaScript渲染的页面建议使用Splash，这样不会破坏Scrapy中的异步处理过程，会大大提高爬取效率，而且Splash的安装和配置比较简单，通过API调用的方式也实现了模块分离，大规模爬取时部署起来也更加方便。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/scrapy_redis框架" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/scrapy_redis框架/" class="article-date">
      <time datetime="2018-10-18T12:22:23.457Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>scrapy_redis框架<br>scrapy_redis的介绍<br>scrapy_redis的官方介绍<br>基于redis的scrapy组件<br>使用scrapy_redis要安装scrapy_redis组件（pip install scrapy_redis）<br>scrapy_redis使用场景<br>增量式抓取、分布式抓取、持久化抓取<br>scrapy_redis实现的功能及与原生scrapy的区别<br>request去重<br>scrapy：可以实现去重，但是scrapy去重不能实现持久化，即当下次如果再次运行程序，会再次存储之前存储过得内容<br>scrapy_redis：会将请求过的指纹集合保存起来，用来判断重复我请求，即便重新运行程序，已经请求过的请求，也不会进入待请求对列中<br>爬虫持久化<br>scrapy：每次运行爬虫都是一次全新的开始<br>scrapy_redis：会将待请求的序列化后的request对象保存起来，这样每次运行程序都会从待请求列队中读取request对象进行请求，即使终止程序，再次运行，依然会从待请求对列中获取请求对象<br>分布式爬虫<br>scrapy：不能实现分布式<br>scrapy_redis：可以通过redis存储爬取的状态（已爬取，和待爬取），让多台机器上的爬虫程序，共用一个redis服务器，这样数据是共享的，实现了分布式的爬虫<br>分布式和集群的区别（备注）：<br>分布式：一个业务拆分成多个子业务，部署在不同的服务器上<br>集群，同一个业务，部署在多个服务器上<br>scrapy_redis的爬虫工作流程</p>
<p>Scrapy_redis和scrapy框架使用的区别<br>克隆scrapy_redis的示例代码<br>克隆 github上的scrapy-redis源码文件<br>git clone <a href="https://github.com/rolando/scrapy-redis.git" target="_blank" rel="noopener">https://github.com/rolando/scrapy-redis.git</a><br>打开example-project的scrapy_redis示例源代码<br>mv scrapy-redis/example-project    项目目录<br>example-project项目目录结构如下</p>
<p>redis_scrapy代码上和普通scrapy爬虫的区别<br>一、spider爬虫文件<br>第一种Scrapy爬虫（如示例dmoz.py）<br>区别：<br>和普通的爬虫文件没有区别<br>文件主要内容：</p>
<p>第二种RedisSpider爬虫（如示例的myspider_redis.py）<br>区别<br>爬虫类继承自RedisSpider<br>要指定爬虫类的redis_key类属性，指定reids中存储start_url的位置<br>新增了爬虫类的方法<br>def make_request_from_data(self, data):   # data 为接收到额redis_key传递过来的url，我们可以自行拼接成想要的url进行请求<br>search_key = data<br>url = self.URL_MAIN % (search_key, 1)<br>result_item = SearchInfoResultItem()<br>result_item[“task_value”] = search_key<br>return Request(url, meta={“result_item”: result_item}, dont_filter=True)<br>作用<br>防止分布式爬虫在不同的程序中，都会去请求该start_url，这样会造成请求的重复，响应也会重复的交给解析函数去处理，从redis中取出的start_url第一次被爬虫取出之后便会删除掉，不会造成重复的请求<br>在持久化和去重的功能上实现了RedisSpider爬虫才实现了分布式<br>代码示例：</p>
<p>第三种RedisCrawSpider爬虫（如示例的myspider_redis.py）<br>区别<br>爬虫类继承自RedisCrawlSpider<br>比RedisSpider爬虫就多了一个rules过滤功能<br>作用<br>在持久化和去重的功能上实现了RedisSpider爬虫才实现了分布式<br>另外可以实现自动的对响应中的连接进行匹配，进行请求<br>代码示例</p>
<p>二、setting文件<br>区别：<br>设置了三个新的属性<br>DUPEFILTER_CLASS = “scrapy_redis.dupefilter.RFPDupeFilter”<br>指定爬虫给request对象去重的方法<br>SCHEDULER = “scrapy_redis.scheduler.Scheduler”<br>指定爬虫处理请求列队的方法<br>SCHEDULER_PERSIST = True<br>列队中的内容是否持久保存，为False的时候，在关闭程序的时候，清空redis<br>添加了一个处理item的pipeline<br> ‘scrapy_redis.pipelines.RedisPipeline’: 400<br>用来将items自动保存到reids中，我们可以取消，不用reids来保存<br>设置了redis的地址<br>REDIS_URL = ‘redis://192.168.207.130: 6379’<br>不写使用哪个数据库的话，默认使用0数据库<br>文件主要内容：</p>
<p>三、运行爬虫后，reids数据库的中多出的键（运行程序后）<br>redis数据库多个三个键<br>爬虫名：dupefilter   用来存储爬取过的request指纹集合，数据类型为set类型<br>爬虫名：requests  用来存储已经序列化的待请求对象，数据类型为zset类型<br>爬虫名：items    用来存储爬取出来的数据，数据类型为list类型<br>每个键存储的内容：</p>
<p>Scrapy_redis功能实现的方法(Scrapy_redis的源码)<br>scrapy_redis之redispipeline（实现item数据自动存储redis数据库）<br>文件位置<br>/python3.5/site-packages/scrapy_redis/pipelines/RedisPipeline<br>源码主要功能<br>实现将item存储在redis中<br>源码片段</p>
<p>scrapy_redis之RFPDupeFilter（主要实现去重）<br>文件位置<br>/python3.5/site-packages/scrapy_redis、dupefilter/RFPDpeFilter<br>源码主要功能<br>判断请求是否存在内存中，如果不存在<br>对请求进行一系列操作后，将其转化为指纹<br>将指纹存储在内存中<br>主要实现去重功能<br>源码片段</p>
<p>加密指纹生成过程<br>  import hashlib<br>  fp = hashlib.sha1()<br>  fp.update(url)<br>  fp.update(request.method)<br>  fp.update(request.body or b””)<br>  return fp.hexdiget()  #sha1结果的16进制的字符串<br>scrapy_redis之Scheduler（主要实现持久化）<br>文件位置<br>/python3.5/site-packages/scrapy_redis/scheduler/Scheduler<br>代码主要功能<br>判断取消过滤是否为true<br>判断url地址是否是第一次看到<br>如果过滤，且是第一次看到的请求，那么将会进入待请求的列队<br>主要实现持久化功能<br>源码片段</p>
<p>scrapy_redis的总结<br>scrapy_redis与scrapy的区别<br>本质区别<br>scrapy_redis_spider相比于之前的scrapy_spider多了持久化和持久化去重的功能<br>主要是因为scrapy_redis将指纹和请求进行了在redis中的存储,正是redis实现了持久化<br>代码区别<br>仅仅是在setting中增加了五行代码<br>拓展：<br>scrapy_redis实现了请求的增量式爬虫，还可以实现内容增量式爬虫<br>可以将dont_filter设置不进行过滤<br>会将每条数据根据发帖人，发帖时间，帖子更新时间等使用md5算法生成指纹<br>在进行存储的时候 进行判断是否已经存在，以及是否更新<br>如果不存在那么便插入<br>如果存在但是更新时间已经更新，那么便更新<br>工作场景常用的数据存储模式<br>优点<br>可以利用redis的读取快速的特点，进行数据的快速读取<br>可以实现读取和存储分离，防止锁表的产生<br>存储数据用mysql或者mongodb<br>先将爬取到的数据去重后存储到mysql或mongodb中<br>也可以先存储到数据库中，在对数据库进行数据的去重<br>redis应用其速度快的特点，用来展示数据<br>定期从mysql或者mongodb中将去重后的数据读取到redis中<br>读取数据从redis中读取，其速度远远优于mysql和mongodb</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/scrapyd部署总结" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/scrapyd部署总结/" class="article-date">
      <time datetime="2018-10-18T12:22:23.438Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>scrapyd部署总结<br>二、环境安装<br>安装scprayd，网址：<a href="https://github.com/scrapy/scrapyd" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd</a><br>安装scrapyd-client，网址：<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd-client</a><br>建议从github上下载最新源码，然后用python setup.py install安装，因为pip安装源有可能不是最新版的。</p>
<p>三、验证安装成功<br>在命令框中输入scrapyd，输出如下说明安装成功</p>
<p>打开<a href="http://localhost:6800/" target="_blank" rel="noopener">http://localhost:6800/</a> 可以看到</p>
<p>点击jobs可以查看爬虫运行情况。</p>
<p>接下来就是让人很头疼的scrapyd-deploy问题了，查看官方文档上说用<br>scrapyd-deploy -l<br>可以看到当前部署的爬虫项目，但是当我输入这段命令的时候显示这个命令不存在或者有错误、不合法之类的。<br>解决方案：<br>在你的python目录下的Scripts文件夹中，我的路径是“D:\program files\python2.7.0\Scripts”，增加一个scrapyd-deploy.bat文件。<br>内容为：<br>@echo off<br>“D:\program files\python2.7.0\python.exe” “D:\program files\python2.7.0\Scripts\scrapyd-deploy” %*<br>然后重新打开命令框，再运行scrapyd-deploy -l 就可以了。</p>
<p>四、发布工程到scrapyd<br>scrapyd-deploy <target> -p <project><br>target为你的服务器命令，project是你的工程名字。<br>首先对你要发布的爬虫工程的scrapy.cfg 文件进行修改，我这个文件的内容如下：<br>[deploy:scrapyd1]<br>url = <a href="http://localhost:6800/" target="_blank" rel="noopener">http://localhost:6800/</a><br>project = baidu</project></target></p>
<p>因此我输入的命令是：<br>scrapyd-deploy scrapyd1 -p baidu</p>
<p>输出如下</p>
<p>五、启动爬虫<br>使用如下命令启动一个爬虫<br>curl <a href="http://localhost:6800/schedule.json" target="_blank" rel="noopener">http://localhost:6800/schedule.json</a> -d project=PROJECT_NAME -d spider=SPIDER_NAME<br>PROJECT_NAME填入你爬虫工程的名字，SPIDER_NAME填入你爬虫的名字<br>我输入的代码如下：<br>curl <a href="http://localhost:6800/schedule.json" target="_blank" rel="noopener">http://localhost:6800/schedule.json</a> -d project=baidu -d spider=baidu</p>
<p>因为这个测试爬虫写的非常简单，一下子就运行完了。查看网站的jobs可以看到有一个爬虫已经运行完，处于Finished一列中</p>
<p>六、停止一个爬虫<br>curl <a href="http://localhost:6800/cancel.json" target="_blank" rel="noopener">http://localhost:6800/cancel.json</a> -d project=PROJECT_NAME -d job=JOB_ID<br>更多API可以查看官网：<a href="http://scrapyd.readthedocs.io/en/latest/api.html" target="_blank" rel="noopener">http://scrapyd.readthedocs.io/en/latest/api.html</a></p>
<p>七、远程开启服务器上的爬虫<br>前提: 已安装scrapyd-client<br>代码如下：<br>[cpp] view plain<br>copy</p>
<h1 id="coding-utf-8"><a href="#coding-utf-8" class="headerlink" title="coding=utf-8"></a>coding=utf-8</h1><p>import urllib<br>import urllib2  </p>
<h1 id="启动爬虫"><a href="#启动爬虫" class="headerlink" title="启动爬虫"></a>启动爬虫</h1><p>test_data = {‘project’:’baidu’, ‘spider’:’baidu’}<br>test_data_urlencode = urllib.urlencode(test_data)  </p>
<p>requrl = “<a href="http://localhost:6800/schedule.json&quot;" target="_blank" rel="noopener">http://localhost:6800/schedule.json&quot;</a>  </p>
<h1 id="以下是post请求"><a href="#以下是post请求" class="headerlink" title="以下是post请求"></a>以下是post请求</h1><p>req = urllib2.Request(url = requrl, data = test_data_urlencode)  </p>
<p>res_data = urllib2.urlopen(req)<br>res = res_data.read()  # res 是str类型<br>print res  </p>
<h1 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h1><h1 id="以下是get请求"><a href="#以下是get请求" class="headerlink" title="以下是get请求"></a>以下是get请求</h1><p>myproject = “baidu”<br>requrl = “<a href="http://localhost:6800/listjobs.json?project=&quot;" target="_blank" rel="noopener">http://localhost:6800/listjobs.json?project=&quot;</a> + myproject<br>req = urllib2.Request(requrl)  </p>
<p>res_data = urllib2.urlopen(req)<br>res = res_data.read()<br>print res  </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/Scrapy-settinglog信息" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/Scrapy-settinglog信息/" class="article-date">
      <time datetime="2018-10-18T12:22:23.417Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Scrapy-setting/log信息<br>配置scrapy的setting文件<br>为什么需要配置文件<br>配置文件存放一些公共的变量（比如数据库的地址，账号密码等）方便自己和别人的使用和修改<br>为了方便识别，一般配置文件中变量的命名都为大写，例如：SQL_HOST = ‘192.168.0.1’<br>在pider和pipeline中访问setting内容<br>在spider中访问setting的内容<br>​         class Myspider(scrapy.Spider):<br>​            name = ‘myspider’<br>​            start_urls = [‘http://<strong>*</strong>‘;]<br>​            def parse(self, response):<br>​                # 可以直接通过访问实例属性的方法，访问setting文件中的内容<br>​                                # self.settings 返回的是一个字典，里面按照键值对存储所有的配置参数<br>​                key = self.settings.attributes.get(‘KEY’，None)<br>在pipeline文件中访问setting的内容<br>​         class Mypipeline(object):<br>​             def open_spider(self, spider):<br>​                  # 可以通过传递过来的spider实例来访问setting中的内容<br>​                  BOT_NAME = spider.setting.get(‘KEY’, None)<br>配置文件的常用的配置<br>​        BOT_NAME = ‘<strong><em>‘   项目名称<br>​        SPIDER_MOUDLES = [‘ys.spiders’]     # 爬虫创建的位置<br>​        NEWSPIDER_MOUDLE = ‘yg.spiders’    # 新建爬虫的位置<br>​        USER_AGENT =’</em></strong>‘      # 设置主句名称，用来告诉服务器请求的身份，注意，scrapy不能将USER_AGENT配置在        HEADERS中<br>​       ITEM_PIPLINES   # 管道<br>​        DOWNLOAD_MIDDLEWARES    # 下载中间件<br>​       OBEY_ROBOTFILES = False    # 是否遵守robots协议<br>​        SPIDER_MIDDLEWARES    # 爬虫中间件<br>​        DEAFAULT_REQUEAT_HEADERS = { ***}   # 配置scrapy 默认的请求头，不能包含USER-AGRNT和cookies、refer<br>​        #  使用增量式，或分布式爬虫需要配置<br>​        DUPEFILTER_CLASS = “scrapy_redis.dupefilter.RFPDupeFilter”<br>​        SCHEDULER = “scrapy_redis.scheduler.Scheduler”<br>​        SCHEDULER_PERSIST = True<br>​        ITEM_PIPELINES = {<br>​            ‘scrapy_redis.pipelines.RedisPipeline’: 400<br>​            }<br>​        REDIS_URL = ‘redis://127.0.0.1: 6379’<br>​<br>       RETRY_HTTP_CODES = []    # 设置会retry的状态码响应<br>        HTTPERROR_ALLOWED_CODES = [302,]    # 设置来指定spider能处理的response返回值<br>        LOG_LEVEL = “WARNING”     # 设置log提示的等级，log的从高到低的级别分别为：error、warning、info、debug<br>        RETRY_ENABLED = False     开启请求失败重新请求，默认是开启的<br>        RETRY_TIMES = 3        设置重新请求的次数<br>        DOWNLOAD_TIMEOUT = 6     设置请求最大等待时间<br>        COOKIES_ENABLE = True        #  是否启用cookie中间件，如果关闭是不会向服务器发送cookie的<br>        COOKIES_DEBUG = True        # 可以看带cookie在函数中传递的过程<br>        DOWNLOAD_DELAY = 3   # 请求延时<br>        CONCURRENT_REQUESTS = 16   # 并发数量，默认值为16<br>        CONCURRENT_REQUESTS_PRE_DOMAIN = 16   # 每个域名最大并发<br>        CONCURRENT_REQUESTS_PR_IP = 16    # 每个ip最大的并发<br>         AUTOTHROTTLE_ENABLED = True    # 动态调整下载延时<br>        CONCURRENT_REQUESTS_PRE_DOMAIN = 1  # 设置允许对同一域名发起请求的并发数量限制<br>        JOBDIR = ‘路径’     # 配置记录爬虫状态目录的文件，使爬虫中途停止再启动的时候会接着上一次的状态继续爬取<br>log的配置与scrapy的debug信息<br>scrapy-log相关的配置<br>log的作用<br>为了让我们自己希望输出到终端的内容能容易看一些<br>配置log的显示等级<br>在setting中设置log显示的级别<br>​            LOG_LEVEL = “WARNING”        #  在setting文件中添加一行<br>log的从高到低的级别分别为：error、warning、info、debug<br>自定义log日志的方法<br>在想要显示log的位置添加log的输出<br>第一种打印方式：（不带log产生的位置）</p>
<p>第二种打印方式：（带log产生的具体文件）</p>
<p>在普通的py文件中配置logger日志的方法<br>作用<br>我们配置完了logger的输出文件后，我们可以在任何其他的程序后，可以导入logger模块，帮助我们来可以看到错误产生的位置和时间等信息<br>我们还可以将log存储的文件命名为每日的日期，这样方便我们分别存储我们每日的bug<br>示例：<br>​                    import logging<br>​                           # 配置logger的输出格式，和输出内容<br>​                    logging.basicConfig(<br>​                                    level=logging.INFO,<br>​                                    format=’%(levelname)s : %(filename)s ‘<br>​                                           ‘[%(lineno)d] : %(message)s’<br>​                                           ‘ - %(asctime)s’, datefmt=’[%d-%b-%Y %H:%M:%S]’<br>​                                            filename=’./logdebug.log’)<br>​<br>                    if <strong>name</strong> == ‘<strong>main</strong>‘:<br>                        logger = logging.getLogger(<strong>name</strong>)<br>                        log_message = ‘error’<br>                        logger.warning(log_message）<br>输出的格式</p>
<p>详细示例</p>
<p>常见的scrapy的debug信息</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-4.python爬虫/scrapy框架/Scrapy-pipeline" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/10/18/4.python爬虫/scrapy框架/Scrapy-pipeline/" class="article-date">
      <time datetime="2018-10-18T12:22:23.400Z" itemprop="datePublished">2018-10-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Scrapy-pipeline<br>pipeline的配置<br>pipeline类的作用<br>接收引擎传递过来的数据，对数据进行处理<br>配置pipeline类（在pipelines文件中）<br>​    class Myspiderpipline(object):<br>​        def process_item(self, item, spider):   # 实现数据处理的方法，方法名不能修改<br>​            with open(‘temp.txt’, ‘a’) as f:<br>​                json.dump( item, f, ensure_ascii=False, indent=2)<br>​                        # 将item打印，并将item传递给后面的pipeline，实现数据在管道pipeline之间的传递<br>​                    return item<br>注意：<br>process_item方法中的spider参数，表示传递参数过来的爬虫对象<br>spider： 传递数据的爬虫对象，spider.name可以返回爬虫的名称<br>item：传递过来的数据<br>setting文件中的设置开启pipeline<br>​    ITEM_PIPELINE = {<br>​        ‘myspider.pipelines.MyspiderPipeline’: 300,<br>​    }<br>注意：<br>‘muspider.pipelines.MyspiderPipeline’是pipeline文件的路径<br>300  是pipeline的权重，pipeline的权重越小，其优先级越高，可以理解为距离引擎的远近<br>如果权重相同的两个pipeline，scrapy会自动的将键值进行排序，根据排序结果定义访问顺序<br>pipeline的方法<br>​      form pymongo import MongoClient<br>​      class Myspiderpipline(object):<br>​            # open_spider在爬虫开启请求start_url前的时候只执行一次，一般用于导入数据库<br>​             def open_spider(self,  spider):<br>​                    # 实例化一个mongoclient<br>​                   con = MongoClient(spider.settings.get(‘HOST’), spider.settings.get(‘PORT’))<br>​                   db = con[spider.settings.get(‘DB’)]<br>​                   self.collection = db[spider.settings.get(‘COLLECTON’)]<br>​         </p>
<pre><code># close_spider在爬虫关闭的时候只执行 一次，一般用于需要关闭的数据库关闭
def close_spider(self,  spider):
        print(&apos;关闭pipeline&apos;)
        return item      
# from_crawler连接到setting.py的配置文件
def from_crawler(cls, crawler):
        return  cls(mongo_uri=crawler.settings.get(&apos;MGONGO_URI&apos;))
</code></pre><p>备注：<br>一个项目会有多个spider，不同的pipeline处理不同的item的内容<br>一个spider的内容可能要做不同的操作，比如存入不同的数据库中<br>pipeline向mongoDB中插入数据<br>在pipeline类的外部定义使用的mongodb集合<br>​     from pymongo import MongoClient<br>​     client = MongoClient(host=’127.0.0.1’, port=27017)    # 实例化mongodb客户端<br>​     collection = client[‘myspider’][‘yangguang’]<br>在pipeline类open_spider中导入mongodb<br>​    form pymongo import MongoClient<br>​    class Myspiderpipline(object):<br>​        # open_spider在爬虫开启的时候只执行一次<br>​        def open_spider(self,  spider):<br>​            # 实例化一个mongoclient<br>​            con = MongoClient(spider.settings.get(‘HOST’), spider.settings.get(‘PORT’))<br>​            db = con[spider.settings.get(‘DB’)]<br>​            self.collection = db[spider.settings.get(‘COLLECTON’)]<br>向集合中插入数据<br>​            def process_item(self, item, spider):<br>​                item[‘collection_text’] = <em>**</em><br>​                collection.insert(dict(item))   # 向mongodb中插入数据<br>​                return item<br>向Mongodb中插入数据<br>要在setting中配置<br>MONGO_URI = ‘127.0.0.1’<br>MONGO_DATABASE = ‘数据库名’</p>
<p>pipeline向Mysql中存入数据<br>数据同步的方式写入Mysql</p>
<p>数据异步的方式写入Mysql</p>
<p>数据自动导出json文件<br>自定义形式导出</p>
<p>使用scrapy自带exportor导出</p>
<p>将爬取去到的图片地址对应的图片进行存储<br>setting文件中设置图片存储位置</p>
<p>pipeline中对图片进行下载</p>
<p>ArticleImagePipeline类的方法<br>get_media_requests(self, item, info)<br>该方法实现将item字段中的url字段取出来，然后直接生成Request对象，请求对象会加入到对列中，等待执行下载<br>file_path(self, request, response=None, info=None)<br>这个方法用来返回保存的文件名<br>item_completed(self, results, item, info)<br>当单个item完成下载后的处理方法<br>results参数就是该item对应的下载结果，它是一个列表形式，列表的每一个元素就是一个元组，其中包含了下载成功或者失败的信息<br>附件：<br>Desktop.zip</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2018 刘小恺
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>